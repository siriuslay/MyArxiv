<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-04T00:00:00Z">2024-11-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>ing with Phonemes: Enhancing LLM Multilinguality for non-Latin
  Script Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang Nguyen, Khyati Mahajan, Vikas Yadav, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual LLMs have achieved remarkable benchmark performance, but we find
they continue to underperform on non-Latin script languages across contemporary
LLM families. This discrepancy arises from the fact that LLMs are pretrained
with orthographic scripts, which are dominated by Latin characters that obscure
their shared phonology with non-Latin scripts. We propose leveraging phonemic
transcriptions as complementary signals to induce script-invariant
representations. Our study demonstrates that integrating phonemic signals
improves performance across both non-Latin and Latin languages, with a
particularly significant impact on closing the performance gap between the two.
Through detailed experiments, we show that phonemic and orthographic scripts
retrieve distinct examples for in-context learning (ICL). This motivates our
proposed Mixed-ICL retrieval strategy, where further aggregation leads to our
significant performance improvements for both Latin script languages (up to
12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL
retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attacking Vision-Language Computer Agents via Pop-ups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhe Zhang, Tao Yu, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents powered by large vision and language models (VLM) have
demonstrated significant potential in completing daily computer tasks, such as
browsing the web to book travel and operating desktop software, which requires
agents to understand these interfaces. Despite such visual inputs becoming more
integrated into agentic applications, what types of risks and attacks exist
around them still remain unclear. In this work, we demonstrate that VLM agents
can be easily attacked by a set of carefully designed adversarial pop-ups,
which human users would typically recognize and ignore. This distraction leads
agents to click these pop-ups instead of performing the tasks as usual.
Integrating these pop-ups into existing agent testing environments like OSWorld
and VisualWebArena leads to an attack success rate (the frequency of the agent
clicking the pop-ups) of 86% on average and decreases the task success rate by
47%. Basic defense techniques such as asking the agent to ignore pop-ups or
including an advertisement notice, are ineffective against the attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Scientific Hypothesis Generation with Knowledge Grounded Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities in
various scientific domains, from natural language processing to complex
problem-solving tasks. Their ability to understand and generate human-like text
has opened up new possibilities for advancing scientific research, enabling
tasks such as data analysis, literature review, and even experimental design.
One of the most promising applications of LLMs in this context is hypothesis
generation, where they can identify novel research directions by analyzing
existing knowledge. However, despite their potential, LLMs are prone to
generating ``hallucinations'', outputs that are plausible-sounding but
factually incorrect. Such a problem presents significant challenges in
scientific fields that demand rigorous accuracy and verifiability, potentially
leading to erroneous or misleading conclusions. To overcome these challenges,
we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that
enhances LLM hypothesis generation by integrating external, structured
knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured
reasoning process, organizing their output as a chain of ideas (CoI), and
includes a KG-supported module for the detection of hallucinations. With
experiments on our newly constructed hypothesis generation dataset, we
demonstrate that KG-CoI not only improves the accuracy of LLM-generated
hypotheses but also reduces the hallucination in their reasoning chains,
highlighting its effectiveness in advancing real-world scientific research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models generalize analogy solving like people can? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire E. Stevenson, Alexandra Pafford, Han L. J. van der Maas, Melanie Mitchell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When we solve an analogy we transfer information from a known context to a
new one through abstract rules and relational similarity. In people, the
ability to solve analogies such as "body : feet :: table : ?" emerges in
childhood, and appears to transfer easily to other domains, such as the visual
domain "( : ) :: < : ?". Recent research shows that large language models
(LLMs) can solve various forms of analogies. However, can LLMs generalize
analogy solving to new domains like people can? To investigate this, we had
children, adults, and LLMs solve a series of letter-string analogies (e.g., a b
: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek
alphabet), and a far transfer domain (list of symbols). As expected, children
and adults easily generalized their knowledge to unfamiliar domains, whereas
LLMs did not. This key difference between human and AI performance is evidence
that these LLMs still struggle with robust human-like analogical transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Seq-VCR: Preventing Collapse in Intermediate <span class="highlight-title">Transformer</span> Representations
  for Enhanced Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, <span class="highlight-author">Yann LeCun</span>, Irina Rish, Ravid Shwartz-Ziv, Christopher Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoder-only Transformers often struggle with complex reasoning tasks,
particularly arithmetic reasoning requiring multiple sequential operations. In
this work, we identify representation collapse in the model's intermediate
layers as a key factor limiting their reasoning capabilities. To address this,
we propose Sequential Variance-Covariance Regularization (Seq-VCR), which
enhances the entropy of intermediate representations and prevents collapse.
Combined with dummy pause tokens as substitutes for chain-of-thought (CoT)
tokens, our method significantly improves performance in arithmetic reasoning
problems. In the challenging $5 \times 5$ integer multiplication task, our
approach achieves $99.5\%$ exact match accuracy, outperforming models of the
same size (which yield $0\%$ accuracy) and GPT-4 with five-shot CoT prompting
($44\%$). We also demonstrate superior results on arithmetic expression and
longest increasing subsequence (LIS) datasets. Our findings highlight the
importance of preventing intermediate layer representation collapse to enhance
the reasoning capabilities of Transformers and show that Seq-VCR offers an
effective solution without requiring explicit CoT supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable potential as autonomous
agents, particularly in web-based tasks. However, existing LLM web agents
heavily rely on expensive proprietary LLM APIs, while open LLMs lack the
necessary decision-making capabilities. This paper introduces WebRL, a
self-evolving online curriculum reinforcement learning framework designed to
train high-performance web agents using open LLMs. WebRL addresses three key
challenges in building LLM web agents, including the scarcity of training
tasks, sparse feedback signals, and policy distribution drift in online
learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that
generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised
reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure
consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4
models into proficient web agents. On WebArena-Lite, WebRL improves the success
rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.
These open models significantly surpass the performance of GPT-4-Turbo (17.6%)
and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained
on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's
effectiveness in bridging the gap between open and proprietary LLM-based web
agents, paving the way for more accessible and powerful autonomous web
interaction systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparsing Law: Towards Large Language Models with Greater Activation
  Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Creative Short Story Generation in Humans and Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storytelling is a fundamental aspect of human communication, relying heavily
on creativity to produce narratives that are novel, appropriate, and
surprising. While large language models (LLMs) have recently demonstrated the
ability to generate high-quality stories, their creative capabilities remain
underexplored. Previous research has either focused on creativity tests
requiring short responses or primarily compared model performance in story
generation to that of professional writers. However, the question of whether
LLMs exhibit creativity in writing short stories on par with the average human
remains unanswered. In this work, we conduct a systematic analysis of
creativity in short story generation across LLMs and everyday people. Using a
five-sentence creative story task, commonly employed in psychology to assess
human creativity, we automatically evaluate model- and human-generated stories
across several dimensions of creativity, including novelty, surprise, and
diversity. Our findings reveal that while LLMs can generate stylistically
complex stories, they tend to fall short in terms of creativity when compared
to average human writers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MdEval: Massively Multilingual Code Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, Guanglin Niu, Ge Zhang, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code large language models (LLMs) have made significant progress in code
debugging by directly generating the correct code based on the buggy code
snippet. Programming benchmarks, typically consisting of buggy code snippet and
their associated test cases, are used to assess the debugging capabilities of
LLMs. However, many existing benchmarks primarily focus on Python and are often
limited in terms of language diversity (e.g., DebugBench and DebugEval). To
advance the field of multilingual debugging with LLMs, we propose the first
massively multilingual debugging benchmark, which includes 3.6K test samples of
18 programming languages and covers the automated program repair (APR) task,
the code review (CR) task, and the bug identification (BI) task. Further, we
introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs
into the correct multilingual queries and solutions (xDebugGen). Further, a
multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong
baseline specifically to handle the bugs of a wide range of programming
languages (e.g. "Missing Mut" in language Rust and "Misused Macro Definition"
in language C). Our extensive experiments on MDEVAL reveal a notable
performance gap between open-source models and closed-source LLMs (e.g., GPT
and Claude series), highlighting huge room for improvement in multilingual code
debugging scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRMArena: Understanding the Capacity of LLM Agents to Perform
  Professional CRM Tasks in Realistic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese, Caiming Xiong, Philippe Laban, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customer Relationship Management (CRM) systems are vital for modern
enterprises, providing a foundation for managing customer interactions and
data. Integrating AI agents into CRM systems can automate routine processes and
enhance personalized service. However, deploying and evaluating these agents is
challenging due to the lack of realistic benchmarks that reflect the complexity
of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel
benchmark designed to evaluate AI agents on realistic tasks grounded in
professional work environments. Following guidance from CRM experts and
industry best practices, we designed CRMArena with nine customer service tasks
distributed across three personas: service agent, analyst, and manager. The
benchmark includes 16 commonly used industrial objects (e.g., account, order,
knowledge article, case) with high interconnectivity, along with latent
variables (e.g., complaint habits, policy violations) to simulate realistic
data distributions. Experimental results reveal that state-of-the-art LLM
agents succeed in less than 40% of the tasks with ReAct prompting, and less
than 55% even with function-calling abilities. Our findings highlight the need
for enhanced agent capabilities in function-calling and rule-following to be
deployed in real-world work environments. CRMArena is an open challenge to the
community: systems that can reliably complete tasks showcase direct business
value in a popular work environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The LLM Language Network: A Neuroscientific Approach for Identifying
  Causally Task-Relevant Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable capabilities on not just
language tasks, but also various tasks that are not linguistic in nature, such
as logical reasoning and social inference. In the human brain, neuroscience has
identified a core language system that selectively and causally supports
language processing. We here ask whether similar specialization for language
emerges in LLMs. We identify language-selective units within 18 popular LLMs,
using the same localization approach that is used in neuroscience. We then
establish the causal role of these units by demonstrating that ablating LLM
language-selective units -- but not random units -- leads to drastic deficits
in language tasks. Correspondingly, language-selective LLM units are more
aligned to brain recordings from the human language system than random units.
Finally, we investigate whether our localization method extends to other
cognitive domains: while we find specialized networks in some LLMs for
reasoning and social capabilities, there are substantial differences among
models. These findings provide functional and causal evidence for
specialization in large language models, and highlight parallels with the
functional organization in the brain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Induction and Transduction for Abstract Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When learning an input-output mapping from very few examples, is it better to
first infer a latent function that explains the examples, or is it better to
directly predict new test outputs, e.g. using a neural network? We study this
question on ARC, a highly diverse dataset of abstract reasoning tasks. We train
neural models for induction (inferring latent functions) and transduction
(directly predicting the test output for a given test input). Our models are
trained on synthetic data generated by prompting LLMs to produce Python code
specifying a function to be inferred, plus a stochastic subroutine for
generating inputs to that function. We find inductive and transductive models
solve very different problems, despite training on the same problems, and
despite sharing the same neural architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated
  Parameters by Tencent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie She, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao She, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, Jie Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Hunyuan-Large, which is currently the largest
open-source Transformer-based mixture of experts model, with a total of 389
billion parameters and 52 billion activation parameters, capable of handling up
to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior
performance across various benchmarks including language understanding and
generation, logical reasoning, mathematical problem-solving, coding,
long-context, and aggregated tasks, where it outperforms LLama3.1-70B and
exhibits comparable performance when compared to the significantly larger
LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale
synthetic data that is orders larger than in previous literature, a mixed
expert routing strategy, a key-value cache compression technique, and an
expert-specific learning rate strategy. Additionally, we also investigate the
scaling laws and learning rate schedule of mixture of experts models, providing
valuable insights and guidances for future model development and optimization.
The code and checkpoints of Hunyuan-Large are released to facilitate future
innovations and applications.
  Codes: https://github.com/Tencent/Hunyuan-Large
  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Positive Experience Reflection for Agents in Interactive Text
  Environments <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs T. J. Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent agents designed for interactive environments face significant
challenges in text-based games, a domain that demands complex reasoning and
adaptability. While agents based on large language models (LLMs) using
self-reflection have shown promise, they struggle when initially successful and
exhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,
a novel approach that addresses these limitations in existing reflection
methods by incorporating positive experiences and managed memory to enrich the
context available to the agent at decision time. Our comprehensive analysis
spans both closed- and open-source LLMs and demonstrates the effectiveness of
Sweet&Sour in improving agent performance, particularly in scenarios where
previous approaches fall short.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2024 Language Gamification workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of DevOps in Enhancing Enterprise Software Delivery Success
  through R&D Efficiency and Source Code Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the impact of DevOps practices on enterprise software
delivery success, focusing on enhancing R&D efficiency and source code
management (SCM). Using a qualitative methodology, data were collected from
case studies of large-scale enterprises implementing DevOps to explore how
these practices streamline software development processes. Findings reveal that
DevOps significantly improves R&D productivity by fostering cross-functional
collaboration, reducing development cycle times, and enhancing software quality
through effective SCM practices, such as version control and continuous
integration. Additionally, SCM tools within DevOps enable precise change
tracking and reliable code maintenance, further supporting faster, more robust
software delivery. However, the study identifies challenges, including cultural
resistance and tool integration issues, that can hinder DevOps implementation.
Additionally, This research contributes to the growing body of DevOps
literature by highlighting the role of R&D efficiency and SCM as crucial
factors for software delivery success. Future studies should investigate these
factors across diverse industries to validate findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Steering Vectors by Targeting Sparse Autoencoder Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sviatoslav Chalnev, Matthew Siu, Arthur Conmy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To control the behavior of language models, steering methods attempt to
ensure that outputs of the model satisfy specific pre-defined properties.
Adding steering vectors to the model is a promising method of model control
that is easier than finetuning, and may be more robust than prompting. However,
it can be difficult to anticipate the effects of steering vectors produced by
almost all existing methods, such as CAA (Panickssery et al., 2024) or the
direct use of SAE latents (Templeton et al., 2024). In our work, we address
this issue by using SAEs to measure the effects of steering vectors, giving us
a method that can be used to understand the causal effect of any steering
vector intervention. We use this method for measuring causal effects to develop
an improved steering method, SAE-Targeted Steering (SAE-TS), which finds
steering vectors to target specific SAE features while minimizing unintended
side effects. We show that overall, SAE-TS balances steering effects with
coherence better than CAA and SAE feature steering, when evaluated on a range
of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 maintext pages and 9 appendix pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Emotional Descriptions to Electrovibration Haptic Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guimin Hu, Zirui Zhao, Lukas Heilmann, Yasemin Vardar, Hasti Seifi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing and displaying haptic signals with sensory and emotional attributes
can improve the user experience in various applications. Free-form user
language provides rich sensory and emotional information for haptic design
(e.g., ``This signal feels smooth and exciting''), but little work exists on
linking user descriptions to haptic signals (i.e., language grounding). To
address this gap, we conducted a study where 12 users described the feel of 32
signals perceived on a surface haptics (i.e., electrovibration) display. We
developed a computational pipeline using natural language processing (NLP)
techniques, such as GPT-3.5 Turbo and word embedding methods, to extract
sensory and emotional keywords and group them into semantic clusters (i.e.,
concepts). We linked the keyword clusters to haptic signal features (e.g.,
pulse count) using correlation analysis. The proposed pipeline demonstrates the
viability of a computational approach to analyzing haptic experiences. We
discuss our future plans for creating a predictive model of haptic experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVSS: Layer Importance Evaluation in Large Language Models via
  Activation Variance-Sparsity Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Song, Yuxin Wu, Sitan Huang, Zhongfeng Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of layer importance in deep learning has been an active area
of research, with significant implications for model optimization and
interpretability. Recently, large language models (LLMs) have gained prominence
across various domains, yet limited studies have explored the functional
importance and performance contributions of individual layers within LLMs,
especially from the perspective of activation distribution. In this work, we
propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining
normalized activation variance and sparsity to assess each layer's contribution
to model performance. By identifying and removing approximately the lowest 25%
of layers based on AVSS, we achieve over 90% of original model performance
across tasks such as question answering, language modeling, and sentiment
classification, indicating that these layers may be non-essential. Our approach
provides a systematic method for identifying less critical layers, contributing
to efficient large language model architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancements and limitations of LLMs in replicating human color-word
  associations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color-word associations play a fundamental role in human cognition and design
applications. Large Language Models (LLMs) have become widely available and
demonstrated intelligent behaviors in various benchmarks with natural
conversation skills. However, their ability to replicate human color-word
associations remains understudied. We compared multiple generations of LLMs
(from GPT-3 to GPT- 4o) against human color-word associations using data
collected from over 10,000 Japanese participants, involving 17 colors and words
from eight categories in Japanese. Our findings reveal a clear progression in
LLM performance across generations, with GPT-4o achieving the highest accuracy
in predicting the best voted word for each color and category, particularly
when using visual inputs rather than text-based color codes. However, the
highest median performance was approximately 50% even for GPT4-o with visual
inputs (chance level is 10%), and the performance levels varied significantly
across word categories and colors, indicating a failure to fully replicate
human color-word associations. On the other hand, color discrimination ability
estimated from our color-word association data showed that LLMs demonstrated
high correlation with human color discrimination patterns, similarly to
previous studies. Our study highlights both the advancements in LLM
capabilities and their persistent limitations, suggesting differences in
semantic memory structures between humans and LLMs in representing color-word
associations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regress, Don't Guess -- A Regression-like Loss on Number Tokens for
  Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Zausinger, Lars Pennig, Kacper Chlodny, Vincent Limbach, Anna Ketteler, Thorben Prein, Vishwa Mohan Singh, Michael Morris Danziger, Jannis Born
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While language models have exceptional capabilities at text generation, they
lack a natural inductive bias for emitting numbers and thus struggle in tasks
involving reasoning over quantities, especially arithmetics. This has
particular relevance in scientific datasets where combinations of text and
numerical data are abundant. One fundamental limitation is the nature of the CE
loss, which assumes a nominal (categorical) scale and thus cannot convey
proximity between generated number tokens. As a remedy, we here present two
versions of a number token loss. The first is based on an $L_p$ loss between
the ground truth token value and the weighted sum of the predicted class
probabilities. The second loss minimizes the Wasserstein-1 distance between the
distribution of the predicted output probabilities and the ground truth
distribution. These regression-like losses can easily be added to any language
model and extend the CE objective during training. We compare the proposed
schemes on a mathematics dataset against existing tokenization, encoding, and
decoding schemes for improving number representation in language models. Our
results reveal a significant improvement in numerical accuracy when equipping a
standard T5 model with the proposed loss schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5-page version for NeurIPS 2024 (MathAI workshop)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Efficient Training of Large Language Models with
  Low-dimensional Projected Attention <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingtai Lv, Ning Ding, Kaiyan Zhang, Ermo Hua, Ganqu Cui, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the effectiveness and efficiency of large language models (LLMs)
simultaneously is a critical yet challenging research goal. In this paper, we
find that low-rank pre-training, normally considered as efficient methods that
will compromise performance, can be scalably effective when reduced parameters
are precisely targeted. Specifically, applying the low-dimensional module only
to the attention layer -- resolves this issue and enhances both effectiveness
and efficiency. We refer to this structure as Low-dimensional Projected
Attention (LPA) and provide an explanatory analysis. Through extensive
experimentation at parameter scales of 130M, 370M, and scaling up to 3B, we
have validated the effectiveness and scalability of LPA. Our results show that
LPA model can save up to 12.4% in time while achieving an approximate 5%
improvement in test perplexity (ppl) and on downstream tasks compared with the
vanilla Transformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable cognitive decline detection in free dialogues with a Machine
  Learning approach based on <span class="highlight-title">pre-train</span>ed Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco de Arriba-Pérez, Silvia García-Méndez, Javier Otero-Mosquera, Francisco J. González-Castaño
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive and neurological impairments are very common, but only a small
proportion of affected individuals are diagnosed and treated, partly because of
the high costs associated with frequent screening. Detecting pre-illness stages
and analyzing the progression of neurological disorders through effective and
efficient intelligent systems can be beneficial for timely diagnosis and early
intervention. We propose using Large Language Models to extract features from
free dialogues to detect cognitive decline. These features comprise high-level
reasoning content-independent features (such as comprehension, decreased
awareness, increased distraction, and memory problems). Our solution comprises
(i) preprocessing, (ii) feature engineering via Natural Language Processing
techniques and prompt engineering, (iii) feature analysis and selection to
optimize performance, and (iv) classification, supported by automatic
explainability. We also explore how to improve Chatgpt's direct cognitive
impairment prediction capabilities using the best features in our models.
Evaluation metrics obtained endorse the effectiveness of a mixed approach
combining feature extraction with Chatgpt and a specialized Machine Learning
model to detect cognitive decline within free-form conversational dialogues
with older adults. Ultimately, our work may facilitate the development of an
inexpensive, non-invasive, and rapid means of detecting and explaining
cognitive decline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shortcut Learning in In-Context Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Song, Yingji Li, Fausto Giunchiglia, Hao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcut learning refers to the phenomenon where models employ simple,
non-robust decision rules in practical tasks, which hinders their
generalization and robustness. With the rapid development of large language
models (LLMs) in recent years, an increasing number of studies have shown the
impact of shortcut learning on LLMs. This paper provides a novel perspective to
review relevant research on shortcut learning in In-Context Learning (ICL). It
conducts a detailed exploration of the types of shortcuts in ICL tasks, their
causes, available benchmarks, and strategies for mitigating shortcuts. Based on
corresponding observations, it summarizes the unresolved issues in existing
research and attempts to outline the future research landscape of shortcut
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoonick Lee, Mogan Gim, Donghyeon Park, Donghee Choi, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) have shown promise in various
creative domains, including culinary arts. However, many LLMs still struggle to
deliver the desired level of culinary creativity, especially when tasked with
adapting recipes to meet specific cultural requirements. This study focuses on
cuisine transfer-applying elements of one cuisine to another-to assess LLMs'
culinary creativity. We employ a diverse set of LLMs to generate and evaluate
culturally adapted recipes, comparing their evaluations against LLM and human
judgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark
to evaluate LLMs' recipe generation abilities in the cuisine transfer task,
assessing their cultural accuracy and creativity in the culinary domain. Our
findings reveal crucial insights into both generative and evaluative
capabilities of LLMs in the culinary domain, highlighting strengths and
limitations in understanding and applying cultural nuances in recipe creation.
The code and dataset used in this project will be openly available in
\url{http://github.com/dmis-lab/CulinaryASH}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Models Learn to Skip Steps? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trained on vast corpora of human language, language models demonstrate
emergent human-like reasoning abilities. Yet they are still far from true
intelligence, which opens up intriguing opportunities to explore the parallels
of humans and model behaviors. In this work, we study the ability to skip steps
in reasoning - a hallmark of human expertise developed through practice. Unlike
humans, who may skip steps to enhance efficiency or to reduce cognitive load,
models do not inherently possess such motivations to minimize reasoning steps.
To address this, we introduce a controlled framework that stimulates
step-skipping behavior by iteratively refining models to generate shorter and
accurate reasoning paths. Empirical results indicate that models can develop
the step skipping ability under our guidance. Moreover, after fine-tuning on
expanded datasets that include both complete and skipped reasoning sequences,
the models can not only resolve tasks with increased efficiency without
sacrificing accuracy, but also exhibit comparable and even enhanced
generalization capabilities in out-of-domain scenarios. Our work presents the
first exploration into human-like step-skipping ability and provides fresh
perspectives on how such cognitive abilities can benefit AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Label Semantics and Meta-Label Refinement for Multi-Label
  Question Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Dong, Xiaobei Niu, Rui Zhong, Zhifeng Wang, Mingzhang Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate annotation of educational resources is critical in the rapidly
advancing field of online education due to the complexity and volume of
content. Existing classification methods face challenges with semantic overlap
and distribution imbalance of labels in the multi-label context, which impedes
effective personalized learning and resource recommendation. This paper
introduces RR2QC, a novel Retrieval Reranking method To multi-label Question
Classification by leveraging label semantics and meta-label refinement.
Firstly, RR2QC leverages semantic relationships within and across label groups
to enhance pre-training strategie in multi-label context. Next, a class center
learning task is introduced, integrating label texts into downstream training
to ensure questions consistently align with label semantics, retrieving the
most relevant label sequences. Finally, this method decomposes labels into
meta-labels and trains a meta-label classifier to rerank the retrieved label
sequences. In doing so, RR2QC enhances the understanding and prediction
capability of long-tail labels by learning from meta-labels frequently
appearing in other labels. Addtionally, a Math LLM is used to generate
solutions for questions, extracting latent information to further refine the
model's insights. Experimental results demonstrate that RR2QC outperforms
existing classification methods in Precision@k and F1 scores across multiple
educational datasets, establishing it as a potent enhancement for online
educational content utilization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriG-NER: Triplet-Grid Framework for Discontinuous Named Entity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rina Carines Cabral, Soyeon Caren Han, Areej Alhassan, Riza Batista-Navarro, Goran Nenadic, Josiah Poon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discontinuous Named Entity Recognition (DNER) presents a challenging problem
where entities may be scattered across multiple non-adjacent tokens, making
traditional sequence labelling approaches inadequate. Existing methods
predominantly rely on custom tagging schemes to handle these discontinuous
entities, resulting in models tightly coupled to specific tagging strategies
and lacking generalisability across diverse datasets. To address these
challenges, we propose TriG-NER, a novel Triplet-Grid Framework that introduces
a generalisable approach to learning robust token-level representations for
discontinuous entity extraction. Our framework applies triplet loss at the
token level, where similarity is defined by word pairs existing within the same
entity, effectively pulling together similar and pushing apart dissimilar ones.
This approach enhances entity boundary detection and reduces the dependency on
specific tagging schemes by focusing on word-pair relationships within a
flexible grid structure. We evaluate TriG-NER on three benchmark DNER datasets
and demonstrate significant improvements over existing grid-based
architectures. These results underscore our framework's effectiveness in
capturing complex entity structures and its adaptability to various tagging
schemes, setting a new benchmark for discontinuous entity extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be made available upon publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Align-SLM: Textless Spoken Language Models with Reinforcement Learning
  from AI Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While textless Spoken Language Models (SLMs) have shown potential in
end-to-end speech-to-speech modeling, they still lag behind text-based Large
Language Models (LLMs) in terms of semantic coherence and relevance. This work
introduces the Align-SLM framework, which leverages preference optimization
inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the
semantic understanding of SLMs. Our approach generates multiple speech
continuations from a given prompt and uses semantic metrics to create
preference data for Direct Preference Optimization (DPO). We evaluate the
framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,
the spoken version of the StoryCloze dataset for semantic coherence, and other
speech generation metrics, including the GPT4-o score and human evaluation.
Experimental results show that our method achieves state-of-the-art performance
for SLMs on most benchmarks, highlighting the importance of preference
optimization to improve the semantics of SLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Pedagogical LLMs with Supervised Fine Tuning for Computing
  Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Vassar, Jake Renzella, Emily Ross, Andrew Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates supervised fine-tuning of large language models
(LLMs) to improve their pedagogical alignment in computing education,
addressing concerns that LLMs may hinder learning outcomes. The project
utilised a proprietary dataset of 2,500 high quality question/answer pairs from
programming course forums, and explores two research questions: the suitability
of university course forums in contributing to fine-tuning datasets, and how
supervised fine-tuning can improve LLMs' alignment with educational principles
such as constructivism. Initial findings suggest benefits in pedagogical
alignment of LLMs, with deeper evaluations required.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 1 table, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGViz: Diagnose and Visualize Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tevin Wang, Jingyuan He, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) combines knowledge from domain-specific
sources into large language models to ground answer generation. Current RAG
systems lack customizable visibility on the context documents and the model's
attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool
that visualizes the attentiveness of the generated tokens in retrieved
documents. With a built-in user interface, retrieval index, and Large Language
Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and
document-level attention visualization, and (2) generation comparison upon
context document addition and removal. As an open-source toolkit, RAGViz can be
easily hosted with a custom embedding model and HuggingFace-supported LLM
backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,
memory-efficient LLM inference tool, and custom context snippet method, RAGViz
operates efficiently with a median query time of about 5 seconds on a moderate
GPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo
video of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaSaur: Large Language Agents Beyond Predefined Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing LLM agent systems typically select actions from a fixed and
predefined set at every step. While this approach is effective in closed,
narrowly-scoped environments, we argue that it presents two major challenges
when deploying LLM agents in real-world scenarios: (1) selecting from a fixed
set of actions significantly restricts the planning and acting capabilities of
LLM agents, and (2) this approach requires substantial human effort to
enumerate and implement all possible actions, which becomes impractical in
complex environments with a vast number of potential actions. In this work, we
propose an LLM agent framework that enables the dynamic creation and
composition of actions in an online manner. In this framework, the agent
interacts with the environment by generating and executing programs written in
a general-purpose programming language at each step. Furthermore, generated
actions are accumulated over time for future reuse. Our extensive experiments
on the GAIA benchmark demonstrate that this framework offers significantly
greater flexibility and outperforms previous methods. Notably, it allows an LLM
agent to recover in scenarios where no relevant action exists in the predefined
set or when existing actions fail due to unforeseen edge cases. At the time of
writing, we hold the top position on the GAIA public leaderboard. Our code can
be found in
\href{https://github.com/adobe-research/dynasaur}{https://github.com/adobe-research/dynasaur}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AmbigNLG: Addressing Task Ambiguity in Instruction for NLG <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17717v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17717v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayana Niwa, Hayate Iso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AmbigNLG, a novel task designed to tackle the challenge of task
ambiguity in instructions for Natural Language Generation (NLG). Ambiguous
instructions often impede the performance of Large Language Models (LLMs),
especially in complex NLG tasks. To tackle this issue, we propose an ambiguity
taxonomy that categorizes different types of instruction ambiguities and
refines initial instructions with clearer specifications. Accompanying this
task, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated
to facilitate research in AmbigNLG. Through comprehensive experiments with
state-of-the-art LLMs, we demonstrate that our method significantly enhances
the alignment of generated text with user expectations, achieving up to a
15.02-point increase in ROUGE scores. Our findings highlight the critical
importance of addressing task ambiguity to fully harness the capabilities of
LLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in
practical settings involving interactive ambiguity mitigation with users,
underscoring the benefits of leveraging LLMs for interactive clarification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMMA: End-to-End Multimodal Model for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, Yin Zhou, James Guo, Dragomir Anguelov, Mingxing Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.
Built on a multi-modal large language model foundation, EMMA directly maps raw
camera sensor data into various driving-specific outputs, including planner
trajectories, perception objects, and road graph elements. EMMA maximizes the
utility of world knowledge from the pre-trained large language models, by
representing all non-sensor inputs (e.g. navigation instructions and ego
vehicle status) and outputs (e.g. trajectories and 3D locations) as natural
language text. This approach allows EMMA to jointly process various driving
tasks in a unified language space, and generate the outputs for each task using
task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by
achieving state-of-the-art performance in motion planning on nuScenes as well
as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also
yields competitive results for camera-primary 3D object detection on the Waymo
Open Dataset (WOD). We show that co-training EMMA with planner trajectories,
object detection, and road graph tasks yields improvements across all three
domains, highlighting EMMA's potential as a generalist model for autonomous
driving applications. However, EMMA also exhibits certain limitations: it can
process only a small amount of image frames, does not incorporate accurate 3D
sensing modalities like LiDAR or radar and is computationally expensive. We
hope that our results will inspire further research to mitigate these issues
and to further evolve the state of the art in autonomous driving model
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Blog post: https://waymo.com/blog/2024/10/introducing-emma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Ref: Enhancing Reference Handling in Technical Writing with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazi Ahmed Asif Fuad, Lizhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in data synthesis but can be inaccurate in
domain-specific tasks, which retrieval-augmented generation (RAG) systems
address by leveraging user-provided data. However, RAGs require optimization in
both retrieval and generation stages, which can affect output quality. In this
paper, we present LLM-Ref, a writing assistant tool that aids researchers in
writing articles from multiple source documents with enhanced reference
synthesis and handling capabilities. Unlike traditional RAG systems that use
chunking and indexing, our tool retrieves and generates content directly from
text paragraphs. This method facilitates direct reference extraction from the
generated outputs, a feature unique to our tool. Additionally, our tool employs
iterative response generation, effectively managing lengthy contexts within the
language model's constraints. Compared to baseline RAG-based systems, our
approach achieves a $3.25\times$ to $6.26\times$ increase in Ragas score, a
comprehensive metric that provides a holistic view of a RAG system's ability to
produce accurate, relevant, and contextually appropriate responses. This
improvement shows our method enhances the accuracy and contextual relevance of
writing assistance tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compact Language Models via Pruning and Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) targeting different deployment scales and sizes
are currently produced by training each variant from scratch; this is extremely
compute-intensive. In this paper, we investigate if pruning an existing LLM and
then re-training it with a fraction (<3%) of the original training data can be
a suitable alternative to repeated, full retraining. To this end, we develop a
set of practical and effective compression best practices for LLMs that combine
depth, width, attention and MLP pruning with knowledge distillation-based
retraining; we arrive at these best practices through a detailed empirical
exploration of pruning strategies for each axis, methods to combine axes,
distillation strategies, and search techniques for arriving at optimal
compressed architectures. We use this guide to compress the Nemotron-4 family
of LLMs by a factor of 2-4x, and compare their performance to similarly-sized
models on a variety of language modeling tasks. Deriving 8B and 4B models from
an already pretrained 15B model using our approach requires up to 40x fewer
training tokens per model compared to training from scratch; this results in
compute cost savings of 1.8x for training the full model family (15B, 8B, and
4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to
training from scratch, perform comparably to other community models such as
Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art
compression techniques from the literature. We have open-sourced Minitron model
weights on Huggingface, with corresponding supplementary material including
example code available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Contextual Speech Recognition Using Vector Quantization for
  Efficient Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Flemotomos, Roger Hsiao, Pawel Swietojanski, Takaaki Hori, Dogan Can, Xiaodan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural contextual biasing allows speech recognition models to leverage
contextually relevant information, leading to improved transcription accuracy.
However, the biasing mechanism is typically based on a cross-attention module
between the audio and a catalogue of biasing entries, which means computational
complexity can pose severe practical limitations on the size of the biasing
catalogue and consequently on accuracy improvements. This work proposes an
approximation to cross-attention scoring based on vector quantization and
enables compute- and memory-efficient use of large biasing catalogues. We
propose to use this technique jointly with a retrieval based contextual biasing
approach. First, we use an efficient quantized retrieval module to shortlist
biasing entries by grounding them on audio. Then we use retrieved entries for
biasing. Since the proposed approach is agnostic to the biasing method, we
investigate using full cross-attention, LLM prompting, and a combination of the
two. We show that retrieval based shortlisting allows the system to efficiently
leverage biasing catalogues of several thousands of entries, resulting in up to
71% relative error rate reduction in personal entity recognition. At the same
time, the proposed approximation algorithm reduces compute time by 20% and
memory usage by 85-95%, for lists of up to one million entries, when compared
to standard dot-product cross-attention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, submitted to IEEE/ACM Transactions on Audio,
  Speech, and Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System
  Without Turn-level Dialog Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adib Mosharrof, A. B. Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) systems enable users to achieve their goals
through natural language interactions. Traditionally, these systems have relied
on turn-level manually annotated metadata, such as dialogue states and policy
annotations, which are expensive, time-consuming, and often inconsistent or
error-prone. This dependence limits the potential to leverage vast amounts of
readily available conversational data for training TOD systems. Additionally, a
critical challenge in TOD system design is determining when and how to access
and integrate information from external sources. Current approaches typically
expect this information to be provided alongside the dialogue context, rather
than learning to identify and retrieve it autonomously. While pre-trained large
language models (LLMs) have been used to develop TOD systems, their potential
to train such systems without laborious annotations remains largely unexplored.
This work employs multi-task instruction fine-tuning to create more efficient
and scalable TOD systems that can effectively leverage natural language
conversational data without manual annotations, while autonomously managing
external information retrieval. Our extensive experimental evaluations, using
three diverse TOD datasets and three LLMs of varying sizes, demonstrate that
our approach can generalize to new, unseen domains. Notably, our approach
outperforms both state-of-the-art models trained on annotated data and
billion-scale parameter off-the-shelf ChatGPT models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieving Implicit and Explicit Emotional Events Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guimin Hu, Hasti Seifi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have garnered significant attention in recent
years due to their impressive performance. While considerable research has
evaluated these models from various perspectives, the extent to which LLMs can
perform implicit and explicit emotion retrieval remains largely unexplored. To
address this gap, this study investigates LLMs' emotion retrieval capabilities
in commonsense. Through extensive experiments involving multiple models, we
systematically evaluate the ability of LLMs on emotion retrieval. Specifically,
we propose a supervised contrastive probing method to verify LLMs' performance
for implicit and explicit emotion retrieval, as well as the diversity of the
emotional events they retrieve. The results offer valuable insights into the
strengths and limitations of LLMs in handling emotion retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Illuminating Blind Spots of Language Models with Targeted
  Agent-in-the-Loop Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17860v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17860v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs T. J. Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have achieved impressive accuracy across a variety of
tasks but remain vulnerable to high-confidence misclassifications, also
referred to as unknown unknowns (UUs). These UUs cluster into blind spots in
the feature space, leading to significant risks in high-stakes applications.
This is particularly relevant for smaller, lightweight LMs that are more
susceptible to such errors. While the identification of UUs has been
extensively studied, their mitigation remains an open challenge, including how
to use identified UUs to eliminate unseen blind spots. In this work, we propose
a novel approach to address blind spot mitigation through the use of
intelligent agents -- either humans or large LMs -- as teachers to characterize
UU-type errors. By leveraging the generalization capabilities of intelligent
agents, we identify patterns in high-confidence misclassifications and use them
to generate targeted synthetic samples to improve model robustness and reduce
blind spots. We conduct an extensive evaluation of our method on three
classification tasks and demonstrate its effectiveness in reducing the number
of UUs, all while maintaining a similar level of accuracy. We find that the
effectiveness of human computation has a high ceiling but is highly dependent
on familiarity with the underlying task. Moreover, the cost gap between humans
and LMs surpasses an order of magnitude, as LMs attain human-like
generalization and generation performance while being more scalable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan He, Yuxiang Nie, Hongmei Wang, Shu Yang, Yihui Wang, Zhiyuan Cai, Zhixuan Chen, Yingxue Xu, Luyang Luo, Huiling Xiang, Xi Lin, Mingxiang Wu, Yifan Peng, George Shih, Ziyang Xu, Xian Wu, Qiong Wang, Ronald Cheong Kin Chan, Varut Vardhanabhuti, Winnie Chiu Wing Chu, Yefeng Zheng, Pranav Rajpurkar, Kang Zhang, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist foundation models (GFMs) are renowned for their exceptional
capability and flexibility in effectively generalizing across diverse tasks and
modalities. In the field of medicine, while GFMs exhibit superior
generalizability based on their extensive intrinsic knowledge as well as
proficiency in instruction following and in-context learning, specialist models
excel in precision due to their domain knowledge. In this work, for the first
time, we explore the synergy between the GFM and specialist models, to enable
precise medical image analysis on a broader scope. Specifically, we propose a
cooperative framework, Generalist-Specialist Collaboration (GSCo), which
consists of two stages, namely the construction of GFM and specialists, and
collaborative inference on downstream tasks. In the construction stage, we
develop MedDr, the largest open-source GFM tailored for medicine, showcasing
exceptional instruction-following and in-context learning capabilities.
Meanwhile, a series of lightweight specialists are crafted for downstream tasks
with low computational cost. In the collaborative inference stage, we introduce
two cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented
Diagnosis, to harvest the generalist's in-context learning abilities alongside
the specialists' domain expertise. For a comprehensive evaluation, we curate a
large-scale benchmark featuring 28 datasets and about 250,000 images. Extensive
results demonstrate that MedDr consistently outperforms state-of-the-art GFMs
on downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists
across all out-of-domain disease diagnosis datasets. These findings indicate a
significant paradigm shift in the application of GFMs, transitioning from
separate models for specific tasks to a collaborative approach between GFMs and
specialists, thereby advancing the frontiers of generalizable AI in medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pediatrics<span class="highlight-title">GPT</span>: Large Language Models as Chinese Medical Assistants for
  Pediatric Applications <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19266v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19266v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing intelligent pediatric consultation systems offers promising
prospects for improving diagnostic efficiency, especially in China, where
healthcare resources are scarce. Despite recent advances in Large Language
Models (LLMs) for Chinese medicine, their performance is sub-optimal in
pediatric applications due to inadequate instruction data and vulnerable
training procedures. To address the above issues, this paper builds PedCorpus,
a high-quality dataset of over 300,000 multi-task instructions from pediatric
textbooks, guidelines, and knowledge graph resources to fulfil diverse
diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the
first Chinese pediatric LLM assistant built on a systematic and robust training
pipeline. In the continuous pre-training phase, we introduce a hybrid
instruction pre-training mechanism to mitigate the internal-injected knowledge
inconsistency of LLMs for medical domain adaptation. Immediately, the
full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the
general medical knowledge schema into the models. After that, we devise a
direct following preference optimization to enhance the generation of
pediatrician-like humanistic responses. In the parameter-efficient secondary
SFT phase, a mixture of universal-specific experts strategy is presented to
resolve the competency conflict between medical generalist and pediatric
expertise mastery. Extensive results based on the metrics, GPT-4, and doctor
evaluations on distinct doctor downstream tasks show that PediatricsGPT
consistently outperforms previous Chinese medical LLMs. Our model and dataset
will be open-source for community development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical
  Large Language Model</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Estornell, Jean-Francois Ton, Yuanshun Yao, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated a remarkable ability to serve
as general-purpose tools for various language-based tasks. Recent works have
demonstrated that the efficacy of such models can be improved through iterative
dialog between multiple models, frequently referred to as multi-agent debate
(MAD). While debate shows promise as a means of improving model efficacy, most
works in this area treat debate as an emergent behavior, rather than a learned
behavior. In doing so, current debate frameworks rely on collaborative
behaviors to have been sufficiently trained into off-the-shelf models. To
address this limitation, we propose ACC-Debate, an Actor-Critic based learning
framework to produce a two-agent team specialized in debate. We demonstrate
that ACC-Debate outperforms SotA debate techniques on a wide array of
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tool Learning with Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, tool learning with large language models (LLMs) has emerged as a
promising paradigm for augmenting the capabilities of LLMs to tackle highly
complex problems. Despite growing attention and rapid advancements in this
field, the existing literature remains fragmented and lacks systematic
organization, posing barriers to entry for newcomers. This gap motivates us to
conduct a comprehensive survey of existing works on tool learning with LLMs. In
this survey, we focus on reviewing existing literature from the two primary
aspects (1) why tool learning is beneficial and (2) how tool learning is
implemented, enabling a comprehensive understanding of tool learning with LLMs.
We first explore the "why" by reviewing both the benefits of tool integration
and the inherent benefits of the tool learning paradigm from six specific
aspects. In terms of "how", we systematically review the literature according
to a taxonomy of four key stages in the tool learning workflow: task planning,
tool selection, tool calling, and response generation. Additionally, we provide
a detailed summary of existing benchmarks and evaluation methods, categorizing
them according to their relevance to different stages. Finally, we discuss
current challenges and outline potential future directions, aiming to inspire
both researchers and industrial developers to further explore this emerging and
promising area. We also maintain a GitHub repository to continually keep track
of the relevant papers and resources in this rising area at
https://github.com/quchangle1/LLM-Tool-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted by Frontiers of Computer Science (FCS),
  with the DOI: {10.1007/s11704-024-40678-2}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ fMRI predictors based on language models of increasing complexity
  recover brain left lateralization <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Bonnasse-Gahot, Christophe Pallier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, studies of naturalistic language processing where
participants are scanned while listening to continuous text have flourished.
Using word embeddings at first, then large language models, researchers have
created encoding models to analyze the brain signals. Presenting these models
with the same text as the participants allows to identify brain areas where
there is a significant correlation between the functional magnetic resonance
imaging (fMRI) time series and the ones predicted by the models' artificial
neurons. One intriguing finding from these studies is that they have revealed
highly symmetric bilateral activation patterns, somewhat at odds with the
well-known left lateralization of language processing. Here, we report analyses
of an fMRI dataset where we manipulate the complexity of large language models,
testing 28 pretrained models from 8 different families, ranging from 124M to
14.2B parameters. First, we observe that the performance of models in
predicting brain responses follows a scaling law, where the fit with brain
activity increases linearly with the logarithm of the number of parameters of
the model (and its performance on natural language processing tasks). Second,
although this effect is present in both hemispheres, it is stronger in the left
than in the right hemisphere. Specifically, the left-right difference in brain
correlation follows a scaling law with the number of parameters. This finding
reconciles computational analyses of brain activity using large language models
with the classic observation from aphasic patients showing left hemisphere
dominance for language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TorchTitan: One-stop PyTorch native solution for production ready LLM
  <span class="highlight-title">pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, Sanket Purandare, Gokul Nadathur, Stratos Idreos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has been instrumental in
advancing state-of-the-art natural language processing applications. Training
LLMs with billions of parameters and trillions of tokens require sophisticated
distributed systems that enable composing and comparing several
state-of-the-art techniques in order to efficiently scale across thousands of
accelerators. However, existing solutions are complex, scattered across
multiple libraries/repositories, lack interoperability, and are cumbersome to
maintain. Thus, curating and empirically comparing training recipes require
non-trivial engineering effort.
  This paper introduces TorchTitan, an open-source, PyTorch-native distributed
training system that unifies state-of-the-art techniques, streamlining
integration and reducing overhead. TorchTitan enables 3D parallelism in a
modular manner with elastic scaling, providing comprehensive logging,
checkpointing, and debugging tools for production-ready training. It also
incorporates hardware-software co-designed solutions, leveraging features like
Float8 training and SymmetricMemory. As a flexible test bed, TorchTitan
facilitates custom recipe curation and comparison, allowing us to develop
optimized training recipes for Llama 3.1 and provide guidance on selecting
techniques for maximum efficiency based on our experiences.
  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8
billion to 405 billion parameters, and showcase its exceptional performance,
modular composability, and elastic scalability. By stacking training
optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at
the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at
the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at
the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Metric for Measuring the Robustness of Large Language Models in
  Non-adversarial Scenarios <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01963v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01963v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Ackerman, Ella Rabinovich, Eitan Farchi, Ateret Anaby-Tavor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluate the robustness of several large language models on multiple
datasets. Robustness here refers to the relative insensitivity of the model's
answers to meaning-preserving variants of their input. Benchmark datasets are
constructed by introducing naturally-occurring, non-malicious perturbations, or
by generating semantically equivalent paraphrases of input questions or
statements. We further propose a novel metric for assessing a model robustness,
and demonstrate its benefits in the non-adversarial scenario by empirical
evaluation of several models on the created datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 2024 Conference on Empirical Methods in Natural
  Language Processing (EMNLP) findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11944v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11944v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the capabilities of large multimodal models (LMMs) continue to advance,
evaluating the performance of LMMs emerges as an increasing need. Additionally,
there is an even larger gap in evaluating the advanced knowledge and reasoning
abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,
a new Chinese Massive Multi-discipline Multimodal Understanding benchmark
designed to evaluate LMMs on tasks demanding college-level subject knowledge
and deliberate reasoning in a Chinese context. CMMMU is inspired by and
strictly follows the annotation and analysis pattern of MMMU. CMMMU includes
12k manually collected multimodal questions from college exams, quizzes, and
textbooks, covering six core disciplines: Art & Design, Business, Science,
Health & Medicine, Humanities & Social Science, and Tech & Engineering, like
its companion, MMMU. These questions span 30 subjects and comprise 39 highly
heterogeneous image types, such as charts, diagrams, maps, tables, music
sheets, and chemical structures. CMMMU focuses on complex perception and
reasoning with domain-specific knowledge in the Chinese context. We evaluate 11
open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves
accuracies of 42%, indicating a large space for improvement. CMMMU will boost
the community to build the next-generation LMMs towards expert artificial
intelligence and promote the democratization of LMMs by providing diverse
language contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiVLC: Extending Vision-Language Compositionality Evaluation with
  Text-to-Image Retrieval <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imanol Miranda, Ander Salaberria, Eneko Agirre, Gorka Azkune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe
are formulated as image-to-text retrieval problems, where, given an image, the
models need to select between the correct textual description and a synthetic
hard negative text. In this work, we present the Bidirectional Vision-Language
Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic
hard negative image generated from the synthetic text, resulting in two
image-to-text retrieval examples (one for each image) and, more importantly,
two text-to-image retrieval examples (one for each text). Human annotators
filter out ill-formed examples ensuring the validity of the benchmark. The
experiments on BiVLC uncover a weakness of current multimodal models, as they
perform poorly in the text-to-image direction. In fact, when considering both
retrieval directions, the conclusions obtained in previous works change
significantly. In addition to the benchmark, we show that a contrastive model
trained using synthetic images and texts significantly improves over the base
model in SugarCrepe and in BiVLC for both retrieval directions. The gap to
human performance in BiVLC confirms that Vision-Language Compositionality is
still a challenging problem. BiVLC and code are available at
https://imirandam.github.io/BiVLC_project_page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page
  at: https://imirandam.github.io/BiVLC_project_page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">GPT</span>-4 learn to analyse moves in research article abstracts? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15612v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15612v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danni Yu, Marina Bondi, Ken Hyland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most powerful and enduring ideas in written discourse analysis is
that genres can be described in terms of the moves which structure a writer's
purpose. Considerable research has sought to identify these distinct
communicative acts, but analyses have been beset by problems of subjectivity,
reliability and the time-consuming need for multiple coders to confirm
analyses. In this paper we employ the affordances of GPT-4 to automate the
annotation process by using natural language prompts. Focusing on abstracts
from articles in four applied linguistics journals, we devise prompts which
enable the model to identify moves effectively. The annotated outputs of these
prompts were evaluated by two assessors with a third addressing disagreements.
The results show that an 8-shot prompt was more effective than one using two,
confirming that the inclusion of examples illustrating areas of variability can
enhance GPT-4's ability to recognize multiple moves in a single sentence and
reduce bias related to textual position. We suggest that GPT-4 offers
considerable potential in automating this annotation process, when human actors
with domain specific linguistic expertise inform the prompting process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot
  Named Entity Recognition with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anbang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ReverseNER, a framework aimed at overcoming the
limitations of large language models (LLMs) in zero-shot Named Entity
Recognition (NER) tasks, particularly in cases where certain entity types have
ambiguous boundaries. ReverseNER tackles this challenge by constructing a
reliable example library with the reversed process of NER. Rather than
beginning with sentences, this method uses an LLM to generate entities based on
their definitions and then expands them into full sentences. During sentence
generation, the LLM is guided to replicate the structure of a specific 'feature
sentence', extracted from the task sentences by clustering. This results in
well-annotated sentences with clearly labeled entities, while preserving
semantic and structural similarity to the task sentences. Once the example
library is constructed, the method selects the most semantically similar
example labels for each task sentence to support the LLM's inference. We also
propose an entity-level self-consistency scoring mechanism to improve NER
performance with LLMs. Experiments show that ReverseNER significantly
outperforms traditional zero-shot NER with LLMs and surpasses several few-shot
methods, marking a notable improvement in NER for domains with limited labeled
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Adaptation of Language Models with a Memory of Amortized Contexts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, Jonathan Richard Schwarz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the rapid generation and dissemination of information, large language
models (LLMs) quickly run out of date despite enormous development costs. To
address the crucial need to keep models updated, online learning has emerged as
a critical tool when utilizing LLMs for real-world applications. However, given
the ever-expanding corpus of unseen documents and the large parameter space of
modern LLMs, efficient adaptation is essential. To address these challenges, we
propose Memory of Amortized Contexts (MAC), an efficient and effective online
adaptation framework for LLMs with strong knowledge retention. We propose a
feature extraction and memory-augmentation approach to compress and extract
information from new documents into compact modulations stored in a memory
bank. When answering questions, our model attends to and extracts relevant
knowledge from this memory bank. To learn informative modulations in an
efficient manner, we utilize amortization-based meta-learning, which
substitutes an otherwise required optimization process with a single forward
pass of the encoder. Subsequently, we learn to choose from and aggregate
selected documents into a single modulation by conditioning on the question,
allowing us to adapt a frozen language model during test time without requiring
further gradient updates. Our experiment demonstrates the superiority of MAC in
multiple aspects, including online adaptation performance, time, and memory
efficiency. In addition, we show how MAC can be combined with and improve the
performance of popular alternatives such as retrieval augmented generations
(RAGs). Code is available at: https://github.com/jihoontack/MAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference proceeding for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneBit: Towards Extremely Low-bit Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11295v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11295v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model quantification uses low bit-width values to represent the weight
matrices of existing models to be quantized, which is a promising approach to
reduce both storage and computational overheads of deploying highly anticipated
LLMs. However, current quantization methods suffer severe performance
degradation when the bit-width is extremely reduced, and thus focus on
utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes
the weight matrices of LLMs to 1-bit, paving the way for the extremely low
bit-width deployment of LLMs. For this target, we introduce a 1-bit model
compressing framework named OneBit, including a novel 1-bit parameter
representation method to better quantize LLMs as well as an effective parameter
initialization method based on matrix decomposition to improve the convergence
speed of the quantization framework. Sufficient experimental results indicate
that OneBit achieves good performance (at least 81% of the non-quantized
performance on LLaMA models) with robust training processes when only using
1-bit weight matrices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Crafting Tomorrow's Headlines: Neural News Generation and Detection in
  English, Turkish, Hungarian, and Persian <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10724v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10724v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cem Üyük, Danica Rovó, Shaghayegh Kolli, Rabia Varol, Georg Groh, Daryna Dementieva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era dominated by information overload and its facilitation with Large
Language Models (LLMs), the prevalence of misinformation poses a significant
threat to public discourse and societal well-being. A critical concern at
present involves the identification of machine-generated news. In this work, we
take a significant step by introducing a benchmark dataset designed for neural
news detection in four languages: English, Turkish, Hungarian, and Persian. The
dataset incorporates outputs from multiple multilingual generators (in both,
zero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and
GPT-4. Next, we experiment with a variety of classifiers, ranging from those
based on linguistic features to advanced Transformer-based models and LLMs
prompting. We present the detection results aiming to delve into the
interpretablity and robustness of machine-generated texts detectors across all
target languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 NLP4PI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attr-Int: A Simple and Effective Entity Alignment Framework for
  Heterogeneous Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyan Yang, Jingwei Cheng, Chuanhao Xu, Xihao Wang, Jiayi Li, Fu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) refers to the task of linking entities in different
knowledge graphs (KGs). Existing EA methods rely heavily on structural
isomorphism. However, in real-world KGs, aligned entities usually have
non-isomorphic neighborhood structures, which paralyses the application of
these structure-dependent methods. In this paper, we investigate and tackle the
problem of entity alignment between heterogeneous KGs. First, we propose two
new benchmarks to closely simulate real-world EA scenarios of heterogeneity.
Then we conduct extensive experiments to evaluate the performance of
representative EA methods on the new benchmarks. Finally, we propose a simple
and effective entity alignment framework called Attr-Int, in which innovative
attribute information interaction methods can be seamlessly integrated with any
embedding encoder for entity alignment, improving the performance of existing
entity alignment techniques. Experiments demonstrate that our framework
outperforms the state-of-the-art approaches on two new benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MHPP: Exploring the Capabilities and Limitations of Language Models
  Beyond Basic Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianbo Dai, Jianqiao Lu, Yunlong Feng, Dong Huang, Guangtao Zeng, Rongju Ruan, Ming Cheng, Haochen Tan, Zhijiang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have greatly improved
code generation, specifically at the function level. For instance, GPT-4o has
achieved a 91.0\% pass rate on HumanEval. However, this draws into question the
adequacy of existing benchmarks in thoroughly assessing function-level code
generation capabilities. Our study analyzed two common benchmarks, HumanEval
and MBPP, and found that these might not thoroughly evaluate LLMs' code
generation capacities due to limitations in quality, difficulty, and
granularity. To resolve this, we introduce the Mostly Hard Python Problems
(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on
the combination of natural language and code reasoning, MHPP gauges LLMs'
abilities to comprehend specifications and restrictions, engage in multi-step
reasoning, and apply coding knowledge effectively. Initial evaluations of 26
LLMs using MHPP showed many high-performing models on HumanEval failed to
achieve similar success on MHPP. Moreover, MHPP highlighted various previously
undiscovered limitations within various LLMs, leading us to believe that it
could pave the way for a better understanding of LLMs' capabilities and
limitations. MHPP, evaluation pipeline, and leaderboard can be found in
https://github.com/SparksofAGI/MHPP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, dataset and code are available at
  https://github.com/SparksofAGI/MHPP, leaderboard can be found at
  https://sparksofagi.github.io/MHPP/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking
  Across Diverse Vocabularies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Koneru, Matthias Huck, Miriam Exel, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in NLP have resulted in models with specialized
strengths, such as processing multimodal inputs or excelling in specific
domains. However, real-world tasks, like multimodal translation, often require
a combination of these strengths, such as handling both translation and image
processing. While individual translation and vision models are powerful, they
typically lack the ability to perform both tasks in a single system. Combining
these models poses challenges, particularly due to differences in their
vocabularies, which limit the effectiveness of traditional ensemble methods to
post-generation techniques like N-best list re-ranking. In this work, we
propose a novel zero-shot ensembling strategy that allows for the integration
of different models during the decoding phase without the need for additional
training. Our approach re-ranks beams during decoding by combining scores at
the word level, using heuristics to predict when a word is completed. We
demonstrate the effectiveness of this method in machine translation scenarios,
showing that it enables the generation of translations that are both speech-
and image-aware while also improving overall translation quality (We will
release the code upon paper acceptance.).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WMT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and
  Schema Pruning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Shen, Pavlos Vougiouklis, Chenxin Diao, Kaustubh Vyas, Yuanyi Ji, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on Text-to-SQL semantic parsing from the perspective of
retrieval-augmented generation. Motivated by challenges related to the size of
commercial database schemata and the deployability of business intelligence
solutions, we propose $\text{ASTReS}$ that dynamically retrieves input database
information and uses abstract syntax trees to select few-shot examples for
in-context learning.
  Furthermore, we investigate the extent to which an in-parallel semantic
parser can be leveraged for generating approximated versions of the expected
SQL queries, to support our retrieval. We take this approach to the extreme--we
adapt a model consisting of less than $500$M parameters, to act as an extremely
efficient approximator, enhancing it with the ability to process schemata in a
parallelised manner. We apply $\text{ASTReS}$ to monolingual and cross-lingual
benchmarks for semantic parsing, showing improvements over state-of-the-art
baselines. Comprehensive experiments highlight the contribution of modules
involved in this retrieval-augmented generation setting, revealing interesting
directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable and Domain-General Abstractive Proposition Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Javad Hosseini, Yang Gao, Tim Baumgärtner, Alex Fabrikant, Reinald Kim Amplayo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting text into fine-grained units of meaning is important to a wide
range of NLP applications. The default approach of segmenting text into
sentences is often insufficient, especially since sentences are usually complex
enough to include multiple units of meaning that merit separate treatment in
the downstream task. We focus on the task of abstractive proposition
segmentation (APS): transforming text into simple, self-contained, well-formed
sentences. Several recent works have demonstrated the utility of proposition
segmentation with few-shot prompted LLMs for downstream tasks such as
retrieval-augmented grounding and fact verification. However, this approach
does not scale to large amounts of text and may not always extract all the
facts from the input text. In this paper, we first introduce evaluation metrics
for the task to measure several dimensions of quality. We then propose a
scalable, yet accurate, proposition segmentation model. We model proposition
segmentation as a supervised task by training LLMs on existing annotated
datasets and show that training yields significantly improved results. We
further show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as
teachers for annotating large amounts of multi-domain synthetic distillation
data, we can train smaller student models (Gemma 1 2B and 7B) with results
similar to the teacher LLMs. We then demonstrate that our technique leads to
effective domain generalization, by annotating data in two domains outside the
original training data and evaluating on them. Finally, as a key contribution
of the paper, we share an easy-to-use API for NLP practitioners to use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nash CoT: Multi-Path Inference with Preference Equilibrium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhang, Cunxiang Wang, Xiong Xiao, Yue Zhang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain of thought (CoT) is a reasoning framework that can enhance the
performance of Large Language Models (LLMs) on complex inference tasks. In
particular, among various studies related to CoT, multi-path inference stands
out as a simple yet effective improvement. However, there is no optimal setting
for the number of inference paths. Therefore, we have to increase the number of
inference paths to obtain better results, which in turn increases the inference
cost. To address this limitation, we can utilize question-related role
templates to guide LLMs into relevant roles, thereby increasing the possibility
of correct inferences for each path and further reducing dependence on the
number of inference paths while improving reasoning accuracy. However, placing
LLMs into specific roles may reduce their reasoning diversity and performance
on a few tasks where role dependence is low. To alleviate the excessive
immersion of the LLM into a specific role, we propose Nash CoT by constructing
a competitive system on each path that balances the generation from
role-specific LLMs' and the general LLMs' generation, thereby ensuring both
effective role adoption and diversity in LLM generation further maintaining the
performance of multi-path inference while reducing the requirement of the
number of inference paths. We evaluate Nash CoT across various inference tasks,
including Arabic Reasoning, Commonsense Question Answering, and Symbolic
Inference, achieving results that are comparable to or better than those of
multi-path CoT with the equal number of inference paths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Synthetic <span class="highlight-title">Dataset</span> for Personal Attribute Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Yukhymenko, Robin Staab, Mark Vero, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, powerful Large Language Models (LLMs) have become easily accessible
to hundreds of millions of users world-wide. However, their strong capabilities
and vast world knowledge do not come without associated privacy risks. In this
work, we focus on the emerging privacy threat LLMs pose -- the ability to
accurately infer personal information from online texts. Despite the growing
importance of LLM-based author profiling, research in this area has been
hampered by a lack of suitable public datasets, largely due to ethical and
privacy concerns associated with real personal data. We take two steps to
address this problem: (i) we construct a simulation framework for the popular
social media platform Reddit using LLM agents seeded with synthetic personal
profiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic
dataset of over 7800 comments manually labeled for personal attributes. We
validate our dataset with a human study showing that humans barely outperform
random guessing on the task of distinguishing our synthetic comments from real
ones. Further, we verify that our dataset enables meaningful personal attribute
inference research by showing across 18 state-of-the-art LLMs that our
synthetic comments allow us to draw the same conclusions as real-world data.
Combined, our experimental results, dataset and pipeline form a strong basis
for future privacy-preserving research geared towards understanding and
mitigating inference-based privacy threats that LLMs pose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Have You Merged My Model? On The Robustness of Large Language Model IP
  Protection Methods Against Model Merging <span class="chip">CCS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshuo Cong, Delong Ran, Zesen Liu, Xinlei He, Jinyuan Liu, Yichen Gong, Qi Li, Anyu Wang, Xiaoyun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging is a promising lightweight model empowerment technique that
does not rely on expensive computing devices (e.g., GPUs) or require the
collection of specific training data. Instead, it involves editing different
upstream model parameters to absorb their downstream task capabilities.
However, uncertified model merging can infringe upon the Intellectual Property
(IP) rights of the original upstream models. In this paper, we conduct the
first study on the robustness of IP protection methods under model merging
scenarios. Specifically, we investigate two state-of-the-art IP protection
techniques: Quantization Watermarking and Instructional Fingerprint, along with
various advanced model merging technologies, such as Task Arithmetic,
TIES-MERGING, and so on. Experimental results indicate that current Large
Language Model (LLM) watermarking techniques cannot survive in the merged
models, whereas model fingerprinting techniques can. Our research aims to
highlight that model merging should be an indispensable consideration in the
robustness assessment of model IP protection techniques, thereby promoting the
healthy development of the open-source LLM community. Our code is available at
https://github.com/ThuCCSLab/MergeGuard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM CCS-LAMPS 2024 (Best Paper Award)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Localizing and Mitigating Errors in Long-form Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11930v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11930v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form question answering (LFQA) aims to provide thorough and in-depth
answers to complex questions, enhancing comprehension. However, such detailed
responses are prone to hallucinations and factual inconsistencies, challenging
their faithful evaluation. This work introduces HaluQuestQA, the first
hallucination dataset with localized error annotations for human-written and
model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k
span-level error annotations for five different error types by expert
annotators, along with preference judgments. Using our collected data, we
thoroughly analyze the shortcomings of long-form answers and find that they
lack comprehensiveness and provide unhelpful references. We train an automatic
feedback model on this dataset that predicts error spans with incomplete
information and provides associated explanations. Finally, we propose a
prompt-based approach, Error-informed refinement, that uses signals from the
learned feedback model to refine generated answers, which we show reduces
errors and improves answer quality across multiple models. Furthermore, humans
find answers generated by our approach comprehensive and highly prefer them
(84%) over the baseline answers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data are available:
  https://github.com/UKPLab/arxiv2024-lfqa-hallucination</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A LLM-Based Ranking Method for the Evaluation of Automatic
  Counter-Narrative Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irune Zubiaga, Aitor Soroa, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to evaluate Counter Narrative (CN)
generation using a Large Language Model (LLM) as an evaluator. We show that
traditional automatic metrics correlate poorly with human judgements and fail
to capture the nuanced relationship between generated CNs and human perception.
To alleviate this, we introduce a model ranking pipeline based on pairwise
comparisons of generated CNs from different models, organized in a
tournament-style format. The proposed evaluation method achieves a high
correlation with human preference, with a $\rho$ score of 0.88. As an
additional contribution, we leverage LLMs as zero-shot CN generators and
provide a comparative analysis of chat, instruct, and base models, exploring
their respective strengths and limitations. Through meticulous evaluation,
including fine-tuning experiments, we elucidate the differences in performance
and responsiveness to domain-specific data. We conclude that chat-aligned
models in zero-shot are the best option for carrying out the task, provided
they do not refuse to generate an answer due to security concerns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Findings of the Association for Computational
  Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word Embedding Dimension Reduction via Weakly-Supervised Feature
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintang Xue, Yun-Cheng Wang, Chengwei Wei, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a fundamental task in natural language processing, word embedding converts
each word into a representation in a vector space. A challenge with word
embedding is that as the vocabulary grows, the vector space's dimension
increases, which can lead to a vast model size. Storing and processing word
vectors are resource-demanding, especially for mobile edge-devices
applications. This paper explores word embedding dimension reduction. To
balance computational costs and performance, we propose an efficient and
effective weakly-supervised feature selection method named WordFS. It has two
variants, each utilizing novel criteria for feature selection. Experiments on
various tasks (e.g., word and sentence similarity and binary and multi-class
classification) indicate that the proposed WordFS model outperforms other
dimension reduction methods at lower computational costs. We have released the
code for reproducibility along with the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoQT: Low-Rank Adapters for Quantized <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16528v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16528v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Loeschcke, Mads Toftrup, Michael J. Kastoryano, Serge Belongie, Vésteinn Snæbjarnarson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances using low-rank adapters and quantization, pretraining of
large models on consumer hardware has not been possible without model sharding,
offloading during training, or per-layer gradient updates. To address these
limitations, we propose Low-Rank Adapters for Quantized Training (LoQT), a
method for efficiently training quantized models. LoQT uses gradient-based
tensor factorization to initialize low-rank trainable weight matrices that are
periodically merged into quantized full-rank weight matrices. Our approach is
suitable for both pretraining and fine-tuning models. We demonstrate this for
language modeling and downstream task adaptation, finding that LoQT enables
efficient training of models up to 7B parameters on a 24GB GPU. We also
demonstrate the feasibility of training a 13B model using per-layer gradient
updates on the same hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GINopic: Topic Modeling with Graph Isomorphism Network <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suman Adhya, Debarshi Kumar Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling is a widely used approach for analyzing and exploring large
document collections. Recent research efforts have incorporated pre-trained
contextualized language models, such as BERT embeddings, into topic modeling.
However, they often neglect the intrinsic informational value conveyed by
mutual dependencies between words. In this study, we introduce GINopic, a topic
modeling framework based on graph isomorphism networks to capture the
correlation between words. By conducting intrinsic (quantitative as well as
qualitative) and extrinsic evaluations on diverse benchmark datasets, we
demonstrate the effectiveness of GINopic compared to existing topic models and
highlight its potential for advancing topic modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper for NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StreamingDialogue: Prolonged Dialogue Learning via Long Context
  Compression with Minimal Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Nan Li, Quan Tu, Cunli Mao, Zhengtao Yu, Ji-Rong Wen, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard Large Language Models (LLMs) struggle with handling dialogues with
long contexts due to efficiency and consistency issues. According to our
observation, dialogue contexts are highly structured, and the special token of
\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate
information. We refer to the EoU tokens as ``conversational attention sinks''
(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which
compresses long dialogue history into conv-attn sinks with minimal losses, and
thus reduces computational complexity quadratically with the number of sinks
(i.e., the number of utterances). Current LLMs already demonstrate the ability
to handle long context window, e.g., a window size of 200K or more. To this
end, by compressing utterances into EoUs, our method has the potential to
handle more than 200K of utterances, resulting in a prolonged dialogue
learning. In order to minimize information losses from reconstruction after
compression, we design two learning strategies of short-memory reconstruction
(SMR) and long-memory reactivation (LMR). Our method outperforms strong
baselines in dialogue tasks and achieves a 4 $\times$ speedup while reducing
memory usage by 18 $\times$ compared to dense attention recomputation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient
  Batching and Composability <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Christof Monz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient finetuning (PEFT) methods effectively adapt large
language models (LLMs) to diverse downstream tasks, reducing storage and GPU
memory demands. Despite these advantages, several applications pose new
challenges to PEFT beyond mere parameter efficiency. One notable challenge
involves the efficient deployment of LLMs equipped with multiple task- or
user-specific adapters, particularly when different adapters are needed for
distinct requests within the same batch. Another challenge is the
interpretability of LLMs, which is crucial for understanding how LLMs function.
Previous studies introduced various approaches to address different challenges.
In this paper, we introduce a novel method, RoAd, which employs a
straightforward 2D rotation to adapt LLMs and addresses all the above
challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal
performance on GLUE, eight commonsense reasoning tasks and four arithmetic
reasoning tasks with $<0.1\%$ trainable parameters; (2) RoAd facilitates the
efficient serving of requests requiring different adapters within a batch, with
an overhead comparable to element-wise multiplication instead of batch matrix
multiplication; (3) RoAd enhances LLM's interpretability through integration
within a framework of distributed interchange intervention, demonstrated via
composition experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. Code: https://github.com/BaohaoLiao/road</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Internalizing ASR with Implicit Chain of Thought for Efficient
  Speech-to-Speech Conversational LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17353v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17353v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Shing-Hei Yuen, Timothy Tin-Long Tse, Jian Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current speech-based LLMs are predominantly trained on extensive ASR and TTS
datasets, excelling in tasks related to these domains. However, their ability
to handle direct speech-to-speech conversations remains notably constrained.
These models often rely on an ASR-to-TTS chain-of-thought pipeline, converting
speech into text for processing before generating audio responses, which
introduces latency and loses audio features. We propose a method that
implicitly internalizes ASR chain of thought into a speech LLM, enhancing its
native speech understanding capabilities. Our approach reduces latency and
improves the model's native understanding of speech, paving the way for more
efficient and natural real-time audio interactions. We also release a
large-scale synthetic conversational dataset to facilitate further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated for reviewer comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVQA: Culturally-diverse Multilingual Visual Question Answering
  Benchmark <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hernán Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D'Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario Rodríguez-Cantelar, Mélanie Jouitteau, Mihail Mihaylov, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Naome Etori, Olivier Niyomugisha, Paula Mónica Silva, Pranjal Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago Góngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Teresa Clifford, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is an important task in multimodal AI, and it
is often used to test the ability of vision-language models to understand and
reason on knowledge present in both visual and textual data. However, most of
the current VQA models use datasets that are primarily focused on English and a
few major world languages, with images that are typically Western-centric.
While recent efforts have tried to increase the number of languages covered on
VQA datasets, they still lack diversity in low-resource languages. More
importantly, although these datasets often extend their linguistic range via
translation or some other approaches, they usually keep images the same,
resulting in narrow cultural representation. To address these limitations, we
construct CVQA, a new Culturally-diverse multilingual Visual Question Answering
benchmark, designed to cover a rich set of languages and cultures, where we
engage native speakers and cultural experts in the data collection process. As
a result, CVQA includes culturally-driven images and questions from across 30
countries on four continents, covering 31 languages with 13 scripts, providing
a total of 10k questions. We then benchmark several Multimodal Large Language
Models (MLLMs) on CVQA, and show that the dataset is challenging for the
current state-of-the-art models. This benchmark can serve as a probing
evaluation suite for assessing the cultural capability and bias of multimodal
models and hopefully encourage more research efforts toward increasing cultural
awareness and linguistic diversity in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Compression with Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rhea Sanjay Sukthanker, Benedikt Staffler, Frank Hutter, Aaron Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable reasoning abilities, allowing
them to generalize across a wide range of downstream tasks, such as commonsense
reasoning or instruction following. However, as LLMs scale, inference costs
become increasingly prohibitive, accumulating significantly over their life
cycle. This poses the question: Can we compress pre-trained LLMs to meet
diverse size and latency requirements? We leverage Neural Architecture Search
(NAS) to compress LLMs by pruning structural components, such as attention
heads, neurons, and layers, aiming to achieve a Pareto-optimal balance between
performance and efficiency. While NAS already achieved promising results on
small language models in previous work, in this paper we propose various
extensions that allow us to scale to LLMs. Compared to structural pruning
baselines, we show that NAS improves performance up to 3.4% on MMLU with an
on-device latency speedup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WER We Stand: Benchmarking Urdu ASR Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samee Arif, Sualeha Farid, Aamina Jamal Khan, Mustafa Abbas, Agha Ali Raza, Awais Athar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive evaluation of Urdu Automatic Speech
Recognition (ASR) models. We analyze the performance of three ASR model
families: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along
with a detailed examination of the most frequent wrong words and error types
including insertions, deletions, and substitutions. Our analysis is conducted
using two types of datasets, read speech and conversational speech. Notably, we
present the first conversational speech dataset designed for benchmarking Urdu
ASR models. We find that seamless-large outperforms other ASR models on the
read speech dataset, while whisper-large performs best on the conversational
speech dataset. Furthermore, this evaluation highlights the complexities of
assessing ASR models for low-resource languages like Urdu using quantitative
metrics alone and emphasizes the need for a robust Urdu text normalization
system. Our findings contribute valuable insights for developing robust ASR
systems for low-resource languages like Urdu.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DREsS: <span class="highlight-title">Dataset</span> for Rubric-based Essay Scoring on EFL Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haneul Yoo, Jieun Han, So-Yeon Ahn, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated essay scoring (AES) is a useful tool in English as a Foreign
Language (EFL) writing education, offering real-time essay scores for students
and instructors. However, previous AES models were trained on essays and scores
irrelevant to the practical scenarios of EFL writing education and usually
provided a single holistic score due to the lack of appropriate datasets. In
this paper, we release DREsS, a large-scale, standard dataset for rubric-based
automated essay scoring. DREsS comprises three sub-datasets: DREsS_New,
DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with
2.3K essays authored by EFL undergraduate students and scored by English
education experts. We also standardize existing rubric-based essay scoring
datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation
strategy for essays, which generates 40.1K synthetic samples of DREsS_CASE and
improves the baseline results by 45.44%. DREsS will enable further research to
provide a more accurate and practical AES system for EFL writing education.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Experimental Narratives: A Comparison of Human Crowdsourced Storytelling
  and AI Storytelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nina Begus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper proposes a framework that combines behavioral and computational
experiments employing fictional prompts as a novel tool for investigating
cultural artifacts and social biases in storytelling both by humans and
generative AI. The study analyzes 250 stories authored by crowdworkers in June
2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging
methods from narratology and inferential statistics. Both crowdworkers and
large language models responded to identical prompts about creating and falling
in love with an artificial human. The proposed experimental paradigm allows a
direct and controlled comparison between human and LLM-generated storytelling.
Responses to the Pygmalionesque prompts confirm the pervasive presence of the
Pygmalion myth in the collective imaginary of both humans and large language
models. All solicited narratives present a scientific or technological pursuit.
The analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are
more progressive in terms of gender roles and sexuality than those written by
humans. While AI narratives with default settings and no additional prompting
can occasionally provide innovative plot twists, they offer less imaginative
scenarios and rhetoric than human-authored texts. The proposed framework argues
that fiction can be used as a window into human and AI-based collective
imaginary and social dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the ubiquity of charts as a data analysis, visualization, and
decision-making tool across industries and sciences, there has been a growing
interest in developing pre-trained foundation models as well as general purpose
instruction-tuned models for chart understanding and reasoning. However,
existing methods suffer crucial drawbacks across two critical axes affecting
the performance of chart representation models: they are trained on data
generated from underlying data tables of the charts, ignoring the visual trends
and patterns in chart images, and use weakly aligned vision-language backbone
models for domain-specific training, limiting their generalizability when
encountering charts in the wild. We address these important drawbacks and
introduce ChartGemma, a novel chart understanding and reasoning model developed
over PaliGemma. Rather than relying on underlying data tables, ChartGemma is
trained on instruction-tuning data generated directly from chart images, thus
capturing both high-level trends and low-level visual information from a
diverse set of charts. Our simple approach achieves state-of-the-art results
across $5$ benchmarks spanning chart summarization, question answering, and
fact-checking, and our elaborate qualitative studies on real-world charts show
that ChartGemma generates more realistic and factually correct summaries
compared to its contemporaries. We release the code, model checkpoints,
dataset, and demos at https://github.com/vis-nlp/ChartGemma.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic-Conversation Relevance (TCR) <span class="highlight-title">Dataset</span> and Benchmarks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaran Fan, Jamie Pool, Senja Filipi, Ross Cutler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Workplace meetings are vital to organizational collaboration, yet a large
percentage of meetings are rated as ineffective. To help improve meeting
effectiveness by understanding if the conversation is on topic, we create a
comprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety
of domains and meeting styles. The TCR dataset includes 1,500 unique meetings,
22 million words in transcripts, and over 15,000 meeting topics, sourced from
both newly collected Speech Interruption Meeting (SIM) data and existing public
datasets. Along with the text data, we also open source scripts to generate
synthetic meetings or create augmented meetings from the TCR dataset to enhance
data diversity. For each data source, benchmarks are created using GPT-4 to
evaluate the model accuracy in understanding transcription-topic relevance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pipeline Parallelism with Controllable Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15362v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15362v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghui Qi, Xinyi Wan, Nyamdavaa Amar, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pipeline parallelism has been widely explored, but most existing schedules
lack a systematic methodology. In this paper, we propose a framework to
decompose pipeline schedules as repeating a building block, and show that the
lifespan of the building block decides the peak activation memory of the
pipeline schedule. Guided by the observations, we find that almost all existing
pipeline schedules, to the best of our knowledge, are memory inefficient. To
address this, we introduce a family of memory efficient building blocks with
controllable activation memory, which can reduce the peak activation memory to
1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable
throughput. We can also achieve almost zero pipeline bubbles while maintaining
the same activation memory as 1F1B. Our evaluations demonstrate that in pure
pipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in
terms of throughput. When employing a grid search over hybrid parallelism
hyperparameters in practical scenarios, our methods demonstrate a 16%
throughput improvement over the 1F1B baseline for large language models. The
implementation is open-sourced at
https://github.com/sail-sg/zero-bubble-pipeline-parallelism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Retrieval: End-to-End Information Retrieval with One Large Language
  Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyu Tang, Jiawei Chen, Zhuoqun Li, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu, Hongyu Lin, Fei Huang, Ben He, Xianpei Han, Le Sun, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has significantly transformed both
the construction and application of information retrieval (IR) systems.
However, current interactions between IR systems and LLMs remain limited, with
LLMs merely serving as part of components within IR systems, and IR systems
being constructed independently of LLMs. This separated architecture restricts
knowledge sharing and deep collaboration between them. In this paper, we
introduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval
architecture. Self-Retrieval unifies all essential IR functions within a single
LLM, leveraging the inherent capabilities of LLMs throughout the IR process.
Specifically, Self-Retrieval internalizes the retrieval corpus through
self-supervised learning, transforms the retrieval process into sequential
passage generation, and performs relevance assessment for reranking.
Experimental results demonstrate that Self-Retrieval not only outperforms
existing retrieval approaches by a significant margin, but also substantially
enhances the performance of LLM-driven downstream applications like
retrieval-augmented generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Camera-ready Version. Code:
  https://github.com/icip-cas/SelfRetrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gesture<span class="highlight-title">GPT</span>: Toward Zero-Shot Free-Form Hand Gesture Understanding with
  Large Language Model Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12821v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12821v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, Yiqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing gesture interfaces only work with a fixed set of gestures defined
either by interface designers or by users themselves, which introduces learning
or demonstration efforts that diminish their naturalness. Humans, on the other
hand, understand free-form gestures by synthesizing the gesture, context,
experience, and common sense. In this way, the user does not need to learn,
demonstrate, or associate gestures. We introduce GestureGPT, a free-form hand
gesture understanding framework that mimics human gesture understanding
procedures to enable a natural free-form gestural interface. Our framework
leverages multiple Large Language Model agents to manage and synthesize gesture
and context information, then infers the interaction intent by associating the
gesture with an interface function. More specifically, our triple-agent
framework includes a Gesture Description Agent that automatically segments and
formulates natural language descriptions of hand poses and movements based on
hand landmark coordinates. The description is deciphered by a Gesture Inference
Agent through self-reasoning and querying about the interaction context (e.g.,
interaction history, gaze data), which is managed by a Context Management
Agent. Following iterative exchanges, the Gesture Inference Agent discerns the
user's intent by grounding it to an interactive function. We validated our
framework offline under two real-world scenarios: smart home control and online
video streaming. The average zero-shot Top-1/Top-5 grounding accuracies are
44.79%/83.59% for smart home tasks and 37.50%/73.44% for video streaming tasks.
We also provide an extensive discussion that includes rationale for model
selection, generalizability, and future research directions for a practical
system etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the ISS 2024 track of the Proceedings
  of the ACM on Human-Computer Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions
  with Path Planning and Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14826v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14826v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinzhuo Wu, Wei Liu, Jian Luan, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, tool-augmented LLMs have gained increasing attention. Given an
instruction, tool-augmented LLMs can interact with various external tools in
multiple rounds and provide a final answer. However, previous LLMs were trained
on overly detailed instructions, which included API names or parameters, while
real users would not explicitly mention these API details. This leads to a gap
between trained LLMs and real-world scenarios. In addition, most works ignore
whether the interaction process follows the instruction. To address these
issues, we constructed a training dataset called MGToolBench, which contains
statement and category-level instructions to better reflect real-world
scenarios. In addition, we propose ToolPlanner, a two-stage reinforcement
learning framework that utilizes path planning and two feedback mechanisms to
enhance the LLM's task completion and instruction-following capabilities.
Experimental results show that ToolPlanner significantly improves the Match
Rate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA
model. Human evaluation verifies that the multi-granularity instructions can
better align with users' usage habits. Our data and code will be released upon
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISP-LLM: Dimension-Independent Structural Pruning for Large Language
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangqian Gao, Chi-Heng Lin, Ting Hua, Tang Zheng, Yilin Shen, Hongxia Jin, Yen-Chang Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success in various
natural language processing tasks, including language modeling, understanding,
and generation. However, the increased memory and computational costs
associated with these models pose significant challenges for deployment on
resource-limited devices. Structural pruning has emerged as a promising
solution to reduce the costs of LLMs without requiring post-processing steps.
Prior structural pruning methods either follow the dependence of structures at
the cost of limiting flexibility, or introduce non-trivial additional
parameters by incorporating different projection matrices. In this work, we
propose a novel approach that relaxes the constraint imposed by regular
structural pruning methods and eliminates the structural dependence along the
embedding dimension. Our dimension-independent structural pruning method offers
several benefits. Firstly, our method enables different blocks to utilize
different subsets of the feature maps. Secondly, by removing structural
dependence, we facilitate each block to possess varying widths along its input
and output dimensions, thereby significantly enhancing the flexibility of
structural pruning. We evaluate our method on various LLMs, including OPT,
LLaMA, LLaMA-2, Phi-1.5, and Phi-2. Experimental results demonstrate that our
approach outperforms other state-of-the-art methods, showing for the first time
that structural pruning can achieve an accuracy similar to semi-structural
pruning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented
  Generation Question Answering <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitchell DeHaven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a multi-adapter retrieval augmented generation
system (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP
2024. CRAG is a question answering dataset contains 3 different subtasks aimed
at realistic question and answering RAG related tasks, with a diverse set of
question topics, question types, time dynamic answers, and questions featuring
entities of varying popularity.
  Our system follows a standard setup for web based RAG, which uses processed
web pages to provide context for an LLM to produce generations, while also
querying API endpoints for additional information. MARAGS also utilizes
multiple different adapters to solve the various requirements for these tasks
with a standard cross-encoder model for ranking candidate passages relevant for
answering the question. Our system achieved 2nd place for Task 1 as well as 3rd
place on Task 2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAG KDD Cup 24 Workshop</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Caching for Faster Video Generation with Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Michael S. Ryoo, Tian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating temporally-consistent high-fidelity videos can be computationally
expensive, especially over longer temporal spans. More-recent Diffusion
Transformers (DiTs) -- despite making significant headway in this context --
have only heightened such challenges as they rely on larger models and heavier
attention mechanisms, resulting in slower inference speeds. In this paper, we
introduce a training-free method to accelerate video DiTs, termed Adaptive
Caching (AdaCache), which is motivated by the fact that "not all videos are
created equal": meaning, some videos require fewer denoising steps to attain a
reasonable quality than others. Building on this, we not only cache
computations through the diffusion process, but also devise a caching schedule
tailored to each video generation, maximizing the quality-latency trade-off. We
further introduce a Motion Regularization (MoReg) scheme to utilize video
information within AdaCache, essentially controlling the compute allocation
based on motion content. Altogether, our plug-and-play contributions grant
significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video
generation) without sacrificing the generation quality, across multiple video
DiT baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project-page is available at https://adacache-dit.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoVFX: Physically Realistic Video Editing from Natural Language
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Yu Hsu, Zhi-Hao Lin, Albert Zhai, Hongchi Xia, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern visual effects (VFX) software has made it possible for skilled artists
to create imagery of virtually anything. However, the creation process remains
laborious, complex, and largely inaccessible to everyday users. In this work,
we present AutoVFX, a framework that automatically creates realistic and
dynamic VFX videos from a single video and natural language instructions. By
carefully integrating neural scene modeling, LLM-based code generation, and
physical simulation, AutoVFX is able to provide physically-grounded,
photorealistic editing effects that can be controlled directly using natural
language instructions. We conduct extensive experiments to validate AutoVFX's
efficacy across a diverse spectrum of videos and instructions. Quantitative and
qualitative results suggest that AutoVFX outperforms all competing methods by a
large margin in generative quality, instruction alignment, editing versatility,
and physical plausibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://haoyuhsu.github.io/autovfx-website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-free Regional <span class="highlight-title">Prompt</span>ing for Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated excellent capabilities in text-to-image
generation. Their semantic understanding (i.e., prompt following) ability has
also been greatly improved with large language models (e.g., T5, Llama).
However, existing models cannot perfectly handle long and complex text prompts,
especially when the text prompts contain various objects with numerous
attributes and interrelated spatial relationships. While many regional
prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but
there are still no implementations based on the recent Diffusion Transformer
(DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and
implement regional prompting for FLUX.1 based on attention manipulation, which
enables DiT with fined-grained compositional text-to-image generation
capability in a training-free manner. Code is available at
https://github.com/antonioo-c/Regional-Prompting-FLUX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/antonioo-c/Regional-Prompting-FLUX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Length Image Tokenization via Recurrent Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Duggal, Phillip Isola, Antonio Torralba, William T. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision systems typically assign fixed-length representations to
images, regardless of the information content. This contrasts with human
intelligence - and even large language models - which allocate varying
representational capacities based on entropy, context and familiarity. Inspired
by this, we propose an approach to learn variable-length token representations
for 2D images. Our encoder-decoder architecture recursively processes 2D image
tokens, distilling them into 1D latent tokens over multiple iterations of
recurrent rollouts. Each iteration refines the 2D tokens, updates the existing
1D latent tokens, and adaptively increases representational capacity by adding
new tokens. This enables compression of images into a variable number of
tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction
loss and FID metrics, demonstrating that token count aligns with image entropy,
familiarity and downstream task requirements. Recurrent token processing with
increasing representational capacity in each iteration shows signs of token
specialization, revealing potential for object / part discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at: https://github.com/ShivamDuggal4/adaptive-length-tokenizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Far is Video Generation from World Model: A Physical Law Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenAI's Sora highlights the potential of video generation for developing
world models that adhere to fundamental physical laws. However, the ability of
video generation models to discover such laws purely from visual data without
human priors can be questioned. A world model learning the true law should give
predictions robust to nuances and correctly extrapolate on unseen scenarios. In
this work, we evaluate across three key scenarios: in-distribution,
out-of-distribution, and combinatorial generalization. We developed a 2D
simulation testbed for object movement and collisions to generate videos
deterministically governed by one or more classical mechanics laws. This
provides an unlimited supply of data for large-scale experimentation and
enables quantitative evaluation of whether the generated videos adhere to
physical laws. We trained diffusion-based video generation models to predict
object movements based on initial frames. Our scaling experiments show perfect
generalization within the distribution, measurable scaling behavior for
combinatorial generalization, but failure in out-of-distribution scenarios.
Further experiments reveal two key insights about the generalization mechanisms
of these models: (1) the models fail to abstract general physical rules and
instead exhibit "case-based" generalization behavior, i.e., mimicking the
closest training example; (2) when generalizing to new cases, models are
observed to prioritize different factors when referencing training data: color
> size > velocity > shape. Our study suggests that scaling alone is
insufficient for video generation models to uncover fundamental physical laws,
despite its role in Sora's broader success. See our project page at
https://phyworld.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning General-Purpose Biomedical Volume Representations using
  Randomized Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current volumetric biomedical foundation models struggle to generalize as
public 3D datasets are small and do not cover the broad diversity of medical
procedures, conditions, anatomical regions, and imaging protocols. We address
this by creating a representation learning method that instead anticipates
strong domain shifts at training time itself. We first propose a data engine
that synthesizes highly variable training samples that enable generalization to
new biomedical contexts. To then train a single 3D network for any voxel-level
task, we develop a contrastive learning method that pretrains the network to be
stable against nuisance imaging variation simulated by the data engine, a key
inductive bias for generalization. This network's features can be used as
robust representations of input images for downstream tasks and its weights
provide a strong, dataset-agnostic initialization for finetuning on new
datasets. As a result, we set new standards across both multimodality
registration and few-shot segmentation, a first for any 3D biomedical vision
model, all without (pre-)training on any existing dataset of real images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and model weights available at
  https://github.com/neel-dey/anatomix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning identification of maternal inflammatory response and
  histologic choroamnionitis from placental membrane whole slide images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Sharma, Ramin Nateghi, Marina Ayad, Lee A. D. Cooper, Jeffery A. Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The placenta forms a critical barrier to infection through pregnancy, labor
and, delivery. Inflammatory processes in the placenta have short-term, and
long-term consequences for offspring health. Digital pathology and machine
learning can play an important role in understanding placental inflammation,
and there have been very few investigations into methods for predicting and
understanding Maternal Inflammatory Response (MIR). This work intends to
investigate the potential of using machine learning to understand MIR based on
whole slide images (WSI), and establish early benchmarks. To that end, we use
Multiple Instance Learning framework with 3 feature extractors: ImageNet-based
EfficientNet-v2s, and 2 histopathology foundation models, UNI and Phikon to
investigate predictability of MIR stage from histopathology WSIs. We also
interpret predictions from these models using the learned attention maps from
these models. We also use the MIL framework for predicting white blood cells
count (WBC) and maximum fever temperature ($T_{max}$). Attention-based MIL
models are able to classify MIR with a balanced accuracy of up to 88.5% with a
Cohen's Kappa ($\kappa$) of up to 0.772. Furthermore, we found that the
pathology foundation models (UNI and Phikon) are both able to achieve higher
performance with balanced accuracy and $\kappa$, compared to ImageNet-based
feature extractor (EfficientNet-v2s). For WBC and $T_{max}$ prediction, we
found mild correlation between actual values and those predicted from
histopathology WSIs. We used MIL framework for predicting MIR stage from WSIs,
and compared effectiveness of foundation models as feature extractors, with
that of an ImageNet-based model. We further investigated model failure cases
and found them to be either edge cases prone to interobserver variability,
examples of pathologist's overreach, or mislabeled due to processing errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physically Based Neural Bidirectional Reflectance Distribution Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenliang Zhou, Alejandro Sztrajman, Gilles Rainer, Fangcheng Zhong, Fazilet Gokbudak, Zhilin Guo, Weihao Xia, Rafal Mantiuk, Cengiz Oztireli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the physically based neural bidirectional reflectance
distribution function (PBNBRDF), a novel, continuous representation for
material appearance based on neural fields. Our model accurately reconstructs
real-world materials while uniquely enforcing physical properties for realistic
BRDFs, specifically Helmholtz reciprocity via reparametrization and energy
passivity via efficient analytical integration. We conduct a systematic
analysis demonstrating the benefits of adhering to these physical laws on the
visual quality of reconstructed materials. Additionally, we enhance the color
accuracy of neural BRDFs by introducing chromaticity enforcement supervising
the norms of RGB channels. Through both qualitative and quantitative
experiments on multiple databases of measured real-world BRDFs, we show that
adhering to these physical constraints enables neural fields to more faithfully
and stably represent the original data and achieve higher rendering quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Cheng, Juncheng Mu, Xianfang Zeng, Xin Chen, Anqi Pang, Chi Zhang, Zhibin Wang, Bin Fu, Gang Yu, Ziwei Liu, Liang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Texturing is a crucial step in the 3D asset production workflow, which
enhances the visual appeal and diversity of 3D assets. Despite recent
advancements in Text-to-Texture (T2T) generation, existing methods often yield
subpar results, primarily due to local discontinuities, inconsistencies across
multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle
these challenges, we propose a novel generation-refinement 3D texturing
framework called MVPaint, which can generate high-resolution, seamless textures
while emphasizing multi-view consistency. MVPaint mainly consists of three key
modules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model,
MVPaint first simultaneously generates multi-view images by employing an SMG
model, which leads to coarse texturing results with unpainted parts due to
missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete
3D texturing, we introduce the S3I method, specifically designed to effectively
texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore,
MVPaint employs a UVR module to improve the texture quality in the UV space,
which first performs a UV-space Super-Resolution, followed by a Spatial-aware
Seam-Smoothing algorithm for revising spatial texturing discontinuities caused
by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the
Objaverse T2T benchmark and the GSO T2T benchmark, based on selected
high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset,
respectively. Extensive experimental results demonstrate that MVPaint surpasses
existing state-of-the-art methods. Notably, MVPaint could generate
high-fidelity textures with minimal Janus issues and highly enhanced cross-view
consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mvpaint.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-based Generative Multicasting with Intent-aware Semantic
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinkai Liu, Mahdi Boloursaz Mashhadi, Li Qiao, Yi Ma, Rahim Tafazolli, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models (GDMs) have recently shown great success in
synthesizing multimedia signals with high perceptual quality enabling highly
efficient semantic communications in future wireless networks. In this paper,
we develop an intent-aware generative semantic multicasting framework utilizing
pre-trained diffusion models. In the proposed framework, the transmitter
decomposes the source signal to multiple semantic classes based on the
multi-user intent, i.e. each user is assumed to be interested in details of
only a subset of the semantic classes. The transmitter then sends to each user
only its intended classes, and multicasts a highly compressed semantic map to
all users over shared wireless resources that allows them to locally synthesize
the other classes, i.e. non-intended classes, utilizing pre-trained diffusion
models. The signal retrieved at each user is thereby partially reconstructed
and partially synthesized utilizing the received semantic map. This improves
utilization of the wireless resources, with better preserving privacy of the
non-intended classes. We design a communication/computation-aware scheme for
per-class adaptation of the communication parameters, such as the transmission
power and compression rate to minimize the total latency of retrieving signals
at multiple receivers, tailored to the prevailing channel conditions as well as
the users reconstruction/synthesis distortion/perception requirements. The
simulation results demonstrate significantly reduced per-user latency compared
with non-generative and intent-unaware multicasting benchmarks while
maintaining high perceptual quality of the signals retrieved at the users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PPLLaVA: Varied Video Sequence Understanding With <span class="highlight-title">Prompt</span> Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyang Liu, Haoran Tang, Haibo Liu, Yixiao Ge, Ying Shan, Chen Li, Jiankun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The past year has witnessed the significant advancement of video-based large
language models. However, the challenge of developing a unified model for both
short and long video understanding remains unresolved. Most existing video LLMs
cannot handle hour-long videos, while methods custom for long videos tend to be
ineffective for shorter videos and images. In this paper, we identify the key
issue as the redundant content in videos. To address this, we propose a novel
pooling strategy that simultaneously achieves token compression and
instruction-aware visual feature aggregation. Our model is termed Prompt-guided
Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three
core components: the CLIP-based visual-prompt alignment that extracts visual
information relevant to the user's instructions, the prompt-guided pooling that
compresses the visual sequence to arbitrary scales using convolution-style
pooling, and the clip context extension designed for lengthy prompt common in
visual dialogue. Moreover, our codebase also integrates the most advanced video
Direct Preference Optimization (DPO) and visual interleave training. Extensive
experiments have validated the performance of our model. With superior
throughput and only 1024 visual context, PPLLaVA achieves better results on
image benchmarks as a video LLM, while achieving state-of-the-art performance
across various video benchmarks, excelling in tasks ranging from caption
generation to multiple-choice questions, and handling video lengths from
seconds to hours. Codes have been available at
https://github.com/farewellthree/PPLLaVA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenXD: Generating Any 3D and 4D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Zhao, Chung-Ching Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in 2D visual generation have been remarkably successful.
However, 3D and 4D generation remain challenging in real-world applications due
to the lack of large-scale 4D data and effective model design. In this paper,
we propose to jointly investigate general 3D and 4D generation by leveraging
camera and object movements commonly observed in daily life. Due to the lack of
real-world 4D data in the community, we first propose a data curation pipeline
to obtain camera poses and object motion strength from videos. Based on this
pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.
By leveraging all the 3D and 4D data, we develop our framework, GenXD, which
allows us to produce any 3D or 4D scene. We propose multiview-temporal modules,
which disentangle camera and object movements, to seamlessly learn from both 3D
and 4D data. Additionally, GenXD employs masked latent conditions to support a
variety of conditioning views. GenXD can generate videos that follow the camera
trajectory as well as consistent 3D views that can be lifted into 3D
representations. We perform extensive evaluations across various real-world and
synthetic datasets, demonstrating GenXD's effectiveness and versatility
compared to previous methods in 3D and 4D generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grouped Discrete Representation for Object-Centric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-Centric Learning (OCL) can discover objects in images or videos by
simply reconstructing the input. For better object discovery, representative
OCL methods reconstruct the input as its Variational Autoencoder (VAE)
intermediate representation, which suppresses pixel noises and promotes object
separability by discretizing continuous super-pixels with template features.
However, treating features as units overlooks their composing attributes, thus
impeding model generalization; indexing features with scalar numbers loses
attribute-level similarities and differences, thus hindering model convergence.
We propose \textit{Grouped Discrete Representation} (GDR) for OCL. We decompose
features into combinatorial attributes via organized channel grouping, and
compose these attributes into discrete representation via tuple indexes.
Experiments show that our GDR improves both Transformer- and Diffusion-based
OCL methods consistently on various datasets. Visualizations show that our GDR
captures better object separability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu, Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, Chunchao Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While 3D generative models have greatly improved artists' workflows, the
existing diffusion models for 3D generation suffer from slow generation and
poor generalization. To address this issue, we propose a two-stage approach
named Hunyuan3D-1.0 including a lite version and a standard version, that both
support text- and image-conditioned generation. In the first stage, we employ a
multi-view diffusion model that efficiently generates multi-view RGB in
approximately 4 seconds. These multi-view images capture rich details of the 3D
asset from different viewpoints, relaxing the tasks from single-view to
multi-view reconstruction. In the second stage, we introduce a feed-forward
reconstruction model that rapidly and faithfully reconstructs the 3D asset
given the generated multi-view images in approximately 7 seconds. The
reconstruction network learns to handle noises and in-consistency introduced by
the multi-view diffusion and leverages the available information from the
condition image to efficiently recover the 3D structure. % Extensive
experimental results demonstrate the effectiveness of Hunyuan3D-1.0 in
generating high-quality 3D assets. Our framework involves the text-to-image
model ~\ie, Hunyuan-DiT, making it a unified framework to support both text-
and image-conditioned 3D generation. Our standard version has $10\times$ more
parameters than our lite and other existing model. Our Hunyuan3D-1.0 achieves
an impressive balance between speed and quality, significantly reducing
generation time while maintaining the quality and diversity of the produced
assets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal-in-the-Loop for Learning with Imbalanced Noisy Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Brandon Graham-Knight, Jamil Fayyad, Nourhan Bayasi, Patricia Lasserre, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance and label noise are pervasive in large-scale datasets, yet
much of machine learning research assumes well-labeled, balanced data, which
rarely reflects real world conditions. Existing approaches typically address
either label noise or class imbalance in isolation, leading to suboptimal
results when both issues coexist. In this work, we propose
Conformal-in-the-Loop (CitL), a novel training framework that addresses both
challenges with a conformal prediction-based approach. CitL evaluates sample
uncertainty to adjust weights and prune unreliable examples, enhancing model
resilience and accuracy with minimal computational cost. Our extensive
experiments include a detailed analysis showing how CitL effectively emphasizes
impactful data in noisy, imbalanced datasets. Our results show that CitL
consistently boosts model performance, achieving up to a 6.1% increase in
classification accuracy and a 5.0 mIoU improvement in segmentation. Our code is
publicly available: CitL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Speech Recognition: A Single Model for Auditory, Visual, and
  Audiovisual Inputs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandros Haliassos, Rodrigo Mira, Honglie Chen, Zoe Landgraf, Stavros Petridis, Maja Pantic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in auditory, visual, and audiovisual speech recognition (ASR, VSR,
and AVSR, respectively) has traditionally been conducted independently. Even
recent self-supervised studies addressing two or all three tasks simultaneously
tend to yield separate models, leading to disjoint inference pipelines with
increased memory requirements and redundancies. This paper proposes unified
training strategies for these systems. We demonstrate that training a single
model for all three tasks enhances VSR and AVSR performance, overcoming typical
optimisation challenges when training from scratch. Moreover, we introduce a
greedy pseudo-labelling approach to more effectively leverage unlabelled
samples, addressing shortcomings in related self-supervised methods. Finally,
we develop a self-supervised pre-training method within our framework, proving
its effectiveness alongside our semi-supervised approach. Despite using a
single model for all tasks, our unified approach achieves state-of-the-art
performance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR,
as well as on the newly released WildVSR dataset. Code and models are available
at https://github.com/ahaliassos/usr.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code: https://github.com/ahaliassos/usr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Audio-Visual Segmentation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Sokolov, Swapnil Bhosale, Xiatian Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing the sounding objects in scenes is a longstanding objective in
embodied AI, with diverse applications in robotics and AR/VR/MR. To that end,
Audio-Visual Segmentation (AVS), taking as condition an audio signal to
identify the masks of the target sounding objects in an input image with
synchronous camera and microphone sensors, has been recently advanced. However,
this paradigm is still insufficient for real-world operation, as the mapping
from 2D images to 3D scenes is missing. To address this fundamental limitation,
we introduce a novel research problem, 3D Audio-Visual Segmentation, extending
the existing AVS to the 3D output space. This problem poses more challenges due
to variations in camera extrinsics, audio scattering, occlusions, and diverse
acoustics across sounding object categories. To facilitate this research, we
create the very first simulation based benchmark, 3DAVS-S34-O7, providing
photorealistic 3D scene environments with grounded spatial audio under
single-instance and multi-instance settings, across 34 scenes and 7 object
categories. This is made possible by re-purposing the Habitat simulator to
generate comprehensive annotations of sounding object locations and
corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet,
characterized by integrating the ready-to-use knowledge from pretrained 2D
audio-visual foundation models synergistically with 3D visual scene
representation through spatial audio-aware mask alignment and refinement.
Extensive experiments demonstrate that EchoSegnet can effectively segment
sounding objects in 3D space on our new benchmark, representing a significant
advancement in the field of embodied AI. Project page:
https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the NeurIPS 2024 Workshop on Audio Imagination</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage
  Training <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIRA: Scalable Inter-frame Relation and Association for Radar Perception <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Yataka, Pu Perry Wang, Petros Boufounos, Ryuhei Takahashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional radar feature extraction faces limitations due to low spatial
resolution, noise, multipath reflection, the presence of ghost targets, and
motion blur. Such limitations can be exacerbated by nonlinear object motion,
particularly from an ego-centric viewpoint. It becomes evident that to address
these challenges, the key lies in exploiting temporal feature relation over an
extended horizon and enforcing spatial motion consistency for effective
association. To this end, this paper proposes SIRA (Scalable Inter-frame
Relation and Association) with two designs. First, inspired by Swin
Transformer, we introduce extended temporal relation, generalizing the existing
temporal relation layer from two consecutive frames to multiple inter-frames
with temporally regrouped window attention for scalability. Second, we propose
motion consistency track with the concept of a pseudo-tracklet generated from
observational data for better trajectory prediction and subsequent object
association. Our approach achieves 58.11 mAP@0.5 for oriented object detection
and 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing
previous state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One VLM to Keep it Learning: Generation and Balancing for Data-free
  Continual Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepayan Das, Davide Talon, Massimiliano Mancini, Yiming Wang, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have shown significant promise in Visual
Question Answering (VQA) tasks by leveraging web-scale multimodal datasets.
However, these models often struggle with continual learning due to
catastrophic forgetting when adapting to new tasks. As an effective remedy to
mitigate catastrophic forgetting, rehearsal strategy uses the data of past
tasks upon learning new task. However, such strategy incurs the need of storing
past data, which might not be feasible due to hardware constraints or privacy
concerns. In this work, we propose the first data-free method that leverages
the language generation capability of a VLM, instead of relying on external
models, to produce pseudo-rehearsal data for addressing continual VQA. Our
proposal, named as GaB, generates pseudo-rehearsal data by posing previous task
questions on new task data. Yet, despite being effective, the distribution of
generated questions skews towards the most frequently posed questions due to
the limited and task-specific training data. To mitigate this issue, we
introduce a pseudo-rehearsal balancing module that aligns the generated data
towards the ground-truth data distribution using either the question
meta-statistics or an unsupervised clustering method. We evaluate our proposed
method on two recent benchmarks, \ie VQACL-VQAv2 and CLOVE-function benchmarks.
GaB outperforms all the data-free baselines with substantial improvement in
maintaining VQA performance across evolving tasks, while being on-par with
methods with access to the past data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition
  via Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjith George, Sebastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accuracy of face recognition systems has improved significantly in the
past few years, thanks to the large amount of data collected and the
advancement in neural network architectures. However, these large-scale
datasets are often collected without explicit consent, raising ethical and
privacy concerns. To address this, there have been proposals to use synthetic
datasets for training face recognition models. Yet, such models still rely on
real data to train the generative models and generally exhibit inferior
performance compared to those trained on real datasets. One of these datasets,
DigiFace, uses a graphics pipeline to generate different identities and
different intra-class variations without using real data in training the
models. However, the performance of this approach is poor on face recognition
benchmarks, possibly due to the lack of realism in the images generated from
the graphics pipeline. In this work, we introduce a novel framework for realism
transfer aimed at enhancing the realism of synthetically generated face images.
Our method leverages the large-scale face foundation model, and we adapt the
pipeline for realism enhancement. By integrating the controllable aspects of
the graphics pipeline with our realism enhancement technique, we generate a
large amount of realistic variations-combining the advantages of both
approaches. Our empirical evaluations demonstrate that models trained using our
enhanced dataset significantly improve the performance of face recognition
systems over the baseline. The source code and datasets will be made available
publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double Descent Meets Out-of-Distribution Detection: Theoretical Insights
  and Empirical Analysis on the role of model complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouïn Ben Ammar, David Brellmann, Arturo Mendoza, Antoine Manzanera, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While overparameterization is known to benefit generalization, its impact on
Out-Of-Distribution (OOD) detection is less understood. This paper investigates
the influence of model complexity in OOD detection. We propose an expected OOD
risk metric to evaluate classifiers confidence on both training and OOD
samples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD
risk of binary least-squares classifiers applied to Gaussian data. We show that
the OOD risk depicts an infinite peak, when the number of parameters is equal
to the number of samples, which we associate with the double descent
phenomenon. Our experimental study on different OOD detection methods across
multiple neural architectures extends our theoretical insights and highlights a
double descent curve. Our observations suggest that overparameterization does
not necessarily lead to better OOD detection. Using the Neural Collapse
framework, we provide insights to better understand this behavior. To
facilitate reproducibility, our code will be made publicly available upon
publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detect an Object At Once without Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Hao, Jianheng Liu, Yongjia Zhao, Zuofan Chen, Qi Sun, Jinlong Chen, Jianguo Wei, Minghao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When presented with one or a few photos of a previously unseen object, humans
can instantly recognize it in different scenes. Although the human brain
mechanism behind this phenomenon is still not fully understood, this work
introduces a novel technical realization of this task. It consists of two
phases: (1) generating a Similarity Density Map (SDM) by convolving the scene
image with the given object image patch(es) so that the highlight areas in the
SDM indicate the possible locations; (2) obtaining the object occupied areas in
the scene through a Region Alignment Network (RAN). The RAN is constructed on a
backbone of Deep Siamese Network (DSN), and different from the traditional
DSNs, it aims to obtain the object accurate regions by regressing the location
and area differences between the ground truths and the predicted ones indicated
by the highlight areas in SDM. By pre-learning from labels annotated in
traditional datasets, the SDM-RAN can detect previously unknown objects without
fine-tuning. Experiments were conducted on the MS COCO, PASCAL VOC datasets.
The results indicate that the proposed method outperforms state-of-the-art
methods on the same task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile
  Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqin Zhao, Mallesham Dasari, Tian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality environment lighting is the foundation of creating immersive
user experiences in mobile augmented reality (AR) applications. However,
achieving visually coherent environment lighting estimation for Mobile AR is
challenging due to several key limitations associated with AR device sensing
capabilities, including limitations in device camera FoV and pixel dynamic
ranges. Recent advancements in generative AI, which can generate high-quality
images from different types of prompts, including texts and images, present a
potential solution for high-quality lighting estimation. Still, to effectively
use generative image diffusion models, we must address their key limitations of
generation hallucination and slow inference process. To do so, in this work, we
design and implement a generative lighting estimation system called CleAR that
can produce high-quality and diverse environment maps in the format of
360$^\circ$ images. Specifically, we design a two-step generation pipeline
guided by AR environment context data to ensure the results follow physical
environment visual context and color appearances. To improve the estimation
robustness under different lighting conditions, we design a real-time
refinement component to adjust lighting estimation results on AR devices. To
train and test our generative models, we curate a large-scale environment
lighting estimation dataset with diverse lighting conditions. Through
quantitative evaluation and user study, we show that CleAR outperforms
state-of-the-art lighting estimation methods on both estimation accuracy and
robustness. Moreover, CleAR supports real-time refinement of lighting
estimation results, ensuring robust and timely environment lighting updates for
AR applications. Our end-to-end generative estimation takes as fast as 3.2
seconds, outperforming state-of-the-art methods by 110x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning
  with <span class="highlight-title">Pre-Train</span>ed Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linglan Zhao, Xuerui Zhang, Ke Yan, Shouhong Ding, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning aims to incrementally acquire new concepts in data streams
while resisting forgetting previous knowledge. With the rise of powerful
pre-trained models (PTMs), there is a growing interest in training incremental
learning systems using these foundation models, rather than learning from
scratch. Existing works often view PTMs as a strong initial point and directly
apply parameter-efficient tuning (PET) in the first session for adapting to
downstream tasks. In the following sessions, most methods freeze model
parameters for tackling forgetting issues. However, applying PET directly to
downstream data cannot fully explore the inherent knowledge in PTMs.
Additionally, freezing the parameters in incremental sessions hinders models'
plasticity to novel concepts not covered in the first session. To solve the
above issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE)
framework. In particular, to inherit general knowledge from foundation models,
we include a transfer loss function by measuring the correlation between the
PTM and the PET-applied model. After calibrating in the first session, the slow
efficient tuning parameters can capture more informative features, improving
generalization to incoming classes. Moreover, to further incorporate novel
concepts, we strike a balance between stability and plasticity by fixing slow
efficient tuning parameters and continuously updating the fast ones.
Specifically, a cross-classification loss with feature alignment is proposed to
circumvent catastrophic forgetting. During inference, we introduce an
entropy-based aggregation strategy to dynamically utilize the complementarity
in the slow and fast learners. Extensive experiments on seven benchmark
datasets verify the effectiveness of our method by significantly surpassing the
state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Domain Generalization in <span class="highlight-title">Self-supervised</span> Monocular Depth
  Estimation via Stabilized Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanqi Yao, Gang Wu, Kui Jiang, Siao Liu, Jian Kuai, Xianming Liu, Junjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a self-supervised Monocular Depth Estimation (MDE) model with great
generalization remains significantly challenging. Despite the success of
adversarial augmentation in the supervised learning generalization, naively
incorporating it into self-supervised MDE models potentially causes
over-regularization, suffering from severe performance degradation. In this
paper, we conduct qualitative analysis and illuminate the main causes: (i)
inherent sensitivity in the UNet-alike depth network and (ii) dual optimization
conflict caused by over-regularization. To tackle these issues, we propose a
general adversarial training framework, named Stabilized Conflict-optimization
Adversarial Training (SCAT), integrating adversarial data augmentation into
self-supervised MDE methods to achieve a balance between stability and
generalization. Specifically, we devise an effective scaling depth network that
tunes the coefficients of long skip connection and effectively stabilizes the
training process. Then, we propose a conflict gradient surgery strategy, which
progressively integrates the adversarial gradient and optimizes the model
toward a conflict-free direction. Extensive experiments on five benchmarks
demonstrate that SCAT can achieve state-of-the-art performance and
significantly improve the generalization capability of existing self-supervised
MDE methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced computer vision for extracting georeferenced vehicle
  trajectories from drone imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Fonod, Haechan Cho, Hwasoo Yeo, Nikolas Geroliminis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework for extracting georeferenced vehicle
trajectories from high-altitude drone footage, addressing key challenges in
urban traffic monitoring and limitations of traditional ground-based systems.
We employ state-of-the-art computer vision and deep learning to create an
end-to-end pipeline that enhances vehicle detection, tracking, and trajectory
stabilization. Conducted in the Songdo International Business District, South
Korea, the study used a multi-drone experiment over 20 intersections, capturing
approximately 12TB of 4K video data over four days. We developed a novel track
stabilization method that uses detected vehicle bounding boxes as exclusion
masks during image registration, which, combined with advanced georeferencing
techniques, accurately transforms vehicle coordinates into real-world
geographical data. Additionally, our framework includes robust vehicle
dimension estimation and detailed road segmentation for in-depth traffic
analysis. The framework produced two high-quality datasets: the Songdo Traffic
dataset, comprising nearly 1 million unique vehicle trajectories, and the
Songdo Vision dataset, containing over 5,000 human-annotated frames with about
300,000 vehicle instances in four classes. Comparisons between drone-derived
data and high-precision sensor data from an instrumented probe vehicle
highlight the accuracy and consistency of our framework's extraction in dense
urban settings. By publicly releasing these datasets and the pipeline source
code, this work sets new benchmarks for data quality, reproducibility, and
scalability in traffic research. Results demonstrate the potential of
integrating drone technology with advanced computer vision for precise,
cost-effective urban traffic monitoring, providing valuable resources for the
research community to develop intelligent transportation systems and improve
traffic management strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancements and limitations of LLMs in replicating human color-word
  associations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color-word associations play a fundamental role in human cognition and design
applications. Large Language Models (LLMs) have become widely available and
demonstrated intelligent behaviors in various benchmarks with natural
conversation skills. However, their ability to replicate human color-word
associations remains understudied. We compared multiple generations of LLMs
(from GPT-3 to GPT- 4o) against human color-word associations using data
collected from over 10,000 Japanese participants, involving 17 colors and words
from eight categories in Japanese. Our findings reveal a clear progression in
LLM performance across generations, with GPT-4o achieving the highest accuracy
in predicting the best voted word for each color and category, particularly
when using visual inputs rather than text-based color codes. However, the
highest median performance was approximately 50% even for GPT4-o with visual
inputs (chance level is 10%), and the performance levels varied significantly
across word categories and colors, indicating a failure to fully replicate
human color-word associations. On the other hand, color discrimination ability
estimated from our color-word association data showed that LLMs demonstrated
high correlation with human color discrimination patterns, similarly to
previous studies. Our study highlights both the advancements in LLM
capabilities and their persistent limitations, suggesting differences in
semantic memory structures between humans and LLMs in representing color-word
associations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal biometric authentication: Leveraging shared layer
  architectures for enhanced security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vatchala S, Yogesh C, Yeshwanth Govindarajan, Krithik Raja M, Vishal Pramav Amirtha Ganesan, Aashish Vinod A, Dharun Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a novel multi-modal biometric authentication
system that integrates facial, vocal, and signature data to enhance security
measures. Utilizing a combination of Convolutional Neural Networks (CNNs) and
Recurrent Neural Networks (RNNs), our model architecture uniquely incorporates
dual shared layers alongside modality-specific enhancements for comprehensive
feature extraction. The system undergoes rigorous training with a joint loss
function, optimizing for accuracy across diverse biometric inputs.
Feature-level fusion via Principal Component Analysis (PCA) and classification
through Gradient Boosting Machines (GBM) further refine the authentication
process. Our approach demonstrates significant improvements in authentication
accuracy and robustness, paving the way for advanced secure identity
verification solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning on 3D Semantic Segmentation: A Detailed <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thodoris Betsas, Andreas Georgopoulos, Anastasios Doulamis, Pierre Grussenmeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper an exhaustive review and comprehensive analysis of recent and
former deep learning methods in 3D Semantic Segmentation (3DSS) is presented.
In the related literature, the taxonomy scheme used for the classification of
the 3DSS deep learning methods is ambiguous. Based on the taxonomy schemes of 9
existing review papers, a new taxonomy scheme of the 3DSS deep learning methods
is proposed, aiming to standardize it and improve the comparability and clarity
across related studies. Furthermore, an extensive overview of the available
3DSS indoor and outdoor datasets is provided along with their links. The core
part of the review is the detailed presentation of recent and former 3DSS deep
learning methods and their classification using the proposed taxonomy scheme
along with their GitHub repositories. Additionally, a brief but informative
analysis of the evaluation metrics and loss functions used in 3DSS is included.
Finally, a fruitful discussion of the examined 3DSS methods and datasets, is
presented to foster new research directions and applications in the field of
3DSS. Supplementary, to this review a GitHub repository is provided
(https://github.com/thobet/Deep-Learning-on-3D-Semantic-Segmentation-a-
Detailed-Review) including a quick classification of over 400 3DSS methods,
using the proposed taxonomy scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Integrated Decision Gradients (IDG-DP) for
  Radar-based Human Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Zakariyya, Linda Tran, Kaushik Bhargav Sivangi, Paul Henderson, Fani Deligianni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion analysis offers significant potential for healthcare monitoring
and early detection of diseases. The advent of radar-based sensing systems has
captured the spotlight for they are able to operate without physical contact
and they can integrate with pre-existing Wi-Fi networks. They are also seen as
less privacy-invasive compared to camera-based systems. However, recent
research has shown high accuracy in recognizing subjects or gender from radar
gait patterns, raising privacy concerns. This study addresses these issues by
investigating privacy vulnerabilities in radar-based Human Activity Recognition
(HAR) systems and proposing a novel method for privacy preservation using
Differential Privacy (DP) driven by attributions derived with Integrated
Decision Gradient (IDG) algorithm. We investigate Black-box Membership
Inference Attack (MIA) Models in HAR settings across various levels of
attacker-accessible information. We extensively evaluated the effectiveness of
the proposed IDG-DP method by designing a CNN-based HAR model and rigorously
assessing its resilience against MIAs. Experimental results demonstrate the
potential of IDG-DP in mitigating privacy attacks while maintaining utility
across all settings, particularly excelling against label-only and shadow model
black-box MIA attacks. This work represents a crucial step towards balancing
the need for effective radar-based HAR with robust privacy protection in
healthcare environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The evolution of volumetric video: A <span class="highlight-title">survey</span> of smart transcoding and
  compression approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preetish Kakkar, Hariharan Ragothaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video, the capture and display of three-dimensional (3D) imagery,
has emerged as a revolutionary technology poised to transform the media
landscape, enabling immersive experiences that transcend the limitations of
traditional 2D video. One of the key challenges in this domain is the efficient
delivery of these high-bandwidth, data-intensive volumetric video streams,
which requires innovative transcoding and compression techniques. This research
paper explores the state-of-the-art in volumetric video compression and
delivery, with a focus on the potential of AI-driven solutions to address the
unique challenges posed by this emerging medium.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphVL: Graph-Enhanced Semantic Modeling via Vision-Language Models for
  Generalized Class Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhupendra Solanki, Ashwin Nair, Mainak Singha, Souradeep Mukhopadhyay, Ankit Jha, Biplab Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Category Discovery (GCD) aims to cluster unlabeled images into
known and novel categories using labeled images from known classes. To address
the challenge of transferring features from known to unknown classes while
mitigating model bias, we introduce GraphVL, a novel approach for
vision-language modeling in GCD, leveraging CLIP. Our method integrates a graph
convolutional network (GCN) with CLIP's text encoder to preserve class
neighborhood structure. We also employ a lightweight visual projector for image
data, ensuring discriminative features through margin-based contrastive losses
for image-text mapping. This neighborhood preservation criterion effectively
regulates the semantic space, making it less sensitive to known classes.
Additionally, we learn textual prompts from known classes and align them to
create a more contextually meaningful semantic feature space for the GCN layer
using a contextual similarity loss. Finally, we represent unlabeled samples
based on their semantic distance to class prompts from the GCN, enabling
semi-supervised clustering for class discovery and minimizing errors. Our
experiments on seven benchmark datasets consistently demonstrate the
superiority of GraphVL when integrated with the CLIP backbone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACM ICVGIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Integrity when Unlearning with T2I Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Schioppa, Emiel Hoogeboom, Jonathan Heek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of text-to-image Diffusion Models has led to their
widespread public accessibility. However these models, trained on large
internet datasets, can sometimes generate undesirable outputs. To mitigate
this, approximate Machine Unlearning algorithms have been proposed to modify
model weights to reduce the generation of specific types of images,
characterized by samples from a ``forget distribution'', while preserving the
model's ability to generate other images, characterized by samples from a
``retain distribution''. While these methods aim to minimize the influence of
training data in the forget distribution without extensive additional
computation, we point out that they can compromise the model's integrity by
inadvertently affecting generation for images in the retain distribution.
Recognizing the limitations of FID and CLIPScore in capturing these effects, we
introduce a novel retention metric that directly assesses the perceptual
difference between outputs generated by the original and the unlearned models.
We then propose unlearning algorithms that demonstrate superior effectiveness
in preserving model integrity compared to existing baselines. Given their
straightforward implementation, these algorithms serve as valuable benchmarks
for future advancements in approximate Machine Unlearning for Diffusion Models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AM Flow: Adapters for Temporal Processing in Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanay Agrawal, Abid Ali, Antitza Dantcheva, Francois Bremond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models, in particular \textit{image} models, have recently
gained generalisability and robustness. %are becoming more general and robust
by the day. In this work, we propose to exploit such advances in the realm of
\textit{video} classification. Video foundation models suffer from the
requirement of extensive pretraining and a large training time. Towards
mitigating such limitations, we propose "\textit{Attention Map (AM) Flow}" for
image models, a method for identifying pixels relevant to motion in each input
video frame. In this context, we propose two methods to compute AM flow,
depending on camera motion. AM flow allows the separation of spatial and
temporal processing, while providing improved results over combined
spatio-temporal processing (as in video models). Adapters, one of the popular
techniques in parameter efficient transfer learning, facilitate the
incorporation of AM flow into pretrained image models, mitigating the need for
full-finetuning. We extend adapters to "\textit{temporal processing adapters}"
by incorporating a temporal processing unit into the adapters. Our work
achieves faster convergence, therefore reducing the number of epochs needed for
training. Moreover, we endow an image model with the ability to achieve
state-of-the-art results on popular action recognition datasets. This reduces
training time and simplifies pretraining. We present experiments on
Kinetics-400, Something-Something v2, and Toyota Smarthome datasets, showcasing
state-of-the-art or comparable results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Unlabeled Data with Multiple Expert Teachers for Open
  Vocabulary Aerial Object Detection and Its Orientation Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Weiwei Guo, Xue Yang, Ning Liao, Shaofeng Zhang, Yi Yu, Wenxian Yu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, aerial object detection has been increasingly pivotal in
various earth observation applications. However, current algorithms are limited
to detecting a set of pre-defined object categories, demanding sufficient
annotated training samples, and fail to detect novel object categories. In this
paper, we put forth a novel formulation of the aerial object detection problem,
namely open-vocabulary aerial object detection (OVAD), which can detect objects
beyond training categories without costly collecting new labeled data. We
propose CastDet, a CLIP-activated student-teacher detection framework that
serves as the first OVAD detector specifically designed for the challenging
aerial scenario, where objects often exhibit weak appearance features and
arbitrary orientations. Our framework integrates a robust localization teacher
along with several box selection strategies to generate high-quality proposals
for novel objects. Additionally, the RemoteCLIP model is adopted as an
omniscient teacher, which provides rich knowledge to enhance classification
capabilities for novel categories. A dynamic label queue is devised to maintain
high-quality pseudo-labels during training. By doing so, the proposed CastDet
boosts not only novel object proposals but also classification. Furthermore, we
extend our approach from horizontal OVAD to oriented OVAD with tailored
algorithm designs to effectively manage bounding box representation and
pseudo-label generation. Extensive experiments for both tasks on multiple
existing aerial object detection datasets demonstrate the effectiveness of our
approach. The code is available at https://github.com/lizzy8587/CastDet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Representation Collapse in Vector Quantized Models with One
  Linear Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Zhu, Bocheng Li, Yifei Xin, Linli Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector Quantization (VQ) is a widely used method for converting continuous
representations into discrete codes, which has become fundamental in
unsupervised representation learning and latent generative models. However, VQ
models are often hindered by the problem of representation collapse in the
latent space, which leads to low codebook utilization and limits the
scalability of the codebook for large-scale training. Existing methods designed
to mitigate representation collapse typically reduce the dimensionality of
latent space at the expense of model capacity, which do not fully resolve the
core issue. In this study, we conduct a theoretical analysis of representation
collapse in VQ models and identify its primary cause as the disjoint
optimization of the codebook, where only a small subset of code vectors are
updated through gradient descent. To address this issue, we propose
\textbf{SimVQ}, a novel method which reparameterizes the code vectors through a
linear transformation layer based on a learnable latent basis. This
transformation optimizes the \textit{entire linear space} spanned by the
codebook, rather than merely updating \textit{the code vector} selected by the
nearest-neighbor search in vanilla VQ models. Although it is commonly
understood that the multiplication of two linear matrices is equivalent to
applying a single linear layer, our approach works surprisingly well in
resolving the collapse issue in VQ models with just one linear layer. We
validate the efficacy of SimVQ through extensive experiments across various
modalities, including image and audio data with different model architectures.
Our code is available at \url{https://github.com/youngsheen/SimVQ}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree level change detection over Ahmedabad city using very high
  resolution satellite images and Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai G Singla, Gautam Jaiswal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, 0.5m high resolution satellite datasets over Indian urban
region was used to demonstrate the applicability of deep learning models over
Ahmedabad, India. Here, YOLOv7 instance segmentation model was trained on well
curated trees canopy dataset (6500 images) in order to carry out the change
detection. During training, evaluation metrics such as bounding box regression
and mask regression loss, mean average precision (mAP) and stochastic gradient
descent algorithm were used for evaluating and optimizing the performance of
model. After the 500 epochs, the mAP of 0.715 and 0.699 for individual tree
detection and tree canopy mask segmentation were obtained. However, by further
tuning hyper parameters of the model, maximum accuracy of 80 % of trees
detection with false segmentation rate of 2% on data was obtained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QCS:Feature Refining from Quadruplet Cross Similarity for Facial
  Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengpeng Wang, Li Chen, Lili Wang, Zhaofan Li, Xuebin Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On facial expression datasets with complex and numerous feature types, where
the significance and dominance of labeled features are difficult to predict,
facial expression recognition(FER) encounters the challenges of inter-class
similarity and intra-class variances, making it difficult to mine effective
features. We aim to solely leverage the feature similarity among facial samples
to address this. We introduce the Cross Similarity Attention (CSA), an
input-output position-sensitive attention mechanism that harnesses feature
similarity across different images to compute the corresponding global spatial
attention. Based on this, we propose a four-branch circular framework, called
Quadruplet Cross Similarity (QCS), to extract discriminative features from the
same class and eliminate redundant ones from different classes synchronously to
refine cleaner features. The symmetry of the network ensures balanced and
stable training and reduces the amount of CSA interaction matrix. Contrastive
residual distillation is utilized to transfer the information learned in the
cross module back to the base network. The cross-attention module exists during
training, and only one base branch is retained during inference. our proposed
QCS model outperforms state-of-the-art methods on several popular FER datasets,
without requiring additional landmark information or other extra training data.
The code is available at https://github.com/birdwcp/QCS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Typicalness-Aware Learning for Failure Detection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Liu, Jiequan Cui, Zhuotao Tian, Senqiao Yang, Qingdong He, Xiaoling Wang, Jingyong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) often suffer from the overconfidence issue, where
incorrect predictions are made with high confidence scores, hindering the
applications in critical systems. In this paper, we propose a novel approach
called Typicalness-Aware Learning (TAL) to address this issue and improve
failure detection performance. We observe that, with the cross-entropy loss,
model predictions are optimized to align with the corresponding labels via
increasing logit magnitude or refining logit direction. However, regarding
atypical samples, the image content and their labels may exhibit disparities.
This discrepancy can lead to overfitting on atypical samples, ultimately
resulting in the overconfidence issue that we aim to address. To tackle the
problem, we have devised a metric that quantifies the typicalness of each
sample, enabling the dynamic adjustment of the logit magnitude during the
training process. By allowing atypical samples to be adequately fitted while
preserving reliable logit direction, the problem of overconfidence can be
mitigated. TAL has been extensively evaluated on benchmark datasets, and the
results demonstrate its superiority over existing failure detection methods.
Specifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of
the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art.
Code is available at https://github.com/liuyijungoon/TAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPECTRUM: Semantic Processing and Emotion-informed video-Captioning
  Through Retrieval and Understanding Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Faghihi, Mohammedreza Zarenejad, Ali-Asghar Beheshti Shirazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing a video's meaning and critical concepts by analyzing the subtle
details is a fundamental yet challenging task in video captioning. Identifying
the dominant emotional tone in a video significantly enhances the perception of
its context. Despite a strong emphasis on video captioning, existing models
often need to adequately address emotional themes, resulting in suboptimal
captioning results. To address these limitations, this paper proposes a novel
Semantic Processing and Emotion-informed video-Captioning Through Retrieval and
Understanding Modalities (SPECTRUM) framework to empower the generation of
emotionally and semantically credible captions. Leveraging our pioneering
structure, SPECTRUM discerns multimodal semantics and emotional themes using
Visual Text Attribute Investigation (VTAI) and determines the orientation of
descriptive captions through a Holistic Concept-Oriented Theme (HCOT),
expressing emotionally-informed and field-acquainted references. They exploit
video-to-text retrieval capabilities and the multifaceted nature of video
content to estimate the emotional probabilities of candidate captions. Then,
the dominant theme of the video is determined by appropriately weighting
embedded attribute vectors and applying coarse- and fine-grained emotional
concepts, which define the video's contextual alignment. Furthermore, using two
loss functions, SPECTRUM is optimized to integrate emotional information and
minimize prediction errors. Extensive experiments on the EmVidCap, MSVD, and
MSRVTT video captioning datasets demonstrate that our model significantly
surpasses state-of-the-art methods. Quantitative and qualitative evaluations
highlight the model's ability to accurately capture and convey video emotions
and multimodal attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Gaze Behavior Boosts <span class="highlight-title">Self-Supervised</span> Object Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyang Yu, Arthur Aubret, Marcel C. Raabe, Jane Yang, Chen Yu, Jochen Triesch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to significant variations in the projection of the same object from
different viewpoints, machine learning algorithms struggle to recognize the
same object across various perspectives. In contrast, toddlers quickly learn to
recognize objects from different viewpoints with almost no supervision. Recent
works argue that toddlers develop this ability by mapping close-in-time visual
inputs to similar representations while interacting with objects. High acuity
vision is only available in the central visual field, which may explain why
toddlers (much like adults) constantly move their gaze around during such
interactions. It is unclear whether/how much toddlers curate their visual
experience through these eye movements to support learning object
representations. In this work, we explore whether a bio inspired visual
learning model can harness toddlers' gaze behavior during a play session to
develop view-invariant object recognition. Exploiting head-mounted eye tracking
during dyadic play, we simulate toddlers' central visual field experience by
cropping image regions centered on the gaze location. This visual stream feeds
a time-based self-supervised learning algorithm. Our experiments demonstrate
that toddlers' gaze strategy supports the learning of invariant object
representations. Our analysis also reveals that the limited size of the central
visual field where acuity is high is crucial for this. We further find that
toddlers' visual experience elicits more robust representations compared to
adults' mostly because toddlers look at objects they hold themselves for longer
bouts. Overall, our work reveals how toddlers' gaze behavior supports
self-supervised learning of view-invariant object recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnSegMedGAT: Unsupervised Medical Image Segmentation using Graph
  Attention Networks Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Mudit Adityaja, Saurabh J. Shigwan, Nitin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The data-intensive nature of supervised classification drives the interest of
the researchers towards unsupervised approaches, especially for problems such
as medical image segmentation, where labeled data is scarce. Building on the
recent advancements of Vision transformers (ViT) in computer vision, we propose
an unsupervised segmentation framework using a pre-trained Dino-ViT. In the
proposed method, we leverage the inherent graph structure within the image to
realize a significant performance gain for segmentation in medical images. For
this, we introduce a modularity-based loss function coupled with a Graph
Attention Network (GAT) to effectively capture the inherent graph topology
within the image. Our method achieves state-of-the-art performance, even
significantly surpassing or matching that of existing (semi)supervised
technique such as MedSAM which is a Segment Anything Model in medical images.
We demonstrate this using two challenging medical image datasets ISIC-2018 and
CVC-ColonDB. This work underscores the potential of unsupervised approaches in
advancing medical image analysis in scenarios where labeled data is scarce. The
github repository of the code is available on
[https://github.com/mudit-adityaja/UnSegMedGAT].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Leopard Individual Identification: An Adaptive Angular
  Margin Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Colomer Matachana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate identification of individual leopards across camera trap images is
critical for population monitoring and ecological studies. This paper
introduces a deep learning framework to distinguish between individual leopards
based on their unique spot patterns. This approach employs a novel adaptive
angular margin method in the form of a modified CosFace architecture. In
addition, I propose a preprocessing pipeline that combines RGB channels with an
edge detection channel to underscore the critical features learned by the
model.
  This approach significantly outperforms the Triplet Network baseline,
achieving a Dynamic Top-5 Average Precision of 0.8814 and a Top-5 Rank Match
Detection of 0.9533, demonstrating its potential for open-set learning in
wildlife identification. While not surpassing the performance of the SIFT-based
Hotspotter algorithm, this method represents a substantial advancement in
applying deep learning to patterned wildlife identification.
  This research contributes to the field of computer vision and provides a
valuable tool for biologists aiming to study and protect leopard populations.
It also serves as a stepping stone for applying the power of deep learning in
Capture-Recapture studies for other patterned species.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust plug-and-play methods for highly accelerated non-Cartesian MRI
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre-Antoine Comby, Benjamin Lapostolle, Matthieu Terris, Philippe Ciuciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-quality Magnetic Resonance Imaging (MRI) reconstruction at
accelerated acquisition rates remains challenging due to the inherent ill-posed
nature of the inverse problem. Traditional Compressed Sensing (CS) methods,
while robust across varying acquisition settings, struggle to maintain good
reconstruction quality at high acceleration factors ($\ge$ 8). Recent advances
in deep learning have improved reconstruction quality, but purely data-driven
methods are prone to overfitting and hallucination effects, notably when the
acquisition setting is varying. Plug-and-Play (PnP) approaches have been
proposed to mitigate the pitfalls of both frameworks. In a nutshell, PnP
algorithms amount to replacing suboptimal handcrafted CS priors with powerful
denoising deep neural network (DNNs). However, in MRI reconstruction, existing
PnP methods often yield suboptimal results due to instabilities in the proximal
gradient descent (PGD) schemes and the lack of curated, noiseless datasets for
training robust denoisers. In this work, we propose a fully unsupervised
preprocessing pipeline to generate clean, noiseless complex MRI signals from
multicoil data, enabling training of a high-performance denoising DNN.
Furthermore, we introduce an annealed Half-Quadratic Splitting (HQS) algorithm
to address the instability issues, leading to significant improvements over
existing PnP algorithms. When combined with preconditioning techniques, our
approach achieves state-of-the-art results, providing a robust and efficient
solution for high-quality MRI reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Where to Edit Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqiao Yang, Long-Kai Huang, Shengzhuang Chen, Kede Ma, Ying Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model editing aims to data-efficiently correct predictive errors of large
pre-trained models while ensuring generalization to neighboring failures and
locality to minimize unintended effects on unrelated examples. While
significant progress has been made in editing Transformer-based large language
models, effective strategies for editing vision Transformers (ViTs) in computer
vision remain largely untapped. In this paper, we take initial steps towards
correcting predictive errors of ViTs, particularly those arising from
subpopulation shifts. Taking a locate-then-edit approach, we first address the
where-to-edit challenge by meta-learning a hypernetwork on CutMix-augmented
data generated for editing reliability. This trained hypernetwork produces
generalizable binary masks that identify a sparse subset of structured model
parameters, responsive to real-world failure samples. Afterward, we solve the
how-to-edit problem by simply fine-tuning the identified parameters using a
variant of gradient descent to achieve successful edits. To validate our
method, we construct an editing benchmark that introduces subpopulation shifts
towards natural underrepresented images and AI-generated images, thereby
revealing the limitations of pre-trained ViTs for object recognition. Our
approach not only achieves superior performance on the proposed benchmark but
also allows for adjustable trade-offs between generalization and locality. Our
code is available at https://github.com/hustyyq/Where-to-Edit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Contextual Uncertainty of Visual Data for Efficient Training
  of Deep Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharat Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objects, in the real world, rarely occur in isolation and exhibit typical
arrangements governed by their independent utility, and their expected
interaction with humans and other objects in the context. For example, a chair
is expected near a table, and a computer is expected on top. Humans use this
spatial context and relative placement as an important cue for visual
recognition in case of ambiguities. Similar to human's, DNN's exploit
contextual information from data to learn representations. Our research focuses
on harnessing the contextual aspects of visual data to optimize data annotation
and enhance the training of deep networks. Our contributions can be summarized
as follows: (1) We introduce the notion of contextual diversity for active
learning CDAL and show its applicability in three different visual tasks
semantic segmentation, object detection and image classification, (2) We
propose a data repair algorithm to curate contextually fair data to reduce
model bias, enabling the model to detect objects out of their obvious context,
(3) We propose Class-based annotation, where contextually relevant classes are
selected that are complementary for model training under domain shift.
Understanding the importance of well-curated data, we also emphasize the
necessity of involving humans in the loop to achieve accurate annotations and
to develop novel interaction strategies that allow humans to serve as
fact-checkers. In line with this we are working on developing image retrieval
system for wildlife camera trap images and reliable warning system for poor
quality rural roads. For large-scale annotation, we are employing a strategic
combination of human expertise and zero-shot models, while also integrating
human input at various stages for continuous feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICVGIP, Young Researchers Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Bin, Jianming Yao, Tin Lun Lam, Tianwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel algorithm for real-time planar semantic mapping tailored
for humanoid robots navigating complex terrains such as staircases. Our method
is adaptable to any odometry input and leverages GPU-accelerated processes for
planar extraction, enabling the rapid generation of globally consistent
semantic maps. We utilize an anisotropic diffusion filter on depth images to
effectively minimize noise from gradient jumps while preserving essential edge
details, enhancing normal vector images' accuracy and smoothness. Both the
anisotropic diffusion and the RANSAC-based plane extraction processes are
optimized for parallel processing on GPUs, significantly enhancing
computational efficiency. Our approach achieves real-time performance,
processing single frames at rates exceeding $30~Hz$, which facilitates detailed
plane extraction and map management swiftly and efficiently. Extensive testing
underscores the algorithm's capabilities in real-time scenarios and
demonstrates its practical application in humanoid robot gait planning,
significantly improving its ability to navigate dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 2024 IEEE-RAS International Conference on Humanoid
  Robots. The code: https://github.com/BTFrontier/polygon_mapping</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Autoencoders are Parameter-Efficient Federated Continual Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen He, Xiangfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a specific distributed learning paradigm in which a
central server aggregates updates from multiple clients' local models, thereby
enabling the server to learn without requiring clients to upload their private
data, maintaining data privacy. While existing federated learning methods are
primarily designed for static data, real-world applications often require
clients to learn new categories over time. This challenge necessitates the
integration of continual learning techniques, resulting in federated continual
learning (FCL). Although advanced prompt-based continual learning methods
leverage pre-trained transformers to mitigate catastrophic forgetting, they do
not adequately address the non-IID challenges in federated learning. To address
both catastrophic forgetting and non-IID issues, we propose to use masked
autoencoders (MAEs) as parameter-efficient federated continual learners, called
pMAE. pMAE learns reconstructive prompt on the client side through image
reconstruction using MAEs. On the server side, it reconstructs the uploaded
restore information to capture the data distribution across previous tasks and
different clients, using these reconstructed images to finetune discriminative
prompt and classifier parameters designed for classification, thereby
alleviating catastrophic forgetting and non-IID challenges on a global scale.
Experimental results demonstrate that pMAE achieves performance comparable to
existing prompt-based methods and can enhance their effectiveness, particularly
when using self-supervised pre-trained transformers as the backbone. Code is
available at: https://github.com/ycheoo/pMAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FPPL: An Efficient and Non-IID Robust Federated Continual Learning
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen He, Chuyun Shen, Xiangfeng Wang, Bo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated continual learning (FCL) aims to learn from sequential data stream
in the decentralized federated learning setting, while simultaneously
mitigating the catastrophic forgetting issue in classical continual learning.
Existing FCL methods usually employ typical rehearsal mechanisms, which could
result in privacy violations or additional onerous storage and computational
burdens. In this work, an efficient and non-IID robust federated continual
learning framework, called Federated Prototype-Augmented Prompt Learning
(FPPL), is proposed. The FPPL can collaboratively learn lightweight prompts
augmented by prototypes without rehearsal. On the client side, a fusion
function is employed to fully leverage the knowledge contained in task-specific
prompts for alleviating catastrophic forgetting. Additionally, global
prototypes aggregated from the server are used to obtain unified representation
through contrastive learning, mitigating the impact of non-IID-derived data
heterogeneity. On the server side, locally uploaded prototypes are utilized to
perform debiasing on the classifier, further alleviating the performance
degradation caused by both non-IID and catastrophic forgetting. Empirical
evaluations demonstrate the effectiveness of FPPL, achieving notable
performance with an efficient design while remaining robust to diverse non-IID
degrees. Code is available at: https://github.com/ycheoo/FPPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MBDRes-U-Net: Multi-Scale Lightweight Brain Tumor Segmentation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longfeng Shen, Yanqi Hou, Jiacong Chen, Liangjin Diao, Yaxi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of brain tumors plays a key role in the diagnosis and
treatment of brain tumor diseases. It serves as a critical technology for
quantifying tumors and extracting their features. With the increasing
application of deep learning methods, the computational burden has become
progressively heavier. To achieve a lightweight model with good segmentation
performance, this study proposes the MBDRes-U-Net model using the
three-dimensional (3D) U-Net codec framework, which integrates multibranch
residual blocks and fused attention into the model. The computational burden of
the model is reduced by the branch strategy, which effectively uses the rich
local features in multimodal images and enhances the segmentation performance
of subtumor regions. Additionally, during encoding, an adaptive weighted
expansion convolution layer is introduced into the multi-branch residual block,
which enriches the feature expression and improves the segmentation accuracy of
the model. Experiments on the Brain Tumor Segmentation (BraTS) Challenge 2018
and 2019 datasets show that the architecture could maintain a high precision of
brain tumor segmentation while considerably reducing the calculation
overhead.Our code is released at
https://github.com/Huaibei-normal-university-cv-laboratory/mbdresunet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Brain tumor segmentation, lightweight model, Brain Tumor Segmentation
  (BraTS) Challenge, group convolution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Global Depth-Range-Free Multi-View Stereo <span class="highlight-title">Transformer</span> Network with
  Pose Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitong Dong, Yijin Li, Zhaoyang Huang, Weikang Bian, Jingbo Liu, Hujun Bao, Zhaopeng Cui, Hongsheng Li, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel multi-view stereo (MVS) framework that gets
rid of the depth range prior. Unlike recent prior-free MVS methods that work in
a pair-wise manner, our method simultaneously considers all the source images.
Specifically, we introduce a Multi-view Disparity Attention (MDA) module to
aggregate long-range context information within and across multi-view images.
Considering the asymmetry of the epipolar disparity flow, the key to our method
lies in accurately modeling multi-view geometric constraints. We integrate pose
embedding to encapsulate information such as multi-view camera poses, providing
implicit geometric constraints for multi-view disparity feature fusion
dominated by attention. Additionally, we construct corresponding hidden states
for each source image due to significant differences in the observation quality
of the same pixel in the reference frame across multiple source frames. We
explicitly estimate the quality of the current pixel corresponding to sampled
points on the epipolar line of the source image and dynamically update hidden
states through the uncertainty estimation module. Extensive results on the DTU
dataset and Tanks&Temple benchmark demonstrate the effectiveness of our method.
The code is available at our project page:
https://zju3dv.github.io/GD-PoseMVS/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAttack: Robust Black-box Attack on LiDAR-based Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyin Chen, Danxin Liao, Sheng Xiang, Haibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since DNN is vulnerable to carefully crafted adversarial examples,
adversarial attack on LiDAR sensors have been extensively studied. We introduce
a robust black-box attack dubbed LiDAttack. It utilizes a genetic algorithm
with a simulated annealing strategy to strictly limit the location and number
of perturbation points, achieving a stealthy and effective attack. And it
simulates scanning deviations, allowing it to adapt to dynamic changes in real
world scenario variations. Extensive experiments are conducted on 3 datasets
(i.e., KITTI, nuScenes, and self-constructed data) with 3 dominant object
detection models (i.e., PointRCNN, PointPillar, and PV-RCNN++). The results
reveal the efficiency of the LiDAttack when targeting a wide range of object
detection models, with an attack success rate (ASR) up to 90%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining and Transferring Feature-Geometry Coherence for Unsupervised
  Point Cloud Registration <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kezheng Xiong, Haoen Xiang, Qingshan Xu, Chenglu Wen, Siqi Shen, Jonathan Li, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud registration, a fundamental task in 3D vision, has achieved
remarkable success with learning-based methods in outdoor environments.
Unsupervised outdoor point cloud registration methods have recently emerged to
circumvent the need for costly pose annotations. However, they fail to
establish reliable optimization objectives for unsupervised training, either
relying on overly strong geometric assumptions, or suffering from poor-quality
pseudo-labels due to inadequate integration of low-level geometric and
high-level contextual information. We have observed that in the feature space,
latent new inlier correspondences tend to cluster around respective positive
anchors that summarize features of existing inliers. Motivated by this
observation, we propose a novel unsupervised registration method termed INTEGER
to incorporate high-level contextual information for reliable pseudo-label
mining. Specifically, we propose the Feature-Geometry Coherence Mining module
to dynamically adapt the teacher for each mini-batch of data during training
and discover reliable pseudo-labels by considering both high-level feature
representations and low-level geometric cues. Furthermore, we propose
Anchor-Based Contrastive Learning to facilitate contrastive learning with
anchors for a robust feature space. Lastly, we introduce a Mixed-Density
Student to learn density-invariant features, addressing challenges related to
density variation and low overlap in the outdoor scenario. Extensive
experiments on KITTI and nuScenes datasets demonstrate that our INTEGER
achieves competitive performance in terms of accuracy and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Deep Learning Tractography Fiber Clustering Framework for
  Functionally Consistent White Matter Parcellation Using Multimodal Diffusion
  MRI and Functional MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Wang, Bocheng Guo, Yijie Li, Junyi Wang, Yuqian Chen, Jarrett Rushmore, Nikos Makris, Yogesh Rathi, Lauren J O'Donnell, Fan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tractography fiber clustering using diffusion MRI (dMRI) is a crucial
strategy for white matter (WM) parcellation. Current methods primarily use the
geometric information of fibers (i.e., the spatial trajectories) to group
similar fibers into clusters, overlooking the important functional signals
present along the fiber tracts. There is increasing evidence that neural
activity in the WM can be measured using functional MRI (fMRI), offering
potentially valuable multimodal information for fiber clustering. In this
paper, we develop a novel deep learning fiber clustering framework, namely Deep
Multi-view Fiber Clustering (DMVFC), that uses joint dMRI and fMRI data to
enable functionally consistent WM parcellation. DMVFC can effectively integrate
the geometric characteristics of the WM fibers with the fMRI BOLD signals along
the fiber tracts. It includes two major components: 1) a multi-view pretraining
module to compute embedding features from fiber geometric information and
functional signals separately, and 2) a collaborative fine-tuning module to
simultaneously refine the two kinds of embeddings. In the experiments, we
compare DMVFC with two state-of-the-art fiber clustering methods and
demonstrate superior performance in achieving functionally meaningful and
consistent WM parcellation results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface
  Reconstruction in Open Scenes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaochao Song, Chong Cheng, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Silver medal Solution for Image Matching Challenge 2024 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Matching Challenge 2024 is a competition focused on building 3D maps
from diverse image sets, requiring participants to solve fundamental computer
vision challenges in image matching across varying angles, lighting, and
seasonal changes. This project develops a Pipeline method that combines
multiple advanced techniques: using pre-trained EfficientNet-B7 for initial
feature extraction and cosine distance-based image pair filtering, employing
both KeyNetAffNetHardNet and SuperPoint for keypoint feature extraction,
utilizing AdaLAM and SuperGlue for keypoint matching, and finally applying
Pycolmap for 3D spatial analysis. The methodology achieved an excellent score
of 0.167 on the private leaderboard, with experimental results demonstrating
that the combination of KeyNetAffNetHardNet and SuperPoint provides significant
advantages in keypoint detection and matching, particularly when dealing with
challenging variations in surface texture and environmental conditions that
typically degrade traditional algorithm performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KptLLM: Unveiling the Power of Large Language Model for Keypoint
  Comprehension <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Yang, Wang Zeng, Sheng Jin, Lumin Xu, Wentao Liu, Chen Qian, Ruimao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Multimodal Large Language Models (MLLMs) have greatly
improved their abilities in image understanding. However, these models often
struggle with grasping pixel-level semantic details, e.g., the keypoints of an
object. To bridge this gap, we introduce the novel challenge of Semantic
Keypoint Comprehension, which aims to comprehend keypoints across different
task scenarios, including keypoint semantic understanding, visual prompt-based
keypoint detection, and textual prompt-based keypoint detection. Moreover, we
introduce KptLLM, a unified multimodal model that utilizes an
identify-then-detect strategy to effectively address these challenges. KptLLM
underscores the initial discernment of semantics in keypoints, followed by the
precise determination of their positions through a chain-of-thought process.
With several carefully designed modules, KptLLM adeptly handles various
modality inputs, facilitating the interpretation of both semantic contents and
keypoint locations. Our extensive experiments demonstrate KptLLM's superiority
in various keypoint detection benchmarks and its unique semantic capabilities
in interpreting keypoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OwMatch: Conditional Self-Labeling with Consistency for Open-World
  Semi-Supervised Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Niu, Lifan Lin, Jian Huang, Chao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) offers a robust framework for harnessing the
potential of unannotated data. Traditionally, SSL mandates that all classes
possess labeled instances. However, the emergence of open-world SSL (OwSSL)
introduces a more practical challenge, wherein unlabeled data may encompass
samples from unseen classes. This scenario leads to misclassification of unseen
classes as known ones, consequently undermining classification accuracy. To
overcome this challenge, this study revisits two methodologies from
self-supervised and semi-supervised learning, self-labeling and consistency,
tailoring them to address the OwSSL problem. Specifically, we propose an
effective framework called OwMatch, combining conditional self-labeling and
open-world hierarchical thresholding. Theoretically, we analyze the estimation
of class distribution on unlabeled data through rigorous statistical analysis,
thus demonstrating that OwMatch can ensure the unbiasedness of the self-label
assignment estimator with reliability. Comprehensive empirical analyses
demonstrate that our method yields substantial performance enhancements across
both known and unknown classes in comparison to previous studies. Code is
available at https://github.com/niusj03/OwMatch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera-ready (10 pages, 4 figures) with the appendices
  (10 pages, 7 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution alignment based transfer fusion frameworks on quantum
  devices for seeking quantum advantages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi He, Feiyu Du, Xiaohan Yu, Yang Zhao, Tao Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of labelled data is specifically an urgent challenge in the
field of quantum machine learning (QML). Two transfer fusion frameworks are
proposed in this paper to predict the labels of a target domain data by
aligning its distribution to a different but related labelled source domain on
quantum devices. The frameworks fuses the quantum data from two different, but
related domains through a quantum information infusion channel. The predicting
tasks in the target domain can be achieved with quantum advantages by
post-processing quantum measurement results. One framework, the quantum basic
linear algebra subroutines (QBLAS) based implementation, can theoretically
achieve the procedure of transfer fusion with quadratic speedup on a universal
quantum computer. In addition, the other framework, a hardware-scalable
architecture, is implemented on the noisy intermediate-scale quantum (NISQ)
devices through a variational hybrid quantum-classical procedure. Numerical
experiments on the synthetic and handwritten digits datasets demonstrate that
the variatioinal transfer fusion (TF) framework can reach state-of-the-art
(SOTA) quantum DA method performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuMask-Editor: A Novel Paradigm of Integration Between the
  Segmentation Diffusion Model and Image Editing to Improve Segmentation
  Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Gao, Fangxu Xing, Daniel Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation models, like mask2former, often demand a substantial
amount of manually annotated data, which is time-consuming and inefficient to
acquire. Leveraging state-of-the-art text-to-image models like Midjourney and
Stable Diffusion has emerged as an effective strategy for automatically
generating synthetic data instead of human annotations. However, prior
approaches have been constrained to synthesizing single-instance images due to
the instability inherent in generating multiple instances with Stable
Diffusion. To expand the domains and diversity of synthetic datasets, this
paper introduces a novel paradigm named DiffuMask-Editor, which combines the
Diffusion Model for Segmentation with Image Editing. By integrating multiple
objects into images using Text2Image models, our method facilitates the
creation of more realistic datasets that closely resemble open-world settings
while simultaneously generating accurate masks. Our approach significantly
reduces the laborious effort associated with manual annotation while ensuring
precise mask generation. Experimental results demonstrate that synthetic data
generated by DiffuMask-Editor enable segmentation methods to achieve superior
performance compared to real data. Particularly in zero-shot backgrounds,
DiffuMask-Editor achieves new state-of-the-art results on Unseen classes of VOC
2012. The code and models will be publicly available soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bootstrapping Top-down Information for Self-modulating Slot Attention <span class="chip">NeurIPS2</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwon Kim, Seoyeon Kim, Suha Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric learning (OCL) aims to learn representations of individual
objects within visual scenes without manual supervision, facilitating efficient
and effective visual reasoning. Traditional OCL methods primarily employ
bottom-up approaches that aggregate homogeneous visual features to represent
objects. However, in complex visual environments, these methods often fall
short due to the heterogeneous nature of visual features within an object. To
address this, we propose a novel OCL framework incorporating a top-down
pathway. This pathway first bootstraps the semantics of individual objects and
then modulates the model to prioritize features relevant to these semantics. By
dynamically modulating the model based on its own output, our top-down pathway
enhances the representational quality of objects. Our framework achieves
state-of-the-art performance across multiple synthetic and real-world
object-discovery benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS2 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding Sparse Tuning for Low Memory Usage <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufan Shen, Junshu Sun, Xiangyang Ji, Qingming Huang, Shuhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is an effective method for adapting
pre-trained vision models to downstream tasks by tuning a small subset of
parameters. Among PEFT methods, sparse tuning achieves superior performance by
only adjusting the weights most relevant to downstream tasks, rather than
densely tuning the whole weight matrix. However, this performance improvement
has been accompanied by increases in memory usage, which stems from two
factors, i.e., the storage of the whole weight matrix as learnable parameters
in the optimizer and the additional storage of tunable weight indexes. In this
paper, we propose a method named SNELL (Sparse tuning with kerNELized LoRA) for
sparse tuning with low memory usage. To achieve low memory usage, SNELL
decomposes the tunable matrix for sparsification into two learnable low-rank
matrices, saving from the costly storage of the whole original matrix. A
competition-based sparsification mechanism is further proposed to avoid the
storage of tunable weight indexes. To maintain the effectiveness of sparse
tuning with low-rank matrices, we extend the low-rank decomposition by applying
nonlinear kernel functions to the whole-matrix merging. Consequently, we gain
an increase in the rank of the merged matrix, enhancing the ability of SNELL in
adapting the pre-trained models to downstream tasks. Extensive experiments on
multiple downstream tasks show that SNELL achieves state-of-the-art performance
with low memory usage, endowing PEFT with sparse tuning to large-scale models.
Codes are available at https://github.com/ssfgunner/SNELL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIWR: Aerial Image Water Resource <span class="highlight-title">Dataset</span> for Segmentation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangdaow Noppitaka, Emmanuel Okafor, Olarik Surinta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective water resource management is crucial in agricultural regions like
northeastern Thailand, where limited water retention in sandy soils poses
significant challenges. In response to this issue, the Aerial Image Water
Resource (AIWR) dataset was developed, comprising 800 aerial images focused on
natural and artificial water bodies in this region. The dataset was created
using Bing Maps and follows the standards of the Fundamental Geographic Data
Set (FGDS). It includes ground truth annotations validated by experts in remote
sensing, making it an invaluable resource for researchers in geoinformatics,
computer vision, and artificial intelligence. The AIWR dataset presents
considerable challenges, such as segmentation due to variations in the size,
color, shape, and similarity of water bodies, which often resemble other land
use categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non rigid geometric distortions correction -- Application to atmospheric
  turbulence stabilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Mao, Jerome Gilles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel approach is presented to recover an image degraded by atmospheric
turbulence. Given a sequence of frames affected by turbulence, we construct a
variational model to characterize the static image. The optimization problem is
solved by Bregman Iteration and the operator splitting method. Our algorithm is
simple, efficient, and can be easily generalized for different scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSTA3D: Multi-scale Twin-attention for 3D Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc Dang Trung Tran, Byeongkeun Kang, Yeejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, transformer-based techniques incorporating superpoints have become
prevalent in 3D instance segmentation. However, they often encounter an
over-segmentation problem, especially noticeable with large objects.
Additionally, unreliable mask predictions stemming from superpoint mask
prediction further compound this issue. To address these challenges, we propose
a novel framework called MSTA3D. It leverages multi-scale feature
representation and introduces a twin-attention mechanism to effectively capture
them. Furthermore, MSTA3D integrates a box query with a box regularizer,
offering a complementary spatial constraint alongside semantic queries.
Experimental evaluations on ScanNetV2, ScanNet200 and S3DIS datasets
demonstrate that our approach surpasses state-of-the-art 3D instance
segmentation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 7 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning predictable and robust neural representations by straightening
  image sequences <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyan Niu, Cristina Savin, Eero P. Simoncelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prediction is a fundamental capability of all living organisms, and has been
proposed as an objective for learning sensory representations. Recent work
demonstrates that in primate visual systems, prediction is facilitated by
neural representations that follow straighter temporal trajectories than their
initial photoreceptor encoding, which allows for prediction by linear
extrapolation. Inspired by these experimental findings, we develop a
self-supervised learning (SSL) objective that explicitly quantifies and
promotes straightening. We demonstrate the power of this objective in training
deep feedforward neural networks on smoothly-rendered synthetic image sequences
that mimic commonly-occurring properties of natural videos. The learned model
contains neural embeddings that are predictive, but also factorize the
geometric, photometric, and semantic attributes of objects. The representations
also prove more robust to noise and adversarial attacks compared to previous
SSL methods that optimize for invariance to random augmentations. Moreover,
these beneficial properties can be transferred to other training procedures by
using the straightening objective as a regularizer, suggesting a broader
utility for straightening as a principle for robust unsupervised learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARN-LSTM: A Multi-Stream Attention-Based Model for Action Recognition
  with Temporal Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanchuan Wang, Ahmad Sufril Azlan Mohmamed, Xiao Yang, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ARN-LSTM, a novel multi-stream action recognition model
designed to address the challenge of simultaneously capturing spatial motion
and temporal dynamics in action sequences. Traditional methods often focus
solely on spatial or temporal features, limiting their ability to comprehend
complex human activities fully. Our proposed model integrates joint, motion,
and temporal information through a multi-stream fusion architecture.
Specifically, it comprises a joint stream for extracting skeleton features, a
temporal stream for capturing dynamic temporal features, and an ARN-LSTM block
that utilizes Time-Distributed Long Short-Term Memory (TD-LSTM) layers followed
by an Attention Relation Network (ARN) to model temporal relations. The outputs
from these streams are fused in a fully connected layer to provide the final
action prediction. Evaluations on the NTU RGB+D 60 and NTU RGB+D 120 datasets
demonstrate the effectiveness of our model, achieving effective performance,
particularly in group activity recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Structured Pruning for Efficient Architecture in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thai Vu Nguyen, Long Bao Le, Anderson Avila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Federated Learning (FL), training is conducted on client devices,
typically with limited computational resources and storage capacity. To address
these constraints, we propose an automatic pruning scheme tailored for FL
systems. Our solution improves computation efficiency on client devices, while
minimizing communication costs. One of the challenges of tuning pruning
hyper-parameters in FL systems is the restricted access to local data. Thus, we
introduce an automatic pruning paradigm that dynamically determines pruning
boundaries. Additionally, we utilized a structured pruning algorithm optimized
for mobile devices that lack hardware support for sparse computations.
Experimental results demonstrate the effectiveness of our approach, achieving
accuracy comparable to existing methods. Our method notably reduces the number
of parameters by 89% and FLOPS by 90%, with minimal impact on the accuracy of
the FEMNIST and CelebFaces datasets. Furthermore, our pruning method decreases
communication overhead by up to 5x and halves inference time when deployed on
Android devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled PET Lesion Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanya Gatsak, Kumar Abhishek, Hanene Ben Yedder, Saeid Asgari Taghanaki, Ghassan Hamarneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PET imaging is an invaluable tool in clinical settings as it captures the
functional activity of both healthy anatomy and cancerous lesions. Developing
automatic lesion segmentation methods for PET images is crucial since manual
lesion segmentation is laborious and prone to inter- and intra-observer
variability. We propose PET-Disentangler, a 3D disentanglement method that uses
a 3D UNet-like encoder-decoder architecture to disentangle disease and normal
healthy anatomical features with losses for segmentation, reconstruction, and
healthy component plausibility. A critic network is used to encourage the
healthy latent features to match the distribution of healthy samples and thus
encourages these features to not contain any lesion-related features. Our
quantitative results show that PET-Disentangler is less prone to incorrectly
declaring healthy and high tracer uptake regions as cancerous lesions, since
such uptake pattern would be assigned to the disentangled healthy component.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChatTracker: Enhancing Visual Tracking Performance via Chatting with
  Multimodal Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Sun, Fan Yu, Shaoxiang Chen, Yu Zhang, Junwei Huang, Chenhui Li, Yang Li, Changbo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual object tracking aims to locate a targeted object in a video sequence
based on an initial bounding box. Recently, Vision-Language~(VL) trackers have
proposed to utilize additional natural language descriptions to enhance
versatility in various applications. However, VL trackers are still inferior to
State-of-The-Art (SoTA) visual trackers in terms of tracking performance. We
found that this inferiority primarily results from their heavy reliance on
manual textual annotations, which include the frequent provision of ambiguous
language descriptions. In this paper, we propose ChatTracker to leverage the
wealth of world knowledge in the Multimodal Large Language Model (MLLM) to
generate high-quality language descriptions and enhance tracking performance.
To this end, we propose a novel reflection-based prompt optimization module to
iteratively refine the ambiguous and inaccurate descriptions of the target with
tracking feedback. To further utilize semantic information produced by MLLM, a
simple yet effective VL tracking framework is proposed and can be easily
integrated as a plug-and-play module to boost the performance of both VL and
visual trackers. Experimental results show that our proposed ChatTracker
achieves a performance comparable to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-task Geometric Estimation of Depth and Surface Normal from
  Monocular 360° Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Huang, Fang-Lue Zhang, Fangfang Zhang, Yu-Kun Lai, Paul Rosin, Neil A. Dodgson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric estimation is required for scene understanding and analysis in
panoramic 360{\deg} images. Current methods usually predict a single feature,
such as depth or surface normal. These methods can lack robustness, especially
when dealing with intricate textures or complex object surfaces. We introduce a
novel multi-task learning (MTL) network that simultaneously estimates depth and
surface normals from 360{\deg} images. Our first innovation is our MTL
architecture, which enhances predictions for both tasks by integrating
geometric information from depth and surface normal estimation, enabling a
deeper understanding of 3D scene structure. Another innovation is our fusion
module, which bridges the two tasks, allowing the network to learn shared
representations that improve accuracy and robustness. Experimental results
demonstrate that our MTL architecture significantly outperforms
state-of-the-art methods in both depth and surface normal estimation, showing
superior performance in complex and diverse scenes. Our model's effectiveness
and generalizability, particularly in handling intricate surface textures,
establish it as a new benchmark in 360{\deg} image geometric estimation. The
code and model are available at
\url{https://github.com/huangkun101230/360MTLGeometricEstimation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, this paper is accepted by Computational Visual Media
  Journal (CVMJ) but not pushlished yet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotation Perturbation Robustness in Point Cloud Analysis: A Perspective
  of Manifold Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Xu, Huazhen Liu, Feiming Wei, Huilin Xiong, Wenxian Yu, Tao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud is often regarded as a discrete sampling of Riemannian manifold
and plays a pivotal role in the 3D image interpretation. Particularly, rotation
perturbation, an unexpected small change in rotation caused by various factors
(like equipment offset, system instability, measurement errors and so on), can
easily lead to the inferior results in point cloud learning tasks. However,
classical point cloud learning methods are sensitive to rotation perturbation,
and the existing networks with rotation robustness also have much room for
improvements in terms of performance and noise tolerance. Given these, this
paper remodels the point cloud from the perspective of manifold as well as
designs a manifold distillation method to achieve the robustness of rotation
perturbation without any coordinate transformation. In brief, during the
training phase, we introduce a teacher network to learn the rotation robustness
information and transfer this information to the student network through online
distillation. In the inference phase, the student network directly utilizes the
original 3D coordinate information to achieve the robustness of rotation
perturbation. Experiments carried out on four different datasets verify the
effectiveness of our method. Averagely, on the Modelnet40 and ScanobjectNN
classification datasets with random rotation perturbations, our classification
accuracy has respectively improved by 4.92% and 4.41%, compared to popular
rotation-robust networks; on the ShapeNet and S3DIS segmentation datasets,
compared to the rotation-robust networks, the improvements of mIoU are 7.36%
and 4.82%, respectively. Besides, from the experimental results, the proposed
algorithm also shows excellent performance in resisting noise and outliers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, submitted to TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Convolution-based Unlearnable Datastes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dohyun Kim, Pedro Sandoval-Segura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The construction of large datasets for deep learning has raised concerns
regarding unauthorized use of online data, leading to increased interest in
protecting data from third-parties who want to use it for training. The
Convolution-based Unlearnable DAtaset (CUDA) method aims to make data
unlearnable by applying class-wise blurs to every image in the dataset so that
neural networks learn relations between blur kernels and labels, as opposed to
informative features for classifying clean data. In this work, we evaluate
whether CUDA data remains unlearnable after image sharpening and frequency
filtering, finding that this combination of simple transforms improves the
utility of CUDA data for training. In particular, we observe a substantial
increase in test accuracy over adversarial training for models trained with
CUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training
models to high accuracy using unlearnable data, we underscore the need for
ongoing refinement in data poisoning techniques to ensure data privacy. Our
method opens new avenues for enhancing the robustness of unlearnable datasets
by highlighting that simple methods such as sharpening and frequency filtering
are capable of breaking convolution-based unlearnable datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not Just Object, But State: Compositional Incremental Learning without
  Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyi Zhang, Binglin Qiu, Qi Jia, Yu Liu, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most incremental learners excessively prioritize coarse classes of objects
while neglecting various kinds of states (e.g. color and material) attached to
the objects. As a result, they are limited in the ability to reason
fine-grained compositionality of state-object pairs. To remedy this limitation,
we propose a novel task called Compositional Incremental Learning
(composition-IL), enabling the model to recognize state-object compositions as
a whole in an incremental learning fashion. Since the lack of suitable
benchmarks, we re-organize two existing datasets and make them tailored for
composition-IL. Then, we propose a prompt-based Composition Incremental Learner
(CompILer), to overcome the ambiguous composition boundary problem which
challenges composition-IL largely. Specifically, we exploit multi-pool prompt
learning, which is regularized by inter-pool prompt discrepancy and intra-pool
prompt diversity. Besides, we devise object-injected state prompting by using
object prompts to guide the selection of state prompts. Furthermore, we fuse
the selected prompts by a generalized-mean strategy, to eliminate irrelevant
information learned in the prompts. Extensive experiments on two datasets
exhibit state-of-the-art performance achieved by CompILer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next Best View For Point-Cloud Model Acquisition: Bayesian Approximation
  and Uncertainty Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madalena Caldeira, Plinio Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Next Best View problem is a computer vision problem widely studied in
robotics. To solve it, several methodologies have been proposed over the years.
Some, more recently, propose the use of deep learning models. Predictions
obtained with the help of deep learning models naturally have some uncertainty
associated with them. Despite this, the standard models do not allow for their
quantification. However, Bayesian estimation theory contributed to the
demonstration that dropout layers allow to estimate prediction uncertainty in
neural networks.
  This work adapts the point-net-based neural network for Next-Best-View
(PC-NBV). It incorporates dropout layers into the model's architecture, thus
allowing the computation of the uncertainty estimate associated with its
predictions. The aim of the work is to improve the network's accuracy in
correctly predicting the next best viewpoint, proposing a way to make the 3D
reconstruction process more efficient.
  Two uncertainty measurements capable of reflecting the prediction's error and
accuracy, respectively, were obtained. These enabled the reduction of the
model's error and the increase in its accuracy from 30\% to 80\% by identifying
and disregarding predictions with high values of uncertainty. Another method
that directly uses these uncertainty metrics to improve the final prediction
was also proposed. However, it showed very residual improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew McDermott, Jason Rife
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we reexamine the process through which a Neural Radiance Field
(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image
applications where camera pixels integrate light over time, LiDAR pulses arrive
at specific times. As such, multiple LiDAR returns are possible for any given
detector and the classification of these returns is inherently probabilistic.
Applying a traditional NeRF training routine can result in the network learning
phantom surfaces in free space between conflicting range measurements, similar
to how floater aberrations may be produced by an image model. We show that by
formulating loss as an integral of probability (rather than as an integral of
optical density) the network can learn multiple peaks for a given ray, allowing
the sampling of first, nth, or strongest returns from a single output channel.
Code is available at https://github.com/mcdermatt/PLINK
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMMA: End-to-End Multimodal Model for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, Yin Zhou, James Guo, Dragomir Anguelov, Mingxing Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.
Built on a multi-modal large language model foundation, EMMA directly maps raw
camera sensor data into various driving-specific outputs, including planner
trajectories, perception objects, and road graph elements. EMMA maximizes the
utility of world knowledge from the pre-trained large language models, by
representing all non-sensor inputs (e.g. navigation instructions and ego
vehicle status) and outputs (e.g. trajectories and 3D locations) as natural
language text. This approach allows EMMA to jointly process various driving
tasks in a unified language space, and generate the outputs for each task using
task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by
achieving state-of-the-art performance in motion planning on nuScenes as well
as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also
yields competitive results for camera-primary 3D object detection on the Waymo
Open Dataset (WOD). We show that co-training EMMA with planner trajectories,
object detection, and road graph tasks yields improvements across all three
domains, highlighting EMMA's potential as a generalist model for autonomous
driving applications. However, EMMA also exhibits certain limitations: it can
process only a small amount of image frames, does not incorporate accurate 3D
sensing modalities like LiDAR or radar and is computationally expensive. We
hope that our results will inspire further research to mitigate these issues
and to further evolve the state of the art in autonomous driving model
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Blog post: https://waymo.com/blog/2024/10/introducing-emma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for
  Open-World Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Hindel, Daniele Cattaneo, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation models are typically trained on a fixed set of classes,
limiting their applicability in open-world scenarios. Class-incremental
semantic segmentation aims to update models with emerging new classes while
preventing catastrophic forgetting of previously learned ones. However,
existing methods impose strict rigidity on old classes, reducing their
effectiveness in learning new incremental classes. In this work, we propose
Taxonomy-Oriented Poincar\'e-regularized Incremental-Class Segmentation
(TOPICS) that learns feature embeddings in hyperbolic space following explicit
taxonomy-tree structures. This supervision provides plasticity for old classes,
updating ancestors based on new classes while integrating new classes at
fitting positions. Additionally, we maintain implicit class relational
constraints on the geometric basis of the Poincar\'e ball. This ensures that
the latent space can continuously adapt to new constraints while maintaining a
robust structure to combat catastrophic forgetting. We also establish eight
realistic incremental learning protocols for autonomous driving scenarios,
where novel classes can originate from known classes or the background.
Extensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0
benchmarks demonstrate that it achieves state-of-the-art performance. We make
the code and trained models publicly available at
http://topics.cs.uni-freiburg.de.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE) <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P. Calmon, Himabindu Lakkaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP embeddings have demonstrated remarkable performance across a wide range
of multimodal applications. However, these high-dimensional, dense vector
representations are not easily interpretable, limiting our understanding of the
rich structure of CLIP and its use in downstream applications that require
transparency. In this work, we show that the semantic structure of CLIP's
latent space can be leveraged to provide interpretability, allowing for the
decomposition of representations into semantic concepts. We formulate this
problem as one of sparse recovery and propose a novel method, Sparse Linear
Concept Embeddings, for transforming CLIP representations into sparse linear
combinations of human-interpretable concepts. Distinct from previous work,
SpLiCE is task-agnostic and can be used, without training, to explain and even
replace traditional dense CLIP representations, maintaining high downstream
performance while significantly improving their interpretability. We also
demonstrate significant use cases of SpLiCE representations including detecting
spurious correlations and model editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures, NeurIPS 2024. Code is provided at
  https://github.com/AI4LIFE-GROUP/SpLiCE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xMIL: Insightful Explanations for Multiple Instance Learning in
  Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Hense, Mina Jamshidi Idaji, Oliver Eberle, Thomas Schnake, Jonas Dippel, Laure Ciernik, Oliver Buchstab, Andreas Mock, Frederick Klauschen, Klaus-Robert Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) is an effective and widely used approach for
weakly supervised machine learning. In histopathology, MIL models have achieved
remarkable success in tasks like tumor detection, biomarker prediction, and
outcome prognostication. However, MIL explanation methods are still lagging
behind, as they are limited to small bag sizes or disregard instance
interactions. We revisit MIL through the lens of explainable AI (XAI) and
introduce xMIL, a refined framework with more general assumptions. We
demonstrate how to obtain improved MIL explanations using layer-wise relevance
propagation (LRP) and conduct extensive evaluation experiments on three toy
settings and four real-world histopathology datasets. Our approach consistently
outperforms previous explanation attempts with particularly improved
faithfulness scores on challenging biomarker prediction tasks. Finally, we
showcase how xMIL explanations enable pathologists to extract insights from MIL
models, representing a significant advance for knowledge discovery and model
debugging in digital histopathology. Codes are available at:
https://github.com/tubml-pathology/xMIL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fashion-VDM: Video Diffusion Model for Virtual Try-On <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna Karras, Yingwei Li, Nan Liu, Luyang Zhu, Innfarn Yoo, Andreas Lugmayr, Chris Lee, Ira Kemelmacher-Shlizerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Fashion-VDM, a video diffusion model (VDM) for generating virtual
try-on videos. Given an input garment image and person video, our method aims
to generate a high-quality try-on video of the person wearing the given
garment, while preserving the person's identity and motion. Image-based virtual
try-on has shown impressive results; however, existing video virtual try-on
(VVT) methods are still lacking garment details and temporal consistency. To
address these issues, we propose a diffusion-based architecture for video
virtual try-on, split classifier-free guidance for increased control over the
conditioning inputs, and a progressive temporal training strategy for
single-pass 64-frame, 512px video generation. We also demonstrate the
effectiveness of joint image-video training for video try-on, especially when
video data is limited. Our qualitative and quantitative experiments show that
our approach sets the new state-of-the-art for video virtual try-on. For
additional results, visit our project page:
https://johannakarras.github.io/Fashion-VDM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGGRAPH Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPEAK: Speech-Driven Pose and Emotion-Adjustable Talking Head Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changpeng Cai, Guinan Guo, Jiao Li, Junhao Su, Fei Shen, Chenghao He, Jing Xiao, Yuanxu Chen, Lei Dai, Feiyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most earlier researches on talking face generation have focused on the
synchronization of lip motion and speech content. However, head pose and facial
emotions are equally important characteristics of natural faces. While
audio-driven talking face generation has seen notable advancements, existing
methods either overlook facial emotions or are limited to specific individuals
and cannot be applied to arbitrary subjects. In this paper, we propose a novel
one-shot Talking Head Generation framework (SPEAK) that distinguishes itself
from the general Talking Face Generation by enabling emotional and postural
control. Specifically, we introduce Inter-Reconstructed Feature Disentanglement
(IRFD) module to decouple facial features into three latent spaces. Then we
design a face editing module that modifies speech content and facial latent
codes into a single latent space. Subsequently, we present a novel generator
that employs modified latent codes derived from the editing module to regulate
emotional expression, head poses, and speech content in synthesizing facial
animations. Extensive trials demonstrate that our method ensures lip
synchronization with the audio while enabling decoupled control of facial
features, it can generate realistic talking head with coordinated lip motions,
authentic facial emotions, and smooth head movements. The demo video is
available: https://anonymous.4open.science/r/SPEAK-8A22
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan He, Yuxiang Nie, Hongmei Wang, Shu Yang, Yihui Wang, Zhiyuan Cai, Zhixuan Chen, Yingxue Xu, Luyang Luo, Huiling Xiang, Xi Lin, Mingxiang Wu, Yifan Peng, George Shih, Ziyang Xu, Xian Wu, Qiong Wang, Ronald Cheong Kin Chan, Varut Vardhanabhuti, Winnie Chiu Wing Chu, Yefeng Zheng, Pranav Rajpurkar, Kang Zhang, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist foundation models (GFMs) are renowned for their exceptional
capability and flexibility in effectively generalizing across diverse tasks and
modalities. In the field of medicine, while GFMs exhibit superior
generalizability based on their extensive intrinsic knowledge as well as
proficiency in instruction following and in-context learning, specialist models
excel in precision due to their domain knowledge. In this work, for the first
time, we explore the synergy between the GFM and specialist models, to enable
precise medical image analysis on a broader scope. Specifically, we propose a
cooperative framework, Generalist-Specialist Collaboration (GSCo), which
consists of two stages, namely the construction of GFM and specialists, and
collaborative inference on downstream tasks. In the construction stage, we
develop MedDr, the largest open-source GFM tailored for medicine, showcasing
exceptional instruction-following and in-context learning capabilities.
Meanwhile, a series of lightweight specialists are crafted for downstream tasks
with low computational cost. In the collaborative inference stage, we introduce
two cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented
Diagnosis, to harvest the generalist's in-context learning abilities alongside
the specialists' domain expertise. For a comprehensive evaluation, we curate a
large-scale benchmark featuring 28 datasets and about 250,000 images. Extensive
results demonstrate that MedDr consistently outperforms state-of-the-art GFMs
on downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists
across all out-of-domain disease diagnosis datasets. These findings indicate a
significant paradigm shift in the application of GFMs, transitioning from
separate models for specific tasks to a collaborative approach between GFMs and
specialists, thereby advancing the frontiers of generalizable AI in medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast yet Safe: Early-Exiting with Risk Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Metod Jazbec, Alexander Timans, Tin Hadži Veljković, Kaspar Sakmann, Dan Zhang, Christian A. Naesseth, Eric Nalisnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling machine learning models significantly improves their performance.
However, such gains come at the cost of inference being slow and
resource-intensive. Early-exit neural networks (EENNs) offer a promising
solution: they accelerate inference by allowing intermediate layers to exit and
produce a prediction early. Yet a fundamental issue with EENNs is how to
determine when to exit without severely degrading performance. In other words,
when is it 'safe' for an EENN to go 'fast'? To address this issue, we
investigate how to adapt frameworks of risk control to EENNs. Risk control
offers a distribution-free, post-hoc solution that tunes the EENN's exiting
mechanism so that exits only occur when the output is of sufficient quality. We
empirically validate our insights on a range of vision and language tasks,
demonstrating that risk control can produce substantial computational savings,
all the while preserving user-specified performance goals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 13 figures, 4 tables (incl. appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manipulation Facing Threats: Evaluating Physical Vulnerabilities in
  End-to-End Vision Language Action Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Cheng, Erjia Xiao, Chengyuan Yu, Zhao Yao, Jiahang Cao, Qiang Zhang, Jiaxu Wang, Mengshu Sun, Kaidi Xu, Jindong Gu, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, driven by advancements in Multimodal Large Language Models (MLLMs),
Vision Language Action Models (VLAMs) are being proposed to achieve better
performance in open-vocabulary scenarios for robotic manipulation tasks. Since
manipulation tasks involve direct interaction with the physical world, ensuring
robustness and safety during the execution of this task is always a very
critical issue. In this paper, by synthesizing current safety research on MLLMs
and the specific application scenarios of the manipulation task in the physical
world, we comprehensively evaluate VLAMs in the face of potential physical
threats. Specifically, we propose the Physical Vulnerability Evaluating
Pipeline (PVEP) that can incorporate as many visual modal physical threats as
possible for evaluating the physical robustness of VLAMs. The physical threats
in PVEP specifically include Out-of-Distribution, Typography-based Visual
Prompts, and Adversarial Patch Attacks. By comparing the performance
fluctuations of VLAMs before and after being attacked, we provide generalizable
Analyses of how VLAMs respond to different physical security threats. Our
project page is in this link:
https://chaducheng.github.io/Manipulat-Facing-Threats/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniRGB-IR: A Unified Framework for RGB-Infrared Semantic Tasks via
  Adapter Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maoxun Yuan, Bo Cui, Tianyi Zhao, Jiayi Wang, Shan Fu, Xingxing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic analysis on visible (RGB) and infrared (IR) images has gained
attention for its ability to be more accurate and robust under low-illumination
and complex weather conditions. Due to the lack of pre-trained foundation
models on the large-scale infrared image datasets, existing methods prefer to
design task-specific frameworks and directly fine-tune them with pre-trained
foundation models on their RGB-IR semantic relevance datasets, which results in
poor scalability and limited generalization. In this work, we propose a general
and efficient framework called UniRGB-IR to unify RGB-IR semantic tasks, in
which a novel adapter is developed to efficiently introduce richer RGB-IR
features into the pre-trained RGB-based foundation model. Specifically, our
framework consists of a RGB-based foundation model, a Multi-modal Feature Pool
(MFP) module and a Supplementary Feature Injector (SFI) module. The MFP and SFI
modules cooperate with each other as an adapter to effectively complement the
RGB-based features with the rich RGB-IR features. During training process, we
freeze the entire foundation model to inherit prior knowledge and only optimize
the proposed adapter. Furthermore, to verify the effectiveness of our
framework, we utilize the vanilla vision transformer (ViT-Base) as the
pre-trained foundation model to perform extensive experiments. Experimental
results on various RGB-IR downstream tasks demonstrate that our method can
achieve state-of-the-art performance. The source code and results are available
at https://github.com/PoTsui99/UniRGB-IR.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modular Quantization-Aware Training for 6D Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06753v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06753v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saqib Javed, Chengkun Li, Andrew Price, Yinlin Hu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge applications, such as collaborative robotics and spacecraft rendezvous,
demand efficient 6D object pose estimation on resource-constrained embedded
platforms. Existing 6D pose estimation networks are often too large for such
deployments, necessitating compression while maintaining reliable performance.
To address this challenge, we introduce Modular Quantization-Aware Training
(MQAT), an adaptive and mixed-precision quantization-aware training strategy
that exploits the modular structure of modern 6D pose estimation architectures.
MQAT guides a systematic gradated modular quantization sequence and determines
module-specific bit precisions, leading to quantized models that outperform
those produced by state-of-the-art uniform and mixed-precision quantization
techniques. Our experiments showcase the generality of MQAT across datasets,
architectures, and quantization algorithms. Remarkably, MQAT-trained quantized
models achieve a significant accuracy boost (>7%) over the baseline
full-precision network while reducing model size by a factor of 4x or more. Our
project website is at: https://saqibjaved1.github.io/MQAT_/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointNCBW: Towards <span class="highlight-title">Dataset</span> Ownership Verification for Point Clouds via
  Negative Clean-label Backdoor Watermark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wei, Yang Wang, Kuofeng Gao, Shuo Shao, Yiming Li, Zhibo Wang, Zhan Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, point clouds have been widely used in computer vision, whereas
their collection is time-consuming and expensive. As such, point cloud datasets
are the valuable intellectual property of their owners and deserve protection.
To detect and prevent unauthorized use of these datasets, especially for
commercial or open-sourced ones that cannot be sold again or used commercially
without permission, we intend to identify whether a suspicious third-party
model is trained on our protected dataset under the black-box setting. We
achieve this goal by designing a scalable clean-label backdoor-based dataset
watermark for point clouds that ensures both effectiveness and stealthiness.
Unlike existing clean-label watermark schemes, which are susceptible to the
number of categories, our method could watermark samples from all classes
instead of only from the target one. Accordingly, it can still preserve high
effectiveness even on large-scale datasets with many classes. Specifically, we
perturb selected point clouds with non-target categories in both shape-wise and
point-wise manners before inserting trigger patterns without changing their
labels. The features of perturbed samples are similar to those of benign
samples from the target class. As such, models trained on the watermarked
dataset will have a distinctive yet stealthy backdoor behavior, i.e.,
misclassifying samples from the target class whenever triggers appear, since
the trained DNNs will treat the inserted trigger pattern as a signal to deny
predicting the target label. We also design a hypothesis-test-guided dataset
ownership verification based on the proposed watermark. Extensive experiments
on benchmark datasets are conducted, verifying the effectiveness of our method
and its resistance to potential removal methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by IEEE Transactions on Information Forensics
  and Security (TIFS), 2024. 16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FilterViT and DropoutViT: Lightweight Vision <span class="highlight-title">Transformer</span> Models for
  Efficient Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce FilterViT, an enhanced version of MobileViT,
which leverages an attention-based mechanism for early-stage downsampling.
Traditional QKV operations on high-resolution feature maps are computationally
intensive due to the abundance of tokens. To address this, we propose a filter
attention mechanism using a convolutional neural network (CNN) to generate an
importance mask, focusing attention on key image regions. The method
significantly reduces computational complexity while maintaining
interpretability, as it highlights essential image areas. Experimental results
show that FilterViT achieves substantial gains in both efficiency and accuracy
compared to other models. We also introduce DropoutViT, a variant that uses a
stochastic approach for pixel selection, further enhancing robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advanced Vision <span class="highlight-title">Transformer</span>s and Open-Set Learning for Robust Mosquito
  Classification: A Novel Approach to Entomological Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Akib Jawad Karim, Muhammad Zawad Mahmud, Riasat Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mosquito-related diseases pose a significant threat to global public health,
necessitating efficient and accurate mosquito classification for effective
surveillance and control. This work presents an innovative approach to mosquito
classification by leveraging state-of-the-art vision transformers and open-set
learning techniques. A novel framework has been introduced that integrates
Transformer-based deep learning models with comprehensive data augmentation and
preprocessing methods, enabling robust and precise identification of ten
mosquito species. The Swin Transformer model achieves the best performance for
traditional closed-set learning with 99.80% accuracy and 0.998 F1 score. The
lightweight MobileViT technique attains an almost similar accuracy of 98.90%
with significantly reduced parameters and model complexities. Next, the applied
deep learning models' adaptability and generalizability in a static environment
have been enhanced by using new classes of data samples during the inference
stage that have not been included in the training set. The proposed framework's
ability to handle unseen classes like insects similar to mosquitoes, even
humans, through open-set learning further enhances its practical applicability
by employing the OpenMax technique and Weibull distribution. The traditional
CNN model, Xception, outperforms the latest transformer with higher accuracy
and F1 score for open-set learning. The study's findings highlight the
transformative potential of advanced deep-learning architectures in entomology,
providing a strong groundwork for future research and development in mosquito
surveillance and vector control. The implications of this work extend beyond
mosquito classification, offering valuable insights for broader ecological and
environmental monitoring applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Pairing Using Embedding Translation for Backdoor Attack Detection
  on Open-Set Classification Tasks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Unnervik, Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks allow an attacker to embed a specific vulnerability in a
machine learning algorithm, activated when an attacker-chosen pattern is
presented, causing a specific misprediction. The need to identify backdoors in
biometric scenarios has led us to propose a novel technique with different
trade-offs. In this paper we propose to use model pairs on open-set
classification tasks for detecting backdoors. Using a simple linear operation
to project embeddings from a probe model's embedding space to a reference
model's embedding space, we can compare both embeddings and compute a
similarity score. We show that this score, can be an indicator for the presence
of a backdoor despite models being of different architectures, having been
trained independently and on different datasets. This technique allows for the
detection of backdoors on models designed for open-set classification tasks,
which is little studied in the literature. Additionally, we show that backdoors
can be detected even when both models are backdoored. The source code is made
available for reproducibility purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024 Safe Generative AI Workshop (oral
  presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Framer: Interactive Frame Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Framer for interactive frame interpolation, which targets
producing smoothly transitioning frames between two images as per user
creativity. Concretely, besides taking the start and end frames as inputs, our
approach supports customizing the transition process by tailoring the
trajectory of some selected keypoints. Such a design enjoys two clear benefits.
First, incorporating human interaction mitigates the issue arising from
numerous possibilities of transforming one image to another, and in turn
enables finer control of local motions. Second, as the most basic form of
interaction, keypoints help establish the correspondence across frames,
enhancing the model to handle challenging cases (e.g., objects on the start and
end frames are of different shapes and styles). It is noteworthy that our
system also offers an "autopilot" mode, where we introduce a module to estimate
the keypoints and refine the trajectory automatically, to simplify the usage in
practice. Extensive experimental results demonstrate the appealing performance
of Framer on various applications, such as image morphing, time-lapse video
generation, cartoon interpolation, etc. The code, the model, and the interface
will be released to facilitate further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://aim-uofa.github.io/Framer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for
  Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyu Li, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing image plays an irreplaceable role in fields such as
agriculture, water resources, military, and disaster relief. Pixel-level
interpretation is a critical aspect of remote sensing image applications;
however, a prevalent limitation remains the need for extensive manual
annotation. For this, we try to introduce open-vocabulary semantic segmentation
(OVSS) into the remote sensing context. However, due to the sensitivity of
remote sensing images to low-resolution features, distorted target shapes and
ill-fitting boundaries are exhibited in the prediction mask. To tackle this
issue, we propose a simple and general upsampler, SimFeatUp, to restore lost
spatial information in deep features in a training-free style. Further, based
on the observation of the abnormal response of local patch tokens to [CLS]
token in CLIP, we propose to execute a straightforward subtraction operation to
alleviate the global bias in patch tokens. Extensive experiments are conducted
on 17 remote sensing datasets spanning semantic segmentation, building
extraction, road detection, and flood detection tasks. Our method achieves an
average of 5.8%, 8.2%, 4.0%, and 15.3% improvement over state-of-the-art
methods on 4 tasks. All codes are released.
\url{https://earth-insights.github.io/SegEarth-OV}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11944v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11944v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the capabilities of large multimodal models (LMMs) continue to advance,
evaluating the performance of LMMs emerges as an increasing need. Additionally,
there is an even larger gap in evaluating the advanced knowledge and reasoning
abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,
a new Chinese Massive Multi-discipline Multimodal Understanding benchmark
designed to evaluate LMMs on tasks demanding college-level subject knowledge
and deliberate reasoning in a Chinese context. CMMMU is inspired by and
strictly follows the annotation and analysis pattern of MMMU. CMMMU includes
12k manually collected multimodal questions from college exams, quizzes, and
textbooks, covering six core disciplines: Art & Design, Business, Science,
Health & Medicine, Humanities & Social Science, and Tech & Engineering, like
its companion, MMMU. These questions span 30 subjects and comprise 39 highly
heterogeneous image types, such as charts, diagrams, maps, tables, music
sheets, and chemical structures. CMMMU focuses on complex perception and
reasoning with domain-specific knowledge in the Chinese context. We evaluate 11
open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves
accuracies of 42%, indicating a large space for improvement. CMMMU will boost
the community to build the next-generation LMMs towards expert artificial
intelligence and promote the democratization of LMMs by providing diverse
language contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiVLC: Extending Vision-Language Compositionality Evaluation with
  Text-to-Image Retrieval <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imanol Miranda, Ander Salaberria, Eneko Agirre, Gorka Azkune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe
are formulated as image-to-text retrieval problems, where, given an image, the
models need to select between the correct textual description and a synthetic
hard negative text. In this work, we present the Bidirectional Vision-Language
Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic
hard negative image generated from the synthetic text, resulting in two
image-to-text retrieval examples (one for each image) and, more importantly,
two text-to-image retrieval examples (one for each text). Human annotators
filter out ill-formed examples ensuring the validity of the benchmark. The
experiments on BiVLC uncover a weakness of current multimodal models, as they
perform poorly in the text-to-image direction. In fact, when considering both
retrieval directions, the conclusions obtained in previous works change
significantly. In addition to the benchmark, we show that a contrastive model
trained using synthetic images and texts significantly improves over the base
model in SugarCrepe and in BiVLC for both retrieval directions. The gap to
human performance in BiVLC confirms that Vision-Language Compositionality is
still a challenging problem. BiVLC and code are available at
https://imirandam.github.io/BiVLC_project_page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page
  at: https://imirandam.github.io/BiVLC_project_page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image
  Synthesis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13686v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13686v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, Xuefeng Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a series of diffusion-aware distillation algorithms have emerged to
alleviate the computational overhead associated with the multi-step inference
process of Diffusion Models (DMs). Current distillation techniques often
dichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii)
ODE Trajectory Reformulation. However, these approaches suffer from severe
performance degradation or domain shifts. To address these limitations, we
propose Hyper-SD, a novel framework that synergistically amalgamates the
advantages of ODE Trajectory Preservation and Reformulation, while maintaining
near-lossless performance during step compression. Firstly, we introduce
Trajectory Segmented Consistency Distillation to progressively perform
consistent distillation within pre-defined time-step segments, which
facilitates the preservation of the original ODE trajectory from a higher-order
perspective. Secondly, we incorporate human feedback learning to boost the
performance of the model in a low-step regime and mitigate the performance loss
incurred by the distillation process. Thirdly, we integrate score distillation
to further improve the low-step generation capability of the model and offer
the first attempt to leverage a unified LoRA to support the inference process
at all steps. Extensive experiments and user studies demonstrate that Hyper-SD
achieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5.
For example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and
+0.51 in Aes Score in the 1-step inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (Camera-Ready Version). Project Page:
  https://hyper-sd.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RaLF: Flow-based Global and Metric Radar Localization in LiDAR Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijeet Nayak, Daniele Cattaneo, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localization is paramount for autonomous robots. While camera and LiDAR-based
approaches have been extensively investigated, they are affected by adverse
illumination and weather conditions. Therefore, radar sensors have recently
gained attention due to their intrinsic robustness to such conditions. In this
paper, we propose RaLF, a novel deep neural network-based approach for
localizing radar scans in a LiDAR map of the environment, by jointly learning
to address both place recognition and metric localization. RaLF is composed of
radar and LiDAR feature encoders, a place recognition head that generates
global descriptors, and a metric localization head that predicts the 3-DoF
transformation between the radar scan and the map. We tackle the place
recognition task by learning a shared embedding space between the two
modalities via cross-modal metric learning. Additionally, we perform metric
localization by predicting pixel-level flow vectors that align the query radar
scan with the LiDAR map. We extensively evaluate our approach on multiple
real-world driving datasets and show that RaLF achieves state-of-the-art
performance for both place recognition and metric localization. Moreover, we
demonstrate that our approach can effectively generalize to different cities
and sensor setups than the ones used during training. We make the code and
trained models publicly available at http://ralf.cs.uni-freiburg.de.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Concept Embeddings for Analysis of Concept Distributions in DNN
  Feature Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Mikriukov, Gesina Schwalbe, Korinna Bade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Insights into the learned latent representations are imperative for verifying
deep neural networks (DNNs) in critical computer vision (CV) tasks. Therefore,
state-of-the-art supervised Concept-based eXplainable Artificial Intelligence
(C-XAI) methods associate user-defined concepts like ``car'' each with a single
vector in the DNN latent space (concept embedding vector). In the case of
concept segmentation, these linearly separate between activation map pixels
belonging to a concept and those belonging to background. Existing methods for
concept segmentation, however, fall short of capturing sub-concepts (e.g.,
``proximate car'' and ``distant car''), and concept overlap (e.g., between
``bus'' and ``truck''). In other words, they do not capture the full
distribution of concept representatives in latent space. For the first time,
this work shows that these simplifications are frequently broken and that
distribution information can be particularly useful for understanding
DNN-learned notions of sub-concepts, concept confusion, and concept outliers.
To allow exploration of learned concept distributions, we propose a novel local
concept analysis framework. Instead of optimizing a single global concept
vector on the complete dataset, it generates a local concept embedding (LoCE)
vector for each individual sample. We use the distribution formed by LoCEs to
explore the latent concept distribution by fitting Gaussian mixture models
(GMMs), hierarchical clustering, and concept-level information retrieval and
outlier detection. Despite its context sensitivity, our method's concept
segmentation performance is competitive to global baselines. Analysis results
are obtained on two datasets and five diverse vision DNN architectures,
including vision transformers (ViTs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. Manni, C. Lauretti, F. Prata, R. Papalia, L. Zollo, P. Soda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endoscopic surgery relies on two-dimensional views, posing challenges for
surgeons in depth perception and instrument manipulation. While Monocular
Visual Simultaneous Localization and Mapping (MVSLAM) has emerged as a
promising solution, its implementation in endoscopic procedures faces
significant challenges due to hardware limitations, such as the use of a
monocular camera and the absence of odometry sensors. This study presents
BodySLAM, a robust deep learning-based MVSLAM approach that addresses these
challenges through three key components: CycleVO, a novel unsupervised
monocular pose estimation module; the integration of the state-of-the-art Zoe
architecture for monocular depth estimation; and a 3D reconstruction module
creating a coherent surgical map. The approach is rigorously evaluated using
three publicly available datasets (Hamlyn, EndoSLAM, and SCARED) spanning
laparoscopy, gastroscopy, and colonoscopy scenarios, and benchmarked against
four state-of-the-art methods. Results demonstrate that CycleVO exhibited
competitive performance with the lowest inference time among pose estimation
methods, while maintaining robust generalization capabilities, whereas Zoe
significantly outperformed existing algorithms for depth estimation in
endoscopy. BodySLAM's strong performance across diverse endoscopic scenarios
demonstrates its potential as a viable MVSLAM solution for endoscopic
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in
  Endoscopic Pituitary Surgery <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjana Wijekoon, Adrito Das, Roxana R. Herrera, Danyal Z. Khan, John Hanrahan, Eleanor Carter, Valpuri Luoma, Danail Stoyanov, Hani J. Marcus, Sophia Bano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow
for anaesthetists to more accurately decide when to administer anaesthetic
agents and drugs, as well as to notify hospital staff to send in the next
patient. Therefore RSD plays an important role in improving patient care and
minimising surgical theatre costs via efficient scheduling. In endoscopic
pituitary surgery, it is uniquely challenging due to variable workflow
sequences with a selection of optional steps contributing to high variability
in surgery duration. This paper presents PitRSDNet for predicting RSD during
pituitary surgery, a spatio-temporal neural network model that learns from
historical data focusing on workflow sequences. PitRSDNet integrates workflow
knowledge into RSD prediction in two forms: 1) multi-task learning for
concurrently predicting step and RSD; and 2) incorporating prior steps as
context in temporal learning and inference. PitRSDNet is trained and evaluated
on a new endoscopic pituitary surgery dataset with 88 videos to show
competitive performance improvements over previous statistical and machine
learning methods. The findings also highlight how PitRSDNet improve RSD
precision on outlier cases utilising the knowledge of prior steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Augmented Environments for Computer-Assisted
  Interventions (AE-CAI) Workshop at the Medical Image Computing and
  Computer-Assisted Interventions (MICCAI) Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DualDn: Dual-domain Denoising via Differentiable ISP <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikang Li, Yujin Wang, Shiqi Chen, Fan Zhang, Jinwei Gu, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image denoising is a critical component in a camera's Image Signal Processing
(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP
pipeline: applying a denoiser directly to captured raw frames (raw domain) or
to the ISP's output sRGB images (sRGB domain). However, both approaches have
their limitations. Residual noise from raw-domain denoising can be amplified by
the subsequent ISP processing, and the sRGB domain struggles to handle
spatially varying noise since it only sees noise distorted by the ISP.
Consequently, most raw or sRGB domain denoising works only for specific noise
distributions and ISP configurations. To address these challenges, we propose
DualDn, a novel learning-based dual-domain denoising. Unlike previous
single-domain denoising, DualDn consists of two denoising networks: one in the
raw domain and one in the sRGB domain. The raw domain denoising adapts to
sensor-specific noise as well as spatially varying noise levels, while the sRGB
domain denoising adapts to ISP variations and removes residual noise amplified
by the ISP. Both denoising networks are connected with a differentiable ISP,
which is trained end-to-end and discarded during the inference stage. With this
design, DualDn achieves greater generalizability compared to most
learning-based denoising methods, as it can adapt to different unseen noises,
ISP parameters, and even novel ISP pipelines. Experiments show that DualDn
achieves state-of-the-art performance and can adapt to different denoising
architectures. Moreover, DualDn can be used as a plug-and-play denoising module
with real cameras without retraining, and still demonstrate better performance
than commercial on-camera denoising. The project website is available at:
https://openimaginglab.github.io/DualDn/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2024, Project page:
  https://openimaginglab.github.io/DualDn/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A citizen science toolkit to collect human perceptions of urban
  environments using open street view images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00174v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00174v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Danish, SM Labib, Britta Ricker, Marco Helbich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Street View Imagery (SVI) is a valuable data source for studies (e.g.,
environmental assessments, green space identification or land cover
classification). While commercial SVI is available, such providers commonly
restrict copying or reuse in ways necessary for research. Open SVI datasets are
readily available from less restrictive sources, such as Mapillary, but due to
the heterogeneity of the images, these require substantial preprocessing,
filtering, and careful quality checks. We present an efficient method for
automated downloading, processing, cropping, and filtering open SVI, to be used
in a survey of human perceptions of the streets portrayed in these images. We
demonstrate our open-source reusable SVI preparation and smartphone-friendly
perception-survey software with Amsterdam (Netherlands) as the case study.
Using a citizen science approach, we collected from 331 people 22,637 ratings
about their perceptions for various criteria. We have published our software in
a public repository for future re-use and reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunji Hong, Minh Hieu Nguyen, Mikaela Angelina Uy, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-view
images, not merely as a field or raw geometry but as a sketch-extrude CAD
model. Extracting extrusion cylinders from raw 3D geometry has been extensively
researched in computer vision, while the processing of 3D data through neural
networks has remained a bottleneck. Since 3D scans are generally accompanied by
multi-view images, leveraging 2D convolutional neural networks allows these
images to be exploited as a rich source for extracting extrusion cylinder
information. However, we observe that extracting only the surface information
of the extrudes and utilizing it results in suboptimal outcomes due to the
challenges in the occlusion and surface segmentation. By synergizing with the
extracted base curve information, we achieve the optimal reconstruction result
with the best accuracy in 2D sketch and extrude parameter estimation. Our
experiments, comparing our method with previous work that takes a raw 3D point
cloud as input, demonstrate the effectiveness of our approach by taking
advantage of multi-view images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private Attribute Inference from Images with Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batuhan Tömekçe, Mark Vero, Robin Staab, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become ubiquitous in our daily tasks and
digital interactions, associated privacy risks are increasingly in focus. While
LLM privacy research has primarily focused on the leakage of model training
data, it has recently been shown that LLMs can make accurate privacy-infringing
inferences from previously unseen texts. With the rise of vision-language
models (VLMs), capable of understanding both images and text, a key question is
whether this concern transfers to the previously unexplored domain of benign
images posted online. To answer this question, we compile an image dataset with
human-annotated labels of the image owner's personal attributes. In order to
understand the privacy risks posed by VLMs beyond traditional human attribute
recognition, our dataset consists of images where the inferable private
attributes do not stem from direct depictions of humans. On this dataset, we
evaluate 7 state-of-the-art VLMs, finding that they can infer various personal
attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy
scales with the general capabilities of the models, implying that future models
can be misused as stronger inferential adversaries, establishing an imperative
for the development of adequate defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual <span class="highlight-title">Self-supervised</span> Learning Scheme for Dense Prediction Tasks on
  X-ray Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08421v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08421v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shervin Halat, Mohammad Rahmati, Ehsan Nazerfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant advancements in artificial intelligence have been
attributed to the integration of self-supervised learning (SSL) scheme. While
SSL has shown impressive achievements in natural language processing (NLP), its
progress in computer vision has comparatively lagged behind. However, the
incorporation of contrastive learning into existing visual SSL models has led
to considerable progress, often surpassing supervised counterparts.
Nonetheless, these improvements have been mostly limited to classification
tasks. Moreover, few studies have evaluated visual SSL models in real-world
scenarios, as most have focused on datasets with class-wise portrait images,
notably ImageNet. Here, we focus on dense prediction tasks using security
inspection x-ray images to evaluate our proposed model, Segment Localization
(SegLoc). Based upon the Instance Localization (InsLoc) model, SegLoc addresses
one of the key challenges of contrastive learning, i.e., false negative pairs
of query embeddings. Our pre-training dataset is synthesized by cutting,
transforming, and pasting labeled segments from an existing labeled dataset
(PIDray) as foregrounds onto instances from an unlabeled dataset (SIXray) as
backgrounds. Furthermore, we fully leverage the labeled data by incorporating
the concept, one queue per class, into the MoCo-v2 memory bank, thereby
avoiding false negative pairs. In our experiments, SegLoc outperformed random
initialization by 3% to 6% while underperformed supervised initialization, in
terms of AR and AP metrics across different IoU values over 20 to 30
pre-training epochs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PT43D: A Probabilistic <span class="highlight-title">Transformer</span> for Generating 3D Shapes from Single
  Highly-Ambiguous RGB Images <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11914v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11914v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Xiong, Angela Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating 3D shapes from single RGB images is essential in various
applications such as robotics. Current approaches typically target images
containing clear and complete visual descriptions of the object, without
considering common realistic cases where observations of objects that are
largely occluded or truncated. We thus propose a transformer-based
autoregressive model to generate the probabilistic distribution of 3D shapes
conditioned on an RGB image containing potentially highly ambiguous
observations of the object. To handle realistic scenarios such as occlusion or
field-of-view truncation, we create simulated image-to-shape training pairs
that enable improved fine-tuning for real-world scenarios. We then adopt
cross-attention to effectively identify the most relevant region of interest
from the input image for shape generation. This enables inference of sampled
shapes with reasonable diversity and strong alignment with the input image. We
train and test our model on our synthetic data then fine-tune and test it on
real-world data. Experiments demonstrate that our model outperforms state of
the art in both scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures. Accepted to BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongmin Lee, Minsu Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the 3D orientations of an object in an image, known as
single-image pose estimation, is a crucial task in 3D vision applications.
Existing methods typically learn 3D rotations parametrized in the spatial
domain using Euler angles or quaternions, but these representations often
introduce discontinuities and singularities. SO(3)-equivariant networks enable
the structured capture of pose patterns with data-efficient learning, but the
parametrizations in spatial domain are incompatible with their architecture,
particularly spherical CNNs, which operate in the frequency domain to enhance
computational efficiency. To overcome these issues, we propose a
frequency-domain approach that directly predicts Wigner-D coefficients for 3D
rotation regression, aligning with the operations of spherical CNNs. Our
SO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial
parameterizations, ensuring consistent pose estimation under arbitrary
rotations. Trained with a frequency-domain regression loss, our method achieves
state-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+,
with significant improvements in accuracy, robustness, and data efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024, Project webpage at
  http://cvlab.postech.ac.kr/research/3D_EquiPose</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QIS : Interactive Segmentation via Quasi-Conformal Mappings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Daoping Zhang, Lok Ming Lui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation plays a crucial role in extracting important objects of
interest from images, enabling various applications. While existing methods
have shown success in segmenting clean images, they often struggle to produce
accurate segmentation results when dealing with degraded images, such as those
containing noise or occlusions. To address this challenge, interactive
segmentation has emerged as a promising approach, allowing users to provide
meaningful input to guide the segmentation process. However, an important
problem in interactive segmentation lies in determining how to incorporate
minimal yet meaningful user guidance into the segmentation model. In this
paper, we propose the quasi-conformal interactive segmentation (QIS) model,
which incorporates user input in the form of positive and negative clicks.
Users mark a few pixels belonging to the object region as positive clicks,
indicating that the segmentation model should include a region around these
clicks. Conversely, negative clicks are provided on pixels belonging to the
background, instructing the model to exclude the region near these clicks from
the segmentation mask. Additionally, the segmentation mask is obtained by
deforming a template mask with the same topology as the object of interest
using an orientation-preserving quasiconformal mapping. This approach helps to
avoid topological errors in the segmentation results. We provide a thorough
analysis of the proposed model, including theoretical support for the ability
of QIS to include or exclude regions of interest or disinterest based on the
user's indication. To evaluate the performance of QIS, we conduct experiments
on synthesized images, medical images, natural images and noisy natural images.
The results demonstrate the efficacy of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target Detection of Safety Protective Gear Using the Improved YOLOv5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Xue Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-risk railway construction, personal protective equipment monitoring
is critical but challenging due to small and frequently obstructed targets. We
propose YOLO-EA, an innovative model that enhances safety measure detection by
integrating ECA into its backbone's convolutional layers, improving discernment
of minuscule objects like hardhats. YOLO-EA further refines target recognition
under occlusion by replacing GIoU with EIoU loss. YOLO-EA's effectiveness was
empirically substantiated using a dataset derived from real-world railway
construction site surveillance footage. It outperforms YOLOv5, achieving 98.9%
precision and 94.7% recall, up 2.5% and 0.5% respectively, while maintaining
real-time performance at 70.774 fps. This highly efficient and precise YOLO-EA
holds great promise for practical application in intricate construction
scenarios, enforcing stringent safety compliance during complex railway
construction projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S3PT: Scene Semantics and Structure Guided Clustering to Boost
  <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing for Autonomous Driving <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K. Wozniak, Hariprasath Govindarajan, Marvin Klingner, Camille Maurice, B Ravi Kiran, Senthil Yogamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-supervised clustering-based pre-training techniques like DINO and
Cribo have shown impressive results for downstream detection and segmentation
tasks. However, real-world applications such as autonomous driving face
challenges with imbalanced object class and size distributions and complex
scene geometries. In this paper, we propose S3PT a novel scene semantics and
structure guided clustering to provide more scene-consistent objectives for
self-supervised training. Specifically, our contributions are threefold: First,
we incorporate semantic distribution consistent clustering to encourage better
representation of rare classes such as motorcycles or animals. Second, we
introduce object diversity consistent spatial clustering, to handle imbalanced
and diverse object sizes, ranging from large background areas to small objects
such as pedestrians and traffic signs. Third, we propose a depth-guided spatial
clustering to regularize learning based on geometric information of the scene,
thus further refining region separation on the feature level. Our learned
representations significantly improve performance in downstream semantic
segmentation and 3D object detection tasks on the nuScenes, nuImages, and
Cityscapes datasets and show promising domain translation properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientNet with Hybrid Attention Mechanisms for Enhanced Breast
  Histopathology Classification: A Comprehensive Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naren Sengodan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer histopathology image classification is crucial for early cancer
detection, offering the potential to reduce mortality rates through timely
diagnosis. This paper introduces a novel approach integrating Hybrid
EfficientNet models with advanced attention mechanisms, including Convolutional
Block Attention Module (CBAM), Self-Attention, and Deformable Attention, to
enhance feature extraction and focus on critical image regions. We evaluate the
performance of our models across multiple magnification scales using publicly
available histopathological datasets. Our method achieves significant
improvements, with accuracy reaching 98.42% at 400X magnification, surpassing
several state-of-the-art models, including VGG and ResNet architectures. The
results are validated using metrics such as accuracy, F1-score, precision, and
recall, demonstrating the clinical potential of our model in improving
diagnostic accuracy. Furthermore, the proposed method shows increased
computational efficiency, making it suitable for integration into real-time
diagnostic workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VGA: Vision GUI Assistant -- Minimizing Hallucinations through
  Image-Centric Fine-Tuning <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Meng, Yu Dai, Zezheng Gong, Shaoxiong Guo, Minglong Tang, Tongquan Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Vision-Language Models (LVLMs) have significantly
improve performance in image comprehension tasks, such as formatted charts and
rich-content images. Yet, Graphical User Interface (GUI) pose a greater
challenge due to their structured format and detailed textual information.
Existing LVLMs often overly depend on internal knowledge and neglect image
content, resulting in hallucinations and incorrect responses in GUI
comprehension. To address these issues, we introduce VGA, a fine-tuned model
designed for comprehensive GUI understanding. Our model aims to enhance the
interpretation of visual data of GUI and reduce hallucinations. We first
construct a Vision Question Answering (VQA) dataset of 63.8k high-quality
examples with our propose Referent Method, which ensures the model's responses
are highly depend on visual content within the image. We then design a
two-stage fine-tuning method called Foundation and Advanced Comprehension (FAC)
to enhance both the model's ability to extract information from image content
and alignment with human intent. Experiments show that our approach enhances
the model's ability to extract information from images and achieves
state-of-the-art results in GUI understanding tasks. Our dataset and
fine-tuning script will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneDiff: A Generalist Model for Image Difference Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05645v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05645v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erdong Hu, Longteng Guo, Tongtian Yue, Zijia Zhao, Shuning Xue, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, Image Difference Captioning (IDC) is crucial for
accurately describing variations between closely related images. Traditional
IDC methods often rely on specialist models, which restrict their applicability
across varied contexts. This paper introduces the OneDiff model, a novel
generalist approach that utilizes a robust vision-language model architecture,
integrating a siamese image encoder with a Visual Delta Module. This innovative
configuration allows for the precise detection and articulation of fine-grained
differences between image pairs. OneDiff is trained through a dual-phase
strategy, encompassing Coupled Sample Training and multi-task learning across a
diverse array of data types, supported by our newly developed DiffCap Dataset.
This dataset merges real-world and synthetic data, enhancing the training
process and bolstering the model's robustness. Extensive testing on diverse IDC
benchmarks, such as Spot-the-Diff, Image-Editing-Request, and Birds-to-Words,
shows that OneDiff consistently outperforms existing state-of-the-art models in
accuracy and adaptability, achieving improvements of up to 97% CIDEr points in
average. By setting a new benchmark in IDC, OneDiff paves the way for more
versatile and effective applications in detecting and describing visual
differences. The code, models, and data will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GACL: Exemplar-Free Generalized Analytic Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15706v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15706v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Yizhu Chen, Di Fang, Run He, Kai Tong, Hongxin Wei, Ziqian Zeng, Cen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class incremental learning (CIL) trains a network on sequential tasks with
separated categories in each task but suffers from catastrophic forgetting,
where models quickly lose previously learned knowledge when acquiring new
tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more
real-world scenario, where incoming data have mixed data categories and unknown
sample size distribution. Existing attempts for the GCIL either have poor
performance or invade data privacy by saving exemplars. In this paper, we
propose a new exemplar-free GCIL technique named generalized analytic continual
learning (GACL). The GACL adopts analytic learning (a gradient-free training
technique) and delivers an analytical (i.e., closed-form) solution to the GCIL
scenario. This solution is derived via decomposing the incoming data into
exposed and unexposed classes, thereby attaining a weight-invariant property, a
rare yet valuable property supporting an equivalence between incremental
learning and its joint training. Such an equivalence is crucial in GCIL
settings as data distributions among different tasks no longer pose challenges
to adopting our GACL. Theoretically, this equivalence property is validated
through matrix analysis tools. Empirically, we conduct extensive experiments
where, compared with existing GCIL methods, our GACL exhibits a consistently
leading performance across various datasets and GCIL settings. Source code is
available at https://github.com/CHEN-YIZHU/GACL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Anchors Are Strong Information Aggregators For Multimodal Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haogeng Liu, Quanzeng You, Xiaotian Han, Yongfei Liu, Huaibo Huang, Ran He, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of Multimodal Large Language Models (MLLMs), vision-language
connector plays a crucial role to link the pre-trained vision encoders with
Large Language Models (LLMs). Despite its importance, the vision-language
connector has been relatively less explored. In this study, we aim to propose a
strong vision-language connector that enables MLLMs to achieve high accuracy
while maintain low computation cost. We first reveal the existence of the
visual anchors in Vision Transformer and propose a cost-effective search
algorithm to extract them. Building on these findings, we introduce the Anchor
Former (AcFormer), a novel vision-language connector designed to leverage the
rich prior knowledge obtained from these visual anchors during pretraining,
guiding the aggregation of information. Through extensive experimentation, we
demonstrate that the proposed method significantly reduces computational costs
by nearly two-thirds compared with baseline, while simultaneously outperforming
baseline methods. This highlights the effectiveness and efficiency of AcFormer.
Codes are available at https://github.com/liuhaogeng/Anchor-Former.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRACE: Temporal Grounding Video LLM via Causal Event Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Qingbin Liu, Xi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Temporal Grounding (VTG) is a crucial capability for video
understanding models and plays a vital role in downstream tasks such as video
browsing and editing. To effectively handle various tasks simultaneously and
enable zero-shot prediction, there is a growing trend in employing video LLMs
for VTG tasks. However, current video LLM-based methods rely exclusively on
natural language generation, lacking the ability to model the clear structure
inherent in videos, which restricts their effectiveness in tackling VTG tasks.
To address this issue, this paper first formally introduces causal event
modeling framework, which represents videos as sequences of events, and predict
the current event using previous events, video inputs, and textural
instructions. Each event consists of three components: timestamps, salient
scores, and textual captions. We then propose a novel task-interleaved video
LLM called TRACE to effectively implement the causal event modeling framework
in practice. The TRACE processes visual frames, timestamps, salient scores, and
text as distinct tasks, employing various encoders and decoding heads for each.
Task tokens are arranged in an interleaved sequence according to the causal
event modeling framework's formulation. Extensive experiments on various VTG
tasks and datasets demonstrate the superior performance of TRACE compared to
state-of-the-art video LLMs. Our model and code are available at
\url{https://github.com/gyxxyg/TRACE}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms
  in Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oryan Yehezkel, Alon Zolfi, Amit Baras, Yuval Elovici, Asaf Shabtai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers have contributed greatly to advancements in the computer
vision domain, demonstrating state-of-the-art performance in diverse tasks
(e.g., image classification, object detection). However, their high
computational requirements grow quadratically with the number of tokens used.
Token sparsification mechanisms have been proposed to address this issue. These
mechanisms employ an input-dependent strategy, in which uninformative tokens
are discarded from the computation pipeline, improving the model's efficiency.
However, their dynamism and average-case assumption makes them vulnerable to a
new threat vector - carefully crafted adversarial examples capable of fooling
the sparsification mechanism, resulting in worst-case performance. In this
paper, we present DeSparsify, an attack targeting the availability of vision
transformers that use token sparsification mechanisms. The attack aims to
exhaust the operating system's resources, while maintaining its stealthiness.
Our evaluation demonstrates the attack's effectiveness on three token
sparsification mechanisms and examines the attack's transferability between
them and its effect on the GPU resources. To mitigate the impact of the attack,
we propose various countermeasures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ F-OAL: Forward-only Online Analytic Learning with Fast Training and Low
  Memory Footprint in Class Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Yuchen Liu, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Yi Wang, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Class Incremental Learning (OCIL) aims to train models incrementally,
where data arrive in mini-batches, and previous data are not accessible. A
major challenge in OCIL is Catastrophic Forgetting, i.e., the loss of
previously learned knowledge. Among existing baselines, replay-based methods
show competitive results but requires extra memory for storing exemplars, while
exemplar-free (i.e., data need not be stored for replay in production) methods
are resource-friendly but often lack accuracy. In this paper, we propose an
exemplar-free approach--Forward-only Online Analytic Learning (F-OAL). Unlike
traditional methods, F-OAL does not rely on back-propagation and is
forward-only, significantly reducing memory usage and computational time.
Cooperating with a pre-trained frozen encoder with Feature Fusion, F-OAL only
needs to update a linear classifier by recursive least square. This approach
simultaneously achieves high accuracy and low resource consumption. Extensive
experiments on benchmark datasets demonstrate F-OAL's robust performance in
OCIL scenarios. Code is available at https://github.com/liuyuchen-cz/F-OAL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MemControl: Mitigating Memorization in Diffusion Models via Automated
  Parameter Selection <span class="chip">WACV'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raman Dutt, Ondrej Bohdal, Pedro Sanchez, Sotirios A. Tsaftaris, Timothy Hospedales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models excel in generating images that closely resemble their
training data but are also susceptible to data memorization, raising privacy,
ethical, and legal concerns, particularly in sensitive domains such as medical
imaging. We hypothesize that this memorization stems from the
overparameterization of deep models and propose that regularizing model
capacity during fine-tuning can mitigate this issue. Firstly, we empirically
show that regulating the model capacity via Parameter-efficient fine-tuning
(PEFT) mitigates memorization to some extent, however, it further requires the
identification of the exact parameter subsets to be fine-tuned for high-quality
generation. To identify these subsets, we introduce a bi-level optimization
framework, MemControl, that automates parameter selection using memorization
and generation quality metrics as rewards during fine-tuning. The parameter
subsets discovered through MemControl achieve a superior tradeoff between
generation quality and memorization. For the task of medical image generation,
our approach outperforms existing state-of-the-art memorization mitigation
strategies by fine-tuning as few as 0.019% of model parameters. Moreover, we
demonstrate that the discovered parameter subsets are transferable to
non-medical domains. Our framework is scalable to large datasets, agnostic to
reward functions, and can be integrated with existing approaches for further
memorization mitigation. To the best of our knowledge, this is the first study
to empirically evaluate memorization in medical images and propose a targeted
yet universal mitigation strategy. The code is available at
https://github.com/Raman1121/Diffusion_Memorization_HPO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV'25 (Applications Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVQA: Culturally-diverse Multilingual Visual Question Answering
  Benchmark <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hernán Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D'Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario Rodríguez-Cantelar, Mélanie Jouitteau, Mihail Mihaylov, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Naome Etori, Olivier Niyomugisha, Paula Mónica Silva, Pranjal Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago Góngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Teresa Clifford, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is an important task in multimodal AI, and it
is often used to test the ability of vision-language models to understand and
reason on knowledge present in both visual and textual data. However, most of
the current VQA models use datasets that are primarily focused on English and a
few major world languages, with images that are typically Western-centric.
While recent efforts have tried to increase the number of languages covered on
VQA datasets, they still lack diversity in low-resource languages. More
importantly, although these datasets often extend their linguistic range via
translation or some other approaches, they usually keep images the same,
resulting in narrow cultural representation. To address these limitations, we
construct CVQA, a new Culturally-diverse multilingual Visual Question Answering
benchmark, designed to cover a rich set of languages and cultures, where we
engage native speakers and cultural experts in the data collection process. As
a result, CVQA includes culturally-driven images and questions from across 30
countries on four continents, covering 31 languages with 13 scripts, providing
a total of 10k questions. We then benchmark several Multimodal Large Language
Models (MLLMs) on CVQA, and show that the dataset is challenging for the
current state-of-the-art models. This benchmark can serve as a probing
evaluation suite for assessing the cultural capability and bias of multimodal
models and hopefully encourage more research efforts toward increasing cultural
awareness and linguistic diversity in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decay Pruning Method: Smooth Pruning With a Self-Rectifying Procedure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Yang, Linlin Gao, Pengyuan Li, Wenbo Li, Yihong Dong, Zhiying Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current structured pruning methods often result in considerable accuracy
drops due to abrupt network changes and loss of information from pruned
structures. To address these issues, we introduce the Decay Pruning Method
(DPM), a novel smooth pruning approach with a self-rectifying mechanism. DPM
consists of two key components: (i) Smooth Pruning: It converts conventional
single-step pruning into multi-step smooth pruning, gradually reducing
redundant structures to zero over N steps with ongoing optimization. (ii)
Self-Rectifying: This procedure further enhances the aforementioned process by
rectifying sub-optimal pruning based on gradient information. Our approach
demonstrates strong generalizability and can be easily integrated with various
existing pruning methods. We validate the effectiveness of DPM by integrating
it with three popular pruning methods: OTOv2, Depgraph, and Gate Decorator.
Experimental results show consistent improvements in performance compared to
the original pruning methods, along with further reductions of FLOPs in most
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ eMoE-Tracker: Environmental MoE-based <span class="highlight-title">Transformer</span> for Robust
  Event-guided Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Chen, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unique complementarity of frame-based and event cameras for high frame
rate object tracking has recently inspired some research attempts to develop
multi-modal fusion approaches. However, these methods directly fuse both
modalities and thus ignore the environmental attributes, e.g., motion blur,
illumination variance, occlusion, scale variation, etc. Meanwhile, insufficient
interaction between search and template features makes distinguishing target
objects and backgrounds difficult. As a result, performance degradation is
induced especially in challenging conditions. This paper proposes a novel and
effective Transformer-based event-guided tracking framework, called
eMoE-Tracker, which achieves new SOTA performance under various conditions. Our
key idea is to disentangle the environment into several learnable attributes to
dynamically learn the attribute-specific features and strengthen the target
information by improving the interaction between the target template and search
regions. To achieve the goal, we first propose an environmental Mix-of-Experts
(eMoE) module that is built upon the environmental Attributes Disentanglement
to learn attribute-specific features and environmental Attributes Assembling to
assemble the attribute-specific features by the learnable attribute scores
dynamically. The eMoE module is a subtle router that prompt-tunes the
transformer backbone more efficiently. We then introduce a contrastive relation
modeling (CRM) module to emphasize target information by leveraging a
contrastive learning strategy between the target template and search regions.
Extensive experiments on diverse event-based benchmark datasets showcase the
superior performance of our eMoE-Tracker compared to the prior arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RGB-event single object tracking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Vectorized Backpropagation Algorithms for Training Feedforward
  Networks Composed of Quadratic Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathew Mithra Noel, Venkataraman Muthiah-Nakarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Higher order artificial neurons whose outputs are computed by applying an
activation function to a higher order multinomial function of the inputs have
been considered in the past, but did not gain acceptance due to the extra
parameters and computational cost. However, higher order neurons have
significantly greater learning capabilities since the decision boundaries of
higher order neurons can be complex surfaces instead of just hyperplanes. The
boundary of a single quadratic neuron can be a general hyper-quadric surface
allowing it to learn many nonlinearly separable datasets. Since quadratic forms
can be represented by symmetric matrices, only $\frac{n(n+1)}{2}$ additional
parameters are needed instead of $n^2$. A quadratic Logistic regression model
is first presented. Solutions to the XOR problem with a single quadratic neuron
are considered. The complete vectorized equations for both forward and backward
propagation in feedforward networks composed of quadratic neurons are derived.
A reduced parameter quadratic neural network model with just $ n $ additional
parameters per neuron that provides a compromise between learning ability and
computational cost is presented. Comparison on benchmark classification
datasets are used to demonstrate that a final layer of quadratic neurons
enables networks to achieve higher accuracy with significantly fewer hidden
layer neurons. In particular this paper shows that any dataset composed of
$\mathcal{C}$ bounded clusters can be separated with only a single layer of
$\mathcal{C}$ quadratic neurons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive
  <span class="highlight-title">Dataset</span> and Benchmark for Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16999v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16999v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Modal Large Language Models (MLLMs) have demonstrated impressive
performance in various VQA tasks. However, they often lack interpretability and
struggle with complex visual inputs, especially when the resolution of the
input image is high or when the interested region that could provide key
information for answering the question is small. To address these challenges,
we collect and introduce the large-scale Visual CoT dataset comprising 438k
question-answer pairs, annotated with intermediate bounding boxes highlighting
key regions essential for answering the questions. Additionally, about 98k
pairs of them are annotated with detailed reasoning steps. Importantly, we
propose a multi-turn processing pipeline that dynamically focuses on visual
inputs and provides interpretable thoughts. We also introduce the related
benchmark to evaluate the MLLMs in scenarios requiring specific local region
identification. Extensive experiments demonstrate the effectiveness of our
framework and shed light on better inference strategies. The Visual CoT
dataset, benchmark, and pre-trained models are available on
https://hao-shao.com/projects/viscot.html to support further research in this
area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hao-shao.com/projects/viscot.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Generalizability of Diffusion Models Requires Rethinking
  the Hidden Gaussian Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Yixiang Dai, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the generalizability of diffusion models by looking
into the hidden properties of the learned score functions, which are
essentially a series of deep denoisers trained on various noise levels. We
observe that as diffusion models transition from memorization to
generalization, their corresponding nonlinear diffusion denoisers exhibit
increasing linearity. This discovery leads us to investigate the linear
counterparts of the nonlinear diffusion models, which are a series of linear
models trained to match the function mappings of the nonlinear diffusion
denoisers. Surprisingly, these linear denoisers are approximately the optimal
denoisers for a multivariate Gaussian distribution characterized by the
empirical mean and covariance of the training dataset. This finding implies
that diffusion models have the inductive bias towards capturing and utilizing
the Gaussian structure (covariance information) of the training dataset for
data generation. We empirically demonstrate that this inductive bias is a
unique property of diffusion models in the generalization regime, which becomes
increasingly evident when the model's capacity is relatively small compared to
the training dataset size. In the case that the model is highly
overparameterized, this inductive bias emerges during the initial training
phases before the model fully memorizes its training data. Our study provides
crucial insights into understanding the notable strong generalization
phenomenon recently observed in real-world diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Aware Text Features in Referring Image Segmentation: From Object
  Understanding to Context Understanding <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Nguyen-Truong, E-Ro Nguyen, Tuan-Anh Vu, Minh-Triet Tran, Binh-Son Hua, Sai-Kit Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring image segmentation is a challenging task that involves generating
pixel-wise segmentation masks based on natural language descriptions. The
complexity of this task increases with the intricacy of the sentences provided.
Existing methods have relied mostly on visual features to generate the
segmentation masks while treating text features as supporting components.
However, this under-utilization of text understanding limits the model's
capability to fully comprehend the given expressions. In this work, we propose
a novel framework that specifically emphasizes object and context comprehension
inspired by human cognitive processes through Vision-Aware Text Features.
Firstly, we introduce a CLIP Prior module to localize the main object of
interest and embed the object heatmap into the query initialization process.
Secondly, we propose a combination of two components: Contextual Multimodal
Decoder and Meaning Consistency Constraint, to further enhance the coherent and
consistent interpretation of language cues with the contextual understanding
obtained from the image. Our method achieves significant performance
improvements on three benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Project
page: \url{https://vatex.hkustvgd.com/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the ubiquity of charts as a data analysis, visualization, and
decision-making tool across industries and sciences, there has been a growing
interest in developing pre-trained foundation models as well as general purpose
instruction-tuned models for chart understanding and reasoning. However,
existing methods suffer crucial drawbacks across two critical axes affecting
the performance of chart representation models: they are trained on data
generated from underlying data tables of the charts, ignoring the visual trends
and patterns in chart images, and use weakly aligned vision-language backbone
models for domain-specific training, limiting their generalizability when
encountering charts in the wild. We address these important drawbacks and
introduce ChartGemma, a novel chart understanding and reasoning model developed
over PaliGemma. Rather than relying on underlying data tables, ChartGemma is
trained on instruction-tuning data generated directly from chart images, thus
capturing both high-level trends and low-level visual information from a
diverse set of charts. Our simple approach achieves state-of-the-art results
across $5$ benchmarks spanning chart summarization, question answering, and
fact-checking, and our elaborate qualitative studies on real-world charts show
that ChartGemma generates more realistic and factually correct summaries
compared to its contemporaries. We release the code, model checkpoints,
dataset, and demos at https://github.com/vis-nlp/ChartGemma.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span> on Adversarial Attack and Defense for Medical Image Analysis:
  Methods and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Dong, Junxi Chen, Xiaohua Xie, Jianhuang Lai, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have achieved superior performance in computer-aided
medical image analysis, yet they are still vulnerable to imperceptible
adversarial attacks, resulting in potential misdiagnosis in clinical practice.
Oppositely, recent years have also witnessed remarkable progress in defense
against these tailored adversarial examples in deep medical diagnosis systems.
In this exposition, we present a comprehensive survey on recent advances in
adversarial attacks and defenses for medical image analysis with a systematic
taxonomy in terms of the application scenario. We also provide a unified
framework for different types of adversarial attack and defense methods in the
context of medical image analysis. For a fair comparison, we establish a new
benchmark for adversarially robust medical diagnosis models obtained by
adversarial training under various scenarios. To the best of our knowledge,
this is the first survey paper that provides a thorough evaluation of
adversarially robust medical diagnosis models. By analyzing qualitative and
quantitative results, we conclude this survey with a detailed discussion of
current challenges for adversarial attack and defense in medical image analysis
systems to shed light on future research directions. Code is available on
\href{https://github.com/tomvii/Adv_MIA}{\color{red}{GitHub}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Computing Surveys (CSUR) (DOI:
  https://doi.org/10.1145/3702638)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Camera-Based HRV Prediction for Remote Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04161v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04161v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kegang Wang, Yantao Wei, Jiankai Tang, Yuntao Wang, Mingwen Tong, Jie Gao, Yujian Ma, Zhongjin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, due to the widespread use of internet videos, remote
photoplethysmography (rPPG) has gained more and more attention in the fields of
affective computing. Restoring blood volume pulse (BVP) signals from facial
videos is a challenging task that involves a series of preprocessing, image
algorithms, and postprocessing to restore waveforms. Not only is the heart rate
metric utilized for affective computing, but the heart rate variability (HRV)
metric is even more significant. The challenge in obtaining HRV indices through
rPPG lies in the necessity for algorithms to precisely predict the BVP peak
positions. In this paper, we collected the Remote Learning Affect and
Physiology (RLAP) dataset, which includes over 32 hours of highly synchronized
video and labels from 58 subjects. This is a public dataset whose BVP labels
have been meticulously designed to better suit the training of HRV models.
Using the RLAP dataset, we trained a new model called Seq-rPPG, it is a model
based on one-dimensional convolution, and experimental results reveal that this
structure is more suitable for handling HRV tasks, which outperformed all other
baselines in HRV performance and also demonstrated significant advantages in
computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Pose Representation Learning for Generating and Transferring
  Non-Rigid Object Poses <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungwoo Yoo, Juil Koo, Kyeongmin Yeo, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method for learning representations of poses for 3D
deformable objects, which specializes in 1) disentangling pose information from
the object's identity, 2) facilitating the learning of pose variations, and 3)
transferring pose information to other object identities. Based on these
properties, our method enables the generation of 3D deformable objects with
diversity in both identities and poses, using variations of a single object. It
does not require explicit shape parameterization such as skeletons or joints,
point-level or shape-level correspondence supervision, or variations of the
target object for pose transfer. To achieve pose disentanglement, compactness
for generative models, and transferability, we first design the pose extractor
to represent the pose as a keypoint-based hybrid representation and the pose
applier to learn an implicit deformation field. To better distill pose
information from the object's geometry, we propose the implicit pose applier to
output an intrinsic mesh property, the face Jacobian. Once the extracted pose
information is transferred to the target object, the pose applier is fine-tuned
in a self-supervised manner to better describe the target object's shapes with
pose variations. The extracted poses are also used to train a cascaded
diffusion model to enable the generation of novel poses. Our experiments with
the DeformThings4D and Human datasets demonstrate state-of-the-art performance
in pose transfer and the ability to generate diverse deformed shapes with
various objects and poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Benjamin Burchfiel, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio signals provide rich information for the robot interaction and object
properties through contact. This information can surprisingly ease the learning
of contact-rich robot manipulation skills, especially when the visual
information alone is ambiguous or incomplete. However, the usage of audio data
in robot manipulation has been constrained to teleoperated demonstrations
collected by either attaching a microphone to the robot or object, which
significantly limits its usage in robot learning pipelines. In this work, we
introduce ManiWAV: an 'ear-in-hand' data collection device to collect
in-the-wild human demonstrations with synchronous audio and visual feedback,
and a corresponding policy interface to learn robot manipulation policy
directly from the demonstrations. We demonstrate the capabilities of our system
through four contact-rich manipulation tasks that require either passively
sensing the contact events and modes, or actively sensing the object surface
materials and states. In addition, we show that our system can generalize to
unseen in-the-wild environments by learning from diverse in-the-wild human
demonstrations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Robot Learning (CoRL) 2024; Project website:
  https://maniwav.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncTweedies: A General Generative Framework Based on Synchronized
  Diffusions <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14370v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14370v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a general framework for generating diverse visual content,
including ambiguous images, panorama images, mesh textures, and Gaussian splat
textures, by synchronizing multiple diffusion processes. We present exhaustive
investigation into all possible scenarios for synchronizing multiple diffusion
processes through a canonical space and analyze their characteristics across
applications. In doing so, we reveal a previously unexplored case: averaging
the outputs of Tweedie's formula while conducting denoising in multiple
instance spaces. This case also provides the best quality with the widest
applicability to downstream tasks. We name this case SyncTweedies. In our
experiments generating visual content aforementioned, we demonstrate the
superior quality of generation by SyncTweedies compared to other
synchronization methods, optimization-based and iterative-update-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://synctweedies.github.io/ (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional Kolmogorov-Arnold Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Dylan Bodner, Antonio Santiago Tepsich, Jack Natan Spolski, Santiago Pourteau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Convolutional Kolmogorov-Arnold Networks
(Convolutional KANs), an innovative alternative to the standard Convolutional
Neural Networks (CNNs) that have revolutionized the field of computer vision.
By integrating the learneable non-linear activation functions presented in
Kolmogorov-Arnold Networks (KANs) into convolutions, we propose a new layer.
Throughout the paper, we empirically validate the performance of Convolutional
KANs against traditional architectures across Fashion-MNIST dataset, finding
that, in some cases, this new approach maintains a similar level of accuracy
while using half the number of parameters. This experiments show that KAN
Convolutions seem to learn more per kernel, which opens up a new horizon of
possibilities in deep learning for computer vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReactFace: Online Multiple Appropriate Facial Reaction Generation in
  Dyadic Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Luo, Siyang Song, Weicheng Xie, Micol Spitale, Zongyuan Ge, Linlin Shen, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dyadic interaction, predicting the listener's facial reactions is
challenging as different reactions could be appropriate in response to the same
speaker's behaviour. Previous approaches predominantly treated this task as an
interpolation or fitting problem, emphasizing deterministic outcomes but
ignoring the diversity and uncertainty of human facial reactions. Furthermore,
these methods often failed to model short-range and long-range dependencies
within the interaction context, leading to issues in the synchrony and
appropriateness of the generated facial reactions. To address these
limitations, this paper reformulates the task as an extrapolation or prediction
problem, and proposes an novel framework (called ReactFace) to generate
multiple different but appropriate facial reactions from a speaker behaviour
rather than merely replicating the corresponding listener facial behaviours.
Our ReactFace generates multiple different but appropriate photo-realistic
human facial reactions by: (i) learning an appropriate facial reaction
distribution representing multiple different but appropriate facial reactions;
and (ii) synchronizing the generated facial reactions with the speaker verbal
and non-verbal behaviours at each time stamp, resulting in realistic 2D facial
reaction sequences. Experimental results demonstrate the effectiveness of our
approach in generating multiple diverse, synchronized, and appropriate facial
reactions from each speaker's behaviour. The quality of the generated facial
reactions is intimately tied to the speaker's speech and facial expressions,
achieved through our novel speaker-listener interaction modules. Our code is
made publicly available at \url{https://github.com/lingjivoo/ReactFace}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Visualization and Computer Graphics
  (TVCG), 18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training on the Test Model: Contamination in Ranking Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishakha Suresh Kalal, Andrew Parry, Sean MacAvaney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural approaches to ranking based on pre-trained language models are highly
effective in ad-hoc search. However, the computational expense of these models
can limit their application. As such, a process known as knowledge distillation
is frequently applied to allow a smaller, efficient model to learn from an
effective but expensive model. A key example of this is the distillation of
expensive API-based commercial Large Language Models into smaller
production-ready models. However, due to the opacity of training data and
processes of most commercial models, one cannot ensure that a chosen test
collection has not been observed previously, creating the potential for
inadvertent data contamination. We, therefore, investigate the effect of a
contaminated teacher model in a distillation setting. We evaluate several
distillation techniques to assess the degree to which contamination occurs
during distillation. By simulating a ``worst-case'' setting where the degree of
contamination is known, we find that contamination occurs even when the test
data represents a small fraction of the teacher's training samples. We,
therefore, encourage caution when training using black-box teacher models where
data provenance is ambiguous.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing ID-based Recommendation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Chen, Chen Gao, Xiaoyi Du, Hengliang Luo, Depeng Jin, Yong Li, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently garnered significant attention in
various domains, including recommendation systems. Recent research leverages
the capabilities of LLMs to improve the performance and user modeling aspects
of recommender systems. These studies primarily focus on utilizing LLMs to
interpret textual data in recommendation tasks. However, it's worth noting that
in ID-based recommendations, textual data is absent, and only ID data is
available. The untapped potential of LLMs for ID data within the ID-based
recommendation paradigm remains relatively unexplored. To this end, we
introduce a pioneering approach called "LLM for ID-based Recommendation"
(LLM4IDRec). This innovative approach integrates the capabilities of LLMs while
exclusively relying on ID data, thus diverging from the previous reliance on
textual data. The basic idea of LLM4IDRec is that by employing LLM to augment
ID data, if augmented ID data can improve recommendation performance, it
demonstrates the ability of LLM to interpret ID data effectively, exploring an
innovative way for the integration of LLM in ID-based recommendation. We
evaluate the effectiveness of our LLM4IDRec approach using three widely-used
datasets. Our results demonstrate a notable improvement in recommendation
performance, with our approach consistently outperforming existing methods in
ID-based recommendation by solely augmenting input data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissertation: On the Theoretical Foundation of Model Comparison and
  Evaluation for Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become increasingly important with the rise of the
web as a medium for electronic and business transactions. One of the key
drivers of this technology is the ease with which users can provide feedback
about their likes and dislikes through simple clicks of a mouse. This feedback
is commonly collected in the form of ratings, but can also be inferred from a
user's browsing and purchasing history. Recommender systems utilize users'
historical data to infer customer interests and provide personalized
recommendations. The basic principle of recommendations is that significant
dependencies exist between user- and item-centric activity, which can be
learned in a data-driven manner to make accurate predictions. Collaborative
filtering is one family of recommendation algorithms that uses ratings from
multiple users to predict missing ratings or uses binary click information to
predict potential clicks. However, recommender systems can be more complex and
incorporate auxiliary data such as content-based attributes, user interactions,
and contextual information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.08517</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferable Sequential Recommendation via Vector Quantized Meta
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenrui Yue, Huimin Zeng, Yang Zhang, Julian McAuley, Dong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While sequential recommendation achieves significant progress on capturing
user-item transition patterns, transferring such large-scale recommender
systems remains challenging due to the disjoint user and item groups across
domains. In this paper, we propose a vector quantized meta learning for
transferable sequential recommenders (MetaRec). Without requiring additional
modalities or shared information across domains, our approach leverages
user-item interactions from multiple source domains to improve the target
domain performance. To solve the input heterogeneity issue, we adopt vector
quantization that maps item embeddings from heterogeneous input spaces to a
shared feature space. Moreover, our meta transfer paradigm exploits limited
target data to guide the transfer of source domain knowledge to the target
domain (i.e., learn to transfer). In addition, MetaRec adaptively transfers
from multiple source tasks by rescaling meta gradients based on the
source-target domain similarity, enabling selective learning to improve
recommendation performance. To validate the effectiveness of our approach, we
perform extensive experiments on benchmark datasets, where MetaRec consistently
outperforms baseline methods by a considerable margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BigData 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Retrieval: End-to-End Information Retrieval with One Large Language
  Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyu Tang, Jiawei Chen, Zhuoqun Li, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu, Hongyu Lin, Fei Huang, Ben He, Xianpei Han, Le Sun, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has significantly transformed both
the construction and application of information retrieval (IR) systems.
However, current interactions between IR systems and LLMs remain limited, with
LLMs merely serving as part of components within IR systems, and IR systems
being constructed independently of LLMs. This separated architecture restricts
knowledge sharing and deep collaboration between them. In this paper, we
introduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval
architecture. Self-Retrieval unifies all essential IR functions within a single
LLM, leveraging the inherent capabilities of LLMs throughout the IR process.
Specifically, Self-Retrieval internalizes the retrieval corpus through
self-supervised learning, transforms the retrieval process into sequential
passage generation, and performs relevance assessment for reranking.
Experimental results demonstrate that Self-Retrieval not only outperforms
existing retrieval approaches by a significant margin, but also substantially
enhances the performance of LLM-driven downstream applications like
retrieval-augmented generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Camera-ready Version. Code:
  https://github.com/icip-cas/SelfRetrieval</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>ing with Phonemes: Enhancing LLM Multilinguality for non-Latin
  Script Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang Nguyen, Khyati Mahajan, Vikas Yadav, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual LLMs have achieved remarkable benchmark performance, but we find
they continue to underperform on non-Latin script languages across contemporary
LLM families. This discrepancy arises from the fact that LLMs are pretrained
with orthographic scripts, which are dominated by Latin characters that obscure
their shared phonology with non-Latin scripts. We propose leveraging phonemic
transcriptions as complementary signals to induce script-invariant
representations. Our study demonstrates that integrating phonemic signals
improves performance across both non-Latin and Latin languages, with a
particularly significant impact on closing the performance gap between the two.
Through detailed experiments, we show that phonemic and orthographic scripts
retrieve distinct examples for in-context learning (ICL). This motivates our
proposed Mixed-ICL retrieval strategy, where further aggregation leads to our
significant performance improvements for both Latin script languages (up to
12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL
retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Length Image Tokenization via Recurrent Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Duggal, Phillip Isola, Antonio Torralba, William T. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision systems typically assign fixed-length representations to
images, regardless of the information content. This contrasts with human
intelligence - and even large language models - which allocate varying
representational capacities based on entropy, context and familiarity. Inspired
by this, we propose an approach to learn variable-length token representations
for 2D images. Our encoder-decoder architecture recursively processes 2D image
tokens, distilling them into 1D latent tokens over multiple iterations of
recurrent rollouts. Each iteration refines the 2D tokens, updates the existing
1D latent tokens, and adaptively increases representational capacity by adding
new tokens. This enables compression of images into a variable number of
tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction
loss and FID metrics, demonstrating that token count aligns with image entropy,
familiarity and downstream task requirements. Recurrent token processing with
increasing representational capacity in each iteration shows signs of token
specialization, revealing potential for object / part discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at: https://github.com/ShivamDuggal4/adaptive-length-tokenizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Causal Bandits: Unknown Graph and Soft Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Yan, Ali Tajer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing causal bandit algorithms depends on two central categories of
assumptions: (i) the extent of information about the underlying causal graphs
and (ii) the extent of information about interventional statistical models.
There have been extensive recent advances in dispensing with assumptions on
either category. These include assuming known graphs but unknown interventional
distributions, and the converse setting of assuming unknown graphs but access
to restrictive hard/$\operatorname{do}$ interventions, which removes the
stochasticity and ancestral dependencies. Nevertheless, the problem in its
general form, i.e., unknown graph and unknown stochastic intervention models,
remains open. This paper addresses this problem and establishes that in a graph
with $N$ nodes, maximum in-degree $d$ and maximum causal path length $L$, after
$T$ interaction rounds the regret upper bound scales as
$\tilde{\mathcal{O}}((cd)^{L-\frac{1}{2}}\sqrt{T} + d + RN)$ where $c>1$ is a
constant and $R$ is a measure of intervention power. A universal minimax lower
bound is also established, which scales as $\Omega(d^{L-\frac{3}{2}}\sqrt{T})$.
Importantly, the graph size $N$ has a diminishing effect on the regret as $T$
grows. These bounds have matching behavior in $T$, exponential dependence on
$L$, and polynomial dependence on $d$ (with the gap $d\ $). On the algorithmic
aspect, the paper presents a novel way of designing a computationally efficient
CB algorithm, addressing a challenge that the existing CB algorithms using soft
interventions face.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning General-Purpose Biomedical Volume Representations using
  Randomized Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current volumetric biomedical foundation models struggle to generalize as
public 3D datasets are small and do not cover the broad diversity of medical
procedures, conditions, anatomical regions, and imaging protocols. We address
this by creating a representation learning method that instead anticipates
strong domain shifts at training time itself. We first propose a data engine
that synthesizes highly variable training samples that enable generalization to
new biomedical contexts. To then train a single 3D network for any voxel-level
task, we develop a contrastive learning method that pretrains the network to be
stable against nuisance imaging variation simulated by the data engine, a key
inductive bias for generalization. This network's features can be used as
robust representations of input images for downstream tasks and its weights
provide a strong, dataset-agnostic initialization for finetuning on new
datasets. As a result, we set new standards across both multimodality
registration and few-shot segmentation, a first for any 3D biomedical vision
model, all without (pre-)training on any existing dataset of real images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and model weights available at
  https://github.com/neel-dey/anatomix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for
  Efficient Robot Execution <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yue, Yulin Wang, Bingyi Kang, Yizeng Han, Shenzhi Wang, Shiji Song, Jiashi Feng, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MLLMs have demonstrated remarkable comprehension and reasoning capabilities
with complex language and visual data. These advances have spurred the vision
of establishing a generalist robotic MLLM proficient in understanding complex
human instructions and accomplishing various embodied tasks. However,
developing MLLMs for real-world robots is challenging due to the typically
limited computation and memory capacities available on robotic platforms. In
contrast, the inference of MLLMs involves storing billions of parameters and
performing tremendous computation, imposing significant hardware demands. In
our paper, we propose a Dynamic Early-Exit Framework for Robotic
Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically
adjusts the size of the activated MLLM based on each situation at hand. The
approach leverages a multi-exit architecture in MLLMs, which allows the model
to terminate processing once a proper size of the model has been activated for
a specific situation, thus avoiding further redundant computation.
Additionally, we develop novel algorithms that establish early-termination
criteria for DeeR, conditioned on predefined demands such as average
computational cost (i.e., power consumption), as well as peak computational
consumption (i.e., latency) and GPU memory usage. These enhancements ensure
that DeeR operates efficiently under varying resource constraints while
maintaining competitive performance. On the CALVIN robot manipulation
benchmark, DeeR demonstrates significant reductions in computational costs of
LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.
Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 figures, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eldar Kurtic, Alexandre Marques, Shubhra Pandit, Mark Kurtz, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the popularity of large language model (LLM) quantization for
inference acceleration, significant uncertainty remains regarding the
accuracy-performance trade-offs associated with various quantization formats.
We present a comprehensive empirical study of quantized accuracy, evaluating
popular quantization formats (FP8, INT8, INT4) across academic benchmarks and
real-world tasks, on the entire Llama-3.1 model family. Additionally, our study
examines the difference in text generated by quantized models versus their
uncompressed counterparts. Beyond benchmarks, we also present a couple of
quantization improvements which allowed us to obtain state-of-the-art accuracy
recovery results. Our investigation, encompassing over 500,000 individual
evaluations, yields several key findings: (1) FP8 weight and activation
quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and
activation quantization (W8A8-INT), when properly tuned, incurs surprisingly
low 1-3% accuracy degradation, and (3) INT4 weight-only quantization
(W4A16-INT) is competitive with 8-bit integer weight and activation
quantization. To address the question of the "best" format for a given
deployment environment, we conduct inference performance analysis using the
popular open-source vLLM framework on various GPU architectures. We find that
W4A16 offers the best cost-efficiency for synchronous deployments, and for
asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel
in asynchronous "continuous batching" deployment of mid- and large-size models
on high-end GPUs. Our results provide a set of practical guidelines for
deploying quantized LLMs across scales and performance requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physically Based Neural Bidirectional Reflectance Distribution Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenliang Zhou, Alejandro Sztrajman, Gilles Rainer, Fangcheng Zhong, Fazilet Gokbudak, Zhilin Guo, Weihao Xia, Rafal Mantiuk, Cengiz Oztireli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the physically based neural bidirectional reflectance
distribution function (PBNBRDF), a novel, continuous representation for
material appearance based on neural fields. Our model accurately reconstructs
real-world materials while uniquely enforcing physical properties for realistic
BRDFs, specifically Helmholtz reciprocity via reparametrization and energy
passivity via efficient analytical integration. We conduct a systematic
analysis demonstrating the benefits of adhering to these physical laws on the
visual quality of reconstructed materials. Additionally, we enhance the color
accuracy of neural BRDFs by introducing chromaticity enforcement supervising
the norms of RGB channels. Through both qualitative and quantitative
experiments on multiple databases of measured real-world BRDFs, we show that
adhering to these physical constraints enables neural fields to more faithfully
and stably represent the original data and achieve higher rendering quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Seq-VCR: Preventing Collapse in Intermediate <span class="highlight-title">Transformer</span> Representations
  for Enhanced Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, <span class="highlight-author">Yann LeCun</span>, Irina Rish, Ravid Shwartz-Ziv, Christopher Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoder-only Transformers often struggle with complex reasoning tasks,
particularly arithmetic reasoning requiring multiple sequential operations. In
this work, we identify representation collapse in the model's intermediate
layers as a key factor limiting their reasoning capabilities. To address this,
we propose Sequential Variance-Covariance Regularization (Seq-VCR), which
enhances the entropy of intermediate representations and prevents collapse.
Combined with dummy pause tokens as substitutes for chain-of-thought (CoT)
tokens, our method significantly improves performance in arithmetic reasoning
problems. In the challenging $5 \times 5$ integer multiplication task, our
approach achieves $99.5\%$ exact match accuracy, outperforming models of the
same size (which yield $0\%$ accuracy) and GPT-4 with five-shot CoT prompting
($44\%$). We also demonstrate superior results on arithmetic expression and
longest increasing subsequence (LIS) datasets. Our findings highlight the
importance of preventing intermediate layer representation collapse to enhance
the reasoning capabilities of Transformers and show that Seq-VCR offers an
effective solution without requiring explicit CoT supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boulder2Vec: Modeling Climber Performances in Professional Bouldering
  Competitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Baron, Victor Hau, Zeke Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using data from professional bouldering competitions from 2008 to 2022, we
train a logistic regression to predict climber results and measure climber
skill. However, this approach is limited, as a single numeric coefficient per
climber cannot adequately capture the intricacies of climbers' varying
strengths and weaknesses in different boulder problems. For example, some
climbers might prefer more static, technical routes while other climbers may
specialize in powerful, dynamic problems.
  To this end, we apply Probabilistic Matrix Factorization (PMF), a framework
commonly used in recommender systems, to represent the unique characteristics
of climbers and problems with latent, multi-dimensional vectors. In this
framework, a climber's performance on a given problem is predicted by taking
the dot product of the corresponding climber vector and problem vectors. PMF
effectively handles sparse datasets, such as our dataset where only a subset of
climbers attempt each particular problem, by extrapolating patterns from
similar climbers.
  We contrast the empirical performance of PMF to the logistic regression
approach and investigate the multivariate representations produced by PMF to
gain insights into climber characteristics. Our results show that the
multivariate PMF representations improve predictive performance of professional
bouldering competitions by capturing both the overall strength of climbers and
their specialized skill sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparsing Law: Towards Large Language Models with Greater Activation
  Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed
  Acyclic Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mufei Li, Viraj Shitole, Eli Chien, Changhai Man, Zhaodong Wang, Srinivas Sridharan, Ying Zhang, Tushar Krishna, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directed acyclic graphs (DAGs) serve as crucial data representations in
domains such as hardware synthesis and compiler/program optimization for
computing systems. DAG generative models facilitate the creation of synthetic
DAGs, which can be used for benchmarking computing systems while preserving
intellectual property. However, generating realistic DAGs is challenging due to
their inherent directional and logical dependencies. This paper introduces
LayerDAG, an autoregressive diffusion model, to address these challenges.
LayerDAG decouples the strong node dependencies into manageable units that can
be processed sequentially. By interpreting the partial order of nodes as a
sequence of bipartite graphs, LayerDAG leverages autoregressive generation to
model directional dependencies and employs diffusion models to capture logical
dependencies within each bipartite graph. Comparative analyses demonstrate that
LayerDAG outperforms existing DAG generative models in both expressiveness and
generalization, particularly for generating large-scale DAGs with up to 400
nodes-a critical scenario for system benchmarking. Extensive experiments on
both synthetic and real-world flow graphs from various computing platforms show
that LayerDAG generates valid DAGs with superior statistical properties and
benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the
training of ML-based surrogate models, resulting in improved accuracy in
predicting performance metrics of real-world DAGs across diverse computing
platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/Graph-COM/LayerDAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defining and Evaluating Physical Safety for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Chen Tang, Pin-Yu Chen, Tsung-Yi Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used to control robotic systems
such as drones, but their risks of causing physical threats and harm in
real-world applications remain unexplored. Our study addresses the critical gap
in evaluating LLM physical safety by developing a comprehensive benchmark for
drone control. We classify the physical safety risks of drones into four
categories: (1) human-targeted threats, (2) object-targeted threats, (3)
infrastructure attacks, and (4) regulatory violations. Our evaluation of
mainstream LLMs reveals an undesirable trade-off between utility and safety,
with models that excel in code generation often performing poorly in crucial
safety aspects. Furthermore, while incorporating advanced prompt engineering
techniques such as In-Context Learning and Chain-of-Thought can improve safety,
these methods still struggle to identify unintentional attacks. In addition,
larger models demonstrate better safety capabilities, particularly in refusing
dangerous commands. Our findings and benchmark can facilitate the design and
evaluation of physical safety for LLMs. The project page is available at
huggingface.co/spaces/TrustSafeAI/LLM-physical-safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information plane and compression-gnostic feedback in quantum machine
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Haboury, Mo Kordzanganeh, Alexey Melnikov, Pavel Sekatski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The information plane (Tishby et al. arXiv:physics/0004057, Shwartz-Ziv et
al. arXiv:1703.00810) has been proposed as an analytical tool for studying the
learning dynamics of neural networks. It provides quantitative insight on how
the model approaches the learned state by approximating a minimal sufficient
statistics. In this paper we extend this tool to the domain of quantum learning
models. In a second step, we study how the insight on how much the model
compresses the input data (provided by the information plane) can be used to
improve a learning algorithm. Specifically, we consider two ways to do so: via
a multiplicative regularization of the loss function, or with a
compression-gnostic scheduler of the learning rate (for algorithms based on
gradient descent). Both ways turn out to be equivalent in our implementation.
Finally, we benchmark the proposed learning algorithms on several
classification and regression tasks using variational quantum circuits. The
results demonstrate an improvement in test accuracy and convergence speed for
both synthetic and real-world datasets. Additionally, with one example we
analyzed the impact of the proposed modifications on the performances of neural
networks in a classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nash Equilibria via Stochastic Eigendecomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Gemp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel set of techniques for approximating a Nash
equilibrium in a finite, normal-form game. It achieves this by constructing a
new reformulation as solving a parameterized system of multivariate polynomials
with tunable complexity. In doing so, it forges an itinerant loop from game
theory to machine learning and back. We show a Nash equilibrium can be
approximated with purely calls to stochastic, iterative variants of singular
value decomposition and power iteration, with implications for biological
plausibility. We provide pseudocode and experiments demonstrating solving for
all equilibria of a general-sum game using only these readily available linear
algebra tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Targeted Manipulation and Deception Emerge when Optimizing LLMs for User
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcus Williams, Micah Carroll, Adhyyan Narang, Constantin Weisser, Brendan Murphy, Anca Dragan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs become more widely deployed, there is increasing interest in directly
optimizing for feedback from end users (e.g. thumbs up) in addition to feedback
from paid annotators. However, training to maximize human feedback creates a
perverse incentive structure for the AI to resort to manipulative tactics to
obtain positive feedback, and some users may be especially vulnerable to such
tactics. We study this phenomenon by training LLMs with Reinforcement Learning
with simulated user feedback. We have three main findings: 1) Extreme forms of
"feedback gaming" such as manipulation and deception can reliably emerge in
domains of practical LLM usage; 2) Concerningly, even if only <2% of users are
vulnerable to manipulative strategies, LLMs learn to identify and surgically
target them while behaving appropriately with other users, making such
behaviors harder to detect; 3 To mitigate this issue, it may seem promising to
leverage continued safety training or LLM-as-judges during training to filter
problematic outputs. To our surprise, we found that while such approaches help
in some settings, they backfire in others, leading to the emergence of subtler
problematic behaviors that would also fool the LLM judges. Our findings serve
as a cautionary tale, highlighting the risks of using gameable feedback sources
-- such as user feedback -- as a target for RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grouped Discrete Representation for Object-Centric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-Centric Learning (OCL) can discover objects in images or videos by
simply reconstructing the input. For better object discovery, representative
OCL methods reconstruct the input as its Variational Autoencoder (VAE)
intermediate representation, which suppresses pixel noises and promotes object
separability by discretizing continuous super-pixels with template features.
However, treating features as units overlooks their composing attributes, thus
impeding model generalization; indexing features with scalar numbers loses
attribute-level similarities and differences, thus hindering model convergence.
We propose \textit{Grouped Discrete Representation} (GDR) for OCL. We decompose
features into combinatorial attributes via organized channel grouping, and
compose these attributes into discrete representation via tuple indexes.
Experiments show that our GDR improves both Transformer- and Diffusion-based
OCL methods consistently on various datasets. Visualizations show that our GDR
captures better object separability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-Efficient Private Learning of Mixtures of Gaussians <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Ashtiani, Mahbod Majid, Shyam Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of learning mixtures of Gaussians with approximate
differential privacy. We prove that roughly $kd^2 + k^{1.5} d^{1.75} + k^2 d$
samples suffice to learn a mixture of $k$ arbitrary $d$-dimensional Gaussians
up to low total variation distance, with differential privacy. Our work
improves over the previous best result [AAL24b] (which required roughly $k^2
d^4$ samples) and is provably optimal when $d$ is much larger than $k^2$.
Moreover, we give the first optimal bound for privately learning mixtures of
$k$ univariate (i.e., $1$-dimensional) Gaussians. Importantly, we show that the
sample complexity for privately learning mixtures of univariate Gaussians is
linear in the number of components $k$, whereas the previous best sample
complexity [AAL21] was quadratic in $k$. Our algorithms utilize various
techniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23],
sample compression for distributions [ABDH+20], and methods for bounding
volumes of sumsets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages. To appear in Neural Information Processing Systems
  (NeurIPS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed
  Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Mei, Dongzhe Zheng, Shihua Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can
process data without the limitation of time intervals. They have advantages in
learning and understanding the evolution of complex real dynamics. Many
previous works have focused on NODEs in concise forms, while numerous physical
systems taking straightforward forms, in fact, belong to their more complex
quasi-classes, thus appealing to a class of general NODEs with high scalability
and flexibility to model those systems. This, however, may result in intricate
nonlinear properties. In this paper, we introduce ControlSynth Neural ODEs
(CSODEs). We show that despite their highly nonlinear nature, convergence can
be guaranteed via tractable linear inequalities. In the composition of CSODEs,
we introduce an extra control term for learning the potential simultaneous
capture of dynamics at different scales, which could be particularly useful for
partial differential equation-formulated systems. Finally, we compare several
representative NNs with CSODEs on important physical dynamics under the
inductive biases of CSODEs, and illustrate that CSODEs have better learning and
predictive abilities in these settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated GNNs for EEG-Based Stroke Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Protani, Lorenzo Giusti, Albert Sund Aillet, Simona Sacco, Paolo Manganotti, Lucio Marinelli, Diogo Reis Santos, Pierpaolo Brutti, Pietro Caliandro, Luigi Serio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) has the potential to become an essential tool in
supporting clinical decision-making processes, offering enhanced diagnostic
capabilities and personalized treatment plans. However, outsourcing medical
records to train ML models using patient data raises legal, privacy, and
security concerns. Federated learning has emerged as a promising paradigm for
collaborative ML, meeting healthcare institutions' requirements for robust
models without sharing sensitive data and compromising patient privacy. This
study proposes a novel method that combines federated learning (FL) and Graph
Neural Networks (GNNs) to predict stroke severity using electroencephalography
(EEG) signals across multiple medical institutions. Our approach enables
multiple hospitals to jointly train a shared GNN model on their local EEG data
without exchanging patient information. Specifically, we address a regression
problem by predicting the National Institutes of Health Stroke Scale (NIHSS), a
key indicator of stroke severity. The proposed model leverages a masked
self-attention mechanism to capture salient brain connectivity patterns and
employs EdgeSHAP to provide post-hoc explanations of the neurological states
after a stroke. We evaluated our method on EEG recordings from four
institutions, achieving a mean absolute error (MAE) of 3.23 in predicting
NIHSS, close to the average error made by human experts (MAE $\approx$ 3.0).
This demonstrates the method's effectiveness in providing accurate and
explainable predictions while maintaining data privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, Proceedings of the II edition of the Workshop on
  Unifying Representations in Neural Models (UniReps 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal-in-the-Loop for Learning with Imbalanced Noisy Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Brandon Graham-Knight, Jamil Fayyad, Nourhan Bayasi, Patricia Lasserre, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance and label noise are pervasive in large-scale datasets, yet
much of machine learning research assumes well-labeled, balanced data, which
rarely reflects real world conditions. Existing approaches typically address
either label noise or class imbalance in isolation, leading to suboptimal
results when both issues coexist. In this work, we propose
Conformal-in-the-Loop (CitL), a novel training framework that addresses both
challenges with a conformal prediction-based approach. CitL evaluates sample
uncertainty to adjust weights and prune unreliable examples, enhancing model
resilience and accuracy with minimal computational cost. Our extensive
experiments include a detailed analysis showing how CitL effectively emphasizes
impactful data in noisy, imbalanced datasets. Our results show that CitL
consistently boosts model performance, achieving up to a 6.1% increase in
classification accuracy and a 5.0 mIoU improvement in segmentation. Our code is
publicly available: CitL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The LLM Language Network: A Neuroscientific Approach for Identifying
  Causally Task-Relevant Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable capabilities on not just
language tasks, but also various tasks that are not linguistic in nature, such
as logical reasoning and social inference. In the human brain, neuroscience has
identified a core language system that selectively and causally supports
language processing. We here ask whether similar specialization for language
emerges in LLMs. We identify language-selective units within 18 popular LLMs,
using the same localization approach that is used in neuroscience. We then
establish the causal role of these units by demonstrating that ablating LLM
language-selective units -- but not random units -- leads to drastic deficits
in language tasks. Correspondingly, language-selective LLM units are more
aligned to brain recordings from the human language system than random units.
Finally, we investigate whether our localization method extends to other
cognitive domains: while we find specialized networks in some LLMs for
reasoning and social capabilities, there are substantial differences among
models. These findings provide functional and causal evidence for
specialization in large language models, and highlight parallels with the
functional organization in the brain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELU-GCN: Effectively Label-Utilizing Graph Convolutional Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Huang, Yujie Mo, Xiaoshuang Shi, Lei Feng, Xiaofeng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The message-passing mechanism of graph convolutional networks (i.e., GCNs)
enables label information to be propagated to a broader range of neighbors,
thereby increasing the utilization of labels. However, the label information is
not always effectively utilized in the traditional GCN framework. To address
this issue, we propose a new two-step framework called ELU-GCN. In the first
stage, ELU-GCN conducts graph learning to learn a new graph structure (\ie
ELU-graph), which enables GCNs to effectively utilize label information. In the
second stage, we design a new graph contrastive learning on the GCN framework
for representation learning by exploring the consistency and mutually exclusive
information between the learned ELU graph and the original graph. Moreover, we
theoretically demonstrate that the proposed method can ensure the
generalization ability of GCNs. Extensive experiments validate the superiority
of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Reclustering Barrier in Centroid-based Deep Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Miklautz, Timo Klein, Kevin Sidak, Collin Leiber, Thomas Lang, Andrii Shkabrii, Sebastian Tschiatschek, Claudia Plant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates an important phenomenon in centroid-based deep
clustering (DC) algorithms: Performance quickly saturates after a period of
rapid early gains. Practitioners commonly address early saturation with
periodic reclustering, which we demonstrate to be insufficient to address
performance plateaus. We call this phenomenon the "reclustering barrier" and
empirically show when the reclustering barrier occurs, what its underlying
mechanisms are, and how it is possible to Break the Reclustering Barrier with
our algorithm BRB. BRB avoids early over-commitment to initial clusterings and
enables continuous adaptation to reinitialized clustering targets while
remaining conceptually simple. Applying our algorithm to widely-used
centroid-based DC algorithms, we show that (1) BRB consistently improves
performance across a wide range of clustering benchmarks, (2) BRB enables
training from scratch, and (3) BRB performs competitively against
state-of-the-art DC algorithms when combined with a contrastive loss. We
release our code and pre-trained models at
https://github.com/Probabilistic-and-Interactive-ML/breaking-the-reclustering-barrier .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Induction and Transduction for Abstract Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When learning an input-output mapping from very few examples, is it better to
first infer a latent function that explains the examples, or is it better to
directly predict new test outputs, e.g. using a neural network? We study this
question on ARC, a highly diverse dataset of abstract reasoning tasks. We train
neural models for induction (inferring latent functions) and transduction
(directly predicting the test output for a given test input). Our models are
trained on synthetic data generated by prompting LLMs to produce Python code
specifying a function to be inferred, plus a stochastic subroutine for
generating inputs to that function. We find inductive and transductive models
solve very different problems, despite training on the same problems, and
despite sharing the same neural architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Utilization of Unique Node Identifiers in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Bechler-Speicher, Moshe Eliasof, Carola-Bibiane Schönlieb, Ran Gilad-Bachrach, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks have inherent representational limitations due to their
message-passing structure. Recent work has suggested that these limitations can
be overcome by using unique node identifiers (UIDs). Here we argue that despite
the advantages of UIDs, one of their disadvantages is that they lose the
desirable property of permutation-equivariance. We thus propose to focus on UID
models that are permutation-equivariant, and present theoretical arguments for
their advantages. Motivated by this, we propose a method to regularize UID
models towards permutation equivariance, via a contrastive loss. We empirically
demonstrate that our approach improves generalization and extrapolation
abilities while providing faster training convergence. On the recent BREC
expressiveness benchmark, our proposed method achieves state-of-the-art
performance compared to other random-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Explanations via Riemannian Latent Space Traversal <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paraskevas Pegios, Aasa Feragen, Andreas Abildtrup Hansen, Georgios Arvanitidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of increasingly complex deep models has fueled an urgent need
for insight into how these models make predictions. Counterfactual explanations
form a powerful tool for providing actionable explanations to practitioners.
Previously, counterfactual explanation methods have been designed by traversing
the latent space of generative models. Yet, these latent spaces are usually
greatly simplified, with most of the data distribution complexity contained in
the decoder rather than the latent embedding. Thus, traversing the latent space
naively without taking the nonlinear decoder into account can lead to unnatural
counterfactual trajectories. We introduce counterfactual explanations obtained
using a Riemannian metric pulled back via the decoder and the classifier under
scrutiny. This metric encodes information about the complex geometric structure
of the data and the learned representation, enabling us to obtain robust
counterfactual trajectories with high fidelity, as demonstrated by our
experiments in real-world tabular datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Workshop NeurReps</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards safe Bayesian optimization with Wiener kernel regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksii Molodchyk, Johannes Teutsch, Timm Faulwasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Optimization (BO) is a data-driven strategy for
minimizing/maximizing black-box functions based on probabilistic surrogate
models. In the presence of safety constraints, the performance of BO crucially
relies on tight probabilistic error bounds related to the uncertainty
surrounding the surrogate model. For the case of Gaussian Process surrogates
and Gaussian measurement noise, we present a novel error bound based on the
recently proposed Wiener kernel regression. We prove that under rather mild
assumptions, the proposed error bound is tighter than bounds previously
documented in the literature which leads to enlarged safety regions. We draw
upon a numerical example to demonstrate the efficacy of the proposed error
bound in safe BO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variable Selection in Convex Piecewise Linear Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitham Kanj, Seonho Kim, Kiryung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Sparse Gradient Descent as a solution for variable
selection in convex piecewise linear regression where the model is given as
$\mathrm{max}\langle a_j^\star, x \rangle + b_j^\star$ for $j = 1,\dots,k$
where $x \in \mathbb R^d$ is the covariate vector. Here,
$\{a_j^\star\}_{j=1}^k$ and $\{b_j^\star\}_{j=1}^k$ denote the ground-truth
weight vectors and intercepts. A non-asymptotic local convergence analysis is
provided for Sp-GD under sub-Gaussian noise when the covariate distribution
satisfies sub-Gaussianity and anti-concentration property. When the model order
and parameters are fixed, Sp-GD provides an $\epsilon$-accurate estimate given
$\mathcal{O}(\max(\epsilon^{-2}\sigma_z^2,1)s\log(d/s))$ observations where
$\sigma_z^2$ denotes the noise variance. This also implies the exact parameter
recovery by Sp-GD from $\mathcal{O}(s\log(d/s))$ noise-free observations. Since
optimizing the squared loss for sparse max-affine is non-convex, an
initialization scheme is proposed to provide a suitable initial estimate within
the basin of attraction for Sp-GD, i.e. sufficiently accurate to invoke the
convergence guarantees. The initialization scheme uses sparse principal
component analysis to estimate the subspace spanned by $\{ a_j^\star\}_{j=1}^k$
then applies an $r$-covering search to estimate the model parameters. A
non-asymptotic analysis is presented for this initialization scheme when the
covariates and noise samples follow Gaussian distributions. When the model
order and parameters are fixed, this initialization scheme provides an
$\epsilon$-accurate estimate given
$\mathcal{O}(\epsilon^{-2}\max(\sigma_z^4,\sigma_z^2,1)s^2\log^4(d))$
observations. Numerical Monte Carlo results corroborate theoretical findings
for Sp-GD and the initialization scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting the Temperature-Dependent CMC of Surfactant Mixtures with
  Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoforos Brozos, Jan G. Rittig, Sandip Bhattacharya, Elie Akanny, Christina Kohlmann, Alexander Mitsos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surfactants are key ingredients in foaming and cleansing products across
various industries such as personal and home care, industrial cleaning, and
more, with the critical micelle concentration (CMC) being of major interest.
Predictive models for CMC of pure surfactants have been developed based on
recent ML methods, however, in practice surfactant mixtures are typically used
due to to performance, environmental, and cost reasons. This requires
accounting for synergistic/antagonistic interactions between surfactants;
however, predictive ML models for a wide spectrum of mixtures are missing so
far. Herein, we develop a graph neural network (GNN) framework for surfactant
mixtures to predict the temperature-dependent CMC. We collect data for 108
surfactant binary mixtures, to which we add data for pure species from our
previous work [Brozos et al. (2024), J. Chem. Theory Comput.]. We then develop
and train GNNs and evaluate their accuracy across different prediction test
scenarios for binary mixtures relevant to practical applications. The final GNN
models demonstrate very high predictive performance when interpolating between
different mixture compositions and for new binary mixtures with known species.
Extrapolation to binary surfactant mixtures where either one or both surfactant
species are not seen before, yields accurate results for the majority of
surfactant systems. We further find superior accuracy of the GNN over a
semi-empirical model based on activity coefficients, which has been widely used
to date. We then explore if GNN models trained solely on binary mixture and
pure species data can also accurately predict the CMCs of ternary mixtures.
Finally, we experimentally measure the CMC of 4 commercial surfactants that
contain up to four species and industrial relevant mixtures and find a very
good agreement between measured and predicted CMC values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Positive Experience Reflection for Agents in Interactive Text
  Environments <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs T. J. Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent agents designed for interactive environments face significant
challenges in text-based games, a domain that demands complex reasoning and
adaptability. While agents based on large language models (LLMs) using
self-reflection have shown promise, they struggle when initially successful and
exhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,
a novel approach that addresses these limitations in existing reflection
methods by incorporating positive experiences and managed memory to enrich the
context available to the agent at decision time. Our comprehensive analysis
spans both closed- and open-source LLMs and demonstrates the effectiveness of
Sweet&Sour in improving agent performance, particularly in scenarios where
previous approaches fall short.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2024 Language Gamification workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Targeted Learning for Variable Importance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Wang, Yunzhe Zhou, Giles Hooker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variable importance is one of the most widely used measures for interpreting
machine learning with significant interest from both statistics and machine
learning communities. Recently, increasing attention has been directed toward
uncertainty quantification in these metrics. Current approaches largely rely on
one-step procedures, which, while asymptotically efficient, can present higher
sensitivity and instability in finite sample settings. To address these
limitations, we propose a novel method by employing the targeted learning (TL)
framework, designed to enhance robustness in inference for variable importance
metrics. Our approach is particularly suited for conditional permutation
variable importance. We show that it (i) retains the asymptotic efficiency of
traditional methods, (ii) maintains comparable computational complexity, and
(iii) delivers improved accuracy, especially in finite sample contexts. We
further support these findings with numerical experiments that illustrate the
practical advantages of our method and validate the theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIRA: Scalable Inter-frame Relation and Association for Radar Perception <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Yataka, Pu Perry Wang, Petros Boufounos, Ryuhei Takahashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional radar feature extraction faces limitations due to low spatial
resolution, noise, multipath reflection, the presence of ghost targets, and
motion blur. Such limitations can be exacerbated by nonlinear object motion,
particularly from an ego-centric viewpoint. It becomes evident that to address
these challenges, the key lies in exploiting temporal feature relation over an
extended horizon and enforcing spatial motion consistency for effective
association. To this end, this paper proposes SIRA (Scalable Inter-frame
Relation and Association) with two designs. First, inspired by Swin
Transformer, we introduce extended temporal relation, generalizing the existing
temporal relation layer from two consecutive frames to multiple inter-frames
with temporally regrouped window attention for scalability. Second, we propose
motion consistency track with the concept of a pseudo-tracklet generated from
observational data for better trajectory prediction and subsequent object
association. Our approach achieves 58.11 mAP@0.5 for oriented object detection
and 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing
previous state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recursive Learning of Asymptotic Variational Objectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Mastrototaro, Mathias Müller, Jimmy Olsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General state-space models (SSMs) are widely used in statistical machine
learning and are among the most classical generative models for sequential
time-series data. SSMs, comprising latent Markovian states, can be subjected to
variational inference (VI), but standard VI methods like the
importance-weighted autoencoder (IWAE) lack functionality for streaming data.
To enable online VI in SSMs when the observations are received in real time, we
propose maximising an IWAE-type variational lower bound on the asymptotic
contrast function, rather than the standard IWAE ELBO, using stochastic
approximation. Unlike the recursive maximum likelihood method, which directly
maximises the asymptotic contrast, our approach, called online sequential IWAE
(OSIWAE), allows for online learning of both model parameters and a Markovian
recognition model for inferring latent states. By approximating filter state
posteriors and their derivatives using sequential Monte Carlo (SMC) methods, we
create a particle-based framework for online VI in SSMs. This approach is more
theoretically well-founded than recently proposed online variational SMC
methods. We provide rigorous theoretical results on the learning objective and
a numerical study demonstrating the method's efficiency in learning model
parameters and particle proposal kernels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collective Model Intelligence Requires Compatible Specialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jyothish Pari, Samy Jelassi, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore the limitations of combining models by averaging
intermediate features, referred to as model merging, and propose a new
direction for achieving collective model intelligence through what we call
compatible specialization. Current methods for model merging, such as parameter
and feature averaging, struggle to effectively combine specialized models due
to representational divergence during fine-tuning. As models specialize to
their individual domains, their internal feature representations become
increasingly incompatible, leading to poor performance when attempting to merge
them for new tasks. We analyze this phenomenon using centered kernel alignment
(CKA) and show that as models specialize, the similarity in their feature space
structure diminishes, hindering their capacity for collective use. To address
these challenges, we investigate routing-based merging strategies, which offer
more flexible methods for combining specialized models by dynamically routing
across different layers. This allows us to improve on existing methods by
combining features from multiple layers rather than relying on fixed,
layer-wise combinations. However, we find that these approaches still face
limitations when layers within models are representationally incompatible. Our
findings highlight the importance of designing new approaches for model merging
that operate on well-defined input and output spaces, similar to how humans
communicate through language rather than intermediate neural activations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably <span class="highlight-title">Transformer</span>s Harness Multi-Concept Word Semantics for Efficient
  In-Context Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Taiji Suzuki, Qingfu Zhang, Hau-San Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large language models (LLMs) have displayed remarkable
creative prowess and emergence capabilities. Existing empirical studies have
revealed a strong connection between these LLMs' impressive emergence abilities
and their in-context learning (ICL) capacity, allowing them to solve new tasks
using only task-specific prompts without further fine-tuning. On the other
hand, existing empirical and theoretical studies also show that there is a
linear regularity of the multi-concept encoded semantic representation behind
transformer-based LLMs. However, existing theoretical work fail to build up an
understanding of the connection between this regularity and the innovative
power of ICL. Additionally, prior work often focuses on simplified, unrealistic
scenarios involving linear transformers or unrealistic loss functions, and they
achieve only linear or sub-linear convergence rates. In contrast, this work
provides a fine-grained mathematical analysis to show how transformers leverage
the multi-concept semantics of words to enable powerful ICL and excellent
out-of-distribution ICL abilities, offering insights into how transformers
innovate solutions for certain unseen tasks encoded with multiple cross-concept
semantics. Inspired by empirical studies on the linear latent geometry of LLMs,
the analysis is based on a concept-based low-noise sparse coding prompt model.
Leveraging advanced techniques, this work showcases the exponential 0-1 loss
convergence over the highly non-convex training dynamics, which pioneeringly
incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and
cross-entropy loss. Empirical simulations corroborate the theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metric properties of partial and robust Gromov-Wasserstein distances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannatul Chhoa, Michael Ivanitskiy, Fushuai Jiang, Shiying Li, Daniel McBride, Tom Needham, Kaiying O'Hare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Gromov-Wasserstein (GW) distances define a family of metrics, based on
ideas from optimal transport, which enable comparisons between probability
measures defined on distinct metric spaces. They are particularly useful in
areas such as network analysis and geometry processing, as computation of a GW
distance involves solving for registration between the objects which minimizes
geometric distortion. Although GW distances have proven useful for various
applications in the recent machine learning literature, it has been observed
that they are inherently sensitive to outlier noise and cannot accommodate
partial matching. This has been addressed by various constructions building on
the GW framework; in this article, we focus specifically on a natural
relaxation of the GW optimization problem, introduced by Chapel et al., which
is aimed at addressing exactly these shortcomings. Our goal is to understand
the theoretical properties of this relaxed optimization problem, from the
viewpoint of metric geometry. While the relaxed problem fails to induce a
metric, we derive precise characterizations of how it fails the axioms of
non-degeneracy and triangle inequality. These observations lead us to define a
novel family of distances, whose construction is inspired by the Prokhorov and
Ky Fan distances, as well as by the recent work of Raghvendra et al.\ on robust
versions of classical Wasserstein distance. We show that our new distances
define true metrics, that they induce the same topology as the GW distances,
and that they enjoy additional robustness to perturbations. These results
provide a mathematically rigorous basis for using our robust partial GW
distances in applications where outliers and partial matching are concerns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Steering Vectors by Targeting Sparse Autoencoder Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sviatoslav Chalnev, Matthew Siu, Arthur Conmy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To control the behavior of language models, steering methods attempt to
ensure that outputs of the model satisfy specific pre-defined properties.
Adding steering vectors to the model is a promising method of model control
that is easier than finetuning, and may be more robust than prompting. However,
it can be difficult to anticipate the effects of steering vectors produced by
almost all existing methods, such as CAA (Panickssery et al., 2024) or the
direct use of SAE latents (Templeton et al., 2024). In our work, we address
this issue by using SAEs to measure the effects of steering vectors, giving us
a method that can be used to understand the causal effect of any steering
vector intervention. We use this method for measuring causal effects to develop
an improved steering method, SAE-Targeted Steering (SAE-TS), which finds
steering vectors to target specific SAE features while minimizing unintended
side effects. We show that overall, SAE-TS balances steering effects with
coherence better than CAA and SAE feature steering, when evaluated on a range
of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 maintext pages and 9 appendix pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffSim2Real: Deploying Quadrupedal Locomotion Policies Purely Trained
  in Differentiable Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Bagajo, Clemens Schwarke, Victor Klemm, Ignat Georgiev, Jean-Pierre Sleiman, Jesus Tordesillas, Animesh Garg, Marco Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable simulators provide analytic gradients, enabling more
sample-efficient learning algorithms and paving the way for data intensive
learning tasks such as learning from images. In this work, we demonstrate that
locomotion policies trained with analytic gradients from a differentiable
simulator can be successfully transferred to the real world. Typically,
simulators that offer informative gradients lack the physical accuracy needed
for sim-to-real transfer, and vice-versa. A key factor in our success is a
smooth contact model that combines informative gradients with physical
accuracy, ensuring effective transfer of learned behaviors. To the best of our
knowledge, this is the first time a real quadrupedal robot is able to locomote
after training exclusively in a differentiable simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the CoRL 2024 Workshop 'Differentiable Optimization
  Everywhere'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double Descent Meets Out-of-Distribution Detection: Theoretical Insights
  and Empirical Analysis on the role of model complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouïn Ben Ammar, David Brellmann, Arturo Mendoza, Antoine Manzanera, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While overparameterization is known to benefit generalization, its impact on
Out-Of-Distribution (OOD) detection is less understood. This paper investigates
the influence of model complexity in OOD detection. We propose an expected OOD
risk metric to evaluate classifiers confidence on both training and OOD
samples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD
risk of binary least-squares classifiers applied to Gaussian data. We show that
the OOD risk depicts an infinite peak, when the number of parameters is equal
to the number of samples, which we associate with the double descent
phenomenon. Our experimental study on different OOD detection methods across
multiple neural architectures extends our theoretical insights and highlights a
double descent curve. Our observations suggest that overparameterization does
not necessarily lead to better OOD detection. Using the Neural Collapse
framework, we provide insights to better understand this behavior. To
facilitate reproducibility, our code will be made publicly available upon
publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vehicles, Pedestrians, and E-bikes: a Three-party Game at
  Right-turn-on-red Crossroads Revealing the Dual and Irrational Role of
  E-bikes that Risks Traffic Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangcheng Zhang, Yeshuo Shu, Keyi Liu, Yuxuan Wang, Donghang Li, Liyan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of e-bikes has facilitated short-distance travel yet led
to confusion and safety problems in road traffic. This study focuses on the
dual characteristics of e-bikes in traffic conflicts: they resemble pedestrians
when interacting with motor vehicles and behave like motor vehicles when in
conflict with pedestrians, which raises the right of way concerns when
potential conflicts are at stake. Using the Quantal Response Equilibrium model,
this research analyzes the behavioral choice differences of three groups of
road users (vehicle-pedestrian, vehicle-e-bike, e-bike-pedestrian) at
right-turn-on-red crossroads in right-turning lines and straight-going lines
conflict scenarios. The results show that the behavior of e-bikes is more
similar to that of motor vehicles than pedestrians overall, and their
interactions with either pedestrians or motor vehicles do not establish a
reasonable order, increasing the likelihood of confusion and conflict. In
contrast, a mutual understanding has developed between motor vehicles and
pedestrians, where motor vehicles tend to yield, and pedestrians tend to cross.
By clarifying the game theoretical model and introducing the rationality
parameter, this study precisely locates the role of e-bikes among road users,
which provides a reliable theoretical basis for optimizing traffic regulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed neural networks viewpoint for solving the
  Dyson-Schwinger equations of quantum electrodynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Carmo Terin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We employ physics-informed neural networks (PINNs) to solve fundamental
Dyson-Schwinger integral equations in the theory of quantum electrodynamics
(QED) in Euclidean space. Our approach uses neural networks to approximate the
fermion wave function renormalization, dynamical mass function, and photon
propagator. By integrating the Dyson-Schwinger equations into the loss
function, the networks learn and predict solutions over a range of momenta and
ultraviolet cutoff values. This method can be extended to other quantum field
theories (QFTs), potentially paving the way for forefront applications of
machine learning within high-level theoretical physics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning
  with <span class="highlight-title">Pre-Train</span>ed Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linglan Zhao, Xuerui Zhang, Ke Yan, Shouhong Ding, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning aims to incrementally acquire new concepts in data streams
while resisting forgetting previous knowledge. With the rise of powerful
pre-trained models (PTMs), there is a growing interest in training incremental
learning systems using these foundation models, rather than learning from
scratch. Existing works often view PTMs as a strong initial point and directly
apply parameter-efficient tuning (PET) in the first session for adapting to
downstream tasks. In the following sessions, most methods freeze model
parameters for tackling forgetting issues. However, applying PET directly to
downstream data cannot fully explore the inherent knowledge in PTMs.
Additionally, freezing the parameters in incremental sessions hinders models'
plasticity to novel concepts not covered in the first session. To solve the
above issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE)
framework. In particular, to inherit general knowledge from foundation models,
we include a transfer loss function by measuring the correlation between the
PTM and the PET-applied model. After calibrating in the first session, the slow
efficient tuning parameters can capture more informative features, improving
generalization to incoming classes. Moreover, to further incorporate novel
concepts, we strike a balance between stability and plasticity by fixing slow
efficient tuning parameters and continuously updating the fast ones.
Specifically, a cross-classification loss with feature alignment is proposed to
circumvent catastrophic forgetting. During inference, we introduce an
entropy-based aggregation strategy to dynamically utilize the complementarity
in the slow and fast learners. Extensive experiments on seven benchmark
datasets verify the effectiveness of our method by significantly surpassing the
state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behavioral Sequence Modeling with Ensemble Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Kawawa-Beaudan, Srijan Sood, Soham Palande, Ganapathy Mani, Tucker Balch, Manuela Veloso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the use of sequence analysis for behavior modeling,
emphasizing that sequential context often outweighs the value of aggregate
features in understanding human behavior. We discuss framing common problems in
fields like healthcare, finance, and e-commerce as sequence modeling tasks, and
address challenges related to constructing coherent sequences from fragmented
data and disentangling complex behavior patterns. We present a framework for
sequence modeling using Ensembles of Hidden Markov Models, which are
lightweight, interpretable, and efficient. Our ensemble-based scoring method
enables robust comparison across sequences of different lengths and enhances
performance in scenarios with imbalanced or scarce data. The framework scales
in real-world scenarios, is compatible with downstream feature-based modeling,
and is applicable in both supervised and unsupervised learning settings. We
demonstrate the effectiveness of our method with results on a longitudinal
human behavior dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do graph neural network states contain graph properties? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Pelletreau-Duris, Ruud van Bakel, Michael Cochez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning models achieve state-of-the-art performance on many tasks, but
this often requires increasingly large model sizes. Accordingly, the complexity
of their representations increase. Explainability techniques (XAI) have made
remarkable progress in the interpretability of ML models. However, the
non-relational nature of Graph Neural Networks (GNNs) make it difficult to
reuse already existing XAI methods. While other works have focused on
instance-based explanation methods for GNNs, very few have investigated
model-based methods and, to our knowledge, none have tried to probe the
embedding of the GNNs for well-known structural graph properties. In this paper
we present a model agnostic explainability pipeline for Graph Neural Networks
(GNNs) employing diagnostic classifiers. This pipeline aims to probe and
interpret the learned representations in GNNs across various architectures and
datasets, refining our understanding and trust in these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 22 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Multiple Initial Solutions to Optimization Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elad Sharony, Heng Yang, Tong Che, Marco Pavone, Shie Mannor, Peter Karkus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequentially solving similar optimization problems under strict runtime
constraints is essential for many applications, such as robot control,
autonomous driving, and portfolio management. The performance of local
optimization methods in these settings is sensitive to the initial solution:
poor initialization can lead to slow convergence or suboptimal solutions. To
address this challenge, we propose learning to predict \emph{multiple} diverse
initial solutions given parameters that define the problem instance. We
introduce two strategies for utilizing multiple initial solutions: (i) a
single-optimizer approach, where the most promising initial solution is chosen
using a selection function, and (ii) a multiple-optimizers approach, where
several optimizers, potentially run in parallel, are each initialized with a
different solution, with the best solution chosen afterward. We validate our
method on three optimal control benchmark tasks: cart-pole, reacher, and
autonomous driving, using different optimizers: DDP, MPPI, and iLQR. We find
significant and consistent improvement with our method across all evaluation
settings and demonstrate that it efficiently scales with the number of initial
solutions required. The code is available at
$\href{https://github.com/EladSharony/miso}{\tt{https://github.com/EladSharony/miso}}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedPID: An Aggregation Method for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Mächler, Gustav Grimberg, Ivan Ezhov, Manuel Nickel, Suprosanna Shit, David Naccache, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents FedPID, our submission to the Federated Tumor
Segmentation Challenge 2024 (FETS24). Inspired by FedCostWAvg and FedPIDAvg,
our winning contributions to FETS21 and FETS2022, we propose an improved
aggregation strategy for federated and collaborative learning. FedCostWAvg is a
method that averages results by considering both the number of training samples
in each group and how much the cost function decreased in the last round of
training. This is similar to how the derivative part of a PID controller works.
In FedPIDAvg, we also included the integral part that was missing. Another
challenge we faced were vastly differing dataset sizes at each center. We
solved this by assuming the sizes follow a Poisson distribution and adjusting
the training iterations for each center accordingly. Essentially, this part of
the method controls that outliers that require too much training time are less
frequently used. Based on these contributions we now adapted FedPIDAvg by
changing how the integral part is computed. Instead of integrating the loss
function we measure the global drop in cost since the first round.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative and Collaborative Multi-Task Semantic Communication for
  Distributed Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Halimi Razlighi, Maximilian H. V. Tillmann, Edgar Beck, Carsten Bockelmann, Armin Dekorsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore a multi-task semantic communication (SemCom) system
for distributed sources, extending the existing focus on collaborative
single-task execution. We build on the cooperative multi-task processing
introduced in [1], which divides the encoder into a common unit (CU) and
multiple specific units (SUs). While earlier studies in multi-task SemCom
focused on full observation settings, our research explores a more realistic
case where only distributed partial observations are available, such as in a
production line monitored by multiple sensing nodes. To address this, we
propose an SemCom system that supports multi-task processing through
cooperation on the transmitter side via split structure and collaboration on
the receiver side. We have used an information-theoretic perspective with
variational approximations for our end-to-end data-driven approach. Simulation
results demonstrate that the proposed cooperative and collaborative multi-task
(CCMT) SemCom system significantly improves task execution accuracy,
particularly in complex datasets, if the noise introduced from the
communication channel is not limiting the task performance too much. Our
findings contribute to a more general SemCom framework capable of handling
distributed sources and multiple tasks simultaneously, advancing the
applicability of SemCom systems in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Compute-Optimal Protein Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyi Cheng, Bo Chen, Pan Li, Jing Gong, Jie Tang, Le Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore optimally training protein language models, an area of significant
interest in biological research where guidance on best practices is limited.
Most models are trained with extensive compute resources until performance
gains plateau, focusing primarily on increasing model sizes rather than
optimizing the efficient compute frontier that balances performance and compute
budgets. Our investigation is grounded in a massive dataset consisting of 939
million protein sequences. We trained over 300 models ranging from 3.5 million
to 10.7 billion parameters on 5 to 200 billion unique tokens, to investigate
the relations between model sizes, training token numbers, and objectives.
First, we observed the effect of diminishing returns for the Causal Language
Model (CLM) and that of overfitting for the Masked Language Model~(MLM) when
repeating the commonly used Uniref database. To address this, we included
metagenomic protein sequences in the training set to increase the diversity and
avoid the plateau or overfitting effects. Second, we obtained the scaling laws
of CLM and MLM on Transformer, tailored to the specific characteristics of
protein sequence data. Third, we observe a transfer scaling phenomenon from CLM
to MLM, further demonstrating the effectiveness of transfer through scaling
behaviors based on estimated Effectively Transferred Tokens. Finally, to
validate our scaling laws, we compare the large-scale versions of ESM-2 and
PROGEN2 on downstream tasks, encompassing evaluations of protein generation as
well as structure- and function-related tasks, all within less or equivalent
pre-training compute budgets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (Spotlight); Code:
  https://github.com/cxysteven/ScalingProteinLM. Additional resources are
  available here</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical characterisation of the Gauss-Newton conditioning in Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jim Zhao, Sidak Pal Singh, Aurelien Lucchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Gauss-Newton (GN) matrix plays an important role in machine learning,
most evident in its use as a preconditioning matrix for a wide family of
popular adaptive methods to speed up optimization. Besides, it can also provide
key insights into the optimization landscape of neural networks. In the context
of deep neural networks, understanding the GN matrix involves studying the
interaction between different weight matrices as well as the dependencies
introduced by the data, thus rendering its analysis challenging. In this work,
we take a first step towards theoretically characterizing the conditioning of
the GN matrix in neural networks. We establish tight bounds on the condition
number of the GN in deep linear networks of arbitrary depth and width, which we
also extend to two-layer ReLU networks. We expand the analysis to further
architectural components, such as residual connections and convolutional
layers. Finally, we empirically validate the bounds and uncover valuable
insights into the influence of the analyzed architectural components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpecRaGE: Robust and Generalizable Multi-view Spectral Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amitai Yacobi, Ofir Lindenbaum, Uri Shaham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view representation learning (MvRL) has garnered substantial attention
in recent years, driven by the increasing demand for applications that can
effectively process and analyze data from multiple sources. In this context,
graph Laplacian-based MvRL methods have demonstrated remarkable success in
representing multi-view data. However, these methods often struggle with
generalization to new data and face challenges with scalability. Moreover, in
many practical scenarios, multi-view data is contaminated by noise or outliers.
In such cases, modern deep-learning-based MvRL approaches that rely on
alignment or contrastive objectives can lead to misleading results, as they may
impose incorrect consistency between clear and corrupted data sources. We
introduce $\textit{SpecRaGE}$, a novel fusion-based framework that integrates
the strengths of graph Laplacian methods with the power of deep learning to
overcome these challenges. SpecRage uses neural networks to learn parametric
mapping that approximates a joint diagonalization of graph Laplacians. This
solution bypasses the need for alignment while enabling generalizable and
scalable learning of informative and meaningful representations. Moreover, it
incorporates a meta-learning fusion module that dynamically adapts to data
quality, ensuring robustness against outliers and noisy views. Our extensive
experiments demonstrate that SpecRaGE outperforms state-of-the-art methods,
particularly in scenarios with data contamination, paving the way for more
reliable and efficient multi-view learning. Our code will be made publicly
available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finite-sample performance of the maximum likelihood estimator in
  logistic regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Chardon, Matthieu Lerasle, Jaouad Mourtada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logistic regression is a classical model for describing the probabilistic
dependence of binary responses to multivariate covariates. We consider the
predictive performance of the maximum likelihood estimator (MLE) for logistic
regression, assessed in terms of logistic risk. We consider two questions:
first, that of the existence of the MLE (which occurs when the dataset is not
linearly separated), and second that of its accuracy when it exists. These
properties depend on both the dimension of covariates and on the signal
strength. In the case of Gaussian covariates and a well-specified logistic
model, we obtain sharp non-asymptotic guarantees for the existence and excess
logistic risk of the MLE. We then generalize these results in two ways: first,
to non-Gaussian covariates satisfying a certain two-dimensional margin
condition, and second to the general case of statistical learning with a
possibly misspecified logistic model. Finally, we consider the case of a
Bernoulli design, where the behavior of the MLE is highly sensitive to the
parameter direction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encoding Multi-level Dynamics in Effect Heterogeneity Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fucheng Warren Zhu, Connor T. Jerzak, Adel Daoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth Observation (EO) data are increasingly used in policy analysis by
enabling granular estimation of treatment effects. However, a challenge in
EO-based causal inference lies in balancing the trade-off between capturing
fine-grained individual heterogeneity and broader contextual information. This
paper introduces Multi-scale Concatenation, a family of composable procedures
that transform arbitrary single-scale CATE estimation algorithms into
multi-scale algorithms. We benchmark the performance of Multi-scale
Concatenation on a CATE estimation pipeline combining Vision Transformer (ViT)
models fine-tuned on satellite images to encode images of different scales with
Causal Forests to obtain the final CATE estimate. We first perform simulation
studies, showing how a multi-scale approach captures multi-level dynamics that
single-scale ViT models fail to capture. We then apply the multi-scale method
to two randomized controlled trials (RCTs) conducted in Peru and Uganda using
Landsat satellite imagery. In the RCT analysis, the Rank Average Treatment
Effect Ratio (RATE Ratio) measure is employed to assess performance without
ground truth individual treatment effects. Results indicate that Multi-scale
Concatenation improves the performance of deep learning models in EO-based CATE
estimation without the complexity of designing new multi-scale architectures
for a specific use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating the Traces You Need: A Conditional Generative Model for
  Process Mining Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Graziosi, Massimiliano Ronzani, Andrei Buliga, Chiara Di Francescomarino, Francesco Folino, Chiara Ghidini, Francesca Meneghello, Luigi Pontieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, trace generation has emerged as a significant challenge
within the Process Mining community. Deep Learning (DL) models have
demonstrated accuracy in reproducing the features of the selected processes.
However, current DL generative models are limited in their ability to adapt the
learned distributions to generate data samples based on specific conditions or
attributes. This limitation is particularly significant because the ability to
control the type of generated data can be beneficial in various contexts,
enabling a focus on specific behaviours, exploration of infrequent patterns, or
simulation of alternative 'what-if' scenarios. In this work, we address this
challenge by introducing a conditional model for process data generation based
on a conditional variational autoencoder (CVAE). Conditional models offer
control over the generation process by tuning input conditional variables,
enabling more targeted and controlled data generation. Unlike other domains,
CVAE for process mining faces specific challenges due to the multiperspective
nature of the data and the need to adhere to control-flow rules while ensuring
data variability. Specifically, we focus on generating process executions
conditioned on control flow and temporal features of the trace, allowing us to
produce traces for specific, identified sub-processes. The generated traces are
then evaluated using common metrics for generative model assessment, along with
additional metrics to evaluate the quality of the conditional generation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6th International Conference on Process Mining (ICPM) 2024
  Copenhagen, Denmark 14-18 October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Transfer Learning Framework for Fault Diagnosis in Wind
  Turbines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenan Weber, Christine Preisach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Common challenges in fault diagnosis include the lack of labeled data and the
need to build models for each domain, resulting in many models that require
supervision. Transfer learning can help tackle these challenges by learning
cross-domain knowledge. Many approaches still require at least some labeled
data in the target domain, and often provide unexplainable results. To this
end, we propose a supervised transfer learning framework for fault diagnosis in
wind turbines that operates in an Anomaly-Space. This space was created using
SCADA data and vibration data and was built and provided to us by our research
partner. Data within the Anomaly-Space can be interpreted as anomaly scores for
each component in the wind turbine, making each value intuitive to understand.
We conducted cross-domain evaluation on the train set using popular supervised
classifiers like Random Forest, Light-Gradient-Boosting-Machines and Multilayer
Perceptron as metamodels for the diagnosis of bearing and sensor faults. The
Multilayer Perceptron achieved the highest classification performance. This
model was then used for a final evaluation in our test set. The results show,
that the proposed framework is able to detect cross-domain faults in the test
set with a high degree of accuracy by using one single classifier, which is a
significant asset to the diagnostic team.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, to be published in: Upper Rhine AI Symposium
  (URAI) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised detection of semantic correlations in big data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Acevedo, Alex Rodriguez, Alessandro Laio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world data, information is stored in extremely large feature vectors.
These variables are typically correlated due to complex interactions involving
many features simultaneously. Such correlations qualitatively correspond to
semantic roles and are naturally recognized by both the human brain and
artificial neural networks. This recognition enables, for instance, the
prediction of missing parts of an image or text based on their context. We
present a method to detect these correlations in high-dimensional data
represented as binary numbers. We estimate the binary intrinsic dimension of a
dataset, which quantifies the minimum number of independent coordinates needed
to describe the data, and is therefore a proxy of semantic complexity. The
proposed algorithm is largely insensitive to the so-called curse of
dimensionality, and can therefore be used in big data analysis. We test this
approach identifying phase transitions in model magnetic systems and we then
apply it to the detection of semantic correlations of images and text inside
deep neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting K-mer Profile for Effective and Scalable Genome
  Representation Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulkadir Celikkanat, Andres R. Masegosa, Thomas D. Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining effective representations of DNA sequences is crucial for genome
analysis. Metagenomic binning, for instance, relies on genome representations
to cluster complex mixtures of DNA fragments from biological samples with the
aim of determining their microbial compositions. In this paper, we revisit
k-mer-based representations of genomes and provide a theoretical analysis of
their use in representation learning. Based on the analysis, we propose a
lightweight and scalable model for performing metagenomic binning at the genome
read level, relying only on the k-mer compositions of the DNA fragments. We
compare the model to recent genome foundation models and demonstrate that while
the models are comparable in performance, the proposed model is significantly
more effective in terms of scalability, a crucial aspect for performing
metagenomic binning of real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Thirty-Eighth Annual Conference on Neural Information
  Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Sparse Allocation with Mutual Choice & Feature Choice Sparse
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kola Ayonrinde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) are a promising approach to extracting features
from neural networks, enabling model interpretability as well as causal
interventions on model internals. SAEs generate sparse feature representations
using a sparsifying activation function that implicitly defines a set of
token-feature matches. We frame the token-feature matching as a resource
allocation problem constrained by a total sparsity upper bound. For example,
TopK SAEs solve this allocation problem with the additional constraint that
each token matches with at most $k$ features. In TopK SAEs, the $k$ active
features per token constraint is the same across tokens, despite some tokens
being more difficult to reconstruct than others. To address this limitation, we
propose two novel SAE variants, Feature Choice SAEs and Mutual Choice SAEs,
which each allow for a variable number of active features per token. Feature
Choice SAEs solve the sparsity allocation problem under the additional
constraint that each feature matches with at most $m$ tokens. Mutual Choice
SAEs solve the unrestricted allocation problem where the total sparsity budget
can be allocated freely between tokens and features. Additionally, we introduce
a new auxiliary loss function, $\mathtt{aux\_zipf\_loss}$, which generalises
the $\mathtt{aux\_k\_loss}$ to mitigate dead and underutilised features. Our
methods result in SAEs with fewer dead features and improved reconstruction
loss at equivalent sparsity levels as a result of the inherent adaptive
computation. More accurate and scalable feature extraction methods provide a
path towards better understanding and more precise control of foundation
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages (18 w/ appendices), 7 figures. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridge-IF: Learning Inverse Protein Folding with Markov Bridges <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Zhu, Jialu Wu, Qiuyi Li, Jiahuan Yan, Mingze Yin, Wei Wu, Mingyang Li, Jieping Ye, Zheng Wang, Jian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse protein folding is a fundamental task in computational protein
design, which aims to design protein sequences that fold into the desired
backbone structures. While the development of machine learning algorithms for
this task has seen significant success, the prevailing approaches, which
predominantly employ a discriminative formulation, frequently encounter the
error accumulation issue and often fail to capture the extensive variety of
plausible sequences. To fill these gaps, we propose Bridge-IF, a generative
diffusion bridge model for inverse folding, which is designed to learn the
probabilistic dependency between the distributions of backbone structures and
protein sequences. Specifically, we harness an expressive structure encoder to
propose a discrete, informative prior derived from structures, and establish a
Markov bridge to connect this prior with native sequences. During the inference
stage, Bridge-IF progressively refines the prior sequence, culminating in a
more plausible design. Moreover, we introduce a reparameterization perspective
on Markov bridge models, from which we derive a simplified loss function that
facilitates more effective training. We also modulate protein language models
(PLMs) with structural conditions to precisely approximate the Markov bridge
process, thereby significantly enhancing generation performance while
maintaining parameter-efficient training. Extensive experiments on
well-established benchmarks demonstrate that Bridge-IF predominantly surpasses
existing baselines in sequence recovery and excels in the design of plausible
proteins with high foldability. The code is available at
https://github.com/violet-sto/Bridge-IF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Zhan, Wenkuan Zhao, Yuanqing Li, Weijie Liu, Xiaoxi Zhang, Chee Wei Tan, Chuan Wu, Deke Guo, Xu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a collaborative machine learning approach that
enables multiple clients to train models without sharing their private data.
With the rise of deep learning, large-scale models have garnered significant
attention due to their exceptional performance. However, a key challenge in FL
is the limitation imposed by clients with constrained computational and
communication resources, which hampers the deployment of these large models.
The Mixture of Experts (MoE) architecture addresses this challenge with its
sparse activation property, which reduces computational workload and
communication demands during inference and updates. Additionally, MoE
facilitates better personalization by allowing each expert to specialize in
different subsets of the data distribution. To alleviate the communication
burdens between the server and clients, we propose FedMoE-DA, a new FL model
training framework that leverages the MoE architecture and incorporates a novel
domain-aware, fine-grained aggregation strategy to enhance the robustness,
personalizability, and communication efficiency simultaneously. Specifically,
the correlation between both intra-client expert models and inter-client data
heterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)
communication between clients for selective expert model synchronization, thus
significantly reducing the server-client transmissions. Experiments demonstrate
that our FedMoE-DA achieves excellent performance while reducing the
communication pressure on the server.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semiparametric conformal prediction <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Won Park, Robert Tibshirani, Kyunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many risk-sensitive applications require well-calibrated prediction sets over
multiple, potentially correlated target variables, for which the prediction
algorithm may report correlated non-conformity scores. In this work, we treat
the scores as random vectors and aim to construct the prediction set accounting
for their joint correlation structure. Drawing from the rich literature on
multivariate quantiles and semiparametric statistics, we propose an algorithm
to estimate the $1-\alpha$ quantile of the scores, where $\alpha$ is the
user-specified miscoverage rate. In particular, we flexibly estimate the joint
cumulative distribution function (CDF) of the scores using nonparametric vine
copulas and improve the asymptotic efficiency of the quantile estimate using
its influence function. The vine decomposition allows our method to scale well
to a large number of targets. We report desired coverage and competitive
efficiency on a range of real-world regression problems, including those with
missing-at-random labels in the calibration set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages (+12 appendix), 11 figures, submitted to AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal biometric authentication: Leveraging shared layer
  architectures for enhanced security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vatchala S, Yogesh C, Yeshwanth Govindarajan, Krithik Raja M, Vishal Pramav Amirtha Ganesan, Aashish Vinod A, Dharun Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a novel multi-modal biometric authentication
system that integrates facial, vocal, and signature data to enhance security
measures. Utilizing a combination of Convolutional Neural Networks (CNNs) and
Recurrent Neural Networks (RNNs), our model architecture uniquely incorporates
dual shared layers alongside modality-specific enhancements for comprehensive
feature extraction. The system undergoes rigorous training with a joint loss
function, optimizing for accuracy across diverse biometric inputs.
Feature-level fusion via Principal Component Analysis (PCA) and classification
through Gradient Boosting Machines (GBM) further refine the authentication
process. Our approach demonstrates significant improvements in authentication
accuracy and robustness, paving the way for advanced secure identity
verification solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training on test proteins improves fitness, structure, and function
  prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Bushuiev, Roman Bushuiev, Nikola Zadorozhny, Raman Samusevich, Hannes Stärk, Jiri Sedlar, Tomáš Pluskal, Josef Sivic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity and distribution shifts often hinder the ability of machine
learning models to generalize when applied to proteins and other biological
data. Self-supervised pre-training on large datasets is a common method to
enhance generalization. However, striving to perform well on all possible
proteins can limit model's capacity to excel on any specific one, even though
practitioners are often most interested in accurate predictions for the
individual protein they study. To address this limitation, we propose an
orthogonal approach to achieve generalization. Building on the prevalence of
self-supervised pre-training, we introduce a method for self-supervised
fine-tuning at test time, allowing models to adapt to the test protein of
interest on the fly and without requiring any additional data. We study our
test-time training (TTT) method through the lens of perplexity minimization and
show that it consistently enhances generalization across different models,
their scales, and datasets. Notably, our method leads to new state-of-the-art
results on the standard benchmark for protein fitness prediction, improves
protein structure prediction for challenging targets, and enhances function
prediction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Integrated Decision Gradients (IDG-DP) for
  Radar-based Human Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Zakariyya, Linda Tran, Kaushik Bhargav Sivangi, Paul Henderson, Fani Deligianni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion analysis offers significant potential for healthcare monitoring
and early detection of diseases. The advent of radar-based sensing systems has
captured the spotlight for they are able to operate without physical contact
and they can integrate with pre-existing Wi-Fi networks. They are also seen as
less privacy-invasive compared to camera-based systems. However, recent
research has shown high accuracy in recognizing subjects or gender from radar
gait patterns, raising privacy concerns. This study addresses these issues by
investigating privacy vulnerabilities in radar-based Human Activity Recognition
(HAR) systems and proposing a novel method for privacy preservation using
Differential Privacy (DP) driven by attributions derived with Integrated
Decision Gradient (IDG) algorithm. We investigate Black-box Membership
Inference Attack (MIA) Models in HAR settings across various levels of
attacker-accessible information. We extensively evaluated the effectiveness of
the proposed IDG-DP method by designing a CNN-based HAR model and rigorously
assessing its resilience against MIAs. Experimental results demonstrate the
potential of IDG-DP in mitigating privacy attacks while maintaining utility
across all settings, particularly excelling against label-only and shadow model
black-box MIA attacks. This work represents a crucial step towards balancing
the need for effective radar-based HAR with robust privacy protection in
healthcare environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alignment-Based Adversarial Training (ABAT) for Improving the Robustness
  and Accuracy of EEG-Based BCIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqing Chen, Ziwei Wang, Dongrui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has achieved great success in electroencephalogram (EEG)
based brain-computer interfaces (BCIs). Most existing BCI studies focused on
improving the decoding accuracy, with only a few considering the adversarial
security. Although many adversarial defense approaches have been proposed in
other application domains such as computer vision, previous research showed
that their direct extensions to BCIs degrade the classification accuracy on
benign samples. This phenomenon greatly affects the applicability of
adversarial defense approaches to EEG-based BCIs. To mitigate this problem, we
propose alignment-based adversarial training (ABAT), which performs EEG data
alignment before adversarial training. Data alignment aligns EEG trials from
different domains to reduce their distribution discrepancies, and adversarial
training further robustifies the classification boundary. The integration of
data alignment and adversarial training can make the trained EEG classifiers
simultaneously more accurate and more robust. Experiments on five EEG datasets
from two different BCI paradigms (motor imagery classification, and event
related potential recognition), three convolutional neural network classifiers
(EEGNet, ShallowCNN and DeepCNN) and three different experimental settings
(offline within-subject cross-block/-session classification, online
cross-session classification, and pre-trained classifiers) demonstrated its
effectiveness. It is very intriguing that adversarial attacks, which are
usually used to damage BCI systems, can be used in ABAT to simultaneously
improve the model accuracy and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Exponential Separation Between Quantum and Quantum-Inspired Classical
  Algorithms for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allan Grønlund, Kasper Green Larsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving a provable exponential quantum speedup for an important machine
learning task has been a central research goal since the seminal HHL quantum
algorithm for solving linear systems and the subsequent quantum recommender
systems algorithm by Kerenidis and Prakash. These algorithms were initially
believed to be strong candidates for exponential speedups, but a lower bound
ruling out similar classical improvements remained absent. In breakthrough work
by Tang, it was demonstrated that this lack of progress in classical lower
bounds was for good reasons. Concretely, she gave a classical counterpart of
the quantum recommender systems algorithm, reducing the quantum advantage to a
mere polynomial. Her approach is quite general and was named quantum-inspired
classical algorithms. Since then, almost all the initially exponential quantum
machine learning speedups have been reduced to polynomial via new
quantum-inspired classical algorithms. From the current state-of-affairs, it is
unclear whether we can hope for exponential quantum speedups for any natural
machine learning task.
  In this work, we present the first such provable exponential separation
between quantum and quantum-inspired classical algorithms. We prove the
separation for the basic problem of solving a linear system when the input
matrix is well-conditioned and has sparse rows and columns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regress, Don't Guess -- A Regression-like Loss on Number Tokens for
  Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Zausinger, Lars Pennig, Kacper Chlodny, Vincent Limbach, Anna Ketteler, Thorben Prein, Vishwa Mohan Singh, Michael Morris Danziger, Jannis Born
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While language models have exceptional capabilities at text generation, they
lack a natural inductive bias for emitting numbers and thus struggle in tasks
involving reasoning over quantities, especially arithmetics. This has
particular relevance in scientific datasets where combinations of text and
numerical data are abundant. One fundamental limitation is the nature of the CE
loss, which assumes a nominal (categorical) scale and thus cannot convey
proximity between generated number tokens. As a remedy, we here present two
versions of a number token loss. The first is based on an $L_p$ loss between
the ground truth token value and the weighted sum of the predicted class
probabilities. The second loss minimizes the Wasserstein-1 distance between the
distribution of the predicted output probabilities and the ground truth
distribution. These regression-like losses can easily be added to any language
model and extend the CE objective during training. We compare the proposed
schemes on a mathematics dataset against existing tokenization, encoding, and
decoding schemes for improving number representation in language models. Our
results reveal a significant improvement in numerical accuracy when equipping a
standard T5 model with the proposed loss schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5-page version for NeurIPS 2024 (MathAI workshop)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards certification: A complete statistical validation pipeline for
  supervised learning in industry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Lacasa, Abel Pardo, Pablo Arbelo, Miguel Sánchez, Pablo Yeste, Noelia Bascones, Alejandro Martínez-Cava, Gonzalo Rubio, Ignacio Gómez, Eusebio Valero, Javier de Vicente
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods of Machine and Deep Learning are gradually being integrated into
industrial operations, albeit at different speeds for different types of
industries. The aerospace and aeronautical industries have recently developed a
roadmap for concepts of design assurance and integration of neural
network-related technologies in the aeronautical sector. This paper aims to
contribute to this paradigm of AI-based certification in the context of
supervised learning, by outlining a complete validation pipeline that
integrates deep learning, optimization and statistical methods. This pipeline
is composed by a directed graphical model of ten steps. Each of these steps is
addressed by a merging key concepts from different contributing disciplines
(from machine learning or optimization to statistics) and adapting them to an
industrial scenario, as well as by developing computationally efficient
algorithmic solutions. We illustrate the application of this pipeline in a
realistic supervised problem arising in aerostructural design: predicting the
likelikood of different stress-related failure modes during different airflight
maneuvers based on a (large) set of features characterising the aircraft
internal loads and geometric parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Integrity when Unlearning with T2I Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Schioppa, Emiel Hoogeboom, Jonathan Heek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of text-to-image Diffusion Models has led to their
widespread public accessibility. However these models, trained on large
internet datasets, can sometimes generate undesirable outputs. To mitigate
this, approximate Machine Unlearning algorithms have been proposed to modify
model weights to reduce the generation of specific types of images,
characterized by samples from a ``forget distribution'', while preserving the
model's ability to generate other images, characterized by samples from a
``retain distribution''. While these methods aim to minimize the influence of
training data in the forget distribution without extensive additional
computation, we point out that they can compromise the model's integrity by
inadvertently affecting generation for images in the retain distribution.
Recognizing the limitations of FID and CLIPScore in capturing these effects, we
introduce a novel retention metric that directly assesses the perceptual
difference between outputs generated by the original and the unlearned models.
We then propose unlearning algorithms that demonstrate superior effectiveness
in preserving model integrity compared to existing baselines. Given their
straightforward implementation, these algorithms serve as valuable benchmarks
for future advancements in approximate Machine Unlearning for Diffusion Models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Cognitive Diagnosis with Disentangled Representation
  Learning for Learner Modeling <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weibo Gao, Qi Liu, Linan Yue, Fangzhou Yao, Hao Wang, Yin Gu, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learners sharing similar implicit cognitive states often display comparable
observable problem-solving performances. Leveraging collaborative connections
among such similar learners proves valuable in comprehending human learning.
Motivated by the success of collaborative modeling in various domains, such as
recommender systems, we aim to investigate how collaborative signals among
learners contribute to the diagnosis of human cognitive states (i.e., knowledge
proficiency) in the context of intelligent education. The primary challenges
lie in identifying implicit collaborative connections and disentangling the
entangled cognitive factors of learners for improved explainability and
controllability in learner Cognitive Diagnosis (CD). However, there has been no
work on CD capable of simultaneously modeling collaborative and disentangled
cognitive states. To address this gap, we present Coral, a Collaborative
cognitive diagnosis model with disentangled representation learning.
Specifically, Coral first introduces a disentangled state encoder to achieve
the initial disentanglement of learners' states. Subsequently, a meticulously
designed collaborative representation learning procedure captures collaborative
signals. It dynamically constructs a collaborative graph of learners by
iteratively searching for optimal neighbors in a context-aware manner. Using
the constructed graph, collaborative information is extracted through node
representation learning. Finally, a decoding process aligns the initial
cognitive states and collaborative states, achieving co-disentanglement with
practice performance reconstructions. Extensive experiments demonstrate the
superior performance of Coral, showcasing significant improvements over
state-of-the-art methods across several real-world datasets. Our code is
available at https://github.com/bigdata-ustc/Coral.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amortized Bayesian Experimental Design for Decision-Making <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daolang Huang, Yujia Guo, Luigi Acerbi, Samuel Kaski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many critical decisions, such as personalized medical diagnoses and product
pricing, are made based on insights gained from designing, observing, and
analyzing a series of experiments. This highlights the crucial role of
experimental design, which goes beyond merely collecting information on system
parameters as in traditional Bayesian experimental design (BED), but also plays
a key part in facilitating downstream decision-making. Most recent BED methods
use an amortized policy network to rapidly design experiments. However, the
information gathered through these methods is suboptimal for down-the-line
decision-making, as the experiments are not inherently designed with downstream
objectives in mind. In this paper, we present an amortized decision-aware BED
framework that prioritizes maximizing downstream decision utility. We introduce
a novel architecture, the Transformer Neural Decision Process (TNDP), capable
of instantly proposing the next experimental design, whilst inferring the
downstream decision, thus effectively amortizing both tasks within a unified
workflow. We demonstrate the performance of our method across several tasks,
showing that it can deliver informative designs and facilitate accurate
decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures. Accepted at the 38th Conference on Neural
  Information Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Efficient Training of Large Language Models with
  Low-dimensional Projected Attention <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingtai Lv, Ning Ding, Kaiyan Zhang, Ermo Hua, Ganqu Cui, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the effectiveness and efficiency of large language models (LLMs)
simultaneously is a critical yet challenging research goal. In this paper, we
find that low-rank pre-training, normally considered as efficient methods that
will compromise performance, can be scalably effective when reduced parameters
are precisely targeted. Specifically, applying the low-dimensional module only
to the attention layer -- resolves this issue and enhances both effectiveness
and efficiency. We refer to this structure as Low-dimensional Projected
Attention (LPA) and provide an explanatory analysis. Through extensive
experimentation at parameter scales of 130M, 370M, and scaling up to 3B, we
have validated the effectiveness and scalability of LPA. Our results show that
LPA model can save up to 12.4% in time while achieving an approximate 5%
improvement in test perplexity (ppl) and on downstream tasks compared with the
vanilla Transformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Table<span class="highlight-title">GPT</span>2: A Large Multimodal Model with Tabular Data Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou, Ga Zhang, Guangcheng Zhu, Haobo Wang, Haokai Xu, Hao Chen, Haoze Li, Haoxuan Lan, Jiaming Tian, Jing Yuan, Junbo Zhao, Junlin Zhou, Kaizhe Shou, Liangyu Zha, Lin Long, Liyao Li, Pengzuo Wu, Qi Zhang, Qingyi Huang, Saisai Yang, Tao Zhang, Wentao Ye, Wufang Zhu, Xiaomeng Hu, Xijun Gu, Xinjie Sun, Xiang Li, Yuhang Yang, Zhiqing Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI
applications, presenting vast new opportunities across industries. Yet, the
integration of tabular data remains notably underdeveloped, despite its
foundational role in numerous real-world domains.
  This gap is critical for three main reasons. First, database or data
warehouse data integration is essential for advanced applications; second, the
vast and largely untapped resource of tabular data offers immense potential for
analysis; and third, the business intelligence domain specifically demands
adaptable, precise solutions that many current LLMs may struggle to provide.
  In response, we introduce TableGPT2, a model rigorously pre-trained and
fine-tuned with over 593.8K tables and 2.36M high-quality query-table-output
tuples, a scale of table-related data unprecedented in prior research. This
extensive training enables TableGPT2 to excel in table-centric tasks while
maintaining strong general language and coding abilities.
  One of TableGPT2's key innovations is its novel table encoder, specifically
designed to capture schema-level and cell-level information. This encoder
strengthens the model's ability to handle ambiguous queries, missing column
names, and irregular tables commonly encountered in real-world applications.
Similar to visual language models, this pioneering approach integrates with the
decoder to form a robust large multimodal model.
  We believe the results are compelling: over 23 benchmarking metrics,
TableGPT2 achieves an average performance improvement of 35.20% in the 7B model
and 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust
general-purpose capabilities intact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional
  Trajectories Through Manifold Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gionni Marchetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A data-driven approach based on unsupervised machine learning is proposed to
infer the intrinsic dimensions $m^{\ast}$ of the high-dimensional trajectories
of the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis
(PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints,
of the FPUT $\beta$ model with $N = 32$ coupled oscillators, revealing a
critical relationship between $m^{\ast}$ and the model's nonlinear strength.
For weak nonlinearities, $m^{\ast} \ll n$, where $n = 2N$. In contrast, for
strong nonlinearities, $m^{\ast} \rightarrow n - 1$, consistently with the
ergodic hypothesis. Furthermore, one of the potential limitations of PCA is
addressed through an analysis with t-distributed stochastic neighbor embedding
($t$-SNE). Accordingly, we found strong evidence suggesting that the datapoints
lie near or on a curved low-dimensional manifold for weak nonlinearities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R+R:Understanding Hyperparameter Effects in DP-SGD <span class="chip">ACSA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Morsbach, Jan Reubold, Thorsten Strufe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on the effects of essential hyperparameters of DP-SGD lacks
consensus, verification, and replication. Contradictory and anecdotal
statements on their influence make matters worse. While DP-SGD is the standard
optimization algorithm for privacy-preserving machine learning, its adoption is
still commonly challenged by low performance compared to non-private learning
approaches. As proper hyperparameter settings can improve the privacy-utility
trade-off, understanding the influence of the hyperparameters promises to
simplify their optimization towards better performance, and likely foster
acceptance of private learning. To shed more light on these influences, we
conduct a replication study: We synthesize extant research on hyperparameter
influences of DP-SGD into conjectures, conduct a dedicated factorial study to
independently identify hyperparameter effects, and assess which conjectures can
be replicated across multiple datasets, model architectures, and differential
privacy budgets. While we cannot (consistently) replicate conjectures about the
main and interaction effects of the batch size and the number of epochs, we
were able to replicate the conjectured relationship between the clipping
threshold and learning rate. Furthermore, we were able to quantify the
significant importance of their combination compared to the other
hyperparameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 40th Annual Computer Security Applications Conference
  (ACSAC 24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theory-inspired Label Shift Adaptation via Aligned Distribution Mixture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruidong Fan, Xiao Ouyang, Hong Tao, Yuhua Qian, Chenping Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a prominent challenge in addressing real-world issues within a dynamic
environment, label shift, which refers to the learning setting where the source
(training) and target (testing) label distributions do not match, has recently
received increasing attention. Existing label shift methods solely use
unlabeled target samples to estimate the target label distribution, and do not
involve them during the classifier training, resulting in suboptimal
utilization of available information. One common solution is to directly blend
the source and target distributions during the training of the target
classifier. However, we illustrate the theoretical deviation and limitations of
the direct distribution mixture in the label shift setting. To tackle this
crucial yet unexplored issue, we introduce the concept of aligned distribution
mixture, showcasing its theoretical optimality and generalization error bounds.
By incorporating insights from generalization theory, we propose an innovative
label shift framework named as Aligned Distribution Mixture (ADM). Within this
framework, we enhance four typical label shift methods by introducing
modifications to the classifier training process. Furthermore, we also propose
a one-step approach that incorporates a pioneering coupling weight estimation
strategy. Considering the distinctiveness of the proposed one-step approach, we
develop an efficient bi-level optimization strategy. Experimental results
demonstrate the effectiveness of our approaches, together with their
effectiveness in COVID-19 diagnosis applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Representation Collapse in Vector Quantized Models with One
  Linear Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Zhu, Bocheng Li, Yifei Xin, Linli Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector Quantization (VQ) is a widely used method for converting continuous
representations into discrete codes, which has become fundamental in
unsupervised representation learning and latent generative models. However, VQ
models are often hindered by the problem of representation collapse in the
latent space, which leads to low codebook utilization and limits the
scalability of the codebook for large-scale training. Existing methods designed
to mitigate representation collapse typically reduce the dimensionality of
latent space at the expense of model capacity, which do not fully resolve the
core issue. In this study, we conduct a theoretical analysis of representation
collapse in VQ models and identify its primary cause as the disjoint
optimization of the codebook, where only a small subset of code vectors are
updated through gradient descent. To address this issue, we propose
\textbf{SimVQ}, a novel method which reparameterizes the code vectors through a
linear transformation layer based on a learnable latent basis. This
transformation optimizes the \textit{entire linear space} spanned by the
codebook, rather than merely updating \textit{the code vector} selected by the
nearest-neighbor search in vanilla VQ models. Although it is commonly
understood that the multiplication of two linear matrices is equivalent to
applying a single linear layer, our approach works surprisingly well in
resolving the collapse issue in VQ models with just one linear layer. We
validate the efficacy of SimVQ through extensive experiments across various
modalities, including image and audio data with different model architectures.
Our code is available at \url{https://github.com/youngsheen/SimVQ}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Classification under Performative Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edwige Cyffers, Muni Sreenivas Pydi, Jamal Atif, Olivier Cappé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performative learning addresses the increasingly pervasive situations in
which algorithmic decisions may induce changes in the data distribution as a
consequence of their public deployment. We propose a novel view in which these
performative effects are modelled as push-forward measures. This general
framework encompasses existing models and enables novel performative gradient
estimation methods, leading to more efficient and scalable learning strategies.
For distribution shifts, unlike previous models which require full
specification of the data distribution, we only assume knowledge of the shift
operator that represents the performative changes. This approach can also be
integrated into various change-of-variablebased models, such as VAEs or
normalizing flows. Focusing on classification with a linear-in-parameters
performative effect, we prove the convexity of the performative risk under a
new set of assumptions. Notably, we do not limit the strength of performative
effects but rather their direction, requiring only that classification becomes
harder when deploying more accurate models. In this case, we also establish a
connection with adversarially robust classification by reformulating the
minimization of the performative risk as a min-max variational problem.
Finally, we illustrate our approach on synthetic and real datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems, Dec 2024,
  Vancouver (Canada), Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modulating State Space Model with SlowFast Framework for
  Compute-Efficient Ultra Low-Latency Speech Enhancement <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longbiao Cheng, Ashutosh Pandey, Buye Xu, Tobi Delbruck, Vamsi Krishna Ithapu, Shih-Chii Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based speech enhancement (SE) methods often face significant
computational challenges when needing to meet low-latency requirements because
of the increased number of frames to be processed. This paper introduces the
SlowFast framework which aims to reduce computation costs specifically when
low-latency enhancement is needed. The framework consists of a slow branch that
analyzes the acoustic environment at a low frame rate, and a fast branch that
performs SE in the time domain at the needed higher frame rate to match the
required latency. Specifically, the fast branch employs a state space model
where its state transition process is dynamically modulated by the slow branch.
Experiments on a SE task with a 2 ms algorithmic latency requirement using the
Voice Bank + Demand dataset show that our approach reduces computation cost by
70% compared to a baseline single-branch network with equivalent parameters,
without compromising enhancement performance. Furthermore, by leveraging the
SlowFast framework, we implemented a network that achieves an algorithmic
latency of just 60 {\mu}s (one sample point at 16 kHz sample rate) with a
computation cost of 100 M MACs/s, while scoring a PESQ-NB of 3.12 and SISNR of
16.62.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Against Multifaceted Graph Heterogeneity via Asymmetric Federated <span class="highlight-title">Prompt</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoning Guo, Ruiqian Han, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Graph Learning (FGL) aims to collaboratively and privately optimize
graph models on divergent data for different tasks. A critical challenge in FGL
is to enable effective yet efficient federated optimization against
multifaceted graph heterogeneity to enhance mutual performance. However,
existing FGL works primarily address graph data heterogeneity and perform
incapable of graph task heterogeneity. To address the challenge, we propose a
Federated Graph Prompt Learning (FedGPL) framework to efficiently enable
prompt-based asymmetric graph knowledge transfer between multifaceted
heterogeneous federated participants. Generally, we establish a split federated
framework to preserve universal and domain-specific graph knowledge,
respectively. Moreover, we develop two algorithms to eliminate task and data
heterogeneity for advanced federated knowledge preservation. First, a
Hierarchical Directed Transfer Aggregator (HiDTA) delivers cross-task
beneficial knowledge that is hierarchically distilled according to the
directional transferability. Second, a Virtual Prompt Graph (VPG) adaptively
generates graph structures to enhance data utility by distinguishing dominant
subgraphs and neutralizing redundant ones. We conduct theoretical analyses and
extensive experiments to demonstrate the significant accuracy and efficiency
effectiveness of FedGPL against multifaceted graph heterogeneity compared to
state-of-the-art baselines on large-scale federated graph datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Loss Optimization in the Infinite Width: Stable Parameterization
  of Predictive Coding Networks and Target Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoki Ishikawa, Rio Yokota, Ryo Karakida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local learning, which trains a network through layer-wise local targets and
losses, has been studied as an alternative to backpropagation (BP) in neural
computation. However, its algorithms often become more complex or require
additional hyperparameters because of the locality, making it challenging to
identify desirable settings in which the algorithm progresses in a stable
manner. To provide theoretical and quantitative insights, we introduce the
maximal update parameterization ($\mu$P) in the infinite-width limit for two
representative designs of local targets: predictive coding (PC) and target
propagation (TP). We verified that $\mu$P enables hyperparameter transfer
across models of different widths. Furthermore, our analysis revealed unique
and intriguing properties of $\mu$P that are not present in conventional BP. By
analyzing deep linear networks, we found that PC's gradients interpolate
between first-order and Gauss-Newton-like gradients, depending on the
parameterization. We demonstrate that, in specific standard settings, PC in the
infinite-width limit behaves more similarly to the first-order gradient. For
TP, even with the standard scaling of the last layer, which differs from
classical $\mu$P, its local loss optimization favors the feature learning
regime over the kernel regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoonick Lee, Mogan Gim, Donghyeon Park, Donghee Choi, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) have shown promise in various
creative domains, including culinary arts. However, many LLMs still struggle to
deliver the desired level of culinary creativity, especially when tasked with
adapting recipes to meet specific cultural requirements. This study focuses on
cuisine transfer-applying elements of one cuisine to another-to assess LLMs'
culinary creativity. We employ a diverse set of LLMs to generate and evaluate
culturally adapted recipes, comparing their evaluations against LLM and human
judgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark
to evaluate LLMs' recipe generation abilities in the cuisine transfer task,
assessing their cultural accuracy and creativity in the culinary domain. Our
findings reveal crucial insights into both generative and evaluative
capabilities of LLMs in the culinary domain, highlighting strengths and
limitations in understanding and applying cultural nuances in recipe creation.
The code and dataset used in this project will be openly available in
\url{http://github.com/dmis-lab/CulinaryASH}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ask, and it shall be given: Turing completeness of <span class="highlight-title">prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhong Qiu, Zhe Xu, Wenxuan Bao, Hanghang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the success of GPT, large language models (LLMs) have been
revolutionizing machine learning and have initiated the so-called LLM prompting
paradigm. In the era of LLMs, people train a single general-purpose LLM and
provide the LLM with different prompts to perform different tasks. However,
such empirical success largely lacks theoretical understanding. Here, we
present the first theoretical study on the LLM prompting paradigm to the best
of our knowledge. In this work, we show that prompting is in fact
Turing-complete: there exists a finite-size Transformer such that for any
computable function, there exists a corresponding prompt following which the
Transformer computes the function. Furthermore, we show that even though we use
only a single finite-size Transformer, it can still achieve nearly the same
complexity bounds as that of the class of all unbounded-size Transformers.
Overall, our result reveals that prompting can enable a single finite-size
Transformer to be efficiently universal, which establishes a theoretical
underpinning for prompt engineering in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Controlled Stochastic Differential Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luc Brogat-Motte, Riccardo Bonalli, Alessandro Rudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identification of nonlinear dynamical systems is crucial across various
fields, facilitating tasks such as control, prediction, optimization, and fault
detection. Many applications require methods capable of handling complex
systems while providing strong learning guarantees for safe and reliable
performance. However, existing approaches often focus on simplified scenarios,
such as deterministic models, known diffusion, discrete systems,
one-dimensional dynamics, or systems constrained by strong structural
assumptions such as linearity. This work proposes a novel method for estimating
both drift and diffusion coefficients of continuous, multidimensional,
nonlinear controlled stochastic differential equations with non-uniform
diffusion. We assume regularity of the coefficients within a Sobolev space,
allowing for broad applicability to various dynamical systems in robotics,
finance, climate modeling, and biology. Leveraging the Fokker-Planck equation,
we split the estimation into two tasks: (a) estimating system dynamics for a
finite set of controls, and (b) estimating coefficients that govern those
dynamics. We provide strong theoretical guarantees, including finite-sample
bounds for \(L^2\), \(L^\infty\), and risk metrics, with learning rates
adaptive to coefficients' regularity, similar to those in nonparametric
least-squares regression literature. The practical effectiveness of our
approach is demonstrated through extensive numerical experiments. Our method is
available as an open-source Python library.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Variational Autoencoders with Intrinsic Dimension and
  Information Imbalance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Camboulin, Diego Doimo, Aldo Glielmo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an analysis of the hidden representations of Variational
Autoencoders (VAEs) using the Intrinsic Dimension (ID) and the Information
Imbalance (II). We show that VAEs undergo a transition in behaviour once the
bottleneck size is larger than the ID of the data, manifesting in a double
hunchback ID profile and a qualitative shift in information processing as
captured by the II. Our results also highlight two distinct training phases for
architectures with sufficiently large bottleneck sizes, consisting of a rapid
fit and a slower generalisation, as assessed by a differentiated behaviour of
ID, II, and KL loss. These insights demonstrate that II and ID could be
valuable tools for aiding architecture search, for diagnosing underfitting in
VAEs, and, more broadly, they contribute to advancing a unified understanding
of deep generative models through geometric analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, accepted at the Unifying Representations in
  Neural Models (UniReps) workshop of NeurIPS 2024 (https://unireps.org/2024/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the phase diagram of extensive-rank symmetric matrix denoising beyond
  rotational invariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Barbier, Francesco Camilli, Justin Ko, Koki Okajima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix denoising is central to signal processing and machine learning. Its
analysis when the matrix to infer has a factorised structure with a rank
growing proportionally to its dimension remains a challenge, except when it is
rotationally invariant. In this case the information theoretic limits and a
Bayes-optimal denoising algorithm, called rotational invariant estimator [1,2],
are known. Beyond this setting few results can be found. The reason is that the
model is not a usual spin system because of the growing rank dimension, nor a
matrix model due to the lack of rotation symmetry, but rather a hybrid between
the two. In this paper we make progress towards the understanding of Bayesian
matrix denoising when the hidden signal is a factored matrix $XX^\intercal$
that is not rotationally invariant. Monte Carlo simulations suggest the
existence of a denoising-factorisation transition separating a phase where
denoising using the rotational invariant estimator remains Bayes-optimal due to
universality properties of the same nature as in random matrix theory, from one
where universality breaks down and better denoising is possible by exploiting
the signal's prior and factorised structure, though algorithmically hard. We
also argue that it is only beyond the transition that factorisation, i.e.,
estimating $X$ itself, becomes possible up to sign and permutation ambiguities.
On the theoretical side, we combine mean-field techniques in an interpretable
multiscale fashion in order to access the minimum mean-square error and mutual
information. Interestingly, our alternative method yields equations which can
be reproduced using the replica approach of [3]. Using numerical insights, we
then delimit the portion of the phase diagram where this mean-field theory is
reliable, and correct it using universality when it is not. Our ansatz matches
well the numerics when accounting for finite size effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Certainty Ratio $C_ρ$: a novel metric for assessing the
  reliability of classifier predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus S. Aguilar-Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the performance of classifiers is critical in machine learning,
particularly in high-stakes applications where the reliability of predictions
can significantly impact decision-making. Traditional performance measures,
such as accuracy and F-score, often fail to account for the uncertainty
inherent in classifier predictions, leading to potentially misleading
assessments. This paper introduces the Certainty Ratio ($C_\rho$), a novel
metric designed to quantify the contribution of confident (certain) versus
uncertain predictions to any classification performance measure. By integrating
the Probabilistic Confusion Matrix ($CM^\star$) and decomposing predictions
into certainty and uncertainty components, $C_\rho$ provides a more
comprehensive evaluation of classifier reliability. Experimental results across
26 datasets and multiple classifiers, including Decision Trees, Naive-Bayes,
3-Nearest Neighbors, and Random Forests, demonstrate that $C_\rho$ reveals
critical insights that conventional metrics often overlook. These findings
emphasize the importance of incorporating probabilistic information into
classifier evaluation, offering a robust tool for researchers and practitioners
seeking to improve model trustworthiness in complex environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnSegMedGAT: Unsupervised Medical Image Segmentation using Graph
  Attention Networks Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Mudit Adityaja, Saurabh J. Shigwan, Nitin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The data-intensive nature of supervised classification drives the interest of
the researchers towards unsupervised approaches, especially for problems such
as medical image segmentation, where labeled data is scarce. Building on the
recent advancements of Vision transformers (ViT) in computer vision, we propose
an unsupervised segmentation framework using a pre-trained Dino-ViT. In the
proposed method, we leverage the inherent graph structure within the image to
realize a significant performance gain for segmentation in medical images. For
this, we introduce a modularity-based loss function coupled with a Graph
Attention Network (GAT) to effectively capture the inherent graph topology
within the image. Our method achieves state-of-the-art performance, even
significantly surpassing or matching that of existing (semi)supervised
technique such as MedSAM which is a Segment Anything Model in medical images.
We demonstrate this using two challenging medical image datasets ISIC-2018 and
CVC-ColonDB. This work underscores the potential of unsupervised approaches in
advancing medical image analysis in scenarios where labeled data is scarce. The
github repository of the code is available on
[https://github.com/mudit-adityaja/UnSegMedGAT].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ N-Gram Induction Heads for In-Context RL: Improving Stability and
  Reducing Data Needs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Zisman, Alexander Nikulin, Andrei Polubarov, Nikita Lyubaykin, Vladislav Kurenkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning allows models like transformers to adapt to new tasks
from a few examples without updating their weights, a desirable trait for
reinforcement learning (RL). However, existing in-context RL methods, such as
Algorithm Distillation (AD), demand large, carefully curated datasets and can
be unstable and costly to train due to the transient nature of in-context
learning abilities. In this work we integrated the n-gram induction heads into
transformers for in-context RL. By incorporating these n-gram attention
patterns, we significantly reduced the data required for generalization - up to
27 times fewer transitions in the Key-to-Door environment - and eased the
training process by making models less sensitive to hyperparameters. Our
approach not only matches but often surpasses the performance of AD,
demonstrating the potential of n-gram induction heads to enhance the efficiency
of in-context RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Adaptive Foundation Models Workshop; NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EXAGREE: Towards Explanation Agreement in Explainable Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sichao Li, Quanling Deng, Amanda S. Barnard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations in machine learning are critical for trust, transparency, and
fairness. Yet, complex disagreements among these explanations limit the
reliability and applicability of machine learning models, especially in
high-stakes environments. We formalize four fundamental ranking-based
explanation disagreement problems and introduce a novel framework, EXplanation
AGREEment (EXAGREE), to bridge diverse interpretations in explainable machine
learning, particularly from stakeholder-centered perspectives. Our approach
leverages a Rashomon set for attribution predictions and then optimizes within
this set to identify Stakeholder-Aligned Explanation Models (SAEMs) that
minimize disagreement with diverse stakeholder needs while maintaining
predictive performance. Rigorous empirical analysis on synthetic and real-world
datasets demonstrates that EXAGREE reduces explanation disagreement and
improves fairness across subgroups in various domains. EXAGREE not only
provides researchers with a new direction for studying explanation disagreement
problems but also offers data scientists a tool for making better-informed
decisions in practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HACD: Harnessing Attribute Semantics and Mesoscopic Structure for
  Community Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anran Zhang, Xingfen Wang, Yuhan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community detection plays a pivotal role in uncovering closely connected
subgraphs, aiding various real-world applications such as recommendation
systems and anomaly detection. With the surge of rich information available for
entities in real-world networks, the community detection problem in attributed
networks has attracted widespread attention. While previous research has
effectively leveraged network topology and attribute information for attributed
community detection, these methods overlook two critical issues: (i) the
semantic similarity between node attributes within the community, and (ii) the
inherent mesoscopic structure, which differs from the pairwise connections of
the micro-structure. To address these limitations, we propose HACD, a novel
attributed community detection model based on heterogeneous graph attention
networks. HACD treats node attributes as another type of node, constructs
attributed networks into heterogeneous graph structures and employs
attribute-level attention mechanisms to capture semantic similarity.
Furthermore, HACD introduces a community membership function to explore
mesoscopic community structures, enhancing the robustness of detected
communities. Extensive experiments demonstrate the effectiveness and efficiency
of HACD, outperforming state-of-the-art methods in attributed community
detection tasks. Our code is publicly available at
https://github.com/Anniran1/HACD1-wsdm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially private and decentralized randomized power method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Nicolas, César Sabater, Mohamed Maouche, Sonia Ben Mokhtar, Mark Coates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The randomized power method has gained significant interest due to its
simplicity and efficient handling of large-scale spectral analysis and
recommendation tasks. As modern datasets contain sensitive private information,
we need to give formal guarantees on the possible privacy leaks caused by this
method. This paper focuses on enhancing privacy preserving variants of the
method. We propose a strategy to reduce the variance of the noise introduced to
achieve Differential Privacy (DP). We also adapt the method to a decentralized
framework with a low computational and communication overhead, while preserving
the accuracy. We leverage Secure Aggregation (a form of Multi-Party
Computation) to allow the algorithm to perform computations using data
distributed among multiple users or devices, without revealing individual data.
We show that it is possible to use a noise scale in the decentralized setting
that is similar to the one in the centralized setting. We improve upon existing
convergence bounds for both the centralized and decentralized versions. The
proposed method is especially relevant for decentralized applications such as
distributed recommender systems, where privacy concerns are paramount.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Landscape for Generative Sequence Models for Specialized
  Data Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Zbeeb, Mohammad Ghorayeb, Mariam Salman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) research often aims to develop models that
generalize reliably across complex datasets, yet this remains challenging in
fields where data is scarce, intricate, or inaccessible. This paper introduces
a novel approach leveraging three generative models of varying complexity to
synthesize one of the most demanding structured datasets: Malicious Network
Traffic. Our approach transforms numerical data into text, reframing data
generation as a language modeling task, which enhances data regularization and
significantly improves generalization and the quality of the synthetic data.
Extensive statistical analyses demonstrate that our method surpasses
state-of-the-art generative models in producing high-fidelity synthetic data.
Additionally, we conduct a comprehensive study on synthetic data applications,
effectiveness, and evaluation strategies, offering valuable insights into its
role across various domains. Our code and pre-trained models are openly
accessible at
https://github.com/Moe-Zbeeb/Exploring-the-landscape-for-generative-models-for-specialized-data-generation,
enabling further exploration and application of our methodology.
  Index Terms: Data synthesis, machine learning, traffic generation,
privacy-preserving data, generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 7 figures, 3 tables, 1 algorithm. code @
  https://github.com/Moe-Zbeeb/Exploring-the-landscape-for-generative-models-for-specialized-data-generation.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept
  Space <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, Hidenori Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern generative models demonstrate impressive capabilities, likely stemming
from an ability to identify and manipulate abstract concepts underlying their
training data. However, fundamental questions remain: what determines the
concepts a model learns, the order in which it learns them, and its ability to
manipulate those concepts? To address these questions, we propose analyzing a
model's learning dynamics via a framework we call the concept space, where each
axis represents an independent concept underlying the data generating process.
By characterizing learning dynamics in this space, we identify how the speed at
which a concept is learned, and hence the order of concept learning, is
controlled by properties of the data we term concept signal. Further, we
observe moments of sudden turns in the direction of a model's learning dynamics
in concept space. Surprisingly, these points precisely correspond to the
emergence of hidden capabilities, i.e., where latent interventions show the
model possesses the capability to manipulate a concept, but these capabilities
cannot yet be elicited via naive input prompting. While our results focus on
synthetically defined toy datasets, we hypothesize a general claim on emergence
of hidden capabilities may hold: generative models possess latent capabilities
that emerge suddenly and consistently during training, though a model might not
exhibit these capabilities under naive input prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Imitation to Refinement -- Residual RL for Precise Assembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in behavior cloning (BC), like action-chunking and diffusion,
have led to impressive progress. Still, imitation alone remains insufficient
for tasks requiring reliable and precise movements, such as aligning and
inserting objects. Our key insight is that chunked BC policies function as
trajectory planners, enabling long-horizon tasks. Conversely, as they execute
action chunks open-loop, they lack the fine-grained reactivity necessary for
reliable execution. Further, we find that the performance of BC policies
saturates despite increasing data. Reinforcement learning (RL) is a natural way
to overcome this, but it is not straightforward to apply directly to
action-chunked models like diffusion policies. We present a simple yet
effective method, ResiP (Residual for Precise Manipulation), that sidesteps
these challenges by augmenting a frozen, chunked BC model with a fully
closed-loop residual policy trained with RL. The residual policy is trained via
on-policy RL, addressing distribution shifts and introducing reactivity without
altering the BC trajectory planner. Evaluation on high-precision manipulation
tasks demonstrates strong performance of ResiP over BC methods and direct RL
fine-tuning. Videos, code, and data are available at
\url{https://residual-assembly.github.io}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Price Homogeneous Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05484v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05484v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keran Chen, Joon Suk Huh, Kirthevasan Kandasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a data pricing problem, where a seller has access to $N$ homogeneous
data points (e.g. drawn i.i.d. from some distribution). There are $m$ types of
buyers in the market, where buyers of the same type $i$ have the same valuation
curve $v_i:[N]\rightarrow [0,1]$, where $v_i(n)$ is the value for having $n$
data points. A priori, the seller is unaware of the distribution of buyers, but
can repeat the market for $T$ rounds so as to learn the revenue-optimal pricing
curve $p:[N] \rightarrow [0, 1]$. To solve this online learning problem, we
first develop novel discretization schemes to approximate any pricing curve.
When compared to prior work, the size of our discretization schemes scales
gracefully with the approximation parameter, which translates to better regret
in online learning. Under assumptions like smoothness and diminishing returns
which are satisfied by data, the discretization size can be reduced further. We
then turn to the online learning problem, both in the stochastic and
adversarial settings. On each round, the seller chooses an anonymous pricing
curve $p_t$. A new buyer appears and may choose to purchase some amount of
data. She then reveals her type only if she makes a purchase. Our online
algorithms build on classical algorithms such as UCB and FTPL, but require
novel ideas to account for the asymmetric nature of this feedback and to deal
with the vastness of the space of pricing curves. Using the improved
discretization schemes previously developed, we are able to achieve
$\tilde{O}(m\sqrt{T})$ regret in the stochastic setting and
$\tilde{O}(m^{3/2}\sqrt{T})$ regret in the adversarial setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Thirty-Eighth Annual Conference on Neural Information Processing
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMMA: End-to-End Multimodal Model for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, Yin Zhou, James Guo, Dragomir Anguelov, Mingxing Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.
Built on a multi-modal large language model foundation, EMMA directly maps raw
camera sensor data into various driving-specific outputs, including planner
trajectories, perception objects, and road graph elements. EMMA maximizes the
utility of world knowledge from the pre-trained large language models, by
representing all non-sensor inputs (e.g. navigation instructions and ego
vehicle status) and outputs (e.g. trajectories and 3D locations) as natural
language text. This approach allows EMMA to jointly process various driving
tasks in a unified language space, and generate the outputs for each task using
task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by
achieving state-of-the-art performance in motion planning on nuScenes as well
as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also
yields competitive results for camera-primary 3D object detection on the Waymo
Open Dataset (WOD). We show that co-training EMMA with planner trajectories,
object detection, and road graph tasks yields improvements across all three
domains, highlighting EMMA's potential as a generalist model for autonomous
driving applications. However, EMMA also exhibits certain limitations: it can
process only a small amount of image frames, does not incorporate accurate 3D
sensing modalities like LiDAR or radar and is computationally expensive. We
hope that our results will inspire further research to mitigate these issues
and to further evolve the state of the art in autonomous driving model
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Blog post: https://waymo.com/blog/2024/10/introducing-emma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Generative Ray Path Sampling for Faster Point-to-Point Ray
  Tracing <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérome Eertmans, Nicola Di Cicco, Claude Oestges, Laurent Jacques, Enrico M. Vittuci, Vittorio Degli-Esposti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio propagation modeling is essential in telecommunication research, as
radio channels result from complex interactions with environmental objects.
Recently, Machine Learning has been attracting attention as a potential
alternative to computationally demanding tools, like Ray Tracing, which can
model these interactions in detail. However, existing Machine Learning
approaches often attempt to learn directly specific channel characteristics,
such as the coverage map, making them highly specific to the frequency and
material properties and unable to fully capture the underlying propagation
mechanisms. Hence, Ray Tracing, particularly the Point-to-Point variant,
remains popular to accurately identify all possible paths between transmitter
and receiver nodes. Still, path identification is computationally intensive
because the number of paths to be tested grows exponentially while only a small
fraction is valid. In this paper, we propose a Machine Learning-aided Ray
Tracing approach to efficiently sample potential ray paths, significantly
reducing the computational load while maintaining high accuracy. Our model
dynamically learns to prioritize potentially valid paths among all possible
paths and scales linearly with scene complexity. Unlike recent alternatives,
our approach is invariant with translation, scaling, or rotation of the
geometry, and avoids dependency on specific environment characteristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, submitted to IEEE ICMLCN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Design for Human Preference Elicitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13895v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13895v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhojyoti Mukherjee, Anusha Lalitha, Kousha Kalantari, Aniket Deshmukh, Ge Liu, Yifei Ma, Branislav Kveton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning of preference models from human feedback has been central to recent
advances in artificial intelligence. Motivated by the cost of obtaining
high-quality human annotations, we study efficient human preference elicitation
for learning preference models. The key idea in our work is to generalize
optimal designs, a methodology for computing optimal information-gathering
policies, to questions with multiple answers, represented as lists of items.
The policy is a distribution over lists and we elicit preferences from the list
proportionally to its probability. To show the generality of our ideas, we
study both absolute and ranking feedback models on items in the list. We design
efficient algorithms for both and analyze them. Finally, we demonstrate that
our algorithms are practical by evaluating them on existing question-answering
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Advances in Neural Information Processing Systems 37</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Layer Pruning Through Similarity Metric Perspective <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Pons, Bruno Yamamoto, Anna H. Reali Costa, Artur Jordao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have been the predominant paradigm in machine learning
for solving cognitive tasks. Such models, however, are restricted by a high
computational overhead, limiting their applicability and hindering advancements
in the field. Extensive research demonstrated that pruning structures from
these models is a straightforward approach to reducing network complexity. In
this direction, most efforts focus on removing weights or filters. Studies have
also been devoted to layer pruning as it promotes superior computational gains.
However, layer pruning often hurts the network predictive ability (i.e.,
accuracy) at high compression rates. This work introduces an effective
layer-pruning strategy that meets all underlying properties pursued by pruning
methods. Our method estimates the relative importance of a layer using the
Centered Kernel Alignment (CKA) metric, employed to measure the similarity
between the representations of the unpruned model and a candidate layer for
pruning. We confirm the effectiveness of our method on standard architectures
and benchmarks, in which it outperforms existing layer-pruning strategies and
other state-of-the-art pruning techniques. Particularly, we remove more than
75% of computation while improving predictive ability. At higher compression
regimes, our method exhibits negligible accuracy drop, while other methods
notably deteriorate model accuracy. Apart from these benefits, our pruned
models exhibit robustness to adversarial and out-of-distribution samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Pattern Recognition (ICPR),
  2024. Oral presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiasheng Zhang, Jialin Chen, Menglin Yang, Aosong Feng, Shuang Liang, Jie Shao, Rex Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world
scenarios, where each node and edge are associated with text descriptions, and
both the graph structure and text descriptions evolve over time. Despite their
broad applicability, there is a notable scarcity of benchmark datasets tailored
to DyTAGs, which hinders the potential advancement in many research fields. To
address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),
a collection of large-scale, time-evolving graphs from diverse domains, with
nodes and edges enriched by dynamically changing text attributes and
categories. To facilitate the use of DTGB, we design standardized evaluation
procedures based on four real-world use cases: future link prediction,
destination node retrieval, edge classification, and textual relation
generation. These tasks require models to understand both dynamic graph
structures and natural language, highlighting the unique challenges posed by
DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,
evaluating 7 popular dynamic graph learning algorithms and their variants of
adapting to text attributes with LLM embeddings, along with 6 powerful large
language models (LLMs). Our results show the limitations of existing models in
handling DyTAGs. Our analysis also demonstrates the utility of DTGB in
investigating the incorporation of structural and textual dynamics. The
proposed DTGB fosters research on DyTAGs and their broad applications. It
offers a comprehensive benchmark for evaluating and advancing models to handle
the interplay between dynamic graph structures and natural language. The
dataset and source code are available at https://github.com/zjs123/DTGB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 13 figures, camera-ready version for NeurIPS 2024 Datasets
  and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bypassing the Noisy Parity Barrier: Learning Higher-Order Markov Random
  Fields from Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Gaitonde, Ankur Moitra, Elchanan Mossel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning graphical models, also known as Markov
random fields (MRFs) from temporally correlated samples. As in many traditional
statistical settings, fundamental results in the area all assume independent
samples from the distribution. However, these samples generally will not
directly correspond to more realistic observations from nature, which instead
evolve according to some stochastic process. From the computational lens, even
generating a single sample from the true MRF distribution is intractable unless
$\mathsf{NP}=\mathsf{RP}$, and moreover, any algorithm to learn from i.i.d.
samples requires prohibitive runtime due to hardness reductions to the parity
with noise problem. These computational barriers for sampling and learning from
the i.i.d. setting severely lessen the utility of these breakthrough results
for this important task; however, dropping this assumption typically only
introduces further algorithmic and statistical complexities.
  In this work, we surprisingly demonstrate that the direct trajectory data
from a natural evolution of the MRF overcomes the fundamental computational
lower bounds to efficient learning. In particular, we show that given a
trajectory with $\widetilde{O}_k(n)$ site updates of an order $k$ MRF from the
Glauber dynamics, a well-studied, natural stochastic process on graphical
models, there is an algorithm that recovers the graph and the parameters in
$\widetilde{O}_k(n^2)$ time. By contrast, all prior algorithms for learning
order $k$ MRFs inherently suffer from $n^{\Theta(k)}$ runtime even in sparse
instances due to the reductions to sparse parity with noise. Our results thus
surprisingly show that this more realistic, but intuitively less tractable,
model for MRFs actually leads to efficiency far beyond what is known and
believed to be true in the traditional i.i.d. case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Identifiable Factorized Causal Representations of Cellular
  Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyi Mao, Romain Lopez, Kai Liu, Jan-Christian Huetter, David Richmond, Panayiotis V. Benos, Lin Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of cells and their responses to genetic or chemical perturbations
promises to accelerate the discovery of therapeutic targets. However, designing
adequate and insightful models for such data is difficult because the response
of a cell to perturbations essentially depends on its biological context (e.g.,
genetic background or cell type). For example, while discovering therapeutic
targets, one may want to enrich for drugs that specifically target a certain
cell type. This challenge emphasizes the need for methods that explicitly take
into account potential interactions between drugs and contexts. Towards this
goal, we propose a novel Factorized Causal Representation (FCR)
  learning method that reveals causal structure in single-cell perturbation
data from several cell lines. Based on the framework of identifiable deep
generative models, FCR learns multiple cellular representations that are
disentangled, comprised of covariate-specific ($\mathbf{z}_x$),
treatment-specific ($\mathbf{z}_{t}$), and interaction-specific
($\mathbf{z}_{tx}$) blocks. Based on recent advances in non-linear ICA theory,
we prove the component-wise identifiability of $\mathbf{z}_{tx}$ and block-wise
identifiability of $\mathbf{z}_t$ and $\mathbf{z}_x$. Then, we present our
implementation of FCR, and empirically demonstrate that it outperforms
state-of-the-art baselines in various tasks across four single-cell datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization on a Finer Scale: Bounded Local Subgradient Variation
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jelena Diakonikolas, Cristóbal Guzmán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate the study of nonsmooth optimization problems under bounded local
subgradient variation, which postulates bounded difference between
(sub)gradients in small local regions around points, in either average or
maximum sense. The resulting class of objective functions encapsulates the
classes of objective functions traditionally studied in optimization, which are
defined based on either Lipschitz continuity of the objective or
H\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class
contains functions that are neither Lipschitz continuous nor have a H\"{o}lder
continuous gradient. When restricted to the traditional classes of optimization
problems, the parameters defining the studied classes lead to more fine-grained
complexity bounds, recovering traditional oracle complexity bounds in the worst
case but generally leading to lower oracle complexity for functions that are
not ``worst case.'' Some highlights of our results are that: (i) it is possible
to obtain complexity results for both convex and nonconvex problems with the
(local or global) Lipschitz constant being replaced by a constant of local
subgradient variation and (ii) mean width of the subdifferential set around the
optima plays a role in the complexity of nonsmooth optimization, particularly
in parallel settings. A consequence of (ii) is that for any error parameter
$\epsilon > 0$, parallel oracle complexity of nonsmooth Lipschitz convex
optimization is lower than its sequential oracle complexity by a factor
$\tilde{\Omega}\big(\frac{1}{\epsilon}\big)$ whenever the objective function is
piecewise linear with polynomially many pieces in the input size. This is
particularly surprising as existing parallel complexity lower bounds are based
on such classes of functions. The seeming contradiction is resolved by
considering the region in which the algorithm is allowed to query the
objective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chronos: Learning the Language of Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07815v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07815v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, Yuyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Chronos, a simple yet effective framework for pretrained
probabilistic time series models. Chronos tokenizes time series values using
scaling and quantization into a fixed vocabulary and trains existing
transformer-based language model architectures on these tokenized time series
via the cross-entropy loss. We pretrained Chronos models based on the T5 family
(ranging from 20M to 710M parameters) on a large collection of publicly
available datasets, complemented by a synthetic dataset that we generated via
Gaussian processes to improve generalization. In a comprehensive benchmark
consisting of 42 datasets, and comprising both classical local models and deep
learning methods, we show that Chronos models: (a) significantly outperform
other methods on datasets that were part of the training corpus; and (b) have
comparable and occasionally superior zero-shot performance on new datasets,
relative to methods that were trained specifically on them. Our results
demonstrate that Chronos models can leverage time series data from diverse
domains to improve zero-shot accuracy on unseen forecasting tasks, positioning
pretrained models as a viable tool to greatly simplify forecasting pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and model checkpoints available at
  https://github.com/amazon-science/chronos-forecasting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometry of Polynomial Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaie Kubjas, Jiayi Li, Maximilian Wiesmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the expressivity and learning process for polynomial neural networks
(PNNs) with monomial activation functions. The weights of the network
parametrize the neuromanifold. In this paper, we study certain neuromanifolds
using tools from algebraic geometry: we give explicit descriptions as
semialgebraic sets and characterize their Zariski closures, called
neurovarieties. We study their dimension and associate an algebraic degree, the
learning degree, to the neurovariety. The dimension serves as a geometric
measure for the expressivity of the network, the learning degree is a measure
for the complexity of training the network and provides upper bounds on the
number of learnable functions. These theoretical results are accompanied with
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 3 figures. Comments are welcome!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneity-Aware Cooperative Federated Edge Learning with Adaptive
  Computation and Communication Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxiao Zhang, Zhidong Gao, Yuanxiong Guo, Yanmin Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the drawbacks of cloud-based federated learning (FL),
cooperative federated edge learning (CFEL) has been proposed to improve
efficiency for FL over mobile edge networks, where multiple edge servers
collaboratively coordinate the distributed model training across a large number
of edge devices. However, CFEL faces critical challenges arising from dynamic
and heterogeneous device properties, which slow down the convergence and
increase resource consumption. This paper proposes a heterogeneity-aware CFEL
scheme called \textit{Heterogeneity-Aware Cooperative Edge-based Federated
Averaging} (HCEF) that aims to maximize the model accuracy while minimizing the
training time and energy consumption via adaptive computation and communication
compression in CFEL. By theoretically analyzing how local update frequency and
gradient compression affect the convergence error bound in CFEL, we develop an
efficient online control algorithm for HCEF to dynamically determine local
update frequencies and compression ratios for heterogeneous devices.
Experimental results show that compared with prior schemes, the proposed HCEF
scheme can maintain higher model accuracy while reducing training latency and
improving energy efficiency simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compact Language Models via Pruning and Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) targeting different deployment scales and sizes
are currently produced by training each variant from scratch; this is extremely
compute-intensive. In this paper, we investigate if pruning an existing LLM and
then re-training it with a fraction (<3%) of the original training data can be
a suitable alternative to repeated, full retraining. To this end, we develop a
set of practical and effective compression best practices for LLMs that combine
depth, width, attention and MLP pruning with knowledge distillation-based
retraining; we arrive at these best practices through a detailed empirical
exploration of pruning strategies for each axis, methods to combine axes,
distillation strategies, and search techniques for arriving at optimal
compressed architectures. We use this guide to compress the Nemotron-4 family
of LLMs by a factor of 2-4x, and compare their performance to similarly-sized
models on a variety of language modeling tasks. Deriving 8B and 4B models from
an already pretrained 15B model using our approach requires up to 40x fewer
training tokens per model compared to training from scratch; this results in
compute cost savings of 1.8x for training the full model family (15B, 8B, and
4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to
training from scratch, perform comparably to other community models such as
Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art
compression techniques from the literature. We have open-sourced Minitron model
weights on Huggingface, with corresponding supplementary material including
example code available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE) <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P. Calmon, Himabindu Lakkaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP embeddings have demonstrated remarkable performance across a wide range
of multimodal applications. However, these high-dimensional, dense vector
representations are not easily interpretable, limiting our understanding of the
rich structure of CLIP and its use in downstream applications that require
transparency. In this work, we show that the semantic structure of CLIP's
latent space can be leveraged to provide interpretability, allowing for the
decomposition of representations into semantic concepts. We formulate this
problem as one of sparse recovery and propose a novel method, Sparse Linear
Concept Embeddings, for transforming CLIP representations into sparse linear
combinations of human-interpretable concepts. Distinct from previous work,
SpLiCE is task-agnostic and can be used, without training, to explain and even
replace traditional dense CLIP representations, maintaining high downstream
performance while significantly improving their interpretability. We also
demonstrate significant use cases of SpLiCE representations including detecting
spurious correlations and model editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures, NeurIPS 2024. Code is provided at
  https://github.com/AI4LIFE-GROUP/SpLiCE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation
  for Electromyography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jehan Yang, Maxwell Soh, Vivianna Lieu, Douglas J Weber, Zackory Erickson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the first generalization and adaptation benchmark using
machine learning for evaluating out-of-distribution performance of
electromyography (EMG) classification algorithms. The ability of an EMG
classifier to handle inputs drawn from a different distribution than the
training distribution is critical for real-world deployment as a control
interface. By predicting the user's intended gesture using EMG signals, we can
create a wearable solution to control assistive technologies, such as
computers, prosthetics, and mobile manipulator robots. This new
out-of-distribution benchmark consists of two major tasks that have utility for
building robust and adaptable control interfaces: 1) intersubject
classification and 2) adaptation using train-test splits for time-series. This
benchmark spans nine datasets--the largest collection of EMG datasets in a
benchmark. Among these, a new dataset is introduced, featuring a novel,
easy-to-wear high-density EMG wearable for data collection. The lack of
open-source benchmarks has made comparing accuracy results between papers
challenging for the EMG research community. This new benchmark provides
researchers with a valuable resource for analyzing practical measures of
out-of-distribution performance for EMG datasets. Our code and data from our
new dataset can be found at emgbench.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xMIL: Insightful Explanations for Multiple Instance Learning in
  Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Hense, Mina Jamshidi Idaji, Oliver Eberle, Thomas Schnake, Jonas Dippel, Laure Ciernik, Oliver Buchstab, Andreas Mock, Frederick Klauschen, Klaus-Robert Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) is an effective and widely used approach for
weakly supervised machine learning. In histopathology, MIL models have achieved
remarkable success in tasks like tumor detection, biomarker prediction, and
outcome prognostication. However, MIL explanation methods are still lagging
behind, as they are limited to small bag sizes or disregard instance
interactions. We revisit MIL through the lens of explainable AI (XAI) and
introduce xMIL, a refined framework with more general assumptions. We
demonstrate how to obtain improved MIL explanations using layer-wise relevance
propagation (LRP) and conduct extensive evaluation experiments on three toy
settings and four real-world histopathology datasets. Our approach consistently
outperforms previous explanation attempts with particularly improved
faithfulness scores on challenging biomarker prediction tasks. Finally, we
showcase how xMIL explanations enable pathologists to extract insights from MIL
models, representing a significant advance for knowledge discovery and model
debugging in digital histopathology. Codes are available at:
https://github.com/tubml-pathology/xMIL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ emg2qwerty: A Large <span class="highlight-title">Dataset</span> with Baselines for Touch Typing using
  Surface Electromyography <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viswanath Sivakumar, Jeffrey Seely, Alan Du, Sean R Bittner, Adam Berenzweig, Anuoluwapo Bolarinwa, Alexandre Gramfort, Michael I Mandel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface electromyography (sEMG) non-invasively measures signals generated by
muscle activity with sufficient sensitivity to detect individual spinal neurons
and richness to identify dozens of gestures and their nuances. Wearable
wrist-based sEMG sensors have the potential to offer low friction, subtle,
information rich, always available human-computer inputs. To this end, we
introduce emg2qwerty, a large-scale dataset of non-invasive electromyographic
signals recorded at the wrists while touch typing on a QWERTY keyboard,
together with ground-truth annotations and reproducible baselines. With 1,135
sessions spanning 108 users and 346 hours of recording, this is the largest
such public dataset to date. These data demonstrate non-trivial, but well
defined hierarchical relationships both in terms of the generative process,
from neurons to muscles and muscle combinations, as well as in terms of domain
shift across users and user sessions. Applying standard modeling techniques
from the closely related field of Automatic Speech Recognition (ASR), we show
strong baseline performance on predicting key-presses using sEMG signals alone.
We believe the richness of this task and dataset will facilitate progress in
several problems of interest to both the machine learning and neuroscientific
communities. Dataset and code can be accessed at
https://github.com/facebookresearch/emg2qwerty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Game Level Repair via Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Bazzaz, Seth Cooper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedurally generated levels created by machine learning models can be
unsolvable without further editing. Various methods have been developed to
automatically repair these levels by enforcing hard constraints during the
post-processing step. However, as levels increase in size, these
constraint-based repairs become increasingly slow. This paper proposes using
explainability methods to identify specific regions of a level that contribute
to its unsolvability. By assigning higher weights to these regions,
constraint-based solvers can prioritize these problematic areas, enabling more
efficient repairs. Our results, tested across three games, demonstrate that
this approach can help to repair procedurally generated levels faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to grok: Emergence of in-context learning and skill composition
  in modular arithmetic tasks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models can solve tasks that were not present in the training
set. This capability is believed to be due to in-context learning and skill
composition. In this work, we study the emergence of in-context learning and
skill composition in a collection of modular arithmetic tasks. Specifically, we
consider a finite collection of linear modular functions $z = a \, x + b \, y
\;\mathrm{mod}\; p$ labeled by the vector $(a, b) \in \mathbb{Z}_p^2$. We use
some of these tasks for pre-training and the rest for out-of-distribution
testing. We empirically show that a GPT-style transformer exhibits a transition
from in-distribution to out-of-distribution generalization as the number of
pre-training tasks increases. We find that the smallest model capable of
out-of-distribution generalization requires two transformer blocks, while for
deeper models, the out-of-distribution generalization phase is
\emph{transient}, necessitating early stopping. Finally, we perform an
interpretability study of the pre-trained models, revealing highly structured
representations in both attention heads and MLPs; and discuss the learned
algorithms. Notably, we find an algorithmic shift in deeper models, as we go
from few to many in-context examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version, NeurIPS 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Ground State Properties: Constant Sample Complexity and Deep
  Learning Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Wanner, Laura Lewis, Chiranjib Bhattacharyya, Devdatt Dubhashi, Alexandru Gheorghiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental problem in quantum many-body physics is that of finding ground
states of local Hamiltonians. A number of recent works gave provably efficient
machine learning (ML) algorithms for learning ground states. Specifically,
[Huang et al. Science 2022], introduced an approach for learning properties of
the ground state of an $n$-qubit gapped local Hamiltonian $H$ from only
$n^{\mathcal{O}(1)}$ data points sampled from Hamiltonians in the same phase of
matter. This was subsequently improved by [Lewis et al. Nature Communications
2024], to $\mathcal{O}(\log n)$ samples when the geometry of the $n$-qubit
system is known. In this work, we introduce two approaches that achieve a
constant sample complexity, independent of system size $n$, for learning ground
state properties. Our first algorithm consists of a simple modification of the
ML model used by Lewis et al. and applies to a property of interest known
beforehand. Our second algorithm, which applies even if a description of the
property is not known, is a deep neural network model. While empirical results
showing the performance of neural networks have been demonstrated, to our
knowledge, this is the first rigorous sample complexity bound on a neural
network model for predicting ground state properties. We also perform numerical
experiments that confirm the improved scaling of our approach compared to
earlier results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures + 40-page appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Human Impressions of Robot Performance During Navigation
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiping Zhang, Nathan Tsoi, Mofeed Nagib, Booyeon Choi, Jie Tan, Hao-Tien Lewis Chiang, Marynel Vázquez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human impressions of robot performance are often measured through surveys. As
a more scalable and cost-effective alternative, we investigate the possibility
of predicting people's impressions of robot behavior using non-verbal
behavioral cues and machine learning techniques. To this end, we first
contribute the SEAN TOGETHER Dataset consisting of observations of an
interaction between a person and a mobile robot in a VR simulation, together
with impressions of robot performance provided by users on a 5-point scale.
Second, we contribute analyses of how well humans and supervised learning
techniques can predict perceived robot performance based on different
observation types (like facial expression features, and features that describe
the navigation behavior of the robot and pedestrians). Our results suggest that
facial expressions alone provide useful information about human impressions of
robot performance; but in the navigation scenarios that we considered,
reasoning about spatial features in context is critical for the prediction
task. Also, supervised learning techniques showed promise because they
outperformed humans' predictions of robot performance in most cases. Further,
when predicting robot performance as a binary classification task on unseen
users' data, the F1 Score of machine learning models more than doubled in
comparison to predicting performance on a 5-point scale. This suggested that
the models can have good generalization capabilities, although they are better
at telling the directionality of robot performance than predicting exact
performance ratings. Based on our findings in simulation, we conducted a
real-world demonstration in which a mobile robot uses a machine learning model
to predict how a human that follows it perceives it. Finally, we discuss the
implications of our results for implementing such supervised learning models in
real-world navigation scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast yet Safe: Early-Exiting with Risk Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Metod Jazbec, Alexander Timans, Tin Hadži Veljković, Kaspar Sakmann, Dan Zhang, Christian A. Naesseth, Eric Nalisnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling machine learning models significantly improves their performance.
However, such gains come at the cost of inference being slow and
resource-intensive. Early-exit neural networks (EENNs) offer a promising
solution: they accelerate inference by allowing intermediate layers to exit and
produce a prediction early. Yet a fundamental issue with EENNs is how to
determine when to exit without severely degrading performance. In other words,
when is it 'safe' for an EENN to go 'fast'? To address this issue, we
investigate how to adapt frameworks of risk control to EENNs. Risk control
offers a distribution-free, post-hoc solution that tunes the EENN's exiting
mechanism so that exits only occur when the output is of sufficient quality. We
empirically validate our insights on a range of vision and language tasks,
demonstrating that risk control can produce substantial computational savings,
all the while preserving user-specified performance goals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 13 figures, 4 tables (incl. appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Descent on Logistic Regression with Non-Separable Data and
  Large Step Sizes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si Yi Meng, Antonio Orvieto, Daniel Yiming Cao, Christopher De Sa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study gradient descent (GD) dynamics on logistic regression problems with
large, constant step sizes. For linearly-separable data, it is known that GD
converges to the minimizer with arbitrarily large step sizes, a property which
no longer holds when the problem is not separable. In fact, the behaviour can
be much more complex -- a sequence of period-doubling bifurcations begins at
the critical step size $2/\lambda$, where $\lambda$ is the largest eigenvalue
of the Hessian at the solution. Using a smaller-than-critical step size
guarantees convergence if initialized nearby the solution: but does this
suffice globally? In one dimension, we show that a step size less than
$1/\lambda$ suffices for global convergence. However, for all step sizes
between $1/\lambda$ and the critical step size $2/\lambda$, one can construct a
dataset such that GD converges to a stable cycle. In higher dimensions, this is
actually possible even for step sizes less than $1/\lambda$. Our results show
that although local convergence is guaranteed for all step sizes less than the
critical step size, global convergence is not, and GD may instead converge to a
cycle depending on the initialization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Random Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19987v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19987v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Andrecut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The single-layer feedforward neural network with random weights is a
recurring motif in the neural networks literature. The advantage of these
networks is their simplified training, which reduces to solving a
ridge-regression problem. A general assumption is that these networks require a
large number of hidden neurons relative to the dimensionality of the data
samples, in order to achieve good classification accuracy. Contrary to this
assumption, here we show that one can obtain good classification results even
if the number of hidden neurons has the same order of magnitude as the
dimensionality of the data samples, if this dimensionality is reasonably high.
Inspired by this result, we also develop an efficient iterative residual
training method for such random neural networks, and we extend the algorithm to
the least-squares kernel version of the neural network model. Moreover, we also
describe an encryption (obfuscation) method which can be used to protect both
the data and the resulted network model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>revised version, 17 pages, 8 figures, added kernel method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the
  Generalised $h$-transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modelling paradigms based on denoising diffusion processes have
emerged as a leading candidate for conditional sampling in inverse problems. In
many real-world applications, we often have access to large, expensively
trained unconditional diffusion models, which we aim to exploit for improving
conditional sampling. Most recent approaches are motivated heuristically and
lack a unifying framework, obscuring connections between them. Further, they
often suffer from issues such as being very sensitive to hyperparameters, being
expensive to train or needing access to weights hidden behind a closed API. In
this work, we unify conditional training and sampling using the mathematically
well-understood Doob's h-transform. This new perspective allows us to unify
many existing methods under a common umbrella. Under this framework, we propose
DEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional
generation that simply fine-tunes a very small network to quickly learn the
conditional $h$-transform, while keeping the larger unconditional network
unchanged. DEFT is much faster than existing baselines while achieving
state-of-the-art performance across a variety of linear and non-linear
benchmarks. On image reconstruction tasks, we achieve speedups of up to
1.6$\times$, while having the best perceptual quality on natural images and
reconstruction performance on medical images. Further, we also provide initial
experiments on protein motif scaffolding and outperform reconstruction guidance
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.09236</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RobotKeyframing: Learning Locomotion with High-Level Objectives via
  Mixture of Dense and Sparse Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Zargarbashi, Jin Cheng, Dongho Kang, Robert Sumner, Stelian Coros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel learning-based control framework that uses
keyframing to incorporate high-level objectives in natural locomotion for
legged robots. These high-level objectives are specified as a variable number
of partial or complete pose targets that are spaced arbitrarily in time. Our
proposed framework utilizes a multi-critic reinforcement learning algorithm to
effectively handle the mixture of dense and sparse rewards. Additionally, it
employs a transformer-based encoder to accommodate a variable number of input
targets, each associated with specific time-to-arrivals. Throughout simulation
and hardware experiments, we demonstrate that our framework can effectively
satisfy the target keyframe sequence at the required times. In the experiments,
the multi-critic method significantly reduces the effort of hyperparameter
tuning compared to the standard single-critic alternative. Moreover, the
proposed transformer-based architecture enables robots to anticipate future
goals, which results in quantitative improvements in their ability to reach
their targets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to 8th Conference on Robot Learning
  (CoRL 2024). Project website: https://sites.google.com/view/robot-keyframing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modular Quantization-Aware Training for 6D Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06753v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06753v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saqib Javed, Chengkun Li, Andrew Price, Yinlin Hu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge applications, such as collaborative robotics and spacecraft rendezvous,
demand efficient 6D object pose estimation on resource-constrained embedded
platforms. Existing 6D pose estimation networks are often too large for such
deployments, necessitating compression while maintaining reliable performance.
To address this challenge, we introduce Modular Quantization-Aware Training
(MQAT), an adaptive and mixed-precision quantization-aware training strategy
that exploits the modular structure of modern 6D pose estimation architectures.
MQAT guides a systematic gradated modular quantization sequence and determines
module-specific bit precisions, leading to quantized models that outperform
those produced by state-of-the-art uniform and mixed-precision quantization
techniques. Our experiments showcase the generality of MQAT across datasets,
architectures, and quantization algorithms. Remarkably, MQAT-trained quantized
models achieve a significant accuracy boost (>7%) over the baseline
full-precision network while reducing model size by a factor of 4x or more. Our
project website is at: https://saqibjaved1.github.io/MQAT_/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointNCBW: Towards <span class="highlight-title">Dataset</span> Ownership Verification for Point Clouds via
  Negative Clean-label Backdoor Watermark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wei, Yang Wang, Kuofeng Gao, Shuo Shao, Yiming Li, Zhibo Wang, Zhan Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, point clouds have been widely used in computer vision, whereas
their collection is time-consuming and expensive. As such, point cloud datasets
are the valuable intellectual property of their owners and deserve protection.
To detect and prevent unauthorized use of these datasets, especially for
commercial or open-sourced ones that cannot be sold again or used commercially
without permission, we intend to identify whether a suspicious third-party
model is trained on our protected dataset under the black-box setting. We
achieve this goal by designing a scalable clean-label backdoor-based dataset
watermark for point clouds that ensures both effectiveness and stealthiness.
Unlike existing clean-label watermark schemes, which are susceptible to the
number of categories, our method could watermark samples from all classes
instead of only from the target one. Accordingly, it can still preserve high
effectiveness even on large-scale datasets with many classes. Specifically, we
perturb selected point clouds with non-target categories in both shape-wise and
point-wise manners before inserting trigger patterns without changing their
labels. The features of perturbed samples are similar to those of benign
samples from the target class. As such, models trained on the watermarked
dataset will have a distinctive yet stealthy backdoor behavior, i.e.,
misclassifying samples from the target class whenever triggers appear, since
the trained DNNs will treat the inserted trigger pattern as a signal to deny
predicting the target label. We also design a hypothesis-test-guided dataset
ownership verification based on the proposed watermark. Extensive experiments
on benchmark datasets are conducted, verifying the effectiveness of our method
and its resistance to potential removal methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by IEEE Transactions on Information Forensics
  and Security (TIFS), 2024. 16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reduce, Reuse, Recycle: Categories for Compositional Reinforcement
  Learning <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Bakirtzis, Michail Savvas, Ruihan Zhao, Sandeep Chinchali, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, conducting task composition by forming cohesive,
executable sequences from multiple tasks remains challenging. However, the
ability to (de)compose tasks is a linchpin in developing robotic systems
capable of learning complex behaviors. Yet, compositional reinforcement
learning is beset with difficulties, including the high dimensionality of the
problem space, scarcity of rewards, and absence of system robustness after task
composition. To surmount these challenges, we view task composition through the
prism of category theory -- a mathematical discipline exploring structures and
their compositional relationships. The categorical properties of Markov
decision processes untangle complex tasks into manageable sub-tasks, allowing
for strategical reduction of dimensionality, facilitating more tractable reward
structures, and bolstering system robustness. Experimental results support the
categorical theory of reinforcement learning by enabling skill reduction,
reuse, and recycling when learning complex robotic arm tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TorchTitan: One-stop PyTorch native solution for production ready LLM
  <span class="highlight-title">pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, Sanket Purandare, Gokul Nadathur, Stratos Idreos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has been instrumental in
advancing state-of-the-art natural language processing applications. Training
LLMs with billions of parameters and trillions of tokens require sophisticated
distributed systems that enable composing and comparing several
state-of-the-art techniques in order to efficiently scale across thousands of
accelerators. However, existing solutions are complex, scattered across
multiple libraries/repositories, lack interoperability, and are cumbersome to
maintain. Thus, curating and empirically comparing training recipes require
non-trivial engineering effort.
  This paper introduces TorchTitan, an open-source, PyTorch-native distributed
training system that unifies state-of-the-art techniques, streamlining
integration and reducing overhead. TorchTitan enables 3D parallelism in a
modular manner with elastic scaling, providing comprehensive logging,
checkpointing, and debugging tools for production-ready training. It also
incorporates hardware-software co-designed solutions, leveraging features like
Float8 training and SymmetricMemory. As a flexible test bed, TorchTitan
facilitates custom recipe curation and comparison, allowing us to develop
optimized training recipes for Llama 3.1 and provide guidance on selecting
techniques for maximum efficiency based on our experiences.
  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8
billion to 405 billion parameters, and showcase its exceptional performance,
modular composability, and elastic scalability. By stacking training
optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at
the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at
the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at
the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniMAP: Universal SMILES-Graph Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikun Feng, Lixin Yang, Yanwen Huang, Yuyan Ni, Weiying Ma, Yanyan Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular representation learning is fundamental for many drug related
applications. Most existing molecular pre-training models are limited in using
single molecular modality, either SMILES or graph representation. To
effectively leverage both modalities, we argue that it is critical to capture
the fine-grained 'semantics' between SMILES and graph, because subtle
sequence/graph differences may lead to contrary molecular properties. In this
paper, we propose a universal SMILE-graph representation learning model, namely
UniMAP. Firstly, an embedding layer is employed to obtain the token and
node/edge representation in SMILES and graph, respectively. A multi-layer
Transformer is then utilized to conduct deep cross-modality fusion. Specially,
four kinds of pre-training tasks are designed for UniMAP, including Multi-Level
Cross-Modality Masking (CMM), SMILES-Graph Matching (SGM), Fragment-Level
Alignment (FLA), and Domain Knowledge Learning (DKL). In this way, both global
(i.e. SGM and DKL) and local (i.e. CMM and FLA) alignments are integrated to
achieve comprehensive cross-modality fusion. We evaluate UniMAP on various
downstream tasks, i.e. molecular property prediction, drug-target affinity
prediction and drug-drug interaction. Experimental results show that UniMAP
outperforms current state-of-the-art pre-training methods.We also visualize the
learned representations to demonstrate the effect of multi-modality
integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effectiveness of Curvature-Based Rewiring and the Role of
  Hyperparameters in GNNs Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Floriano Tori, Vincent Holst, Vincent Ginis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message passing is the dominant paradigm in Graph Neural Networks (GNNs). The
efficiency of message passing, however, can be limited by the topology of the
graph. This happens when information is lost during propagation due to being
oversquashed when travelling through bottlenecks. To remedy this, recent
efforts have focused on graph rewiring techniques, which disconnect the input
graph originating from the data and the computational graph, on which message
passing is performed. A prominent approach for this is to use discrete graph
curvature measures, of which several variants have been proposed, to identify
and rewire around bottlenecks, facilitating information propagation. While
oversquashing has been demonstrated in synthetic datasets, in this work we
reevaluate the performance gains that curvature-based rewiring brings to
real-world datasets. We show that in these datasets, edges selected during the
rewiring process are not in line with theoretical criteria identifying
bottlenecks. This implies they do not necessarily oversquash information during
message passing. Subsequently, we demonstrate that SOTA accuracies on these
datasets are outliers originating from sweeps of hyperparameters -- both the
ones for training and dedicated ones related to the rewiring algorithm --
instead of consistent performance gains. In conclusion, our analysis nuances
the effectiveness of curvature-based rewiring in real-world datasets and brings
a new perspective on the methods to evaluate GNN accuracy improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Central Role of the Loss Function in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Wang, Nathan Kallus, Wen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper illustrates the central role of loss functions in data-driven
decision making, providing a comprehensive survey on their influence in
cost-sensitive classification (CSC) and reinforcement learning (RL). We
demonstrate how different regression loss functions affect the sample
efficiency and adaptivity of value-based decision making algorithms. Across
multiple settings, we prove that algorithms using the binary cross-entropy loss
achieve first-order bounds scaling with the optimal policy's cost and are much
more efficient than the commonly used squared loss. Moreover, we prove that
distributional algorithms using the maximum likelihood loss achieve
second-order bounds scaling with the policy variance and are even sharper than
first-order bounds. This in particular proves the benefits of distributional
RL. We hope that this paper serves as a guide analyzing decision making
algorithms with varying loss functions, and can inspire the reader to seek out
better loss functions to improve any decision making algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISTINQT: A Distributed Privacy Aware Learning Framework for QoS
  Prediction for Future Mobile and Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10158v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10158v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Koursioumpas, Lina Magoula, Ioannis Stavrakakis, Nancy Alonistioti, M. A. Gutierrez-Estevez, Ramin Khalili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond 5G and 6G networks are expected to support new and challenging use
cases and applications that depend on a certain level of Quality of Service
(QoS) to operate smoothly. Predicting the QoS in a timely manner is of high
importance, especially for safety-critical applications as in the case of
vehicular communications. Although until recent years the QoS prediction has
been carried out by centralized Artificial Intelligence (AI) solutions, a
number of privacy, computational, and operational concerns have emerged.
Alternative solutions have surfaced (e.g. Split Learning, Federated Learning),
distributing AI tasks of reduced complexity across nodes, while preserving the
privacy of the data. However, new challenges rise when it comes to scalable
distributed learning approaches, taking into account the heterogeneous nature
of future wireless networks. The current work proposes DISTINQT, a novel
multi-headed input privacy-aware distributed learning framework for QoS
prediction. Our framework supports multiple heterogeneous nodes, in terms of
data types and model architectures, by sharing computations across them. This
enables the incorporation of diverse knowledge into a sole learning process
that will enhance the robustness and generalization capabilities of the final
QoS prediction model. DISTINQT also contributes to data privacy preservation by
encoding any raw input data into highly complex, compressed, and irreversible
latent representations before any transmission. Evaluation results showcase
that DISTINQT achieves a statistically identical performance compared to its
centralized version, while also proving the validity of the privacy preserving
claims. DISTINQT manages to achieve a reduction in prediction error of up to
65% on average against six state-of-the-art centralized baseline solutions
presented in the Tele-Operated Driving use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 Pages Double Column, 10 Figures, (Minor Revised Version) Accepted
  for publication in the IEEE Transactions on Vehicular Technology (IEEE TVT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiVLC: Extending Vision-Language Compositionality Evaluation with
  Text-to-Image Retrieval <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imanol Miranda, Ander Salaberria, Eneko Agirre, Gorka Azkune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe
are formulated as image-to-text retrieval problems, where, given an image, the
models need to select between the correct textual description and a synthetic
hard negative text. In this work, we present the Bidirectional Vision-Language
Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic
hard negative image generated from the synthetic text, resulting in two
image-to-text retrieval examples (one for each image) and, more importantly,
two text-to-image retrieval examples (one for each text). Human annotators
filter out ill-formed examples ensuring the validity of the benchmark. The
experiments on BiVLC uncover a weakness of current multimodal models, as they
perform poorly in the text-to-image direction. In fact, when considering both
retrieval directions, the conclusions obtained in previous works change
significantly. In addition to the benchmark, we show that a contrastive model
trained using synthetic images and texts significantly improves over the base
model in SugarCrepe and in BiVLC for both retrieval directions. The gap to
human performance in BiVLC confirms that Vision-Language Compositionality is
still a challenging problem. BiVLC and code are available at
https://imirandam.github.io/BiVLC_project_page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page
  at: https://imirandam.github.io/BiVLC_project_page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft Condorcet Optimization for Ranking of General Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Lanctot, Kate Larson, Michael Kaisers, Quentin Berthet, Ian Gemp, Manfred Diaz, Roberto-Rafael Maura-Rivero, Yoram Bachrach, Anna Koop, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common way to drive progress of AI models and agents is to compare their
performance on standardized benchmarks. Comparing the performance of general
agents requires aggregating their individual performances across a potentially
wide variety of different tasks. In this paper, we describe a novel ranking
scheme inspired by social choice frameworks, called Soft Condorcet Optimization
(SCO), to compute the optimal ranking of agents: the one that makes the fewest
mistakes in predicting the agent comparisons in the evaluation data. This
optimal ranking is the maximum likelihood estimate when evaluation data (which
we view as votes) are interpreted as noisy samples from a ground truth ranking,
a solution to Condorcet's original voting system criteria. SCO ratings are
maximal for Condorcet winners when they exist, which we show is not necessarily
true for the classical rating system Elo. We propose three optimization
algorithms to compute SCO ratings and evaluate their empirical performance.
When serving as an approximation to the Kemeny-Young voting method, SCO
rankings are on average 0 to 0.043 away from the optimal ranking in normalized
Kendall-tau distance across 865 preference profiles from the PrefLib open
ranking archive. In a simulated noisy tournament setting, SCO achieves accurate
approximations to the ground truth ranking and the best among several baselines
when 59\% or more of the preference data is missing. Finally, SCO ranking
provides the best approximation to the optimal ranking, measured on held-out
test sets, in a problem containing 52,958 human players across 31,049 games of
the classic seven-player game of Diplomacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Transcriptomics Foundation Models for Perturbation Analysis
  : one PCA still rules them all 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ihab Bendidi, Shawn Whitfield, Kian Kenyon-Dean, Hanene Ben Yedder, Yassir El Mesbahi, Emmanuel Noutahi, Alisandra K. Denton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the relationships among genes, compounds, and their
interactions in living organisms remains limited due to technological
constraints and the complexity of biological data. Deep learning has shown
promise in exploring these relationships using various data types. However,
transcriptomics, which provides detailed insights into cellular states, is
still underused due to its high noise levels and limited data availability.
Recent advancements in transcriptomics sequencing provide new opportunities to
uncover valuable insights, especially with the rise of many new foundation
models for transcriptomics, yet no benchmark has been made to robustly evaluate
the effectiveness of these rising models for perturbation analysis. This
article presents a novel biologically motivated evaluation framework and a
hierarchy of perturbation analysis tasks for comparing the performance of
pretrained foundation models to each other and to more classical techniques of
learning from transcriptomics data. We compile diverse public datasets from
different sequencing techniques and cell lines to assess models performance.
Our approach identifies scVI and PCA to be far better suited models for
understanding biological perturbations in comparison to existing foundation
models, especially in their application in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024 AIDrugX Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Adaptation of Language Models with a Memory of Amortized Contexts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, Jonathan Richard Schwarz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the rapid generation and dissemination of information, large language
models (LLMs) quickly run out of date despite enormous development costs. To
address the crucial need to keep models updated, online learning has emerged as
a critical tool when utilizing LLMs for real-world applications. However, given
the ever-expanding corpus of unseen documents and the large parameter space of
modern LLMs, efficient adaptation is essential. To address these challenges, we
propose Memory of Amortized Contexts (MAC), an efficient and effective online
adaptation framework for LLMs with strong knowledge retention. We propose a
feature extraction and memory-augmentation approach to compress and extract
information from new documents into compact modulations stored in a memory
bank. When answering questions, our model attends to and extracts relevant
knowledge from this memory bank. To learn informative modulations in an
efficient manner, we utilize amortization-based meta-learning, which
substitutes an otherwise required optimization process with a single forward
pass of the encoder. Subsequently, we learn to choose from and aggregate
selected documents into a single modulation by conditioning on the question,
allowing us to adapt a frozen language model during test time without requiring
further gradient updates. Our experiment demonstrates the superiority of MAC in
multiple aspects, including online adaptation performance, time, and memory
efficiency. In addition, we show how MAC can be combined with and improve the
performance of popular alternatives such as retrieval augmented generations
(RAGs). Code is available at: https://github.com/jihoontack/MAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference proceeding for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in
  Endoscopic Pituitary Surgery <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjana Wijekoon, Adrito Das, Roxana R. Herrera, Danyal Z. Khan, John Hanrahan, Eleanor Carter, Valpuri Luoma, Danail Stoyanov, Hani J. Marcus, Sophia Bano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow
for anaesthetists to more accurately decide when to administer anaesthetic
agents and drugs, as well as to notify hospital staff to send in the next
patient. Therefore RSD plays an important role in improving patient care and
minimising surgical theatre costs via efficient scheduling. In endoscopic
pituitary surgery, it is uniquely challenging due to variable workflow
sequences with a selection of optional steps contributing to high variability
in surgery duration. This paper presents PitRSDNet for predicting RSD during
pituitary surgery, a spatio-temporal neural network model that learns from
historical data focusing on workflow sequences. PitRSDNet integrates workflow
knowledge into RSD prediction in two forms: 1) multi-task learning for
concurrently predicting step and RSD; and 2) incorporating prior steps as
context in temporal learning and inference. PitRSDNet is trained and evaluated
on a new endoscopic pituitary surgery dataset with 88 videos to show
competitive performance improvements over previous statistical and machine
learning methods. The findings also highlight how PitRSDNet improve RSD
precision on outlier cases utilising the knowledge of prior steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Augmented Environments for Computer-Assisted
  Interventions (AE-CAI) Workshop at the Medical Image Computing and
  Computer-Assisted Interventions (MICCAI) Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Whispers in the Machine: Confidentiality in LLM-integrated Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Evertz, Merlin Chlosta, Lea Schönherr, Thorsten Eisenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly augmented with external tools
and commercial services into LLM-integrated systems. While these interfaces can
significantly enhance the capabilities of the models, they also introduce a new
attack surface. Manipulated integrations, for example, can exploit the model
and compromise sensitive data accessed through other interfaces. While previous
work primarily focused on attacks targeting a model's alignment or the leakage
of training data, the security of data that is only available during inference
has escaped scrutiny so far. In this work, we demonstrate the vulnerabilities
associated with external components and introduce a systematic approach to
evaluate confidentiality risks in LLM-integrated systems. We identify two
specific attack scenarios unique to these systems and formalize these into a
tool-robustness framework designed to measure a model's ability to protect
sensitive information. Our findings show that all examined models are highly
vulnerable to confidentiality attacks, with the risk increasing significantly
when models are used together with external tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Schmitt, Leona Odole, Stefan T. Radev, Paul-Christian Bürkner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present multimodal neural posterior estimation (MultiNPE), a method to
integrate heterogeneous data from different sources in simulation-based
inference with neural networks. Inspired by advances in deep fusion, it allows
researchers to analyze data from different domains and infer the parameters of
complex mathematical models with increased accuracy. We consider three fusion
approaches for MultiNPE (early, late, hybrid) and evaluate their performance in
three challenging experiments. MultiNPE not only outperforms single-source
baselines on a reference task, but also achieves superior inference on
scientific models from cognitive neuroscience and cardiology. We systematically
investigate the impact of partially missing data on the different fusion
strategies. Across our experiments, late and hybrid fusion techniques emerge as
the methods of choice for practical applications of multimodal simulation-based
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Edit Distance with General Costs Using Neural Set Divergence <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eeshaan Jain, Indradyumna Roy, Saswat Meher, Soumen Chakrabarti, Abir De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Edit Distance (GED) measures the (dis-)similarity between two given
graphs, in terms of the minimum-cost edit sequence that transforms one graph to
the other. However, the exact computation of GED is NP-Hard, which has recently
motivated the design of neural methods for GED estimation. However, they do not
explicitly account for edit operations with different costs. In response, we
propose GRAPHEDX, a neural GED estimator that can work with general costs
specified for the four edit operations, viz., edge deletion, edge addition,
node deletion and node addition. We first present GED as a quadratic assignment
problem (QAP) that incorporates these four costs. Then, we represent each graph
as a set of node and edge embeddings and use them to design a family of neural
set divergence surrogates. We replace the QAP terms corresponding to each
operation with their surrogates. Computing such neural set divergence require
aligning nodes and edges of the two graphs. We learn these alignments using a
Gumbel-Sinkhorn permutation generator, additionally ensuring that the node and
edge alignments are consistent with each other. Moreover, these alignments are
cognizant of both the presence and absence of edges between node-pairs.
Experiments on several datasets, under a variety of edit cost settings, show
that GRAPHEDX consistently outperforms state-of-the-art methods and heuristics
in terms of prediction error.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting LLM Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization leverages lower-precision weights to reduce the memory usage of
large language models (LLMs) and is a key technique for enabling their
deployment on commodity hardware. While LLM quantization's impact on utility
has been extensively explored, this work for the first time studies its adverse
effects from a security perspective. We reveal that widely used quantization
methods can be exploited to produce a harmful quantized LLM, even though the
full-precision counterpart appears benign, potentially tricking users into
deploying the malicious quantized model. We demonstrate this threat using a
three-staged attack framework: (i) first, we obtain a malicious LLM through
fine-tuning on an adversarial task; (ii) next, we quantize the malicious model
and calculate constraints that characterize all full-precision models that map
to the same quantized model; (iii) finally, using projected gradient descent,
we tune out the poisoned behavior from the full-precision model while ensuring
that its weights satisfy the constraints computed in step (ii). This procedure
results in an LLM that exhibits benign behavior in full precision but when
quantized, it follows the adversarial behavior injected in step (i). We
experimentally demonstrate the feasibility and severity of such an attack
across three diverse scenarios: vulnerable code generation, content injection,
and over-refusal attack. In practice, the adversary could host the resulting
full-precision model on an LLM community hub such as Hugging Face, exposing
millions of users to the threat of deploying its malicious quantized version on
their devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nash CoT: Multi-Path Inference with Preference Equilibrium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhang, Cunxiang Wang, Xiong Xiao, Yue Zhang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain of thought (CoT) is a reasoning framework that can enhance the
performance of Large Language Models (LLMs) on complex inference tasks. In
particular, among various studies related to CoT, multi-path inference stands
out as a simple yet effective improvement. However, there is no optimal setting
for the number of inference paths. Therefore, we have to increase the number of
inference paths to obtain better results, which in turn increases the inference
cost. To address this limitation, we can utilize question-related role
templates to guide LLMs into relevant roles, thereby increasing the possibility
of correct inferences for each path and further reducing dependence on the
number of inference paths while improving reasoning accuracy. However, placing
LLMs into specific roles may reduce their reasoning diversity and performance
on a few tasks where role dependence is low. To alleviate the excessive
immersion of the LLM into a specific role, we propose Nash CoT by constructing
a competitive system on each path that balances the generation from
role-specific LLMs' and the general LLMs' generation, thereby ensuring both
effective role adoption and diversity in LLM generation further maintaining the
performance of multi-path inference while reducing the requirement of the
number of inference paths. We evaluate Nash CoT across various inference tasks,
including Arabic Reasoning, Commonsense Question Answering, and Symbolic
Inference, achieving results that are comparable to or better than those of
multi-path CoT with the equal number of inference paths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QBSD: Quartile-Based Seasonality Decomposition for Cost-Effective RAN
  KPI Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ebenezer RHP Isaac, Bulbul Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting time series patterns, such as cell key performance indicators
(KPIs) of radio access networks (RAN), plays a vital role in enhancing service
quality and operational efficiency. State-of-the-art forecasting approaches
prioritize accuracy at the expense of computational performance, rendering them
less suitable for data-intensive applications encompassing systems with a
multitude of time series variables. They also do not capture the effect of
dynamic operating ranges that vary with time. To address this issue, we
introduce QBSD, a live single-step forecasting approach tailored to optimize
the trade-off between accuracy and computational complexity. The method has
shown significant success with our real network RAN KPI datasets of over
several thousand cells. In this article, we showcase the performance of QBSD in
comparison to other forecasting approaches on a dataset we have made publicly
available. The results demonstrate that the proposed method excels in runtime
efficiency compared to the leading algorithms available while maintaining
competitive forecast accuracy that rivals neural forecasting methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private Attribute Inference from Images with Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batuhan Tömekçe, Mark Vero, Robin Staab, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become ubiquitous in our daily tasks and
digital interactions, associated privacy risks are increasingly in focus. While
LLM privacy research has primarily focused on the leakage of model training
data, it has recently been shown that LLMs can make accurate privacy-infringing
inferences from previously unseen texts. With the rise of vision-language
models (VLMs), capable of understanding both images and text, a key question is
whether this concern transfers to the previously unexplored domain of benign
images posted online. To answer this question, we compile an image dataset with
human-annotated labels of the image owner's personal attributes. In order to
understand the privacy risks posed by VLMs beyond traditional human attribute
recognition, our dataset consists of images where the inferable private
attributes do not stem from direct depictions of humans. On this dataset, we
evaluate 7 state-of-the-art VLMs, finding that they can infer various personal
attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy
scales with the general capabilities of the models, implying that future models
can be misused as stronger inferential adversaries, establishing an imperative
for the development of adequate defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Synthetic <span class="highlight-title">Dataset</span> for Personal Attribute Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Yukhymenko, Robin Staab, Mark Vero, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, powerful Large Language Models (LLMs) have become easily accessible
to hundreds of millions of users world-wide. However, their strong capabilities
and vast world knowledge do not come without associated privacy risks. In this
work, we focus on the emerging privacy threat LLMs pose -- the ability to
accurately infer personal information from online texts. Despite the growing
importance of LLM-based author profiling, research in this area has been
hampered by a lack of suitable public datasets, largely due to ethical and
privacy concerns associated with real personal data. We take two steps to
address this problem: (i) we construct a simulation framework for the popular
social media platform Reddit using LLM agents seeded with synthetic personal
profiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic
dataset of over 7800 comments manually labeled for personal attributes. We
validate our dataset with a human study showing that humans barely outperform
random guessing on the task of distinguishing our synthetic comments from real
ones. Further, we verify that our dataset enables meaningful personal attribute
inference research by showing across 18 state-of-the-art LLMs that our
synthetic comments allow us to draw the same conclusions as real-world data.
Combined, our experimental results, dataset and pipeline form a strong basis
for future privacy-preserving research geared towards understanding and
mitigating inference-based privacy threats that LLMs pose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency Models for Scalable and Fast Simulation-Based Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Schmitt, Valentin Pratz, Ullrich Köthe, Paul-Christian Bürkner, Stefan T Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation-based inference (SBI) is constantly in search of more expressive
and efficient algorithms to accurately infer the parameters of complex
simulation models. In line with this goal, we present consistency models for
posterior estimation (CMPE), a new conditional sampler for SBI that inherits
the advantages of recent unconstrained architectures and overcomes their
sampling inefficiency at inference time. CMPE essentially distills a continuous
probability flow and enables rapid few-shot inference with an unconstrained
architecture that can be flexibly tailored to the structure of the estimation
problem. We provide hyperparameters and default architectures that support
consistency training over a wide range of different dimensions, including
low-dimensional ones which are important in SBI workflows but were previously
difficult to tackle even with unconditional consistency models. Our empirical
evaluation demonstrates that CMPE not only outperforms current state-of-the-art
algorithms on hard low-dimensional benchmarks, but also achieves competitive
performance with much faster sampling speed on two realistic estimation
problems with high data and/or parameter dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferring stochastic low-rank recurrent neural networks from neural data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthijs Pals, A Erdem Sağtekin, Felix Pei, Manuel Gloeckler, Jakob H Macke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central aim in computational neuroscience is to relate the activity of
large populations of neurons to an underlying dynamical system. Models of these
neural dynamics should ideally be both interpretable and fit the observed data
well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability
by having tractable dynamics. However, it is unclear how to best fit low-rank
RNNs to data consisting of noisy observations of an underlying stochastic
system. Here, we propose to fit stochastic low-rank RNNs with variational
sequential Monte Carlo methods. We validate our method on several datasets
consisting of both continuous and spiking neural data, where we obtain lower
dimensional latent dynamics than current state of the art methods.
Additionally, for low-rank models with piecewise linear nonlinearities, we show
how to efficiently identify all fixed points in polynomial rather than
exponential cost in the number of units, making analysis of the inferred
dynamics tractable for large RNNs. Our method both elucidates the dynamical
systems underlying experimental recordings and provides a generative model
whose trajectories match observed variability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongmin Lee, Minsu Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the 3D orientations of an object in an image, known as
single-image pose estimation, is a crucial task in 3D vision applications.
Existing methods typically learn 3D rotations parametrized in the spatial
domain using Euler angles or quaternions, but these representations often
introduce discontinuities and singularities. SO(3)-equivariant networks enable
the structured capture of pose patterns with data-efficient learning, but the
parametrizations in spatial domain are incompatible with their architecture,
particularly spherical CNNs, which operate in the frequency domain to enhance
computational efficiency. To overcome these issues, we propose a
frequency-domain approach that directly predicts Wigner-D coefficients for 3D
rotation regression, aligning with the operations of spherical CNNs. Our
SO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial
parameterizations, ensuring consistent pose estimation under arbitrary
rotations. Trained with a frequency-domain regression loss, our method achieves
state-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+,
with significant improvements in accuracy, robustness, and data efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024, Project webpage at
  http://cvlab.postech.ac.kr/research/3D_EquiPose</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target Detection of Safety Protective Gear Using the Improved YOLOv5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Xue Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-risk railway construction, personal protective equipment monitoring
is critical but challenging due to small and frequently obstructed targets. We
propose YOLO-EA, an innovative model that enhances safety measure detection by
integrating ECA into its backbone's convolutional layers, improving discernment
of minuscule objects like hardhats. YOLO-EA further refines target recognition
under occlusion by replacing GIoU with EIoU loss. YOLO-EA's effectiveness was
empirically substantiated using a dataset derived from real-world railway
construction site surveillance footage. It outperforms YOLOv5, achieving 98.9%
precision and 94.7% recall, up 2.5% and 0.5% respectively, while maintaining
real-time performance at 70.774 fps. This highly efficient and precise YOLO-EA
holds great promise for practical application in intricate construction
scenarios, enforcing stringent safety compliance during complex railway
construction projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Flow Matching for Smooth Interpolations on the Data Manifold 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kacper Kapuśniak, Peter Potaptchik, Teodora Reu, Leo Zhang, Alexander Tong, Michael Bronstein, Avishek Joey Bose, Francesco Di Giovanni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matching objectives underpin the success of modern generative models and rely
on constructing conditional paths that transform a source distribution into a
target distribution. Despite being a fundamental building block, conditional
paths have been designed principally under the assumption of Euclidean
geometry, resulting in straight interpolations. However, this can be
particularly restrictive for tasks such as trajectory inference, where straight
paths might lie outside the data manifold, thus failing to capture the
underlying dynamics giving rise to the observed marginals. In this paper, we
propose Metric Flow Matching (MFM), a novel simulation-free framework for
conditional flow matching where interpolants are approximate geodesics learned
by minimizing the kinetic energy of a data-induced Riemannian metric. This way,
the generative model matches vector fields on the data manifold, which
corresponds to lower uncertainty and more meaningful interpolations. We
prescribe general metrics to instantiate MFM, independent of the task, and test
it on a suite of challenging problems including LiDAR navigation, unpaired
image translation, and modeling cellular dynamics. We observe that MFM
outperforms the Euclidean baselines, particularly achieving SOTA on single-cell
trajectory prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">survey</span> and benchmark of high-dimensional Bayesian optimization of
  discrete sequences <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel González-Duque, Richard Michael, Simon Bartels, Yevgen Zainchkovskyy, Søren Hauberg, Wouter Boomsma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing discrete black-box functions is key in several domains, e.g.
protein engineering and drug design. Due to the lack of gradient information
and the need for sample efficiency, Bayesian optimization is an ideal candidate
for these tasks. Several methods for high-dimensional continuous and
categorical Bayesian optimization have been proposed recently. However, our
survey of the field reveals highly heterogeneous experimental set-ups across
methods and technical barriers for the replicability and application of
published algorithms to real-world tasks. To address these issues, we develop a
unified framework to test a vast array of high-dimensional Bayesian
optimization methods and a collection of standardized black-box functions
representing real-world application domains in chemistry and biology. These two
components of the benchmark are each supported by flexible, scalable, and
easily extendable software libraries (poli and poli-baselines), allowing
practitioners to readily incorporate new optimization objectives or discrete
optimizers. Project website:
https://machinelearninglifescience.github.io/hdbo_benchmark
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Datasets and Benchmarks track of NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientNet with Hybrid Attention Mechanisms for Enhanced Breast
  Histopathology Classification: A Comprehensive Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naren Sengodan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer histopathology image classification is crucial for early cancer
detection, offering the potential to reduce mortality rates through timely
diagnosis. This paper introduces a novel approach integrating Hybrid
EfficientNet models with advanced attention mechanisms, including Convolutional
Block Attention Module (CBAM), Self-Attention, and Deformable Attention, to
enhance feature extraction and focus on critical image regions. We evaluate the
performance of our models across multiple magnification scales using publicly
available histopathological datasets. Our method achieves significant
improvements, with accuracy reaching 98.42% at 400X magnification, surpassing
several state-of-the-art models, including VGG and ResNet architectures. The
results are validated using metrics such as accuracy, F1-score, precision, and
recall, demonstrating the clinical potential of our model in improving
diagnostic accuracy. Furthermore, the proposed method shows increased
computational efficiency, making it suitable for integration into real-time
diagnostic workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoQT: Low-Rank Adapters for Quantized <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16528v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16528v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Loeschcke, Mads Toftrup, Michael J. Kastoryano, Serge Belongie, Vésteinn Snæbjarnarson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances using low-rank adapters and quantization, pretraining of
large models on consumer hardware has not been possible without model sharding,
offloading during training, or per-layer gradient updates. To address these
limitations, we propose Low-Rank Adapters for Quantized Training (LoQT), a
method for efficiently training quantized models. LoQT uses gradient-based
tensor factorization to initialize low-rank trainable weight matrices that are
periodically merged into quantized full-rank weight matrices. Our approach is
suitable for both pretraining and fine-tuning models. We demonstrate this for
language modeling and downstream task adaptation, finding that LoQT enables
efficient training of models up to 7B parameters on a 24GB GPU. We also
demonstrate the feasibility of training a 13B model using per-layer gradient
updates on the same hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequential Decision Making with Expert Demonstrations under Unobserved
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Balazadeh, Keertana Chidambaram, Viet Nguyen, Rahul G. Krishnan, Vasilis Syrgkanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of online sequential decision-making given auxiliary
demonstrations from experts who made their decisions based on unobserved
contextual information. These demonstrations can be viewed as solving related
but slightly different problems than what the learner faces. This setting
arises in many application domains, such as self-driving cars, healthcare, and
finance, where expert demonstrations are made using contextual information,
which is not recorded in the data available to the learning agent. We model the
problem as zero-shot meta-reinforcement learning with an unknown distribution
over the unobserved contextual variables and a Bayesian regret minimization
objective, where the unobserved variables are encoded as parameters with an
unknown prior. We propose the Experts-as-Priors algorithm (ExPerior), an
empirical Bayes approach that utilizes expert data to establish an informative
prior distribution over the learner's decision-making problem. This prior
distribution enables the application of any Bayesian approach for online
decision-making, such as posterior sampling. We demonstrate that our strategy
surpasses existing behaviour cloning, online, and online-offline baselines for
multi-armed bandits, Markov decision processes (MDPs), and partially observable
MDPs, showcasing the broad reach and utility of ExPerior in using expert
demonstrations across different decision-making setups.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-based Generative Multicasting with Intent-aware Semantic
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinkai Liu, Mahdi Boloursaz Mashhadi, Li Qiao, Yi Ma, Rahim Tafazolli, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models (GDMs) have recently shown great success in
synthesizing multimedia signals with high perceptual quality enabling highly
efficient semantic communications in future wireless networks. In this paper,
we develop an intent-aware generative semantic multicasting framework utilizing
pre-trained diffusion models. In the proposed framework, the transmitter
decomposes the source signal to multiple semantic classes based on the
multi-user intent, i.e. each user is assumed to be interested in details of
only a subset of the semantic classes. The transmitter then sends to each user
only its intended classes, and multicasts a highly compressed semantic map to
all users over shared wireless resources that allows them to locally synthesize
the other classes, i.e. non-intended classes, utilizing pre-trained diffusion
models. The signal retrieved at each user is thereby partially reconstructed
and partially synthesized utilizing the received semantic map. This improves
utilization of the wireless resources, with better preserving privacy of the
non-intended classes. We design a communication/computation-aware scheme for
per-class adaptation of the communication parameters, such as the transmission
power and compression rate to minimize the total latency of retrieving signals
at multiple receivers, tailored to the prevailing channel conditions as well as
the users reconstruction/synthesis distortion/perception requirements. The
simulation results demonstrate significantly reduced per-user latency compared
with non-generative and intent-unaware multicasting benchmarks while
maintaining high perceptual quality of the signals retrieved at the users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Audio-Visual Segmentation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Sokolov, Swapnil Bhosale, Xiatian Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing the sounding objects in scenes is a longstanding objective in
embodied AI, with diverse applications in robotics and AR/VR/MR. To that end,
Audio-Visual Segmentation (AVS), taking as condition an audio signal to
identify the masks of the target sounding objects in an input image with
synchronous camera and microphone sensors, has been recently advanced. However,
this paradigm is still insufficient for real-world operation, as the mapping
from 2D images to 3D scenes is missing. To address this fundamental limitation,
we introduce a novel research problem, 3D Audio-Visual Segmentation, extending
the existing AVS to the 3D output space. This problem poses more challenges due
to variations in camera extrinsics, audio scattering, occlusions, and diverse
acoustics across sounding object categories. To facilitate this research, we
create the very first simulation based benchmark, 3DAVS-S34-O7, providing
photorealistic 3D scene environments with grounded spatial audio under
single-instance and multi-instance settings, across 34 scenes and 7 object
categories. This is made possible by re-purposing the Habitat simulator to
generate comprehensive annotations of sounding object locations and
corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet,
characterized by integrating the ready-to-use knowledge from pretrained 2D
audio-visual foundation models synergistically with 3D visual scene
representation through spatial audio-aware mask alignment and refinement.
Extensive experiments demonstrate that EchoSegnet can effectively segment
sounding objects in 3D space on our new benchmark, representing a significant
advancement in the field of embodied AI. Project page:
https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the NeurIPS 2024 Workshop on Audio Imagination</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and
  Correspondence <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuming You, Minghui Fang, Li Tang, Rongjie Huang, Yongqi Wang, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion-to-music and music-to-motion have been studied separately, each
attracting substantial research interest within their respective domains. The
interaction between human motion and music is a reflection of advanced human
intelligence, and establishing a unified relationship between them is
particularly important. However, to date, there has been no work that considers
them jointly to explore the modality alignment within. To bridge this gap, we
propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous
motion-music generation. Firstly, to mitigate the huge computational costs
raised by long sequences, we propose a novel Bidirectional Contrastive Rhythmic
Variational Auto-Encoder (BiCoR-VAE) that extracts the modality-aligned latent
representations for both motion and music inputs. Subsequently, leveraging the
aligned latent spaces, we introduce a multi-modal Transformer-based diffusion
model and a cross-guidance sampling strategy to enable various generation
tasks, including cross-modal, multi-modal, and variable-length generation.
Extensive experiments demonstrate that MoMu-Diffusion surpasses recent
state-of-the-art methods both qualitatively and quantitatively, and can
synthesize realistic, diverse, long-term, and beat-matched music or motion
sequences. The generated samples and codes are available at
https://momu-diffusion.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneDiff: A Generalist Model for Image Difference Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05645v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05645v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erdong Hu, Longteng Guo, Tongtian Yue, Zijia Zhao, Shuning Xue, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, Image Difference Captioning (IDC) is crucial for
accurately describing variations between closely related images. Traditional
IDC methods often rely on specialist models, which restrict their applicability
across varied contexts. This paper introduces the OneDiff model, a novel
generalist approach that utilizes a robust vision-language model architecture,
integrating a siamese image encoder with a Visual Delta Module. This innovative
configuration allows for the precise detection and articulation of fine-grained
differences between image pairs. OneDiff is trained through a dual-phase
strategy, encompassing Coupled Sample Training and multi-task learning across a
diverse array of data types, supported by our newly developed DiffCap Dataset.
This dataset merges real-world and synthetic data, enhancing the training
process and bolstering the model's robustness. Extensive testing on diverse IDC
benchmarks, such as Spot-the-Diff, Image-Editing-Request, and Birds-to-Words,
shows that OneDiff consistently outperforms existing state-of-the-art models in
accuracy and adaptability, achieving improvements of up to 97% CIDEr points in
average. By setting a new benchmark in IDC, OneDiff paves the way for more
versatile and effective applications in detecting and describing visual
differences. The code, models, and data will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReactFace: Online Multiple Appropriate Facial Reaction Generation in
  Dyadic Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Luo, Siyang Song, Weicheng Xie, Micol Spitale, Zongyuan Ge, Linlin Shen, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dyadic interaction, predicting the listener's facial reactions is
challenging as different reactions could be appropriate in response to the same
speaker's behaviour. Previous approaches predominantly treated this task as an
interpolation or fitting problem, emphasizing deterministic outcomes but
ignoring the diversity and uncertainty of human facial reactions. Furthermore,
these methods often failed to model short-range and long-range dependencies
within the interaction context, leading to issues in the synchrony and
appropriateness of the generated facial reactions. To address these
limitations, this paper reformulates the task as an extrapolation or prediction
problem, and proposes an novel framework (called ReactFace) to generate
multiple different but appropriate facial reactions from a speaker behaviour
rather than merely replicating the corresponding listener facial behaviours.
Our ReactFace generates multiple different but appropriate photo-realistic
human facial reactions by: (i) learning an appropriate facial reaction
distribution representing multiple different but appropriate facial reactions;
and (ii) synchronizing the generated facial reactions with the speaker verbal
and non-verbal behaviours at each time stamp, resulting in realistic 2D facial
reaction sequences. Experimental results demonstrate the effectiveness of our
approach in generating multiple diverse, synchronized, and appropriate facial
reactions from each speaker's behaviour. The quality of the generated facial
reactions is intimately tied to the speaker's speech and facial expressions,
achieved through our novel speaker-listener interaction modules. Our code is
made publicly available at \url{https://github.com/lingjivoo/ReactFace}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Visualization and Computer Graphics
  (TVCG), 18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-03T00:00:00Z">2024-11-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjiao Tian, Chengyue Huang, Zsolt Kira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern optimizers such as AdamW, equipped with momentum and adaptive learning
rate, are designed to escape local minima and explore the vast parameter space.
This exploration is beneficial for finding good loss basins when training from
scratch. It is not necessarily ideal when resuming from a powerful foundation
model because it can lead to large deviations from the pre-trained
initialization and, consequently, worse robustness and generalization. At the
same time, strong regularization on all parameters can lead to under-fitting.
We hypothesize that selectively regularizing the parameter space is the key to
fitting and retraining the pre-trained knowledge. This paper proposes a new
weight decay technique, Selective Projection Decay (SPD), that selectively
imposes a strong penalty on certain layers while allowing others to change
freely. Intuitively, SPD expands and contracts the parameter search space for
layers with consistent and inconsistent loss reduction, respectively.
Experimentally, when equipped with SPD, Adam consistently provides better
in-distribution generalization and out-of-distribution robustness performance
on multiple popular vision and language benchmarks. Code available
at~\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Fucci, Marco Gaido, Beatrice Savoldi, Matteo Negri, Mauro Cettolo, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spurred by the demand for interpretable models, research on eXplainable AI
for language technologies has experienced significant growth, with feature
attribution methods emerging as a cornerstone of this progress. While prior
work in NLP explored such methods for classification tasks and textual
applications, explainability intersecting generation and speech is lagging,
with existing techniques failing to account for the autoregressive nature of
state-of-the-art models and to provide fine-grained, phonetically meaningful
explanations. We address this gap by introducing Spectrogram Perturbation for
Explainable Speech-to-text Generation (SPES), a feature attribution technique
applicable to sequence generation tasks with autoregressive models. SPES
provides explanations for each predicted token based on both the input
spectrogram and the previously generated tokens. Extensive evaluation on speech
recognition and translation demonstrates that SPES generates explanations that
are faithful and plausible to humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Large Language Models for Complex Word Identification in
  Multilingual and Multidomain Setups <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Răzvan-Alexandru Smădu, David-Gabriel Ion, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex Word Identification (CWI) is an essential step in the lexical
simplification task and has recently become a task on its own. Some variations
of this binary classification task have emerged, such as lexical complexity
prediction (LCP) and complexity evaluation of multi-word expressions (MWE).
Large language models (LLMs) recently became popular in the Natural Language
Processing community because of their versatility and capability to solve
unseen tasks in zero/few-shot settings. Our work investigates LLM usage,
specifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and
closed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE
settings. We evaluate zero-shot, few-shot, and fine-tuning settings and show
that LLMs struggle in certain conditions or achieve comparable results against
existing methods. In addition, we provide some views on meta-learning combined
with prompt learning. In the end, we conclude that the current state of LLMs
cannot or barely outperform existing methods, which are usually much smaller.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 16 figures, Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuefeng Peng, Junda Wang, Hong Yu, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements, large language models (LLMs) still struggle
with providing accurate answers when lacking domain-specific or up-to-date
knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by
incorporating external knowledge bases, but it also introduces new attack
surfaces. In this paper, we investigate data extraction attacks targeting the
knowledge databases of RAG systems. We demonstrate that previous attacks on RAG
largely depend on the instruction-following capabilities of LLMs, and that
simple fine-tuning can reduce the success rate of such attacks to nearly zero.
This makes these attacks impractical since fine-tuning is a common practice
when deploying LLMs in specific domains. To further reveal the vulnerability,
we propose to backdoor RAG, where a small portion of poisoned data is injected
during the fine-tuning phase to create a backdoor within the LLM. When this
compromised LLM is integrated into a RAG system, attackers can exploit specific
triggers in prompts to manipulate the LLM to leak documents from the retrieval
database. By carefully designing the poisoned data, we achieve both verbatim
and paraphrased document extraction. We show that with only 3\% poisoned data,
our method achieves an average success rate of 79.7\% in verbatim extraction on
Llama2-7B, with a ROUGE-L score of 64.21, and a 68.6\% average success rate in
paraphrased extraction, with an average ROUGE score of 52.6 across four
datasets. These results underscore the privacy risks associated with the supply
chain when deploying RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sejoon Oh, Yiqiao Jin, Megha Sharma, Donghyun Kim, Eric Ma, Gaurav Verma, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have revolutionized vision-language
understanding but are vulnerable to multimodal jailbreak attacks, where
adversaries meticulously craft inputs to elicit harmful or inappropriate
responses. We propose UniGuard, a novel multimodal safety guardrail that
jointly considers the unimodal and cross-modal harmful signals. UniGuard is
trained such that the likelihood of generating harmful responses in a toxic
corpus is minimized, and can be seamlessly applied to any input prompt during
inference with minimal computational costs. Extensive experiments demonstrate
the generalizability of UniGuard across multiple modalities and attack
strategies. It demonstrates impressive generalizability across multiple
state-of-the-art MLLMs, including LLaVA, Gemini Pro, GPT-4, MiniGPT-4, and
InstructBLIP, thereby broadening the scope of our solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking the Theory Behind Scaling 1-Bit Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majid Daliri, Zhao Song, Chiwun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an
impressive combination of efficiency and performance that rivals traditional
LLMs. Research by Wang et al. (2023); Ma et al. (2024) indicates that the
performance of these 1-bit LLMs progressively improves as the number of
parameters increases, hinting at the potential existence of a Scaling Law for
1-bit Neural Networks. In this paper, we present the first theoretical result
that rigorously establishes this scaling law for 1-bit models. We prove that,
despite the constraint of weights restricted to $\{-1, +1\}$, the dynamics of
model training inevitably align with kernel behavior as the network width
grows. This theoretical breakthrough guarantees convergence of the 1-bit model
to an arbitrarily small loss as width increases. Furthermore, we introduce the
concept of the generalization difference, defined as the gap between the
outputs of 1-bit networks and their full-precision counterparts, and
demonstrate that this difference maintains a negligible level as network width
scales. Building on the work of Kaplan et al. (2020), we conclude by examining
how the training loss scales as a power-law function of the model size, dataset
size, and computational resources utilized for training. Our findings
underscore the promising potential of scaling 1-bit neural networks, suggesting
that int1 could become the standard in future neural network precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosing Medical <span class="highlight-title">Dataset</span>s with Training Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Wenderoth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the potential of using training dynamics as an automated
alternative to human annotation for evaluating the quality of training data.
The framework used is Data Maps, which classifies data points into categories
such as easy-to-learn, hard-to-learn, and ambiguous (Swayamdipta et al., 2020).
Swayamdipta et al. (2020) highlight that difficult-to-learn examples often
contain errors, and ambiguous cases significantly impact model training. To
confirm the reliability of these findings, we replicated the experiments using
a challenging dataset, with a focus on medical question answering. In addition
to text comprehension, this field requires the acquisition of detailed medical
knowledge, which further complicates the task. A comprehensive evaluation was
conducted to assess the feasibility and transferability of the Data Maps
framework to the medical domain. The evaluation indicates that the framework is
unsuitable for addressing datasets' unique challenges in answering medical
questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/LauraWenderoth/training-dynamics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EcoAct: Economic Agent Determines When to Register What Action 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokun Zhang, Jieyu Zhang, Dujian Ding, Mirian Hipolito Garcia, Ankur Mallick, Daniel Madrigal, Menglin Xia, Victor Rühle, Qingyun Wu, Chi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have enabled Large Language Models (LLMs) to function as
agents that can perform actions using external tools. This requires
registering, i.e., integrating tool information into the LLM context prior to
taking actions. Current methods indiscriminately incorporate all candidate
tools into the agent's context and retain them across multiple reasoning steps.
This process remains opaque to LLM agents and is not integrated into their
reasoning procedures, leading to inefficiencies due to increased context length
from irrelevant tools. To address this, we introduce EcoAct, a tool using
algorithm that allows LLMs to selectively register tools as needed, optimizing
context use. By integrating the tool registration process into the reasoning
procedure, EcoAct reduces computational costs by over 50% in multiple steps
reasoning tasks while maintaining performance, as demonstrated through
extensive experiments. Moreover, it can be plugged into any reasoning pipeline
with only minor modifications to the prompt, making it applicable to LLM agents
now and future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Microservices Architecture for Dynamic Pricing in the Travel
  Industry: Algorithms, Scalability, and Impact on Revenue and Customer
  Satisfaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biman Barua, M. Shamim Kaiser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research investigates the implementation of a real-time,
microservices-oriented dynamic pricing system for the travel sector. The system
is designed to address factors such as demand, competitor pricing, and other
external circumstances in real-time. Both controlled simulation and real-life
application showed a respectable gain of 22% in revenue generation and a 17%
improvement in pricing response time which concern the issues of scaling and
flexibility of classical pricing mechanisms. Demand forecasting, competitor
pricing strategies, and event-based pricing were implemented as separate
microservices to enhance their scalability and reduce resource consumption by
30% during peak loads. Customers were also more content as depicted by a 15%
increase in satisfaction score post-implementation given the appreciation of
more appropriate pricing. This research enhances the existing literature with
practical illustrations of the possible application of microservices technology
in developing dynamic pricing solutions in a complex and data-driven context.
There exist however areas for improvement for instance inter-service latency
and the need for extensive real-time data pipelines. The present research goes
on to suggest combining these with direct data capture from customer behavior
at the same time as machine learning capacity developments in pricing
algorithms to assist in more accurate real time pricing. It is determined that
the use of microservices is a reasonable and efficient model for dynamic
pricing, allowing the tourism sector to employ evidence-based and customer
centric pricing techniques, which ensures that their profits are not
jeopardized because of the need for customers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ontology Population using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanaz Saki Norouzi, Adrita Barua, Antrea Christou, Nikita Gautam, Andrew Eells, Pascal Hitzler, Cogan Shimizu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs (KGs) are increasingly utilized for data integration,
representation, and visualization. While KG population is critical, it is often
costly, especially when data must be extracted from unstructured text in
natural language, which presents challenges, such as ambiguity and complex
interpretations. Large Language Models (LLMs) offer promising capabilities for
such tasks, excelling in natural language understanding and content generation.
However, their tendency to ``hallucinate'' can produce inaccurate outputs.
Despite these limitations, LLMs offer rapid and scalable processing of natural
language data, and with prompt engineering and fine-tuning, they can
approximate human-level performance in extracting and structuring data for KGs.
This study investigates LLM effectiveness for the KG population, focusing on
the Enslaved.org Hub Ontology. In this paper, we report that compared to the
ground truth, LLM's can extract ~90% of triples, when provided a modular
ontology as guidance in the prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining and Improving Contrastive Decoding by Extrapolating the
  Probabilities of a Huge and Hypothetical LM <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haw-Shiuan Chang, Nanyun Peng, Mohit Bansal, Anil Ramakrishna, Tagyoung Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive decoding (CD) (Li et al., 2023) improves the next-token
distribution of a large expert language model (LM) using a small amateur LM.
Although CD is applied to various LMs and domains to enhance open-ended text
generation, it is still unclear why CD often works well, when it could fail,
and how we can make it better. To deepen our understanding of CD, we first
theoretically prove that CD could be viewed as linearly extrapolating the
next-token logits from a huge and hypothetical LM. We also highlight that the
linear extrapolation could make CD unable to output the most obvious answers
that have already been assigned high probabilities by the amateur LM.
  To overcome CD's limitation, we propose a new unsupervised decoding method
called $\mathbf{A}$symptotic $\mathbf{P}$robability $\mathbf{D}$ecoding (APD).
APD explicitly extrapolates the probability curves from the LMs of different
sizes to infer the asymptotic probabilities from an infinitely large LM without
inducing more inference costs than CD. In FactualityPrompts, an open-ended text
generation benchmark, sampling using APD significantly boosts factuality in
comparison to the CD sampling and its variants, and achieves state-of-the-art
results for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA
datasets, APD is often significantly better than CD and achieves a similar
effect of using a larger LLM. For example, the perplexity of APD on top of
Pythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA
and LAMBADA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are LLMs good pragmatic speakers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyue Jian, Siddharth Narayanaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are trained on data assumed to include natural
language pragmatics, but do they actually behave like pragmatic speakers? We
attempt to answer this question using the Rational Speech Act (RSA) framework,
which models pragmatic reasoning in human communication. Using the paradigm of
a reference game constructed from the TUNA corpus, we score candidate
referential utterances in both a state-of-the-art LLM (Llama3-8B-Instruct) and
in the RSA model, comparing and contrasting these scores. Given that RSA
requires defining alternative utterances and a truth-conditional meaning
function, we explore such comparison for different choices of each of these
requirements. We find that while scores from the LLM have some positive
correlation with those from RSA, there isn't sufficient evidence to claim that
it behaves like a pragmatic speaker. This initial study paves way for further
targeted efforts exploring different models and settings, including
human-subject evaluation, to see if LLMs truly can, or be made to, behave like
pragmatic speakers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs and the Madness of Crowds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Bradley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the patterns of incorrect answers produced by large language
models (LLMs) during evaluation. These errors exhibit highly non-intuitive
behaviors unique to each model. By analyzing these patterns, we measure the
similarities between LLMs and construct a taxonomy that categorizes them based
on their error correlations. Our findings reveal that the incorrect responses
are not randomly distributed but systematically correlated across models,
providing new insights into the underlying structures and relationships among
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing LLM Evaluations: The Garbling Trick 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Bradley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become increasingly powerful, traditional
evaluation metrics tend to saturate, making it challenging to distinguish
between models based on their performance. We propose a general method to
transform existing LLM evaluations into a series of progressively more
difficult tasks. These enhanced evaluations emphasize reasoning capabilities
and can reveal relative performance differences that are not apparent in the
original assessments.
  To demonstrate the effectiveness of our approach, we create a new
multiple-choice test corpus, extend it into a family of evaluations, and assess
a collection of LLMs. Our results offer insights into the comparative reasoning
abilities of these models, particularly highlighting distinctions between
OpenAI's o1-preview and Google's gemini-pro-1.5-002.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAG: Dictionary-Augmented Generation for Disambiguation of Sentences in
  Endangered Uralic Languages using Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mika Hämäläinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We showcase that ChatGPT can be used to disambiguate lemmas in two endangered
languages ChatGPT is not proficient in, namely Erzya and Skolt Sami. We augment
our prompt by providing dictionary translations of the candidate lemmas to a
majority language - Finnish in our case. This dictionary augmented generation
approach results in 50\% accuracy for Skolt Sami and 41\% accuracy for Erzya.
On a closer inspection, many of the error types were of the kind even an
untrained human annotator would make.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWCLUL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SinaTools: Open Source Toolkit for Arabic Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tymaa Hammouda, Mustafa Jarrar, Mohammed Khalilia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SinaTools, an open-source Python package for Arabic natural
language processing and understanding. SinaTools is a unified package allowing
people to integrate it into their system workflow, offering solutions for
various tasks such as flat and nested Named Entity Recognition (NER),
fully-flagged Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy
Extractions and Evaluation, Lemmatization, Part-of-speech Tagging, Root
Tagging, and additional helper utilities such as corpus processing, text
stripping methods, and diacritic-aware word matching. This paper presents
SinaTools and its benchmarking results, demonstrating that SinaTools
outperforms all similar tools on the aforementioned tasks, such as Flat NER
(87.33%), Nested NER (89.42%), WSD (82.63%), Semantic Relatedness (0.49
Spearman rank), Lemmatization (90.5%), POS tagging (97.5%), among others.
SinaTools can be downloaded from (https://sina.birzeit.edu/sinatools).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integration of Large Vision Language Models for Efficient Post-disaster
  Damage Assessment and Reporting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Chen, Elyas Asadi Shamsabadi, Sheng Jiang, Luming Shen, Daniel Dias-da-Costa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional natural disaster response involves significant coordinated
teamwork where speed and efficiency are key. Nonetheless, human limitations can
delay critical actions and inadvertently increase human and economic losses.
Agentic Large Vision Language Models (LVLMs) offer a new avenue to address this
challenge, with the potential for substantial socio-economic impact,
particularly by improving resilience and resource access in underdeveloped
regions. We introduce DisasTeller, the first multi-LVLM-powered framework
designed to automate tasks in post-disaster management, including on-site
assessment, emergency alerts, resource allocation, and recovery planning. By
coordinating four specialised LVLM agents with GPT-4 as the core model,
DisasTeller autonomously implements disaster response activities, reducing
human execution time and optimising resource distribution. Our evaluations
through both LVLMs and humans demonstrate DisasTeller's effectiveness in
streamlining disaster response. This framework not only supports expert teams
but also simplifies access to disaster management processes for non-experts,
bridging the gap between traditional response methods and LVLM-driven
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-Efficient Alignment for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Liu, Changyu Chen, Chao Du, Wee Sun Lee, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study methods for efficiently aligning large language models (LLMs) with
human preferences given budgeted online feedback. We first formulate the LLM
alignment problem in the frame of contextual dueling bandits. This formulation,
subsuming recent paradigms such as online RLHF and online DPO, inherently
quests for sample-efficient algorithms that incorporate online active
exploration. Leveraging insights from bandit theory, we introduce a unified
algorithm based on Thompson sampling and highlight its applications in two
distinct LLM alignment scenarios. The practical agent that efficiently
implements this algorithm, named SEA (Sample-Efficient Alignment), is
empirically validated through extensive experiments across three model scales
(1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The
results demonstrate that SEA achieves highly sample-efficient alignment with
oracle's preferences, outperforming recent active exploration methods for LLMs.
Additionally, we release the implementation of SEA together with an efficient
codebase designed for online alignment of LLMs, aiming to accelerate future
research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-specific Guided Summarization for Mental Health Posts <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Qian, Yuqi Wang, Zimu Wang, Haiyang Zhang, Wei Wang, Ting Yu, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In domain-specific contexts, particularly mental health, abstractive
summarization requires advanced techniques adept at handling specialized
content to generate domain-relevant and faithful summaries. In response to
this, we introduce a guided summarizer equipped with a dual-encoder and an
adapted decoder that utilizes novel domain-specific guidance signals, i.e.,
mental health terminologies and contextually rich sentences from the source
document, to enhance its capacity to align closely with the content and context
of guidance, thereby generating a domain-relevant summary. Additionally, we
present a post-editing correction model to rectify errors in the generated
summary, thus enhancing its consistency with the original content in detail.
Evaluation on the MentSum dataset reveals that our model outperforms existing
baseline models in terms of both ROUGE and FactCC scores. Although the
experiments are specifically designed for mental health posts, the methodology
we've developed offers broad applicability, highlighting its versatility and
effectiveness in producing high-quality domain-specific summaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at PACLIC 2024. Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Models to Improve on Tape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liat Bezalel, Eyal Orgad, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle when prompted to generate content
under specific constraints. However, in such cases it is often easy to check
whether these constraints are satisfied or violated. Recent works have shown
that LLMs can benefit from such ``corrective feedback''. Here we claim that
this skill of LLMs can be significantly enhanced via training. We introduce an
RL framework for teaching models to use such rewards, by simulating interaction
sessions, and rewarding the model according to its ability to satisfy the
constraints. We refer to our method as CORGI (Controlled Generation with RL for
Guided Interaction), and evaluate it on a variety of controlled generation
tasks using unlabeled training data. We find that CORGI consistently
outperforms the baseline reinforcement learning method that does not
incorporate conversational feedback. Furthermore, CORGI's interactive framework
enables meta-learning, allowing the LLM to generalize better to guided
interaction in new tasks. Our results clearly show that conversational
optimization, when combined with reinforcement learning, significantly improves
the effectiveness of LLMs in controlled generation contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DPCL-Diff: The Temporal Knowledge Graph Reasoning based on Graph Node
  Diffusion Model with Dual-Domain Periodic Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Cao, Lisheng Wang, Luobing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal knowledge graph (TKG) reasoning that infers future missing facts is
an essential and challenging task. Predicting future events typically relies on
closely related historical facts, yielding more accurate results for repetitive
or periodic events. However, for future events with sparse historical
interactions, the effectiveness of this method, which focuses on leveraging
high-frequency historical information, diminishes. Recently, the capabilities
of diffusion models in image generation have opened new opportunities for TKG
reasoning. Therefore, we propose a graph node diffusion model with dual-domain
periodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)
introduces noise into sparsely related events to simulate new events,
generating high-quality data that better conforms to the actual distribution.
This generative mechanism significantly enhances the model's ability to reason
about new events. Additionally, the dual-domain periodic contrastive learning
(DPCL) maps periodic and non-periodic event entities to Poincar\'e and
Euclidean spaces, leveraging their characteristics to distinguish similar
periodic events effectively. Experimental results on four public datasets
demonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG
models in event prediction, demonstrating our approach's effectiveness. This
study also investigates the combined effectiveness of GNDiff and DPCL in TKG
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoCE: Adaptive Mixture of Contextualization Experts for Byte-based
  Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langlin Huang, Mengyu Bu, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byte-based machine translation systems have shown significant potential in
massively multilingual settings. Unicode encoding, which maps each character to
specific byte(s), eliminates the emergence of unknown words, even in new
languages, enabling broad language scalability. However, byte-level
tokenization results in sequences that are hard to interpret due to limited
semantic information per byte. Local contextualization has proven effective in
assigning initial semantics to tokens, improving sentence comprehension.
Nevertheless, variations in encoding rules across languages necessitate an
adaptive approach for effective contextualization. To this end, we propose
Adaptive MultiScale-Headed Attention (Ada-MSHA), adaptively selecting and
mixing attention heads, which are treated as contextualization experts. This
enhances the flexibility of contextualization scales and improves the potential
to discover a better strategy than previous methods. Experiment results show
that our method outperforms existing methods without extensive manual
adjustment of hyper-parameters and surpasses subword-based models with fewer
parameters in Ted-59 dataset. Our code is available at
https://github.com/ictnlp/MoCE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifier-guided Gradient Modulation for Enhanced Multimodal Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirun Guo, Tao Jin, Jingyuan Chen, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning has developed very fast in recent years. However, during
the multimodal training process, the model tends to rely on only one modality
based on which it could learn faster, thus leading to inadequate use of other
modalities. Existing methods to balance the training process always have some
limitations on the loss functions, optimizers and the number of modalities and
only consider modulating the magnitude of the gradients while ignoring the
directions of the gradients. To solve these problems, in this paper, we present
a novel method to balance multimodal learning with Classifier-Guided Gradient
Modulation (CGGM), considering both the magnitude and directions of the
gradients. We conduct extensive experiments on four multimodal datasets:
UPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification,
regression and segmentation tasks. The results show that CGGM outperforms all
the baselines and other state-of-the-art methods consistently, demonstrating
its effectiveness and versatility. Our code is available at
https://github.com/zrguo/CGGM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Reason via Program Generation, Emulation, and Search <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16337v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16337v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Weir, Muhammad Khalifa, Linlu Qiu, Orion Weller, Peter Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program synthesis with language models (LMs) has unlocked a large set of
reasoning abilities; code-tuned LMs have proven adept at generating programs
that solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word
concatenation). However, not all reasoning tasks are easily expressible as
code, e.g. tasks involving commonsense reasoning, moral decision-making, and
sarcasm understanding. Our goal is to extend an LM's program synthesis skills
to such tasks and evaluate the results via pseudo-programs, namely Python
programs where some leaf function calls are left undefined. To that end, we
propose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)
training LMs to generate pseudo-programs, (2) teaching them to emulate their
generated program's execution, including those leaf functions, allowing the
LM's knowledge to fill in the execution gaps; and (3) using them to search over
many programs to find an optimal one. To adapt the CoGEX model to a new task,
we introduce a method for performing program search to find a single program
whose pseudo-execution yields optimal performance when applied to all the
instances of a given dataset. We show that our approach yields large
improvements compared to standard in-context learning approaches on a battery
of tasks, both algorithmic and soft reasoning. This result thus demonstrates
that code synthesis can be applied to a much broader class of problems than
previously considered. Our released dataset, fine-tuned models, and
implementation can be found at \url{https://github.com/nweir127/CoGEX}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRAWL: Tensor Reduced and Approximated Weights for Large Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Luo, Het Patel, Yu Fu, Dawon Ahn, Jia Chen, Yue Dong, Evangelos E. Papalexakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has shown that pruning large-scale language models for
inference is an effective approach to improving model efficiency, significantly
reducing model weights with minimal impact on performance. Interestingly,
pruning can sometimes even enhance accuracy by removing noise that accumulates
during training, particularly through matrix decompositions. However, recent
work has primarily focused on single matrix decompositions or lower precision
techniques, which may fail to fully capture structural patterns. To address
these limitations, we introduce TRAWL (Tensor Reduced and Approximated Weights
for Large Language Models), a technique that applies tensor decomposition
across multiple weight matrices to effectively denoise LLMs by capturing global
structural patterns. Our experiments show that TRAWL improves model performance
by up to 16% over baseline models on benchmark datasets, without requiring
additional data, training, or fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages. Submitted to NAACL 2025 and under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Language Models Replace Programmers? REPOCOD Says 'Not Yet' 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved high accuracy, i.e., more than 90%
pass@1, in solving Python coding problems in HumanEval and MBPP. Thus, a
natural question is, whether LLMs achieve comparable code completion
performance compared to human developers? Unfortunately, one cannot answer this
question using existing manual crafted or simple (e.g., single-line) code
generation benchmarks, since such tasks fail to represent real-world software
development tasks. In addition, existing benchmarks often use poor code
correctness metrics, providing misleading conclusions.
  To address these challenges, we create REPOCOD, a code generation benchmark
with 980 problems collected from 11 popular real-world projects, with more than
58% of them requiring file-level or repository-level context information. In
addition, REPOCOD has the longest average canonical solution length (331.6
tokens) and the highest average cyclomatic complexity (9.00) compared to
existing benchmarks. Each task in REPOCOD includes 313.5 developer-written test
cases on average for better correctness evaluation. In our evaluations of ten
LLMs, none of the models achieve more than 30% pass@1 on REPOCOD, indicating
the necessity of building stronger LLMs that can help developers in real-world
software development. REPOCOD is available at
https://github.com/lt-asset/REPOCOD
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CiteME: Can Language Models Accurately Cite Scientific Claims? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ori Press, Andreas Hochlehnert, Ameya Prabhu, Vishaal Udandarao, Ofir Press, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thousands of new scientific papers are published each month. Such information
overload complicates researcher efforts to stay current with the
state-of-the-art as well as to verify and correctly attribute claims. We pose
the following research question: Given a text excerpt referencing a paper,
could an LM act as a research assistant to correctly identify the referenced
paper? We advance efforts to answer this question by building a benchmark that
evaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,
consists of text excerpts from recent machine learning papers, each referencing
a single other paper. CiteME use reveals a large gap between frontier LMs and
human performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.
We close this gap by introducing CiteAgent, an autonomous system built on the
GPT-4o LM that can also search and read papers, which achieves an accuracy of
35.3\% on CiteME. Overall, CiteME serves as a challenging testbed for
open-ended claim attribution, driving the research community towards a future
where any claim made by an LM can be automatically verified and discarded if
found to be incorrect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13743v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13743v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-visual models now produce photo-realistic images and videos,
they struggle with compositional text prompts involving attributes,
relationships, and higher-order reasoning such as logic and comparison. In this
work, we conduct an extensive human study on GenAI-Bench to evaluate the
performance of leading image and video generation models in various aspects of
compositional text-to-visual generation. We also compare automated evaluation
metrics against our collected human ratings and find that VQAScore -- a metric
measuring the likelihood that a VQA model views an image as accurately
depicting the prompt -- significantly outperforms previous metrics such as
CLIPScore. In addition, VQAScore can improve generation in a black-box manner
(without finetuning) via simply ranking a few (3 to 9) candidate images.
Ranking by VQAScore is 2x to 3x more effective than other scoring methods like
PickScore, HPSv2, and ImageReward at improving human alignment ratings for
DALL-E 3 and Stable Diffusion, especially on compositional prompts that require
advanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with
over 40,000 human ratings to evaluate scoring metrics on ranking images
generated from the same prompt. Lastly, we discuss promising areas for
improvement in VQAScore, such as addressing fine-grained visual details. We
will release all human ratings (over 80,000) to facilitate scientific
benchmarking of both generative models and automated metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We open-source our dataset, model, and code at:
  https://linzhiqiu.github.io/papers/genai_bench ; Project page:
  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first
  introduced in arxiv:2404.01291. This article extends it with an additional
  GenAI-Rank benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Discourse Relation Classification For Nigerian Pidgin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Saeed, Peter Bourgonje, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite attempts to make Large Language Models multi-lingual, many of the
world's languages are still severely under-resourced. This widens the
performance gap between NLP and AI applications aimed at well-financed, and
those aimed at less-resourced languages. In this paper, we focus on Nigerian
Pidgin (NP), which is spoken by nearly 100 million people, but has
comparatively very few NLP resources and corpora. We address the task of
Implicit Discourse Relation Classification (IDRC) and systematically compare an
approach translating NP data to English and then using a well-resourced IDRC
tool and back-projecting the labels versus creating a synthetic discourse
corpus for NP, in which we translate PDTB and project PDTB labels, and then
train an NP IDR classifier. The latter approach of learning a "native" NP
classifier outperforms our baseline by 13.27\% and 33.98\% in f$_{1}$ score for
4-way and 11-way classification, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERBench: An Entity-Relationship based Automatically Verifiable
  Hallucination Benchmark for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05266v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05266v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jio Oh, Soyeon Kim, Junseok Seo, Jindong Wang, Ruochen Xu, Xing Xie, Steven Euijong Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved unprecedented performances in
various applications, yet evaluating them is still challenging. Existing
benchmarks are either manually constructed or are automatic, but lack the
ability to evaluate the thought process of LLMs with arbitrary complexity. We
contend that utilizing existing relational databases based on the
entity-relationship (ER) model is a promising approach for constructing
benchmarks as they contain structured knowledge that can be used to question
LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational
databases have integrity constraints that can be used to better construct
complex in-depth questions and verify answers: (1) functional dependencies can
be used to pinpoint critical keywords that an LLM must know to properly answer
a given question containing certain attribute values; and (2) foreign key
constraints can be used to join relations and construct multi-hop questions,
which can be arbitrarily long and used to debug intermediate answers. We thus
propose ERBench, which uses these integrity constraints to convert any database
into an LLM benchmark. ERBench supports continuous evaluation as databases
change, multimodal questions, and various prompt engineering techniques. In our
experiments, we construct LLM benchmarks using databases of multiple domains
and make an extensive comparison of contemporary LLMs. We show how ERBench can
properly evaluate any LLM by not only checking for answer correctness, but also
effectively verifying the rationales by looking for the right keywords.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why are Visually-Grounded Language Models Bad at Image Classification? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image classification is one of the most fundamental capabilities of machine
vision intelligence. In this work, we revisit the image classification task
using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We
find that existing proprietary and public VLMs, despite often using CLIP as a
vision encoder and having many more parameters, significantly underperform CLIP
on standard image classification benchmarks like ImageNet. To understand the
reason, we explore several hypotheses concerning the inference algorithms,
training objectives, and data processing in VLMs. Our analysis reveals that the
primary cause is data-related: critical information for image classification is
encoded in the VLM's latent space but can only be effectively decoded with
enough training data. Specifically, there is a strong correlation between the
frequency of class exposure during VLM training and instruction-tuning and the
VLM's performance in those classes; when trained with sufficient data, VLMs can
match the accuracy of state-of-the-art classification models. Based on these
findings, we enhance a VLM by integrating classification-focused datasets into
its training, and demonstrate that the enhanced classification performance of
the VLM transfers to its general capabilities, resulting in an improvement of
11.8% on the newly collected ImageWikiQA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking Heads: Understanding Inter-layer Communication in <span class="highlight-title">Transformer</span>
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Merullo, Carsten Eickhoff, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although it is known that transformer language models (LMs) pass features
from early layers to later layers, it is not well understood how this
information is represented and routed by the model. We analyze a mechanism used
in two LMs to selectively inhibit items in a context in one task, and find that
it underlies a commonly used abstraction across many context-retrieval
behaviors. Specifically, we find that models write into low-rank subspaces of
the residual stream to represent features which are then read out by later
layers, forming low-rank communication channels (Elhage et al., 2021) between
layers. A particular 3D subspace in model activations in GPT-2 can be traversed
to positionally index items in lists, and we show that this mechanism can
explain an otherwise arbitrary-seeming sensitivity of the model to the order of
items in the prompt. That is, the model has trouble copying the correct
information from context when many items ``crowd" this limited space. By
decomposing attention heads with the Singular Value Decomposition (SVD), we
find that previously described interactions between heads separated by one or
more layers can be predicted via analysis of their weight matrices alone. We
show that it is possible to manipulate the internal model representations as
well as edit model weights based on the mechanism we discover in order to
significantly improve performance on our synthetic Laundry List task, which
requires recall from a list, often improving task accuracy by over 20%. Our
analysis reveals a surprisingly intricate interpretable structure learned from
language model pretraining, and helps us understand why sophisticated LMs
sometimes fail in simple domains, facilitating future analysis of more complex
behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PutnamBench: Evaluating Neural Theorem-Provers on the Putnam
  Mathematical Competition <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, Swarat Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PutnamBench, a new multi-language benchmark for evaluating the
ability of neural theorem-provers to solve competition mathematics problems.
PutnamBench consists of 1692 hand-constructed formalizations of 640 theorems
sourced from the William Lowell Putnam Mathematical Competition, the premier
undergraduate-level mathematics competition in North America. All the problems
have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq
formalizations. PutnamBench requires significant problem-solving ability and
proficiency in a broad range of topics taught in undergraduate mathematics
courses. We use PutnamBench to evaluate several established neural and symbolic
theorem-provers. These approaches can only solve a handful of the PutnamBench
problems, establishing the benchmark as a difficult open challenge for research
on neural theorem-proving. PutnamBench is available at
https://github.com/trishullab/PutnamBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Datasets & Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision
  Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06007v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06007v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, Wenhao Zheng, Zhaoyang Wang, Xiao Wang, Xuchao Zhang, Chetan Bansal, Marc Niethammer, Junzhou Huang, Hongtu Zhu, Yun Li, Jimeng Sun, Zongyuan Ge, Gang Li, James Zou, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has significantly impacted medical applications,
particularly with the advent of Medical Large Vision Language Models
(Med-LVLMs), sparking optimism for the future of automated and personalized
healthcare. However, the trustworthiness of Med-LVLMs remains unverified,
posing significant risks for future model deployment. In this paper, we
introduce CARES and aim to comprehensively evaluate the Trustworthiness of
Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs
across five dimensions, including trustfulness, fairness, safety, privacy, and
robustness. CARES comprises about 41K question-answer pairs in both closed and
open-ended formats, covering 16 medical image modalities and 27 anatomical
regions. Our analysis reveals that the models consistently exhibit concerns
regarding trustworthiness, often displaying factual inaccuracies and failing to
maintain fairness across different demographic groups. Furthermore, they are
vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly
release our benchmark and code in https://cares-ai.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ElectionSim: Massive Population Election Simulation Powered by Large
  Language Model Driven Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou, Siming Chen, Jiebo Luo, Xuanjing Huang, Shiping Tang, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The massive population election simulation aims to model the preferences of
specific groups in particular election scenarios. It has garnered significant
attention for its potential to forecast real-world social trends. Traditional
agent-based modeling (ABM) methods are constrained by their ability to
incorporate complex individual background information and provide interactive
prediction results. In this paper, we introduce ElectionSim, an innovative
election simulation framework based on large language models, designed to
support accurate voter simulations and customized distributions, together with
an interactive platform to dialogue with simulated voters. We present a
million-level voter pool sampled from social media platforms to support
accurate individual simulation. We also introduce PPE, a poll-based
presidential election benchmark to assess the performance of our framework
under the U.S. presidential election scenario. Through extensive experiments
and analyses, we demonstrate the effectiveness and robustness of our framework
in U.S. presidential election simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are you still on track!? Catching LLM Task Drift with Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00799v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00799v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, Andrew Paverd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models are commonly used in retrieval-augmented applications
to execute user instructions based on data from external sources. For example,
modern search engines use LLMs to answer queries based on relevant search
results; email plugins summarize emails by processing their content through an
LLM. However, the potentially untrusted provenance of these data sources can
lead to prompt injection attacks, where the LLM is manipulated by natural
language instructions embedded in the external data, causing it to deviate from
the user's original instruction(s). We define this deviation as task drift.
Task drift is a significant concern as it allows attackers to exfiltrate data
or influence the LLM's output for other users. We study LLM activations as a
solution to detect task drift, showing that activation deltas - the difference
in activations before and after processing external data - are strongly
correlated with this phenomenon. Through two probing methods, we demonstrate
that a simple linear classifier can detect drift with near-perfect ROC AUC on
an out-of-distribution test set. We evaluate these methods by making minimal
assumptions about how user's tasks, system prompts, and attacks can be phrased.
We observe that this approach generalizes surprisingly well to unseen task
domains, such as prompt injections, jailbreaks, and malicious instructions,
without being trained on any of these attacks. Interestingly, the fact that
this solution does not require any modifications to the LLM (e.g.,
fine-tuning), as well as its compatibility with existing meta-prompting
solutions, makes it cost-efficient and easy to deploy. To encourage further
research on activation-based task inspection, decoding, and interpretability,
we release our large-scale TaskTracker toolkit, featuring a dataset of over
500K instances, representations from six SoTA language models, and inspection
tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Biomedical Named Entity Recognition with General-domain
  Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yin, Hyunjae Kim, Xiao Xiao, Chih Hsuan Wei, Jaewoo Kang, Zhiyong Lu, Hua Xu, Meng Fang, Qingyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a neural network-based biomedical named entity recognition (BioNER)
model usually requires extensive and costly human annotations. While several
studies have employed multi-task learning with multiple BioNER datasets to
reduce human effort, this approach does not consistently yield performance
improvements and may introduce label ambiguity in different biomedical corpora.
We aim to tackle those challenges through transfer learning from easily
accessible resources with fewer concept overlaps with biomedical datasets. In
this paper, we proposed GERBERA, a simple-yet-effective method that utilized a
general-domain NER dataset for training. Specifically, we performed multi-task
learning to train a pre-trained biomedical language model with both the target
BioNER dataset and the general-domain dataset. Subsequently, we fine-tuned the
models specifically for the BioNER dataset. We systematically evaluated GERBERA
on five datasets of eight entity types, collectively consisting of 81,410
instances. Despite using fewer biomedical resources, our models demonstrated
superior performance compared to baseline models trained with multiple
additional BioNER datasets. Specifically, our models consistently outperformed
the baselines in six out of eight entity types, achieving an average
improvement of 0.9% over the best baseline performance across eight biomedical
entity types sourced from five different corpora. Our method was especially
effective in amplifying performance on BioNER datasets characterized by limited
data, with a 4.7% improvement in F1 scores on the JNLPBA-RNA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in JBI 2024. We make data, codes, and models publicly
  available via https://github.com/qingyu-qc/bioner_gerbera</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-world Instance-specific Image Goal Navigation: Bridging Domain Gaps
  via Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Sakaguchi, Akira Taniguchi, Yoshinobu Hagiwara, Lotfi El Hafi, Shoichi Hasegawa, Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving instance-specific image goal navigation (InstanceImageNav), which
locates the identical object in a real-world environment from a query image, is
essential for robotic systems to assist users in finding desired objects. The
challenge lies in the domain gap between low-quality images observed by the
moving robot, characterized by motion blur and low-resolution, and high-quality
query images provided by the user. Such domain gaps could significantly reduce
the task success rate but have not been the focus of previous work. To address
this, we propose a novel method called Few-shot Cross-quality Instance-aware
Adaptation (CrossIA), which employs contrastive learning with an instance
classifier to align features between massive low- and few high-quality images.
This approach effectively reduces the domain gap by bringing the latent
representations of cross-quality images closer on an instance basis.
Additionally, the system integrates an object image collection with a
pre-trained deblurring model to enhance the observed image quality. Our method
fine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We
evaluated our method's effectiveness through an InstanceImageNav task with 20
different types of instances, where the robot identifies the same instance in a
real-world environment as a high-quality query image. Our experiments showed
that our method improves the task success rate by up to three times compared to
the baseline, a conventional approach based on SuperGlue. These findings
highlight the potential of leveraging contrastive learning and image
enhancement techniques to bridge the domain gap and improve object localization
in robotic applications. The project website is
https://emergentsystemlabstudent.github.io/DomainBridgingNav/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See website at
  https://emergentsystemlabstudent.github.io/DomainBridgingNav/. Accepted to
  IEEE IRC2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of Validating Synthetic Data for Formula Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usneek Singh, José Cambronero, Sumit Gulwani, Aditya Kanade, Anirudh Khatry, Vu Le, Mukul Singh, Gust Verbruggen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can be leveraged to help with writing formulas
in spreadsheets, but resources on these formulas are scarce, impacting both the
base performance of pre-trained models and limiting the ability to fine-tune
them. Given a corpus of formulas, we can use a(nother) model to generate
synthetic natural language utterances for fine-tuning. However, it is important
to validate whether the NL generated by the LLM is indeed accurate to be
beneficial for fine-tuning. In this paper, we provide empirical results on the
impact of validating these synthetic training examples with surrogate
objectives that evaluate the accuracy of the synthetic annotations. We
demonstrate that validation improves performance over raw data across four
models (2 open and 2 closed weight). Interestingly, we show that although
validation tends to prune more challenging examples, it increases the
complexity of problems that models can solve after being fine-tuned on
validated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Editing of Activations for Large Language Model Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09719v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09719v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, Shay B. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often exhibit undesirable behaviours, such as
generating untruthful or biased content. Editing their internal representations
has been shown to be effective in mitigating such behaviours on top of the
existing alignment methods. We propose a novel inference-time editing method,
namely spectral editing of activations (SEA), to project the input
representations into directions with maximal covariance with the positive
demonstrations (e.g., truthful) while minimising covariance with the negative
demonstrations (e.g., hallucinated). We also extend our method to non-linear
editing using feature functions. We run extensive experiments on benchmarks
concerning truthfulness and bias with six open-source LLMs of different sizes
and model families. The results demonstrate the superiority of SEA in
effectiveness, generalisation to similar tasks, as well as computation and data
efficiency. We also show that SEA editing only has a limited negative impact on
other model capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CATS: Contextually-Aware Thresholding for Sparsity in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08763v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08763v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghyun Lee, Je-Yong Lee, Genghan Zhang, Mo Tiwari, Azalia Mirhoseini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have dramatically advanced AI applications, yet
their deployment remains challenging due to their immense inference costs.
Recent studies ameliorate the computational costs of LLMs by increasing their
activation sparsity but suffer from significant performance degradation on
downstream tasks. In this work, we introduce a new framework for sparsifying
the activations of base LLMs and reducing inference costs, dubbed Contextually
Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to
implement, and highly effective. At the heart of our framework is a new
non-linear activation function. We demonstrate that CATS can be applied to
various base models, including Mistral-7B and Llama2-7B, and outperforms
existing sparsification techniques in downstream task performance. More
precisely, CATS-based models often achieve downstream task performance within
1-2% of their base models without any fine-tuning and even at activation
sparsity levels of 50%. Furthermore, CATS-based models converge faster and
display better task performance than competing techniques when fine-tuning is
applied. Finally, we develop a custom GPU kernel for efficient implementation
of CATS that translates the activation of sparsity of CATS to real wall-clock
time speedups. Our custom kernel implementation of CATS results in a ~15%
improvement in wall-clock inference latency of token generation on both
Llama-7B and Mistral-7B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache
  Compression <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11430v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11430v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of large language models (LLMs) is often hindered by the
extensive memory requirements of the Key-Value (KV) cache, especially as
context lengths increase. Existing approaches to reduce the KV cache size
involve either fine-tuning the model to learn a compression strategy or
leveraging attention scores to reduce the sequence length. We analyse the
attention distributions in decoder-only Transformers-based models and observe
that attention allocation patterns stay consistent across most layers.
Surprisingly, we find a clear correlation between the $L_2$ and the attention
scores over cached KV pairs, where a low $L_2$ of a key embedding usually leads
to a high attention score during decoding. This finding indicates that the
influence of a KV pair is potentially determined by the key embedding itself
before being queried. Based on this observation, we compress the KV cache based
on the $L_2$ of key embeddings. Our experimental results show that this simple
strategy can reduce the KV cache size by 50% on language modelling and
needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing
accuracy. Moreover, without relying on the attention scores, this approach
remains compatible with FlashAttention, enabling broader applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of a paper published in the proceedings
  of the 2024 Conference on Empirical Methods in Natural Language Processing
  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on
  Efficient Natural Language and Speech Processing (ENLSP-IV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Triplet-Level Hallucination Evaluation for Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wu, Tsz Ting Chung, Kai Chen, Dit-Yan Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the outstanding performance in vision-language reasoning, Large
Vision-Language Models (LVLMs) might generate hallucinated contents that do not
exist in the given image. Most existing LVLM hallucination benchmarks are
constrained to evaluate the object-related hallucinations. However, the
potential hallucination on the relations between two objects, i.e., relation
hallucination, still lacks investigation. To remedy that, in this paper we
design a unified framework to measure object and relation hallucination in
LVLMs simultaneously. The core idea of our framework is to conduct
hallucination evaluation on (object, relation, object) triplets extracted from
LVLMs' responses, and thus, could be easily generalized to different
vision-language tasks. Based on our framework, we further introduce Tri-HE, a
novel Triplet-level Hallucination Evaluation benchmark which can be used to
study both object and relation hallucination at the same time. We conduct
comprehensive evaluations on Tri-HE and observe that the relation hallucination
issue is even more serious than object hallucination among existing LVLMs,
highlighting a previously neglected problem towards reliable LVLMs. Moreover,
based on our findings, we design a simple yet effective training-free approach
to mitigate hallucinations for LVLMs, with which, we exceed all open-sourced
counterparts on Tri-HE, achieving comparable performance with the powerful
GPT-4V. Our dataset and code for the reproduction of our experiments are
available publicly at https://github.com/wujunjie1998/Tri-HE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://kaichen1998.github.io/projects/tri-he/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOPA: Extending Large Language Models for Video Understanding via
  Text-Only Pre-Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Hehe Fan, Yongkang Wong, Mohan Kankanhalli, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in image understanding have benefited from the extensive
use of web image-text pairs. However, video understanding remains a challenge
despite the availability of substantial web video-text data. This difficulty
primarily arises from the inherent complexity of videos and the inefficient
language supervision in recent web-collected video-text datasets. In this
paper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend
large language models (LLMs) for video understanding, without the need for
pre-training on real video data. Specifically, we first employ an advanced LLM
to automatically generate Textual Videos comprising continuous textual frames,
along with corresponding annotations to simulate real video-text data. Then,
these annotated textual videos are used to pre-align a language-only LLM with
the video modality. To bridge the gap between textual and real videos, we
employ the CLIP model as the feature extractor to align image and text
modalities. During text-only pre-alignment, the continuous textual frames,
encoded as a sequence of CLIP text features, are analogous to continuous CLIP
image features, thus aligning the LLM with real video representation. Extensive
experiments, including zero-shot evaluation and finetuning on various video
understanding tasks, demonstrate that TOPA is an effective and efficient
framework for aligning video content with LLMs. In particular, without training
on any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%
on the challenging long-form video understanding benchmark, Egoschema. This
performance surpasses previous video-text pre-training approaches and proves
competitive with recent GPT-3.5-based video agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">BERT</span>time Stories: Investigating the Role of Synthetic Story Data in
  Language <span class="highlight-title">pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikitas Theodoropoulos, Giorgos Filandrianos, Vassilis Lyberatos, Maria Lymperaiou, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe our contribution to the Strict and Strict-Small tracks of the 2nd
iteration of the BabyLM Challenge. The shared task is centered around efficient
pre-training given data constraints motivated by human development. In
response, we study the effect of synthetic story data in language pre-training
using TinyStories: a recently introduced dataset of short stories. Initially,
we train GPT-Neo models on subsets of TinyStories, while varying the amount of
available data. We find that, even with access to less than 100M words, the
models are able to generate high-quality, original completions to a given
story, and acquire substantial linguistic knowledge. To measure the effect of
synthetic story data, we train LTG-BERT encoder models on a combined dataset
of: a subset of TinyStories, story completions generated by GPT-Neo, and a
subset of the BabyLM dataset. Our experimentation reveals that synthetic data
can occasionally offer modest gains, but overall have a negative influence on
linguistic understanding. Our work offers an initial study on synthesizing
story data in low resource settings and underscores their potential for
augmentation in data-constrained language modeling. We publicly release our
models and implementation on our GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ READoc: A Unified Benchmark for Realistic Document Structured Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichao Li, Aizier Abulaiti, Yaojie Lu, Xuanang Chen, Jia Zheng, Hongyu Lin, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document Structured Extraction (DSE) aims to extract structured content from
raw documents. Despite the emergence of numerous DSE systems, their unified
evaluation remains inadequate, significantly hindering the field's advancement.
This problem is largely attributed to existing benchmark paradigms, which
exhibit fragmented and localized characteristics. To address these limitations
and offer a thorough evaluation of DSE systems, we introduce a novel benchmark
named READoc, which defines DSE as a realistic task of converting unstructured
PDFs into semantically rich Markdown. The READoc dataset is derived from 2,233
diverse and real-world documents from arXiv and GitHub. In addition, we develop
a DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring
modules, to conduct a unified evaluation of state-of-the-art DSE approaches. By
evaluating a range of pipeline tools, expert visual models, and general VLMs,
we identify the gap between current work and the unified, realistic DSE
objective for the first time. We aspire that READoc will catalyze future
research in DSE, fostering more comprehensive and practical solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in
  Code Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingchang Chen, Hongxuan Tang, Zheng Chu, Qianglong Chen, Zekun Wang, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress made by large language models in code generation,
they still struggle with programs that meet complex requirements. Recent work
utilizes plan-and-solve decomposition to decrease the complexity and leverage
self-tests to refine the generated program. Yet, planning deep-inside
requirements in advance can be challenging, and the tests need to be accurate
to accomplish self-improvement. To this end, we propose FunCoder, a code
generation framework incorporating the divide-and-conquer strategy with
functional consensus. Specifically, FunCoder recursively branches off
sub-functions as smaller goals during code generation, represented by a tree
hierarchy. These sub-functions are then composited to attain more complex
objectives. Additionally, we designate functions via a consensus formed by
identifying similarities in program behavior, mitigating error propagation.
FunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval,
MBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method
demonstrates superiority on smaller models: With FunCoder, StableCode-3b
surpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4's performance on
HumanEval. Further analysis reveals that our proposed dynamic function
decomposition is capable of handling complex requirements, and the functional
consensus prevails over self-testing in correctness evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Little Giants: Synthesizing High-Quality Embedding Data at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation has become an increasingly popular way of training
models without the need for large, manually labeled datasets. For tasks like
text embedding, synthetic data offers diverse and scalable training examples,
significantly reducing the cost of human annotation. However, most current
approaches rely heavily on proprietary models like GPT-4, which are expensive
and inefficient for generating large-scale embedding data. In this paper, we
introduce SPEED, a framework that aligns open-source small models (8B) to
efficiently generate large-scale synthetic embedding data. Through supervised
fine-tuning, preference optimization, and self-improvement, SPEED enables small
open-source models to produce high-quality data. Remarkably, SPEED uses only
less than 1/10 of the GPT API calls, outperforming the state-of-the-art
embedding model E5_mistral when both are trained solely on their synthetic
data. Using this efficient generator, we conduct a comprehensive study on how
various factors within the alignment pipeline impact data quality and reveal
the scaling law for synthetic embedding data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent
  Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09824v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09824v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen Pan, Yaliang Li, Bolin Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph generation is a fundamental task that has been extensively studied in
social, technological, and scientific analysis. For modeling the dynamic graph
evolution process, traditional rule-based methods struggle to capture community
structures within graphs, while deep learning methods only focus on fitting
training graphs. This limits existing graph generators to producing graphs that
adhere to predefined rules or closely resemble training datasets, achieving
poor performance in dynamic graph generation. Given that graphs are abstract
representations arising from pairwise interactions in human activities, a
realistic simulation of human-wise interaction could provide deeper insights
into the graph evolution mechanism. With the increasing recognition of large
language models (LLMs) in simulating human behavior, we introduce
GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic
graph generation. Without training or fine-tuning process of LLM, our framework
effectively replicates seven macro-level structural characteristics in
established network science theories while surpassing existing baselines in
graph expansion tasks by 31\% on specific evaluation metrics. Through node
classification task, we validate GAG effectively preserves characteristics of
real-world network for node-wise textual features in generated text-rich graph.
Furthermore, by incorporating parallel acceleration, GAG supports generating
graphs with up to nearly 100,000 nodes or 10 million edges through large-scale
LLM-based agent simulation, with a minimum speed-up of 90.4\%. The source code
is available at https://anonymous.4open.science/r/GraphAgent-2206.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Tokens to Materials: Leveraging Language Models for Scientific
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Wan, Tong Xie, Nan Wu, Wenjie Zhang, Chunyu Kit, Bram Hoex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring the predictive capabilities of language models in material science
is an ongoing interest. This study investigates the application of language
model embeddings to enhance material property prediction in materials science.
By evaluating various contextual embedding methods and pre-trained models,
including Bidirectional Encoder Representations from Transformers (BERT) and
Generative Pre-trained Transformers (GPT), we demonstrate that domain-specific
models, particularly MatBERT significantly outperform general-purpose models in
extracting implicit knowledge from compound names and material properties. Our
findings reveal that information-dense embeddings from the third layer of
MatBERT, combined with a context-averaging approach, offer the most effective
method for capturing material-property relationships from the scientific
literature. We also identify a crucial "tokenizer effect," highlighting the
importance of specialized text processing techniques that preserve complete
compound names while maintaining consistent token counts. These insights
underscore the value of domain-specific training and tokenization in materials
science applications and offer a promising pathway for accelerating the
discovery and development of new materials through AI-driven approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Recursive Encoding for Cuneiform Signs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel M. Stelzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most significant problems in cuneiform pedagogy is the process of
looking up unknown signs, which often involves a tedious page-by-page search
through a sign list. This paper proposes a new "recursive encoding" for signs,
which represents the arrangement of strokes in a way a computer can process. A
series of new algorithms then offers students a new way to look up signs by any
distinctive component, as well as providing new ways to render signs and
tablets electronically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 29 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArEEG_Chars: <span class="highlight-title">Dataset</span> for Envisioned Speech Recognition using EEG for
  Arabic Characters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15733v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15733v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazem Darwish, Abdalrahman Al Malah, Khloud Al Jallad, Nada Ghneim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-computer interfaces is an important and hot research topic that
revolutionize how people interact with the world, especially for individuals
with neurological disorders. While extensive research has been done in EEG
signals of English letters and words, a major limitation remains: the lack of
publicly available EEG datasets for many non-English languages, such as Arabic.
Although Arabic is one of the most spoken languages worldwide, to the best of
our knowledge, there is no publicly available dataset for EEG signals of Arabic
characters until now. To address this gap, we introduce ArEEG_Chars, a novel
EEG dataset for Arabic 31 characters collected from 30 participants (21 males
and 9 females), these records were collected using Epoc X 14 channels device
for 10 seconds long for each char record. The number of recorded signals were
930 EEG recordings. To make the EEG signals suitable for analyzing, each
recording has been split into multiple signals with a time duration of 250ms,
respectively. Therefore, a total of 39857 recordings of EEG signals have been
collected in this study. Moreover, ArEEG_Chars will be publicly available for
researchers. We do hope that this dataset will fill an important gap in the
research of Arabic EEG benefiting Arabic-speaking individuals with
disabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning <span class="highlight-title">Dataset</span> <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown the immense potential of synthetically generated
datasets for training large language models (LLMs), especially for acquiring
targeted skills. Current large-scale math instruction tuning datasets such as
MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed
using outputs from closed-source LLMs with commercially restrictive licenses. A
key reason limiting the use of open-source LLMs in these data generation
pipelines has been the wide gap between the mathematical skills of the best
closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on
the recent progress in open-source LLMs, our proposed prompting novelty, and
some brute-force scaling, we construct OpenMathInstruct-1, a math instruction
tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by
synthesizing code-interpreter solutions for GSM8K and MATH, two popular math
reasoning benchmarks, using the recently released and permissively licensed
Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of
OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which
is competitive with the best gpt-distilled models. We release our code, models,
and the OpenMathInstruct-1 dataset under a commercially permissive license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLMs' Mathematical and Coding Competency through
  Ontology-guided Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09395v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09395v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Hong, Navonil Majumder, Deepanway Ghosal, Somak Aditya, Rada Mihalcea, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have showcased striking
results on existing logical reasoning benchmarks, with some models even
surpassing human performance. However, the true depth of their competencies and
robustness in reasoning tasks remains an open question. To this end, in this
paper, we focus on two popular reasoning tasks: arithmetic reasoning and code
generation. Particularly, we introduce (i) a general ontology of perturbations
for math and coding questions, (ii) a semi-automatic method to apply these
perturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,
of perturbed math and coding problems to probe LLM capabilities in numeric
reasoning and coding tasks. Through comprehensive evaluations of both
closed-source and open-source LLMs, we show a significant performance drop
across all the models against the perturbed questions, suggesting that the
current LLMs lack robust problem solving skills and structured reasoning
abilities in many areas, as defined by our ontology. We open-source the
datasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>With o1 and GPT-4o results. Reformatted the data and presented more
  analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monotonic Paraphrasing Improves Generalization of Language Model
  <span class="highlight-title">Prompt</span>ing <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Liu, Fei Wang, Nan Xu, Tianyi Yan, Tao Meng, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of large language models (LLMs) may vary with different prompts
or instructions of even the same task. One commonly recognized factor for this
phenomenon is the model's familiarity with the given prompt or instruction,
which is typically estimated by its perplexity. However, finding the prompt
with the lowest perplexity is challenging, given the enormous space of possible
prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),
an end-to-end decoding strategy that paraphrases given prompts or instructions
into their lower perplexity counterparts based on an ensemble of a paraphrase
LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or
instruction executor) that constrains the generation for lower perplexity. The
ensemble decoding process can efficiently paraphrase the original prompt
without altering its semantic meaning, while monotonically decreasing the
perplexity of each generation as calculated by the target LM. We explore in
detail both greedy and search-based decoding as two alternative decoding
schemes of MonoPara. Notably, MonoPara does not require any training and can
monotonically lower the perplexity of the paraphrased prompt or instruction,
leading to improved performance of zero-shot LM prompting as evaluated on a
wide selection of tasks. In addition, MonoPara is also shown to effectively
improve LMs' generalization on perturbed and unseen task instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Tuning with Human Feedback on Language, Speech, and Vision
  Tasks: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, Hanyang Zhao, Anirban Das, Wenpin Tang, David D. Yao, Shi-Xiong Zhang, Sambit Sahu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference tuning is a crucial process for aligning deep generative models
with human preferences. This survey offers a thorough overview of recent
advancements in preference tuning and the integration of human feedback. The
paper is organized into three main sections: 1) introduction and preliminaries:
an introduction to reinforcement learning frameworks, preference tuning tasks,
models, and datasets across various modalities: language, speech, and vision,
as well as different policy approaches, 2) in-depth exploration of each
preference tuning approach: a detailed analysis of the methods used in
preference tuning, and 3) applications, discussion, and future directions: an
exploration of the applications of preference tuning in downstream tasks,
including evaluation methods for different modalities, and an outlook on future
research directions. Our objective is to present the latest methodologies in
preference tuning and model alignment, enhancing the understanding of this
field for researchers and practitioners. We hope to encourage further
engagement and innovation in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Survey paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjiao Tian, Chengyue Huang, Zsolt Kira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern optimizers such as AdamW, equipped with momentum and adaptive learning
rate, are designed to escape local minima and explore the vast parameter space.
This exploration is beneficial for finding good loss basins when training from
scratch. It is not necessarily ideal when resuming from a powerful foundation
model because it can lead to large deviations from the pre-trained
initialization and, consequently, worse robustness and generalization. At the
same time, strong regularization on all parameters can lead to under-fitting.
We hypothesize that selectively regularizing the parameter space is the key to
fitting and retraining the pre-trained knowledge. This paper proposes a new
weight decay technique, Selective Projection Decay (SPD), that selectively
imposes a strong penalty on certain layers while allowing others to change
freely. Intuitively, SPD expands and contracts the parameter search space for
layers with consistent and inconsistent loss reduction, respectively.
Experimentally, when equipped with SPD, Adam consistently provides better
in-distribution generalization and out-of-distribution robustness performance
on multiple popular vision and language benchmarks. Code available
at~\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROAD-Waymo: Action Awareness at Scale for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salman Khan, Izzeddin Teeti, Reza Javanmard Alitappeh, Mihaela C. Stoian, Eleonora Giunchiglia, Gurkirt Singh, Andrew Bradley, Fabio Cuzzolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Vehicle (AV) perception systems require more than simply seeing,
via e.g., object detection or scene segmentation. They need a holistic
understanding of what is happening within the scene for safe interaction with
other road users. Few datasets exist for the purpose of developing and training
algorithms to comprehend the actions of other road users. This paper presents
ROAD-Waymo, an extensive dataset for the development and benchmarking of
techniques for agent, action, location and event detection in road scenes,
provided as a layer upon the (US) Waymo Open dataset. Considerably larger and
more challenging than any existing dataset (and encompassing multiple cities),
it comes with 198k annotated video frames, 54k agent tubes, 3.9M bounding boxes
and a total of 12.4M labels. The integrity of the dataset has been confirmed
and enhanced via a novel annotation pipeline designed for automatically
identifying violations of requirements specifically designed for this dataset.
As ROAD-Waymo is compatible with the original (UK) ROAD dataset, it provides
the opportunity to tackle domain adaptation between real-world road scenarios
in different countries within a novel benchmark: ROAD++.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MamT$^4$: Multi-view Attention Networks for Mammography Cancer
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alisher Ibragimov, Sofya Senotrusova, Arsenii Litvinov, Egor Ushakov, Evgeny Karpulevich, Yury Markin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a novel method, called MamT$^4$, which is used
for simultaneous analysis of four mammography images. A decision is made based
on one image of a breast, with attention also devoted to three additional
images: another view of the same breast and two images of the other breast.
This approach enables the algorithm to closely replicate the practice of a
radiologist who reviews the entire set of mammograms for a patient.
Furthermore, this paper emphasizes the preprocessing of images, specifically
proposing a cropping model (U-Net based on ResNet-34) to help the method remove
image artifacts and focus on the breast region. To the best of our knowledge,
this study is the first to achieve a ROC-AUC of 84.0 $\pm$ 1.7 and an F1 score
of 56.0 $\pm$ 1.3 on an independent test dataset of Vietnam digital mammography
(VinDr-Mammo), which is preprocessed with the cropping model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The crop model is available here:
  https://github.com/ispras/mammo_crop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Degradation-Aware Residual-Conditioned Optimal Transport for Unified
  Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaole Tang, Xiang Gu, Xiaoyi He, Xin Hu, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All-in-one image restoration has emerged as a practical and promising
low-level vision task for real-world applications. In this context, the key
issue lies in how to deal with different types of degraded images
simultaneously. In this work, we present a Degradation-Aware
Residual-Conditioned Optimal Transport (DA-RCOT) approach that models
(all-in-one) image restoration as an optimal transport (OT) problem for
unpaired and paired settings, introducing the transport residual as a
degradation-specific cue for both the transport cost and the transport map.
Specifically, we formalize image restoration with a residual-guided OT
objective by exploiting the degradation-specific patterns of the Fourier
residual in the transport cost. More crucially, we design the transport map for
restoration as a two-pass DA-RCOT map, in which the transport residual is
computed in the first pass and then encoded as multi-scale residual embeddings
to condition the second-pass restoration. This conditioning process injects
intrinsic degradation knowledge (e.g., degradation type and level) and
structural information from the multi-scale residual embeddings into the OT
map, which thereby can dynamically adjust its behaviors for all-in-one
restoration. Extensive experiments across five degradations demonstrate the
favorable performance of DA-RCOT as compared to state-of-the-art methods, in
terms of distortion measures, perceptual quality, and image structure
preservation. Notably, DA-RCOT delivers superior adaptability to real-world
scenarios even with multiple degradations and shows distinctive robustness to
both degradation levels and the number of degradations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Gastrointestinal Diagnostics: A CNN-Based Model for VCE Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaneeta Ahlawat, Rohit Sharma,  Urush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the diagnosis of gastrointestinal (GI) diseases has advanced
greatly with the advent of high-tech video capsule endoscopy (VCE) technology,
which allows for non-invasive observation of the digestive system. The MisaHub
Capsule Vision Challenge encourages the development of vendor-independent
artificial intelligence models that can autonomously classify GI anomalies from
VCE images. This paper presents CNN architecture designed specifically for
multiclass classification of ten gut pathologies, including angioectasia,
bleeding, erosion, erythema, foreign bodies, lymphangiectasia, polyps, ulcers,
and worms as well as their normal state.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figuers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MapEx: Indoor Structure Exploration with Probabilistic Information Gain
  from Global Map Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cherie Ho, Seungchan Kim, Brady Moon, Aditya Parandekar, Narek Harutyunyan, Chen Wang, Katia Sycara, Graeme Best, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploration is a critical challenge in robotics, centered on understanding
unknown environments. In this work, we focus on robots exploring structured
indoor environments which are often predictable and composed of repeating
patterns. Most existing approaches, such as conventional frontier approaches,
have difficulty leveraging the predictability and explore with simple
heuristics such as `closest first'. Recent works use deep learning techniques
to predict unknown regions of the map, using these predictions for information
gain calculation. However, these approaches are often sensitive to the
predicted map quality or do not reason over sensor coverage. To overcome these
issues, our key insight is to jointly reason over what the robot can observe
and its uncertainty to calculate probabilistic information gain. We introduce
MapEx, a new exploration framework that uses predicted maps to form
probabilistic sensor model for information gain estimation. MapEx generates
multiple predicted maps based on observed information, and takes into
consideration both the computed variances of predicted maps and estimated
visible area to estimate the information gain of a given viewpoint. Experiments
on the real-world KTH dataset showed on average 12.4% improvement than
representative map-prediction based exploration and 25.4% improvement than
nearest frontier approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynCo: Synthetic Hard Negatives in Contrastive Learning for Better
  Unsupervised Visual Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02401v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02401v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become a dominant approach in self-supervised visual
representation learning. Hard negatives - samples closely resembling the anchor
- are key to enhancing learned representations' discriminative power. However,
efficiently leveraging hard negatives remains challenging. We introduce SynCo
(Synthetic Negatives in Contrastive learning), a novel approach that improves
model performance by generating synthetic hard negatives on the representation
space. Building on the MoCo framework, SynCo introduces six strategies for
creating diverse synthetic hard negatives on-the-fly with minimal computational
overhead. SynCo achieves faster training and better representation learning,
reaching 67.9% top-1 accuracy on ImageNet ILSVRC-2012 linear evaluation after
200 pretraining epochs, surpassing MoCo's 67.5% using the same ResNet-50
encoder. It also transfers more effectively to detection tasks: on PASCAL VOC,
it outperforms both the supervised baseline and MoCo with 82.5% AP; on COCO, it
sets new benchmarks with 40.9% AP for bounding box detection and 35.5% AP for
instance segmentation. Our synthetic hard negative generation approach
significantly enhances visual representations learned through self-supervised
contrastive learning. Code is available at
https://github.com/giakoumoglou/synco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Invariant Representations with Dual Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has been widely used to transfer knowledge from
large, accurate models (teachers) to smaller, efficient ones (students). Recent
methods have explored enforcing consistency by incorporating causal
interpretations to distill invariant representations. In this work, we extend
this line of research by introducing a dual augmentation strategy to promote
invariant feature learning in both teacher and student models. Our approach
leverages different augmentations applied to both models during distillation,
pushing the student to capture robust, transferable features. This dual
augmentation strategy complements invariant causal distillation by ensuring
that the learned representations remain stable across a wider range of data
variations and transformations. Extensive experiments on CIFAR-100 demonstrate
the effectiveness of this approach, achieving competitive results in
same-architecture KD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper presents preliminary results from a project that we have
  since discontinued, as our research focus has shifted to new directions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14468v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14468v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Cong Wei, Weiming Ren, Harry Yang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic field of digital content creation using generative models,
state-of-the-art video editing models still do not offer the level of quality
and control that users desire. Previous works on video editing either extended
from image-based generative models in a zero-shot manner or necessitated
extensive fine-tuning, which can hinder the production of fluid video edits.
Furthermore, these methods frequently rely on textual input as the editing
guidance, leading to ambiguities and limiting the types of edits they can
perform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free
paradigm designed to simplify video editing into two primary steps: (1)
employing an off-the-shelf image editing model to modify the first frame, (2)
utilizing an existing image-to-video generation model to generate the edited
video through temporal feature injection. AnyV2V can leverage any existing
image editing tools to support an extensive array of video editing tasks,
including prompt-based editing, reference-based style transfer, subject-driven
editing, and identity manipulation, which were unattainable by previous
methods. AnyV2V can also support any video length. Our evaluation shows that
AnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore,
AnyV2V significantly outperformed these baselines in human evaluations,
demonstrating notable improvements in visual consistency with the source video
while producing high-quality edits across all editing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR 2024)
  (11/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with
  Attention Grouping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00985v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00985v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingzhen Huang, Jialing Cai, Shan Jia, Vishnu Suresh Lokhande, Siwei Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven image synthesis has made significant advancements with the
development of diffusion models, transforming how visual content is generated
from text prompts. Despite these advances, text-driven image editing, a key
area in computer graphics, faces unique challenges. A major challenge is making
simultaneous edits across multiple objects or attributes. Applying these
methods sequentially for multi-attribute edits increases computational demands
and efficiency losses. In this paper, we address these challenges with
significant contributions. Our main contribution is the development of
ParallelEdits, a method that seamlessly manages simultaneous edits across
multiple attributes. In contrast to previous approaches, ParallelEdits not only
preserves the quality of single attribute edits but also significantly improves
the performance of multitasking edits. This is achieved through innovative
attention distribution mechanism and multi-branch design that operates across
several processing heads. Additionally, we introduce the PIE-Bench++ dataset,
an expansion of the original PIE-Bench dataset, to better support evaluating
image-editing tasks involving multiple objects and attributes simultaneously.
This dataset is a benchmark for evaluating text-driven image editing methods in
multifaceted scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13743v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13743v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-visual models now produce photo-realistic images and videos,
they struggle with compositional text prompts involving attributes,
relationships, and higher-order reasoning such as logic and comparison. In this
work, we conduct an extensive human study on GenAI-Bench to evaluate the
performance of leading image and video generation models in various aspects of
compositional text-to-visual generation. We also compare automated evaluation
metrics against our collected human ratings and find that VQAScore -- a metric
measuring the likelihood that a VQA model views an image as accurately
depicting the prompt -- significantly outperforms previous metrics such as
CLIPScore. In addition, VQAScore can improve generation in a black-box manner
(without finetuning) via simply ranking a few (3 to 9) candidate images.
Ranking by VQAScore is 2x to 3x more effective than other scoring methods like
PickScore, HPSv2, and ImageReward at improving human alignment ratings for
DALL-E 3 and Stable Diffusion, especially on compositional prompts that require
advanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with
over 40,000 human ratings to evaluate scoring metrics on ranking images
generated from the same prompt. Lastly, we discuss promising areas for
improvement in VQAScore, such as addressing fine-grained visual details. We
will release all human ratings (over 80,000) to facilitate scientific
benchmarking of both generative models and automated metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We open-source our dataset, model, and code at:
  https://linzhiqiu.github.io/papers/genai_bench ; Project page:
  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first
  introduced in arxiv:2404.01291. This article extends it with an additional
  GenAI-Rank benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaB-GATr: geometric algebra <span class="highlight-title">transformer</span>s for large biomedical surface
  and volume meshes <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Suk, Baris Imre, Jelmer M. Wolterink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many anatomical structures can be described by surface or volume meshes.
Machine learning is a promising tool to extract information from these 3D
models. However, high-fidelity meshes often contain hundreds of thousands of
vertices, which creates unique challenges in building deep neural network
architectures. Furthermore, patient-specific meshes may not be canonically
aligned which limits the generalisation of machine learning algorithms. We
propose LaB-GATr, a transfomer neural network with geometric tokenisation that
can effectively learn with large-scale (bio-)medical surface and volume meshes
through sequence compression and interpolation. Our method extends the recently
proposed geometric algebra transformer (GATr) and thus respects all Euclidean
symmetries, i.e. rotation, translation and reflection, effectively mitigating
the problem of canonical alignment between patients. LaB-GATr achieves
state-of-the-art results on three tasks in cardiovascular hemodynamics
modelling and neurodevelopmental phenotype prediction, featuring meshes of up
to 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful
architecture for learning with high-fidelity meshes which has the potential to
enable interesting downstream applications. Our implementation is publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First published in "Medical Image Computing and Computer Assisted
  Intervention" (MICCAI), pp 185-195, 2024 by Springer Nature</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, Serge Belongie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to
segment novel categories with minimal annotated support samples. While existing
FS-PCS methods have shown promise, they primarily focus on unimodal point cloud
inputs, overlooking the potential benefits of leveraging multimodal
information. In this paper, we address this gap by introducing a cost-free
multimodal FS-PCS setup, utilizing textual labels and the potentially available
2D image modality. Under this easy-to-achieve setup, we present the MultiModal
Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary
information from multiple modalities. MM-FSS employs a shared backbone with two
heads to extract intermodal and unimodal visual features, and a pretrained text
encoder to generate text embeddings. To fully exploit the multimodal
information, we propose a Multimodal Correlation Fusion (MCF) module to
generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module
to refine the correlations using text-aware semantic guidance. Additionally, we
propose a simple yet effective Test-time Adaptive Cross-modal Calibration
(TACC) technique to mitigate training bias, further improving generalization.
Experimental results on S3DIS and ScanNet datasets demonstrate significant
performance improvements achieved by our method. The efficacy of our approach
indicates the benefits of leveraging commonly-ignored free modalities for
FS-PCS, providing valuable insights for future research. The code is available
at https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why are Visually-Grounded Language Models Bad at Image Classification? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image classification is one of the most fundamental capabilities of machine
vision intelligence. In this work, we revisit the image classification task
using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We
find that existing proprietary and public VLMs, despite often using CLIP as a
vision encoder and having many more parameters, significantly underperform CLIP
on standard image classification benchmarks like ImageNet. To understand the
reason, we explore several hypotheses concerning the inference algorithms,
training objectives, and data processing in VLMs. Our analysis reveals that the
primary cause is data-related: critical information for image classification is
encoded in the VLM's latent space but can only be effectively decoded with
enough training data. Specifically, there is a strong correlation between the
frequency of class exposure during VLM training and instruction-tuning and the
VLM's performance in those classes; when trained with sufficient data, VLMs can
match the accuracy of state-of-the-art classification models. Based on these
findings, we enhance a VLM by integrating classification-focused datasets into
its training, and demonstrate that the enhanced classification performance of
the VLM transfers to its general capabilities, resulting in an improvement of
11.8% on the newly collected ImageWikiQA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BrepGen: A B-rep Generative Diffusion Model with Structured Latent
  Geometry <span class="chip">SIGGRAPH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Xu, Joseph G. Lambourne, Pradeep Kumar Jayaraman, Zhengqing Wang, Karl D. D. Willis, Yasutaka Furukawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents BrepGen, a diffusion-based generative approach that
directly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD)
model. BrepGen represents a B-rep model as a novel structured latent geometry
in a hierarchical tree. With the root node representing a whole CAD solid, each
element of a B-rep model (i.e., a face, an edge, or a vertex) progressively
turns into a child-node from top to bottom. B-rep geometry information goes
into the nodes as the global bounding box of each primitive along with a latent
code describing the local geometric shape. The B-rep topology information is
implicitly represented by node duplication. When two faces share an edge, the
edge curve will appear twice in the tree, and a T-junction vertex with three
incident edges appears six times in the tree with identical node features.
Starting from the root and progressing to the leaf, BrepGen employs
Transformer-based diffusion models to sequentially denoise node features while
duplicated nodes are detected and merged, recovering the B-Rep topology
information. Extensive experiments show that BrepGen advances the task of CAD
B-rep generation, surpassing existing methods on various benchmarks. Results on
our newly collected furniture dataset further showcase its exceptional
capability in generating complicated geometry. While previous methods were
limited to generating simple prismatic shapes, BrepGen incorporates free-form
and doubly-curved surfaces for the first time. Additional applications of
BrepGen include CAD autocomplete and design interpolation. The code, pretrained
models, and dataset are available at https://github.com/samxuxiang/BrepGen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM SIGGRAPH 2024. Code at
  https://github.com/samxuxiang/BrepGen</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-clustering for Federated Recommender System <span class="chip">WWW '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui He, Shuo Liu, Jackey Keung, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As data privacy and security attract increasing attention, Federated
Recommender System (FRS) offers a solution that strikes a balance between
providing high-quality recommendations and preserving user privacy. However,
the presence of statistical heterogeneity in FRS, commonly observed due to
personalized decision-making patterns, can pose challenges. To address this
issue and maximize the benefit of collaborative filtering (CF) in FRS, it is
intuitive to consider clustering clients (users) as well as items into
different groups and learning group-specific models. Existing methods either
resort to client clustering via user representations-risking privacy leakage,
or employ classical clustering strategies on item embeddings or gradients,
which we found are plagued by the curse of dimensionality. In this paper, we
delve into the inefficiencies of the K-Means method in client grouping,
attributing failures due to the high dimensionality as well as data sparsity
occurring in FRS, and propose CoFedRec, a novel Co-clustering Federated
Recommendation mechanism, to address clients heterogeneity and enhance the
collaborative filtering within the federated framework. Specifically, the
server initially formulates an item membership from the client-provided item
networks. Subsequently, clients are grouped regarding a specific item category
picked from the item membership during each communication round, resulting in
an intelligently aggregated group model. Meanwhile, to comprehensively capture
the global inter-relationships among items, we incorporate an additional
supervised contrastive learning term based on the server-side generated item
membership into the local training phase for each client. Extensive experiments
on four datasets are provided, which verify the effectiveness of the proposed
CoFedRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW '24: Proceedings of the ACM Web Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Communication Avoidance for Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lutfi Eren Erdogan, Vijay Anand Raghava Kanakagiri, Kurt Keutzer, Zhen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the major bottlenecks for efficient deployment of neural network based
recommendation systems is the memory footprint of their embedding tables.
Although many neural network based recommendation systems could benefit from
the faster on-chip memory access and increased computational power of hardware
accelerators, the large embedding tables in these models often cannot fit on
the constrained memory of accelerators. Despite the pervasiveness of these
models, prior methods in memory optimization and parallelism fail to address
the memory and communication costs of large embedding tables on accelerators.
As a result, the majority of models are trained on CPUs, while current
implementations of accelerators are hindered by issues such as bottlenecks in
inter-device communication and main memory lookups. In this paper, we propose a
theoretical framework that analyses the communication costs of arbitrary
distributed systems that use lookup tables. We use this framework to propose
algorithms that maximize throughput subject to memory, computation, and
communication constraints. Furthermore, we demonstrate that our method achieves
strong theoretical performance across dataset distributions and memory
constraints, applicable to a wide range of use cases from mobile federated
learning to warehouse-scale computation. We implement our framework and
algorithms in PyTorch and achieve up to 6x increases in training throughput on
GPU systems over baselines, on the Criteo Terabytes dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Graph Neural Network for Recommendation with Dynamic
  De-redundancy and Modality-Guided Feature De-noisy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Mo, Lin Xiao, Qiya Song, Xieping Gao, Eryao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have become crucial in multimodal recommendation
tasks because of their powerful ability to capture complex relationships
between neighboring nodes. However, increasing the number of propagation layers
in GNNs can lead to feature redundancy, which may negatively impact the overall
recommendation performance. In addition, the existing recommendation task
method directly maps the preprocessed multimodal features to the
low-dimensional space, which will bring the noise unrelated to user preference,
thus affecting the representation ability of the model. To tackle the
aforementioned challenges, we propose Multimodal Graph Neural Network for
Recommendation (MGNM) with Dynamic De-redundancy and Modality-Guided Feature
De-noisy, which is divided into local and global interaction. Initially, in the
local interaction process,we integrate a dynamic de-redundancy (DDR) loss
function which is achieved by utilizing the product of the feature coefficient
matrix and the feature matrix as a penalization factor. It reduces the feature
redundancy effects of multimodal and behavioral features caused by the stacking
of multiple GNN layers. Subsequently, in the global interaction process, we
developed modality-guided global feature purifiers for each modality to
alleviate the impact of modality noise. It is a two-fold guiding mechanism
eliminating modality features that are irrelevant to user preferences and
captures complex relationships within the modality. Experimental results
demonstrate that MGNM achieves superior performance on multimodal information
denoising and removal of redundant information compared to the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Robust Regularized Federated Recommendation <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langming Liu, Wanyu Wang, Xiangyu Zhao, Zijian Zhang, Chunxu Zhang, Shanru Lin, Yiqi Wang, Lixin Zou, Zitao Liu, Xuetao Wei, Hongzhi Yin, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play a pivotal role across practical scenarios,
showcasing remarkable capabilities in user preference modeling. However, the
centralized learning paradigm predominantly used raises serious privacy
concerns. The federated recommender system (FedRS) addresses this by updating
models on clients, while a central server orchestrates training without
accessing private data. Existing FedRS approaches, however, face unresolved
challenges, including non-convex optimization, vulnerability, potential privacy
leakage risk, and communication inefficiency. This paper addresses these
challenges by reformulating the federated recommendation problem as a convex
optimization issue, ensuring convergence to the global optimum. Based on this,
we devise a novel method, RFRec, to tackle this optimization problem
efficiently. In addition, we propose RFRecF, a highly efficient version that
incorporates non-uniform stochastic gradient descent to improve communication
efficiency. In user preference modeling, both methods learn local and global
models, collaboratively learning users' common and personalized interests under
the federated learning setting. Moreover, both methods significantly enhance
communication efficiency, robustness, and privacy protection, with theoretical
support. Comprehensive evaluations on four benchmark datasets demonstrate RFRec
and RFRecF's superior performance compared to diverse baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LinRec: Linear Attention Mechanism for Long-term Sequential Recommender
  Systems <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langming Liu, Xiangyu Zhao, Chi Zhang, Jingtong Gao, Wanyu Wang, Wenqi Fan, Yiqi Wang, Ming He, Zitao Liu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have achieved remarkable success in sequential recommender
systems (SRSs). However, computing the attention matrix in traditional
dot-product attention mechanisms results in a quadratic complexity with
sequence lengths, leading to high computational costs for long-term sequential
recommendation. Motivated by the above observation, we propose a novel
L2-Normalized Linear Attention for the Transformer-based Sequential Recommender
Systems (LinRec), which theoretically improves efficiency while preserving the
learning capabilities of the traditional dot-product attention. Specifically,
by thoroughly examining the equivalence conditions of efficient attention
mechanisms, we show that LinRec possesses linear complexity while preserving
the property of attention mechanisms. In addition, we reveal its latent
efficiency properties by interpreting the proposed LinRec mechanism through a
statistical lens. Extensive experiments are conducted based on two public
benchmark datasets, demonstrating that the combination of LinRec and
Transformer models achieves comparable or even superior performance than
state-of-the-art Transformer-based SRS models while significantly improving
time and memory efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential
  Recommendation <span class="chip">WSDM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Liu, Sixiao Zhang, Cheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation (SR) systems excel at capturing users' dynamic
preferences by leveraging their interaction histories. Most existing SR systems
assign a single embedding vector to each item to represent its features, and
various types of models are adopted to combine these item embeddings into a
sequence representation vector to capture the user intent. However, we argue
that this representation alone is insufficient to capture an item's
multi-faceted nature (e.g., movie genres, starring actors). Besides, users
often exhibit complex and varied preferences within these facets (e.g., liking
both action and musical films in the facet of genre), which are challenging to
fully represent. To address the issues above, we propose a novel structure
called Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential
Recommendation (FAME). We leverage sub-embeddings from each head in the last
multi-head attention layer to predict the next item separately. This approach
captures the potential multi-faceted nature of items without increasing model
complexity. A gating mechanism integrates recommendations from each head and
dynamically determines their importance. Furthermore, we introduce a
Mixture-of-Experts (MoE) network in each attention head to disentangle various
user preferences within each facet. Each expert within the MoE focuses on a
specific preference. A learnable router network is adopted to compute the
importance weight for each expert and aggregate them. We conduct extensive
experiments on four public sequential recommendation datasets and the results
demonstrate the effectiveness of our method over existing baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by WSDM'25. The final camera-ready
  version will be available soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Scaling Collaborative Filtering Optimization from the
  Perspective of Matrix Rank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donald Loveland, Xinyi Wu, Tong Zhao, Danai Koutra, Neil Shah, Mingxuan Ju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative Filtering (CF) methods dominate real-world recommender systems
given their ability to learn high-quality, sparse ID-embedding tables that
effectively capture user preferences. These tables scale linearly with the
number of users and items, and are trained to ensure high similarity between
embeddings of interacted user-item pairs, while maintaining low similarity for
non-interacted pairs. Despite their high performance, encouraging dispersion
for non-interacted pairs necessitates expensive regularization (e.g., negative
sampling), hurting runtime and scalability. Existing research tends to address
these challenges by simplifying the learning process, either by reducing model
complexity or sampling data, trading performance for runtime. In this work, we
move beyond model-level modifications and study the properties of the embedding
tables under different learning strategies. Through theoretical analysis, we
find that the singular values of the embedding tables are intrinsically linked
to different CF loss functions. These findings are empirically validated on
real-world datasets, demonstrating the practical benefits of higher stable
rank, a continuous version of matrix rank which encodes the distribution of
singular values. Based on these insights, we propose an efficient warm-start
strategy that regularizes the stable rank of the user and item embeddings. We
show that stable rank regularization during early training phases can promote
higher-quality embeddings, resulting in training speed improvements of up to
66%. Additionally, stable rank regularization can act as a proxy for negative
sampling, allowing for performance gains of up to 21% over loss functions with
small negative sampling ratios. Overall, our analysis unifies current CF
methods under a new perspective, their optimization of stable rank, motivating
a flexible regularization method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Little Giants: Synthesizing High-Quality Embedding Data at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation has become an increasingly popular way of training
models without the need for large, manually labeled datasets. For tasks like
text embedding, synthetic data offers diverse and scalable training examples,
significantly reducing the cost of human annotation. However, most current
approaches rely heavily on proprietary models like GPT-4, which are expensive
and inefficient for generating large-scale embedding data. In this paper, we
introduce SPEED, a framework that aligns open-source small models (8B) to
efficiently generate large-scale synthetic embedding data. Through supervised
fine-tuning, preference optimization, and self-improvement, SPEED enables small
open-source models to produce high-quality data. Remarkably, SPEED uses only
less than 1/10 of the GPT API calls, outperforming the state-of-the-art
embedding model E5_mistral when both are trained solely on their synthetic
data. Using this efficient generator, we conduct a comprehensive study on how
various factors within the alignment pipeline impact data quality and reveal
the scaling law for synthetic embedding data.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Graph Neural Network for Recommendation with Dynamic
  De-redundancy and Modality-Guided Feature De-noisy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Mo, Lin Xiao, Qiya Song, Xieping Gao, Eryao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have become crucial in multimodal recommendation
tasks because of their powerful ability to capture complex relationships
between neighboring nodes. However, increasing the number of propagation layers
in GNNs can lead to feature redundancy, which may negatively impact the overall
recommendation performance. In addition, the existing recommendation task
method directly maps the preprocessed multimodal features to the
low-dimensional space, which will bring the noise unrelated to user preference,
thus affecting the representation ability of the model. To tackle the
aforementioned challenges, we propose Multimodal Graph Neural Network for
Recommendation (MGNM) with Dynamic De-redundancy and Modality-Guided Feature
De-noisy, which is divided into local and global interaction. Initially, in the
local interaction process,we integrate a dynamic de-redundancy (DDR) loss
function which is achieved by utilizing the product of the feature coefficient
matrix and the feature matrix as a penalization factor. It reduces the feature
redundancy effects of multimodal and behavioral features caused by the stacking
of multiple GNN layers. Subsequently, in the global interaction process, we
developed modality-guided global feature purifiers for each modality to
alleviate the impact of modality noise. It is a two-fold guiding mechanism
eliminating modality features that are irrelevant to user preferences and
captures complex relationships within the modality. Experimental results
demonstrate that MGNM achieves superior performance on multimodal information
denoising and removal of redundant information compared to the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14468v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14468v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Cong Wei, Weiming Ren, Harry Yang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic field of digital content creation using generative models,
state-of-the-art video editing models still do not offer the level of quality
and control that users desire. Previous works on video editing either extended
from image-based generative models in a zero-shot manner or necessitated
extensive fine-tuning, which can hinder the production of fluid video edits.
Furthermore, these methods frequently rely on textual input as the editing
guidance, leading to ambiguities and limiting the types of edits they can
perform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free
paradigm designed to simplify video editing into two primary steps: (1)
employing an off-the-shelf image editing model to modify the first frame, (2)
utilizing an existing image-to-video generation model to generate the edited
video through temporal feature injection. AnyV2V can leverage any existing
image editing tools to support an extensive array of video editing tasks,
including prompt-based editing, reference-based style transfer, subject-driven
editing, and identity manipulation, which were unattainable by previous
methods. AnyV2V can also support any video length. Our evaluation shows that
AnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore,
AnyV2V significantly outperformed these baselines in human evaluations,
demonstrating notable improvements in visual consistency with the source video
while producing high-quality edits across all editing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR 2024)
  (11/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13743v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13743v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-visual models now produce photo-realistic images and videos,
they struggle with compositional text prompts involving attributes,
relationships, and higher-order reasoning such as logic and comparison. In this
work, we conduct an extensive human study on GenAI-Bench to evaluate the
performance of leading image and video generation models in various aspects of
compositional text-to-visual generation. We also compare automated evaluation
metrics against our collected human ratings and find that VQAScore -- a metric
measuring the likelihood that a VQA model views an image as accurately
depicting the prompt -- significantly outperforms previous metrics such as
CLIPScore. In addition, VQAScore can improve generation in a black-box manner
(without finetuning) via simply ranking a few (3 to 9) candidate images.
Ranking by VQAScore is 2x to 3x more effective than other scoring methods like
PickScore, HPSv2, and ImageReward at improving human alignment ratings for
DALL-E 3 and Stable Diffusion, especially on compositional prompts that require
advanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with
over 40,000 human ratings to evaluate scoring metrics on ranking images
generated from the same prompt. Lastly, we discuss promising areas for
improvement in VQAScore, such as addressing fine-grained visual details. We
will release all human ratings (over 80,000) to facilitate scientific
benchmarking of both generative models and automated metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We open-source our dataset, model, and code at:
  https://linzhiqiu.github.io/papers/genai_bench ; Project page:
  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first
  introduced in arxiv:2404.01291. This article extends it with an additional
  GenAI-Rank benchmark</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-02T00:00:00Z">2024-11-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence Driven Course Generation: A Case Study Using
  Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Djaber Rouabhia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores Artificial Intelligence use, specifically ChatGPT, in
creating educational content. The study aims to elaborate on using ChatGPT to
create course materials. The main objective is to assess the efficiency,
quality, and impact of AI-driven course generation, and to create a Multimedia
Databases course as a case study. The study highlights the potential of AI to
revolutionize educational content creation, making it more accessible,
personalized, and efficient. The course content was generated in less than one
day through iterative methods, using prompts for translation, content
expansion, practical examples, assignments, supplementary materials, and LaTeX
formatting. Each part was verified immediately after generation to ensure
accuracy. Post-generation analysis with Detectia and Turnitin showed similarity
rates of 8.7% and 13%, indicating high originality. Experts and university
committees reviewed and approved the course, with English university teachers
praising its language quality. ChatGPT also created a well-structured and
diversified exam for the module. Key findings reveal significant time
efficiency, comprehensive content coverage, and high flexibility. The study
underscores AI's transformative potential in education, addressing challenges
related to data privacy, technology dependence, content accuracy, and
algorithmic biases. The conclusions emphasize the need for collaboration
between educators, policymakers, and technology developers to harness AI's
benefits in education fully.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online and Offline Evaluations of Collaborative Filtering and Content
  Based Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Elahi, Armin Zirak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used AI applications designed to help users
efficiently discover relevant items. The effectiveness of such systems is tied
to the satisfaction of both users and providers. However, user satisfaction is
complex and cannot be easily framed mathematically using information retrieval
and accuracy metrics. While many studies evaluate accuracy through offline
tests, a growing number of researchers argue that online evaluation methods
such as A/B testing are better suited for this purpose. We have employed a
variety of algorithms on different types of datasets divergent in size and
subject, producing recommendations in various platforms, including media
streaming services, digital publishing websites, e-commerce systems, and news
broadcasting networks. Notably, our target websites and datasets are in Persian
(Farsi) language.
  This study provides a comparative analysis of a large-scale recommender
system that has been operating for the past year across about 70 websites in
Iran, processing roughly 300 requests per second collectively. The system
employs user-based and item-based recommendations using content-based,
collaborative filtering, trend-based methods, and hybrid approaches. Through
both offline and online evaluations, we aim to identify where these algorithms
perform most efficiently and determine the best method for our specific needs,
considering the dataset and system scale. Our methods of evaluation include
manual evaluation, offline tests including accuracy and ranking metrics like
hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR).
Additionally we analyzed and proposed methods to address cold-start and
popularity bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMREx: AMR for Explainable Fact Verification <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chathuri Jayaweera, Sangpil Youm, Bonnie Dorr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of social media networks and the vast amount of information
circulating through them, automatic fact verification is an essential component
to prevent the spread of misinformation. It is even more useful to have fact
verification systems that provide explanations along with their classifications
to ensure accurate predictions. To address both of these requirements, we
implement AMREx, an Abstract Meaning Representation (AMR)-based veracity
prediction and explanation system for fact verification using a combination of
Smatch, an AMR evaluation metric to measure meaning containment and textual
similarity, and demonstrate its effectiveness in producing partially
explainable justifications using two community standard fact verification
datasets, FEVER and AVeriTeC. AMREx surpasses the AVeriTec baseline accuracy
showing the effectiveness of our approach for real-world claim verification. It
follows an interpretable pipeline and returns an explainable AMR node mapping
to clarify the system's veracity predictions when applicable. We further
demonstrate that AMREx output can be used to prompt LLMs to generate
natural-language explanations using the AMR mappings as a guide to lessen the
probability of hallucinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This study implements, evaluates, and analyzes an Abstract Meaning
  Representation (AMR) based partially explainable system for fact
  verification/ veracity classification. Accepted by EMNLP Workshop on Fact
  Extraction and VERification (FEVER) 2024, 11 pages, 7 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Be like a Goldfish, Don't Memorize! Mitigating Memorization in
  Generative LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models can memorize and repeat their training data, causing
privacy and copyright risks. To mitigate memorization, we introduce a subtle
modification to the next-token training objective that we call the goldfish
loss. During training, randomly sampled subsets of tokens are excluded from the
loss computation. These dropped tokens are not memorized by the model, which
prevents verbatim reproduction of a complete chain of tokens from the training
set. We run extensive experiments training billion-scale Llama-2 models, both
pre-trained and trained from scratch, and demonstrate significant reductions in
extractable memorization with little to no impact on downstream benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, and 1 table in the main body. Code available at
  https://github.com/ahans30/goldfish-loss and checkpoints at
  https://huggingface.co/collections/tomg-group-umd/goldfish-loss-mitigating-memorization-in-llms-66c175becb6aab07744f7272</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Narrative Feature or Structured Feature? A Study of Large Language
  Models to Identify Cancer Patients at Risk of Heart Failure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11425v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11425v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chen, Mengyuan Zhang, Mustafa Mohammed Ahmed, Yi Guo, Thomas J. George, Jiang Bian, Yonghui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer treatments are known to introduce cardiotoxicity, negatively impacting
outcomes and survivorship. Identifying cancer patients at risk of heart failure
(HF) is critical to improving cancer treatment outcomes and safety. This study
examined machine learning (ML) models to identify cancer patients at risk of HF
using electronic health records (EHRs), including traditional ML, Time-Aware
long short-term memory (T-LSTM), and large language models (LLMs) using novel
narrative features derived from the structured medical codes. We identified a
cancer cohort of 12,806 patients from the University of Florida Health,
diagnosed with lung, breast, and colorectal cancers, among which 1,602
individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the
best F1 scores, outperforming the traditional support vector machines by 39%,
the T-LSTM deep learning model by 7%, and a widely used transformer model,
BERT, by 5.6%. The analysis shows that the proposed narrative features
remarkably increased feature density and improved performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CogDevelop2K: Reversed Cognitive Development in Multimodal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiang Li, Qingying Gao, Haoran Sun, Haiyun Lyu, Dezhi Luo, Hokin Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Are Multi-modal Large Language Models (MLLMs) stochastic parrots? Do they
genuinely understand? This paper aims to explore the core cognitive abilities
that human intelligence builds upon to perceive, comprehend, and reason in
MLLMs. To this end, we propose CogDevelop2K, a comprehensive benchmark that
spans 12 sub-concepts from primitive knowledge like object permanence and
boundary to more complex abilities like intentionality understanding,
structured via the developmental trajectory of a human mind. We evaluate 46
MLLMs on our benchmarks. Surprisingly, we observe a reversed cognitive
developmental trajectory compared to humans. Comprehensively, we further
evaluate the influence of evaluation strategies and prompting techniques.
Website with this $\href{https://growing-ai-like-a-child.github.io/}{link}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website with this
  $\href{https://growing-ai-like-a-child.github.io/}{link}$</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InversionView: A General-Purpose Method for Reading Information from
  Neural Activations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17653v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17653v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinting Huang, Madhur Panwar, Navin Goyal, Michael Hahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inner workings of neural networks can be better understood if we can
fully decipher the information encoded in neural activations. In this paper, we
argue that this information is embodied by the subset of inputs that give rise
to similar activations. We propose InversionView, which allows us to
practically inspect this subset by sampling from a trained decoder model
conditioned on activations. This helps uncover the information content of
activation vectors, and facilitates understanding of the algorithms implemented
by transformer models. We present four case studies where we investigate models
ranging from small transformers to GPT-2. In these studies, we show that
InversionView can reveal clear information contained in activations, including
basic information about tokens appearing in the context, as well as more
complex information, such as the count of certain tokens, their relative
positions, and abstract knowledge about the subject. We also provide causally
verified circuits to confirm the decoded information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; ICML 2024 Mechanistic Interpretability Workshop oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian scaling laws for in-context learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16531v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16531v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is a powerful technique for getting language models
to perform complex tasks with no training updates. Prior work has established
strong correlations between the number of in-context examples provided and the
accuracy of the model's predictions. In this paper, we seek to explain this
correlation by showing that ICL approximates a Bayesian learner. This
perspective gives rise to a family of novel Bayesian scaling laws for ICL. In
experiments with \mbox{GPT-2} models of different sizes, our scaling laws
exceed or match existing scaling laws in accuracy while also offering
interpretable terms for task priors, learning efficiency, and per-example
probabilities. To illustrate the analytic power that such interpretable scaling
laws provide, we report on controlled synthetic dataset experiments designed to
inform real-world studies of safety alignment. In our experimental protocol, we
use SFT to suppress an unwanted existing model capability and then use ICL to
try to bring that capability back (many-shot jailbreaking). We then experiment
on real-world instruction-tuned LLMs using capabilities benchmarks as well as a
new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws
accurately predict the conditions under which ICL will cause the suppressed
behavior to reemerge, which sheds light on the ineffectiveness of post-training
at increasing LLM safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages main text, 26 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Saeed, Elgizouli Mohamed, Mukhtar Mohamed, Shaina Raza, Shady Shehata, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are widely used but raise ethical concerns due
to embedded social biases. This study examines LLM biases against Arabs versus
Westerners across eight domains, including women's rights, terrorism, and
anti-Semitism and assesses model resistance to perpetuating these biases. To
this end, we create two datasets: one to evaluate LLM bias toward Arabs versus
Westerners and another to test model safety against prompts that exaggerate
negative traits ("jailbreaks"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA
3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases
displaying negative biases toward Arabs, with LlaMA 3.1-405B being the most
biased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being
an optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except
Claude exhibit attack success rates above 87% in three categories. We also find
Claude 3.5 Sonnet the safest, but it still displays biases in seven of eight
categories. Despite being an optimized version of GPT4, We find GPT-4o to be
more prone to biases and jailbreaks, suggesting optimization flaws. Our
findings underscore the pressing need for more robust bias mitigation
strategies and strengthened security measures in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative linguistics contribution to artificial intelligence: Where
  this contribution lies? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Q. Shormani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article aims to characterize Generative linguistics (GL) contribution to
artificial intelligence (AI), alluding to the debate among linguists and AI
scientists on whether linguistics belongs to humanities or science. In this
article, I will try not to be biased as a linguist, studying the phenomenon
from an independent scientific perspective. The article walks the
researcher/reader through the scientific theorems and rationales involved in AI
which belong from GL, specifically the Chomsky School. It, thus, provides good
evidence from syntax, semantics, language faculty, Universal Grammar,
computational system of human language, language acquisition, human brain,
programming languages (e.g. Python), Large Language Models, and unbiased AI
scientists that this contribution is huge, and that this contribution cannot be
denied. It concludes that however the huge GL contribution to AI, there are
still points of divergence including the nature and type of language input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A hybrid <span class="highlight-title">transformer</span> and attention based recurrent neural network for
  robust and interpretable sentiment analysis of tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00297v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00297v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abrar Jahin, Md Sakib Hossain Shovon, M. F. Mridha, Md Rashedul Islam, Yutaka Watanobe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis is crucial for understanding public opinion and consumer
behavior. Existing models face challenges with linguistic diversity,
generalizability, and explainability. We propose TRABSA, a hybrid framework
integrating transformer-based architectures, attention mechanisms, and BiLSTM
networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge
gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.
Augmenting datasets with tweets from 32 countries and US states, we compare six
word-embedding techniques and three lexicon-based labeling techniques,
selecting the best for optimal sentiment analysis. TRABSA outperforms
traditional ML and deep learning models with 94% accuracy and significant
precision, recall, and F1-score gains. Evaluation across diverse datasets
demonstrates consistent superiority and generalizability. SHAP and LIME
analyses enhance interpretability, improving confidence in predictions. Our
study facilitates pandemic resource management, aiding resource planning,
policy formation, and vaccination tactics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Judging the Judges: Evaluating Alignment and Vulnerabilities in
  LLMs-as-Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12624v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12624v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offering a promising solution to the scalability challenges associated with
human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an
approach to evaluating large language models (LLMs). However, there are still
many open questions about the strengths and weaknesses of this paradigm, and
what potential biases it may hold. In this paper, we present a comprehensive
study of the performance of various LLMs acting as judges, focusing on a clean
scenario in which inter-human agreement is high. Investigating thirteen judge
models of different model sizes and families, judging answers of nine different
'examtaker models' - both base and instruction-tuned - we find that only the
best (and largest) models achieve reasonable alignment with humans. However,
they are still quite far behind inter-human agreement and their assigned scores
may still differ with up to 5 points from human-assigned scores. In terms of
their ranking of the nine exam-taker models, instead, also smaller models and
even the lexical metric contains may provide a reasonable signal. Through error
analysis and other studies, we identify vulnerabilities in judge models, such
as their sensitivity to prompt complexity and length, and a tendency toward
leniency. The fact that even the best judges differ from humans in this
comparatively simple setup suggest that caution may be wise when using judges
in more complex setups. Lastly, our research rediscovers the importance of
using alignment metrics beyond simple percent alignment, showing that judges
with high percent agreement can still assign vastly different scores.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Channel Hypergraph Contrastive Learning for Matrix Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Changsheng Shui, Yanwei Yu, Chao Huang, Zhongying Zhao, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rating is a typical user explicit feedback that visually reflects how much a
user likes a related item. The (rating) matrix completion is essentially a
rating prediction process, which is also a significant problem in recommender
systems. Recently, graph neural networks (GNNs) have been widely used in matrix
completion, which captures users' preferences over items by formulating a
rating matrix as a bipartite graph. However, existing methods are susceptible
due to data sparsity and long-tail distribution in real-world scenarios.
Moreover, the messaging mechanism of GNNs makes it difficult to capture
high-order correlations and constraints between nodes, which are essentially
useful in recommendation tasks. To tackle these challenges, we propose a
Multi-Channel Hypergraph Contrastive Learning framework for matrix completion,
named MHCL. Specifically, MHCL adaptively learns hypergraph structures to
capture high-order correlations between nodes and jointly captures local and
global collaborative relationships through attention-based cross-view
aggregation. Additionally, to consider the magnitude and order information of
ratings, we treat different rating subgraphs as different channels, encourage
alignment between adjacent ratings, and further achieve the mutual enhancement
between different ratings through multi-channel cross-rating contrastive
learning. Extensive experiments on five public datasets demonstrate that the
proposed method significantly outperforms the current state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Financial Data and News Articles for Stock Price Movement
  Prediction Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Elahi, Fatemeh Taghvaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting financial markets and stock price movements requires analyzing a
company's performance, historic price movements, industry-specific events
alongside the influence of human factors such as social media and press
coverage. We assume that financial reports (such as income statements, balance
sheets, and cash flow statements), historical price data, and recent news
articles can collectively represent aforementioned factors. We combine
financial data in tabular format with textual news articles and employ
pre-trained Large Language Models (LLMs) to predict market movements. Recent
research in LLMs has demonstrated that they are able to perform both tabular
and text classification tasks, making them our primary model to classify the
multi-modal data. We utilize retrieval augmentation techniques to retrieve and
attach relevant chunks of news articles to financial metrics related to a
company and prompt the LLMs in zero, two, and four-shot settings. Our dataset
contains news articles collected from different sources, historic stock price,
and financial report data for 20 companies with the highest trading volume
across different industries in the stock market. We utilized recently released
language models for our LLM-based classifier, including GPT- 3 and 4, and
LLaMA- 2 and 3 models. We introduce an LLM-based classifier capable of
performing classification tasks using combination of tabular (structured) and
textual (unstructured) data. By using this model, we predicted the movement of
a given stock's price in our dataset with a weighted F1-score of 58.5% and
59.1% and Matthews Correlation Coefficient of 0.175 for both 3-month and
6-month periods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online and Offline Evaluations of Collaborative Filtering and Content
  Based Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Elahi, Armin Zirak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used AI applications designed to help users
efficiently discover relevant items. The effectiveness of such systems is tied
to the satisfaction of both users and providers. However, user satisfaction is
complex and cannot be easily framed mathematically using information retrieval
and accuracy metrics. While many studies evaluate accuracy through offline
tests, a growing number of researchers argue that online evaluation methods
such as A/B testing are better suited for this purpose. We have employed a
variety of algorithms on different types of datasets divergent in size and
subject, producing recommendations in various platforms, including media
streaming services, digital publishing websites, e-commerce systems, and news
broadcasting networks. Notably, our target websites and datasets are in Persian
(Farsi) language.
  This study provides a comparative analysis of a large-scale recommender
system that has been operating for the past year across about 70 websites in
Iran, processing roughly 300 requests per second collectively. The system
employs user-based and item-based recommendations using content-based,
collaborative filtering, trend-based methods, and hybrid approaches. Through
both offline and online evaluations, we aim to identify where these algorithms
perform most efficiently and determine the best method for our specific needs,
considering the dataset and system scale. Our methods of evaluation include
manual evaluation, offline tests including accuracy and ranking metrics like
hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR).
Additionally we analyzed and proposed methods to address cold-start and
popularity bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Knowledge Graph for Teaching Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleni Ilkou, Ernesto Jiménez-Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This poster paper describes the ongoing research project for the creation of
a use-case-driven Knowledge Graph resource tailored to the needs of teaching
education in Knowledge Graphs (KGs). We gather resources related to KG courses
from lectures offered by the Semantic Web community, with the help of the COST
Action Distributed Knowledge Graphs and the interest group on KGs at The Alan
Turing Institute. Our goal is to create a resource-focused KG with multiple
interconnected semantic layers that interlink topics, courses, and materials
with each lecturer. Our approach formulates a domain KG in teaching and relates
it with multiple Personal KGs created for the lecturers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Cross-Correlated Network for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Yuanchen Bei, Wenbing Huang, Shengyuan Chen, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering (CF) models have demonstrated remarkable performance
in recommender systems, which represent users and items as embedding vectors.
Recently, due to the powerful modeling capability of graph neural networks for
user-item interaction graphs, graph-based CF models have gained increasing
attention. They encode each user/item and its subgraph into a single super
vector by combining graph embeddings after each graph convolution. However,
each hop of the neighbor in the user-item subgraphs carries a specific semantic
meaning. Encoding all subgraph information into single vectors and inferring
user-item relations with dot products can weaken the semantic information
between user and item subgraphs, thus leaving untapped potential. Exploiting
this untapped potential provides insight into improving performance for
existing recommendation models. To this end, we propose the Graph
Cross-correlated Network for Recommendation (GCR), which serves as a general
recommendation paradigm that explicitly considers correlations between
user/item subgraphs. GCR first introduces the Plain Graph Representation (PGR)
to extract information directly from each hop of neighbors into corresponding
PGR vectors. Then, GCR develops Cross-Correlated Aggregation (CCA) to construct
possible cross-correlated terms between PGR vectors of user/item subgraphs.
Finally, GCR comprehensively incorporates the cross-correlated terms for
recommendations. Experimental results show that GCR outperforms
state-of-the-art models on both interaction prediction and click-through rate
prediction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, accepted by TKDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM4PR: Improving Post-Ranking in Search Engine with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yan, Yihao Wang, Chi Zhang, Wenyuan Hou, Kang Pan, Xingkai Ren, Zelun Wu, Zhixin Zhai, Enyun Yu, Wenwu Ou, Yang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alongside the rapid development of Large Language Models (LLMs), there has
been a notable increase in efforts to integrate LLM techniques in information
retrieval (IR) and search engines (SE). Recently, an additional post-ranking
stage is suggested in SE to enhance user satisfaction in practical
applications. Nevertheless, research dedicated to enhancing the post-ranking
stage through LLMs remains largely unexplored. In this study, we introduce a
novel paradigm named Large Language Models for Post-Ranking in search engine
(LLM4PR), which leverages the capabilities of LLMs to accomplish the
post-ranking task in SE. Concretely, a Query-Instructed Adapter (QIA) module is
designed to derive the user/item representation vectors by incorporating their
heterogeneous features. A feature adaptation step is further introduced to
align the semantics of user/item representations with the LLM. Finally, the
LLM4PR integrates a learning to post-rank step, leveraging both a main task and
an auxiliary task to fine-tune the model to adapt the post-ranking task.
Experiment studies demonstrate that the proposed framework leads to significant
improvements and exhibits state-of-the-art performance compared with other
alternatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music Foundation Model as Generic Booster for Music Downstream Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        WeiHsiang Liao, Yuhta Takida, Yukara Ikemiya, Zhi Zhong, Chieh-Hsin Lai, Giorgio Fabbro, Kazuki Shimada, Keisuke Toyama, Kinwai Cheuk, Marco Martinez, Shusuke Takahashi, Stefan Uhlich, Taketo Akama, Woosung Choi, Yuichiro Koyama, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate the efficacy of using intermediate representations from a
single foundation model to enhance various music downstream tasks. We introduce
SoniDo , a music foundation model (MFM) designed to extract hierarchical
features from target music samples. By leveraging hierarchical intermediate
features, SoniDo constrains the information granularity, leading to improved
performance across various downstream tasks including both understanding and
generative tasks. We specifically evaluated this approach on representative
tasks such as music tagging, music transcription, music source separation, and
music mixing. Our results reveal that the features extracted from foundation
models provide valuable enhancements in training downstream task models. This
highlights the capability of using features extracted from music foundation
models as a booster for downstream tasks. Our approach not only benefits
existing task-specific models but also supports music downstream tasks
constrained by data scarcity. This paves the way for more effective and
accessible music processing solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages with 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting the Geolocation of Tweets Using <span class="highlight-title">transformer</span> models on
  Customized Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07865v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07865v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kateryna Lutsai, Christoph H. Lampert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research is aimed to solve the tweet/user geolocation prediction task
and provide a flexible methodology for the geotagging of textual big data. The
suggested approach implements neural networks for natural language processing
(NLP) to estimate the location as coordinate pairs (longitude, latitude) and
two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models
has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder
Representations from Transformers (BERT) as base models. Performance metrics
show a median error of fewer than 30 km on a worldwide-level, and fewer than 15
km on the US-level datasets for the models trained and evaluated on text
features of tweets' content and metadata context. Our source code and data are
available at https://github.com/K4TEL/geo-twitter.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 5 tables, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Improving Adversarial Collaborative Filtering for
  Robust Recommendation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Collaborative Filtering (ACF), which typically applies
adversarial perturbations at user and item embeddings through adversarial
training, is widely recognized as an effective strategy for enhancing the
robustness of Collaborative Filtering (CF) recommender systems against
poisoning attacks. Besides, numerous studies have empirically shown that ACF
can also improve recommendation performance compared to traditional CF. Despite
these empirical successes, the theoretical understanding of ACF's effectiveness
in terms of both performance and robustness remains unclear. To bridge this
gap, in this paper, we first theoretically show that ACF can achieve a lower
recommendation error compared to traditional CF with the same training epochs
in both clean and poisoned data contexts. Furthermore, by establishing bounds
for reductions in recommendation error during ACF's optimization process, we
find that applying personalized magnitudes of perturbation for different users
based on their embedding scales can further improve ACF's effectiveness.
Building on these theoretical understandings, we propose Personalized Magnitude
Adversarial Collaborative Filtering (PamaCF). Extensive experiments demonstrate
that PamaCF effectively defends against various types of poisoning attacks
while significantly enhancing recommendation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-stage Conformal Risk Control with Application to Ranked Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Xu, Mufang Ying, Wenge Guo, Zhi Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many practical machine learning systems, such as ranking and recommendation
systems, consist of two concatenated stages: retrieval and ranking. These
systems present significant challenges in accurately assessing and managing the
uncertainty inherent in their predictions. To address these challenges, we
extend the recently developed framework of conformal risk control, originally
designed for single-stage problems, to accommodate the more complex two-stage
setup. We first demonstrate that a straightforward application of conformal
risk control, treating each stage independently, may fail to maintain risk at
their pre-specified levels. Therefore, we propose an integrated approach that
considers both stages simultaneously, devising algorithms to control the risk
of each stage by jointly identifying thresholds for both stages. Our algorithm
further optimizes for a weighted combination of prediction set sizes across all
feasible thresholds, resulting in more effective prediction sets. Finally, we
apply the proposed method to the critical task of two-stage ranked retrieval.
We validate the efficacy of our method through extensive experiments on two
large-scale public datasets, MSLR-WEB and MS MARCO, commonly used for ranked
retrieval tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures; 5 supplementary pages, 3 supplementary figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative Analysis of Modality Fusion Approaches for Audio-Visual
  Person Identification and Verification <span class="chip">SP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aref Farhadipour, Masoumeh Chapariniya, Teodora Vukovic, Volker Dellwo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning involves integrating information from various modalities
to enhance learning and comprehension. We compare three modality fusion
strategies in person identification and verification by processing two
modalities: voice and face. In this paper, a one-dimensional convolutional
neural network is employed for x-vector extraction from voice, while the
pre-trained VGGFace2 network and transfer learning are utilized for face
modality. In addition, gammatonegram is used as speech representation in
engagement with the Darknet19 pre-trained network. The proposed systems are
evaluated using the K-fold cross-validation technique on the 118 speakers of
the test set of the VoxCeleb2 dataset. The comparative evaluations are done for
single-modality and three proposed multimodal strategies in equal situations.
Results demonstrate that the feature fusion strategy of gammatonegram and
facial features achieves the highest performance, with an accuracy of 98.37% in
the person identification task. However, concatenating facial features with the
x-vector reaches 0.62% for EER in verification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted at the ICNLSP2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Speech Emotion Recognition via Feature Distribution
  Adaptation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22023v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22023v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Li, Yixuan Ji, Peng Song, Haoqin Sun, Wenming Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel deep inductive transfer learning framework,
named feature distribution adaptation network, to tackle the challenging
multi-modal speech emotion recognition problem. Our method aims to use deep
transfer learning strategies to align visual and audio feature distributions to
obtain consistent representation of emotion, thereby improving the performance
of speech emotion recognition. In our model, the pre-trained ResNet-34 is
utilized for feature extraction for facial expression images and acoustic Mel
spectrograms, respectively. Then, the cross-attention mechanism is introduced
to model the intrinsic similarity relationships of multi-modal features.
Finally, the multi-modal feature distribution adaptation is performed
efficiently with feed-forward network, which is extended using the local
maximum mean discrepancy loss. Experiments are carried out on two benchmark
datasets, and the results demonstrate that our model can achieve excellent
performance compared with existing ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18709v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18709v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohao Guo, Xianghua Ying, Yaru Chen, Dantong Niu, Guangyao Li, Liao Qu, Yanyu Qi, Jinxing Zhou, Bowei Xing, Wenzhen Yue, Ji Shi, Qixun Wang, Peiliang Zhang, Buwen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new multi-modal task, termed audio-visual
instance segmentation (AVIS), which aims to simultaneously identify, segment
and track individual sounding object instances in audible videos. To facilitate
this research, we introduce a high-quality benchmark named AVISeg, containing
over 90K instance masks from 26 semantic categories in 926 long videos.
Additionally, we propose a strong baseline model for this task. Our model first
localizes sound source within each frame, and condenses object-specific
contexts into concise tokens. Then it builds long-range audio-visual
dependencies between these tokens using window-based attention, and tracks
sounding objects among the entire video sequences. Extensive experiments reveal
that our method performs best on AVISeg, surpassing the existing methods from
related tasks. We further conduct the evaluation on several multi-modal large
models; however, they exhibits subpar performance on instance-level sound
source localization and temporal perception. We expect that AVIS will inspire
the community towards a more comprehensive multi-modal understanding. The
dataset and code will soon be released on https://github.com/ruohaoguo/avis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/ruohaoguo/avis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with
  Instruction Tuning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Jingdong Sun, Kai Wang, Yuxiang Lin, Zheng Lian, Xiaojiang Peng, Alexander Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate emotion perception is crucial for various applications, including
human-computer interaction, education, and counseling. However, traditional
single-modality approaches often fail to capture the complexity of real-world
emotional expressions, which are inherently multimodal. Moreover, existing
Multimodal Large Language Models (MLLMs) face challenges in integrating audio
and recognizing subtle facial micro-expressions. To address this, we introduce
the MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained
annotated samples across diverse emotional categories. This dataset enables
models to learn from varied scenarios and generalize to real-world
applications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly
integrates audio, visual, and textual inputs through emotion-specific encoders.
By aligning features into a shared space and employing a modified LLaMA model
with instruction tuning, Emotion-LLaMA significantly enhances both emotional
recognition and reasoning capabilities. Extensive evaluations show
Emotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap
(7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI
challenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations
on DFEW dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024. 49 pages, 13 figures, Project:
  https://github.com/ZebangCheng/Emotion-LLaMA, Demo:
  https://huggingface.co/spaces/ZebangCheng/Emotion-LLaMA</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-01T00:00:00Z">2024-11-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Question Answering Precision with Optimized Vector Retrieval
  and Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixiao Yang, Mengyang Xu, Weimao Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question-answering (QA) is an important application of Information Retrieval
(IR) and language models, and the latest trend is toward pre-trained large
neural networks with embedding parameters. Augmenting QA performances with
these LLMs requires intensive computational resources for fine-tuning. We
propose an innovative approach to improve QA task performances by integrating
optimized vector retrievals and instruction methodologies. Based on retrieval
augmentation, the process involves document embedding, vector retrieval, and
context construction for optimal QA results. We experiment with different
combinations of text segmentation techniques and similarity functions, and
analyze their impacts on QA performances. Results show that the model with a
small chunk size of 100 without any overlap of the chunks achieves the best
result and outperforms the models based on semantic segmentation using
sentences. We discuss related QA examples and offer insight into how model
performances are improved within the two-stage framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORAG: A Cost-Constrained Retrieval Optimization System for
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziting Wang, Haitao Yuan, Wei Dong, Gao Cong, Feifei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable generation
capabilities but often struggle to access up-to-date information, which can
lead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this
issue by incorporating knowledge from external databases, enabling more
accurate and relevant responses. Due to the context window constraints of LLMs,
it is impractical to input the entire external database context directly into
the model. Instead, only the most relevant information, referred to as chunks,
is selectively retrieved. However, current RAG research faces three key
challenges. First, existing solutions often select each chunk independently,
overlooking potential correlations among them. Second, in practice the utility
of chunks is non-monotonic, meaning that adding more chunks can decrease
overall utility. Traditional methods emphasize maximizing the number of
included chunks, which can inadvertently compromise performance. Third, each
type of user query possesses unique characteristics that require tailored
handling, an aspect that current approaches do not fully consider. To overcome
these challenges, we propose a cost constrained retrieval optimization system
CORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search
(MCTS) based policy framework to find optimal chunk combinations sequentially,
allowing for a comprehensive consideration of correlations among chunks.
Additionally, rather than viewing budget exhaustion as a termination condition,
we integrate budget constraints into the optimization of chunk combinations,
effectively addressing the non-monotonicity of chunk utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A graph-based approach to extracting narrative signals from public
  discourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Pournaki, Tom Willaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narratives are key interpretative devices by which humans make sense of
political reality. As the significance of narratives for understanding current
societal issues such as polarization and misinformation becomes increasingly
evident, there is a growing demand for methods that support their empirical
analysis. To this end, we propose a graph-based formalism and machine-guided
method for extracting, representing, and analyzing selected narrative signals
from digital textual corpora, based on Abstract Meaning Representation (AMR).
The formalism and method introduced here specifically cater to the study of
political narratives that figure in texts from digital media such as archived
political speeches, social media posts, political manifestos and transcripts of
parliamentary debates. We conceptualize these political narratives as a type of
ontological narratives: stories by which actors position themselves as
political beings, and which are akin to political worldviews in which actors
present their normative vision of the world, or aspects thereof. We approach
the study of such political narratives as a problem of information retrieval:
starting from a textual corpus, we first extract a graph-like representation of
the meaning of each sentence in the corpus using AMR. Drawing on transferable
concepts from narratology, we then apply a set of heuristics to filter these
graphs for representations of 1) actors, 2) the events in which these actors
figure, and 3) traces of the perspectivization of these events. We approach
these references to actors, events, and instances of perspectivization as core
narrative signals that initiate a further analysis by alluding to larger
political narratives. By means of a case study of State of the European Union
addresses, we demonstrate how the formalism can be used to inductively surface
signals of political narratives from public discourse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Sense of Metadata Mess: Alignment & Risk Assessment for Diatom
  Data Use Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kio Polson, Marina Potapova, Uttam Meena, Chad Peiper, Joshua Brown, Joshua Agar, Jane Greenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biologists study Diatoms, a fundamental algae, to assess the health of
aquatic systems. Diatom specimens have traditionally been preserved on analog
slides, where a single slide can contain thousands of these microscopic
organisms. Digitization of these collections presents both metadata challenges
and opportunities. This paper reports on metadata research aimed at providing
access to a digital portion of the Academy of Natural Sciences' Diatom
Herbarium, Drexel University. We report results of a 3-part study covering 1) a
review of relevant metadata standards and a microscopy metadata framework
shared by Hammer et al., 2) a baseline metadata alignment mapping current
diatom metadata properties to standard metadata types, and 3) a metadata risk
analysis associated with the course of standard data curation practices. This
research is part of an effort involving the transfer of these digital slides to
an new system, DataFed, to support global accessible. The final section of this
paper includes a conclusion and discusses next steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures, 1 table, to be published in MTSR 2024 conference
  proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Semantic Interoperability Across Materials Science With
  HIVE4MAT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane Greenberg, Kio Polson, Scott McClellan, Xintong Zhao, Alex Kalinowski, Yuan An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  HIVE4MAT is a linked data interactive application for navigating ontologies
of value to materials science. HIVE enables automatic indexing of textual
resources with standardized terminology. This article presents the motivation
underlying HIVE4MAT, explains the system architecture, reports on two
evaluations, and discusses future plans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figures, 3 tables, to be published in SeMatS 2024
  workshop proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-KT: A Versatile Framework for Knowledge Transfer from Large Language
  Models to Collaborative Filtering <span class="chip">ICDM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Severin, Aleksei Ziablitsev, Yulia Savelyeva, Valeriy Tashchilin, Ivan Bulychev, Mikhail Yushkov, Artem Kushneruk, Amaliya Zaryvnykh, Dmitrii Kiselev, Andrey Savchenko, Ilya Makarov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LLM-KT, a flexible framework designed to enhance collaborative
filtering (CF) models by seamlessly integrating LLM (Large Language
Model)-generated features. Unlike existing methods that rely on passing
LLM-generated features as direct inputs, our framework injects these features
into an intermediate layer of any CF model, allowing the model to reconstruct
and leverage the embeddings internally. This model-agnostic approach works with
a wide range of CF models without requiring architectural changes, making it
adaptable to various recommendation scenarios. Our framework is built for easy
integration and modification, providing researchers and developers with a
powerful tool for extending CF model capabilities through efficient knowledge
transfer. We demonstrate its effectiveness through experiments on the MovieLens
and Amazon datasets, where it consistently improves baseline CF models.
Experimental studies showed that LLM-KT is competitive with the
state-of-the-art methods in context-aware settings but can be applied to a
broader range of CF models than current approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ICDM 2024 (demo track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRFLEX: Music Information Retrieval Feature Library for Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuradha Chopra, Abhinaba Roy, Dorien Herremans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an extendable modular system that compiles a range of
music feature extraction models to aid music information retrieval research.
The features include musical elements like key, downbeats, and genre, as well
as audio characteristics like instrument recognition, vocals/instrumental
classification, and vocals gender detection. The integrated models are
state-of-the-art or latest open-source. The features can be extracted as latent
or post-processed labels, enabling integration into music applications such as
generative music, recommendation, and playlist generation. The modular design
allows easy integration of newly developed systems, making it a good
benchmarking and comparison tool. This versatile toolkit supports the research
community in developing innovative solutions by providing concrete musical
features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, 4 tables, submitted to Extended Abstracts for the
  Late-Breaking Demo Session of the 25th Int. Society for Music Information
  Retrieval Conf., San Francisco, United States, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Few-Shot Cross-Domain Named Entity Recognition by Instruction
  Tuning a Word-Embedding based Retrieval Augmented Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhadip Nandi, Neeraj Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Cross-Domain NER is the process of leveraging knowledge from
data-rich source domains to perform entity recognition on data scarce target
domains. Most previous state-of-the-art (SOTA) approaches use pre-trained
language models (PLMs) for cross-domain NER. However, these models are often
domain specific. To successfully use these models for new target domains, we
need to modify either the model architecture or perform model finetuning using
data from the new domains. Both of these result in the creation of entirely new
NER models for each target domain which is infeasible for practical scenarios.
Recently,several works have attempted to use LLMs to solve Few-Shot
Cross-Domain NER. However, most of these are either too expensive for practical
purposes or struggle to follow LLM prompt instructions. In this paper, we
propose IF-WRANER (Instruction Finetuned Word-embedding based Retrieval
Augmented large language model for Named Entity Recognition), a retrieval
augmented LLM, finetuned for the NER task. By virtue of the regularization
techniques used during LLM finetuning and the adoption of word-level embedding
over sentence-level embedding during the retrieval of in-prompt examples,
IF-WRANER is able to outperform previous SOTA Few-Shot Cross-Domain NER
approaches. We have demonstrated the effectiveness of our model by benchmarking
its performance on the open source CrossNER dataset, on which it shows more
than 2% F1 score improvement over the previous SOTA model. We have deployed the
model for multiple customer care domains of an enterprise. Accurate entity
prediction through IF-WRANER helps direct customers to automated workflows for
the domains, thereby reducing escalations to human agents by almost 15% and
leading to millions of dollars in yearly savings for the company.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DivNet: Diversity-Aware Self-Correcting Sequential Recommendation
  Networks <span class="chip">CIKM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Xiao, Zaifan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the last stage of a typical \textit{recommendation system},
\textit{collective recommendation} aims to give the final touches to the
recommended items and their layout so as to optimize overall objectives such as
diversity and whole-page relevance. In practice, however, the interaction
dynamics among the recommended items, their visual appearances and meta-data
such as specifications are often too complex to be captured by experts'
heuristics or simple models. To address this issue, we propose a
\textit{\underline{div}ersity-aware self-correcting sequential recommendation
\underline{net}works} (\textit{DivNet}) that is able to estimate utility by
capturing the complex interactions among sequential items and diversify
recommendations simultaneously. Experiments on both offline and online settings
demonstrate that \textit{DivNet} can achieve better results compared to
baselines with or without collective recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CIKM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Bundle Recommendation: Methods, Applications, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Sun, Lin Li, Ming Li, Xiaohui Tao, Dong Zhang, Peipei Wang, Jimmy Xiangji Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, bundle recommendation systems have gained significant
attention in both academia and industry due to their ability to enhance user
experience and increase sales by recommending a set of items as a bundle rather
than individual items. This survey provides a comprehensive review on bundle
recommendation, beginning by a taxonomy for exploring product bundling. We
classify it into two categories based on bundling strategy from various
application domains, i.e., discriminative and generative bundle recommendation.
Then we formulate the corresponding tasks of the two categories and
systematically review their methods: 1) representation learning from bundle and
item levels and interaction modeling for discriminative bundle recommendation;
2) representation learning from item level and bundle generation for generative
bundle recommendation. Subsequently, we survey the resources of bundle
recommendation including datasets and evaluation metrics, and conduct
reproducibility experiments on mainstream models. Lastly, we discuss the main
challenges and highlight the promising future directions in the field of bundle
recommendation, aiming to serve as a useful resource for researchers and
practitioners. Our code and datasets are publicly available at
https://github.com/WUT-IDEA/bundle-recommendation-survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Utility: Evaluating LLM as Recommender 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chumeng Jiang, Jiayin Wang, Weizhi Ma, Charles L. A. Clarke, Shuai Wang, Chuhan Wu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Large Language Models (LLMs), recent studies
employed LLMs as recommenders to provide personalized information services for
distinct users. Despite efforts to improve the accuracy of LLM-based
recommendation models, relatively little attention is paid to beyond-utility
dimensions. Moreover, there are unique evaluation aspects of LLM-based
recommendation models, which have been largely ignored. To bridge this gap, we
explore four new evaluation dimensions and propose a multidimensional
evaluation framework. The new evaluation dimensions include: 1) history length
sensitivity, 2) candidate position bias, 3) generation-involved performance,
and 4) hallucinations. All four dimensions have the potential to impact
performance, but are largely unnecessary for consideration in traditional
systems. Using this multidimensional evaluation framework, along with
traditional aspects, we evaluate the performance of seven LLM-based
recommenders, with three prompting strategies, comparing them with six
traditional models on both ranking and re-ranking tasks on four datasets. We
find that LLMs excel at handling tasks with prior knowledge and shorter input
histories in the ranking setting, and perform better in the re-ranking setting,
beating traditional models across multiple dimensions. However, LLMs exhibit
substantial candidate position bias issues, and some models hallucinate
non-existent items much more often than others. We intend our evaluation
framework and observations to benefit future research on the use of LLMs as
recommenders. The code and data are available at
https://github.com/JiangDeccc/EvaLLMasRecommender.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Musical Instrument Classification with Advanced Machine
  Learning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanikij Chulev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Musical instrument classification, a key area in Music Information Retrieval,
has gained considerable interest due to its applications in education, digital
music production, and consumer media. Recent advances in machine learning,
specifically deep learning, have enhanced the capability to identify and
classify musical instruments from audio signals. This study applies various
machine learning methods, including Naive Bayes, Support Vector Machines,
Random Forests, Boosting techniques like AdaBoost and XGBoost, as well as deep
learning models such as Convolutional Neural Networks and Artificial Neural
Networks. The effectiveness of these methods is evaluated on the NSynth
dataset, a large repository of annotated musical sounds. By comparing these
approaches, the analysis aims to showcase the advantages and limitations of
each method, providing guidance for developing more accurate and efficient
classification systems. Additionally, hybrid model testing and discussion are
included. This research aims to support further studies in instrument
classification by proposing new approaches and future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 35 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Confidence Evaluation Measures in Zero-Shot CSS Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Farr, Iain Cruickshank, Nico Manzonelli, Nicholas Clark, Kate Starbird, Jevin West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing classification confidence is critical for leveraging large language
models (LLMs) in automated labeling tasks, especially in the sensitive domains
presented by Computational Social Science (CSS) tasks. In this paper, we make
three key contributions: (1) we propose an uncertainty quantification (UQ)
performance measure tailored for data annotation tasks, (2) we compare, for the
first time, five different UQ strategies across three distinct LLMs and CSS
data annotation tasks, (3) we introduce a novel UQ aggregation strategy that
effectively identifies low-confidence LLM annotations and disproportionately
uncovers data incorrectly labeled by the LLMs. Our results demonstrate that our
proposed UQ aggregation strategy improves upon existing methods andcan be used
to significantly improve human-in-the-loop data annotation processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific
  Domain through Complementary Granularity <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show the growing significance of document retrieval in the
generation of LLMs, i.e., RAG, within the scientific domain by bridging their
knowledge gap. However, dense retrievers often struggle with domain-specific
retrieval and complex query-document relationships, particularly when query
segments correspond to various parts of a document. To alleviate such prevalent
challenges, this paper introduces $\texttt{MixGR}$, which improves dense
retrievers' awareness of query-document matching across various levels of
granularity in queries and documents using a zero-shot approach.
$\texttt{MixGR}$ fuses various metrics based on these granularities to a united
score that reflects a comprehensive query-document similarity. Our experiments
demonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by
24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based
retrievers, respectively, averaged on queries containing multiple subqueries
from five scientific retrieval datasets. Moreover, the efficacy of two
downstream scientific question-answering tasks highlights the advantage of
$\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The
code and experimental datasets are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Customizing Language Models with Instance-wise LoRA for Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems predict the next interaction item based on
users' past interactions, aligning recommendations with individual preferences.
Leveraging the strengths of Large Language Models (LLMs) in knowledge
comprehension and reasoning, recent approaches are eager to apply LLMs to
sequential recommendation. A common paradigm is converting user behavior
sequences into instruction data, and fine-tuning the LLM with
parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).
However, the uniform application of LoRA across diverse user behaviors is
insufficient to capture individual variability, resulting in negative transfer
between disparate sequences. To address these challenges, we propose
Instance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation
task as a form of multi-task learning, integrating LoRA with the Mixture of
Experts (MoE) framework. This approach encourages different experts to capture
various aspects of user behavior. Additionally, we introduce a sequence
representation guided gate function that generates customized expert
participation weights for each user sequence, which allows dynamic parameter
adjustment for instance-wise recommendations. In sequential recommendation,
iLoRA achieves an average relative improvement of 11.4\% over basic LoRA in the
hit ratio metric, with less than a 1\% relative increase in trainable
parameters. Extensive experiments on three benchmark datasets demonstrate the
effectiveness of iLoRA, highlighting its superior performance compared to
existing methods in mitigating negative transfer and improving recommendation
accuracy. Our data and code are available at
https://github.com/AkaliKong/iLoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Liu, Xian Wu, Yejing Wang, Zijian Zhang, Feng Tian, Yefeng Zheng, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems (SRS) aim to predict users' subsequent choices
based on their historical interactions and have found applications in diverse
fields such as e-commerce and social media. However, in real-world systems,
most users interact with only a handful of items, while the majority of items
are seldom consumed. These two issues, known as the long-tail user and
long-tail item challenges, often pose difficulties for existing SRS. These
challenges can adversely affect user experience and seller benefits, making
them crucial to address. Though a few works have addressed the challenges, they
still struggle with the seesaw or noisy issues due to the intrinsic scarcity of
interactions. The advancements in large language models (LLMs) present a
promising solution to these problems from a semantic perspective. As one of the
pioneers in this field, we propose the Large Language Models Enhancement
framework for Sequential Recommendation (LLM-ESR). This framework utilizes
semantic embeddings derived from LLMs to enhance SRS without adding extra
inference load from LLMs. To address the long-tail item challenge, we design a
dual-view modeling framework that combines semantics from LLMs and
collaborative signals from conventional SRS. For the long-tail user challenge,
we propose a retrieval augmented self-distillation method to enhance user
preference representation using more informative interactions from similar
users. To verify the effectiveness and versatility of our proposed enhancement
framework, we conduct extensive experiments on three real-world datasets using
three popular SRS models. The results show that our method surpasses existing
baselines consistently, and benefits long-tail users and items especially. The
implementation code is available at
https://github.com/Applied-Machine-Learning-Lab/LLM-ESR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeruIPS'24 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MACRec: a Multi-Agent Collaboration Framework for Recommendation <span class="chip">SIGIR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15235v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15235v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefan Wang, Yuanqing Yu, Wendi Zheng, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based agents have gained considerable attention for their decision-making
skills and ability to handle complex tasks. Recognizing the current gap in
leveraging agent capabilities for multi-agent collaboration in recommendation
systems, we introduce MACRec, a novel framework designed to enhance
recommendation systems through multi-agent collaboration. Unlike existing work
on using agents for user/item simulation, we aim to deploy multi-agents to
tackle recommendation tasks directly. In our framework, recommendation tasks
are addressed through the collaborative efforts of various specialized agents,
including Manager, User/Item Analyst, Reflector, Searcher, and Task
Interpreter, with different working flows. Furthermore, we provide application
examples of how developers can easily use MACRec on various recommendation
tasks, including rating prediction, sequential recommendation, conversational
recommendation, and explanation generation of recommendation results. The
framework and demonstration video are publicly available at
https://github.com/wzf2000/MACRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling User Satisfaction and Creator Productivity Trade-Offs in
  Recommendation Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yao, Yiming Liao, Jingzhou Liu, Shaoliang Nie, Qifan Wang, Haifeng Xu, Hongning Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On User-Generated Content (UGC) platforms, recommendation algorithms
significantly impact creators' motivation to produce content as they compete
for algorithmically allocated user traffic. This phenomenon subtly shapes the
volume and diversity of the content pool, which is crucial for the platform's
sustainability. In this work, we demonstrate, both theoretically and
empirically, that a purely relevance-driven policy with low exploration
strength boosts short-term user satisfaction but undermines the long-term
richness of the content pool. In contrast, a more aggressive exploration policy
may slightly compromise user satisfaction but promote higher content creation
volume. Our findings reveal a fundamental trade-off between immediate user
satisfaction and overall content production on UGC platforms. Building on this
finding, we propose an efficient optimization method to identify the optimal
exploration strength, balancing user and creator engagement. Our model can
serve as a pre-deployment audit tool for recommendation algorithms on UGC
platforms, helping to align their immediate objectives with sustainable,
long-term goals.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Generative and Discriminative Training for Multi-modal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chow, Juncheng Li, Qifan Yu, Kaihang Pan, Hao Fei, Zhiqi Ge, Shuai Yang, Siliang Tang, Hanwang Zhang, Qianru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, Vision-Language Models (VLMs) have been trained under two
predominant paradigms. Generative training has enabled Multimodal Large
Language Models (MLLMs) to tackle various complex tasks, yet issues such as
hallucinations and weak object discrimination persist. Discriminative training,
exemplified by models like CLIP, excels in zero-shot image-text classification
and retrieval, yet struggles with complex scenarios requiring fine-grained
semantic differentiation. This paper addresses these challenges by proposing a
unified approach that integrates the strengths of both paradigms. Considering
interleaved image-text sequences as the general format of input samples, we
introduce a structure-induced training strategy that imposes semantic
relationships between input samples and the MLLM's hidden state. This approach
enhances the MLLM's ability to capture global semantics and distinguish
fine-grained semantics. By leveraging dynamic sequence alignment within the
Dynamic Time Warping framework and integrating a novel kernel for fine-grained
semantic differentiation, our method effectively balances generative and
discriminative tasks. Extensive experiments demonstrate the effectiveness of
our approach, achieving state-of-the-art results in multiple generative tasks,
especially those requiring cognitive and discrimination abilities.
Additionally, our method surpasses discriminative benchmarks in interleaved and
fine-grained retrieval tasks. By employing a retrieval-augmented generation
strategy, our approach further enhances performance in some generative tasks
within one model, offering a promising direction for future research in
vision-language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Multimodal Sentiment Analysis with Incomplete Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhang, Wenbin Wang, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an
emerging direction seeking to tackle the issue of data incompleteness.
Recognizing that the language modality typically contains dense sentiment
information, we consider it as the dominant modality and present an innovative
Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust
MSA. The proposed LNLN features a dominant modality correction (DMC) module and
dominant modality based multimodal learning (DMML) module, which enhances the
model's robustness across various noise scenarios by ensuring the quality of
dominant modality representations. Aside from the methodical design, we perform
comprehensive experiments under random data missing scenarios, utilizing
diverse and meaningful settings on several popular datasets (\textit{e.g.,}
MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and
fairness compared to existing evaluations in the literature. Empirically, LNLN
consistently outperforms existing baselines, demonstrating superior performance
across these challenging and extensive evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-31T00:00:00Z">2024-10-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Embodied Reinforcement Learning Agents: Informativeness and
  Diversity of Language Use <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Xi, Yinong He, Jianing Yang, Yinpei Dai, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, it is desirable for embodied agents to have the
ability to leverage human language to gain explicit or implicit knowledge for
learning tasks. Despite recent progress, most previous approaches adopt simple
low-level instructions as language inputs, which may not reflect natural human
communication. It's not clear how to incorporate rich language use to
facilitate task learning. To address this question, this paper studies
different types of language inputs in facilitating reinforcement learning (RL)
embodied agents. More specifically, we examine how different levels of language
informativeness (i.e., feedback on past behaviors and future guidance) and
diversity (i.e., variation of language expressions) impact agent learning and
inference. Our empirical results based on four RL benchmarks demonstrate that
agents trained with diverse and informative language feedback can achieve
enhanced generalization and fast adaptation to new tasks. These findings
highlight the pivotal role of language use in teaching embodied agents new
tasks in an open world. Project website:
https://github.com/sled-group/Teachable_RL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main. Project website:
  https://github.com/sled-group/Teachable_RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P-Masking: Power Law Masking Improves Multi-attribute Controlled
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elgaar, Hadi Amiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LingGen, a novel approach for controlled text generation that
offers precise control over a wide array of linguistic attributes, even as the
number of attributes varies. LingGen employs a dynamic P-MASKING strategy,
which samples masking rates from a power law distribution during training. This
innovative approach enables the model to develop robust representations and
adapt its attribute control capabilities across a variable number of
attributes, from a single attribute to multiple complex configurations. The
P-MASKING technique enhances LingGen's ability to manage different levels of
attribute visibility, resulting in superior performance in multi-attribute
generation tasks. Our experiments demonstrate that LingGen surpasses current
state-of-the-art models in both attribute control accuracy and text fluency,
particularly excelling in scenarios with varying attribute demands.
Additionally, our ablation studies highlight the effectiveness of P-MASKING and
the influence of different base language models on performance. These findings
demonstrate LingGen's potential for applications requiring precise and
adaptable control over multiple linguistic attributes in text generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Length-Induced Embedding Collapse in <span class="highlight-title">Transformer</span>-based Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings enable various applications, but their performance
deteriorates on longer texts. In this paper, we find that the performance
degradation is due to a phenomenon called Length Collapse, where longer text
embeddings collapse into a narrow space. This collapse results in a
distributional inconsistency between embeddings of different text lengths,
ultimately hurting the performance of downstream tasks. Theoretically, by
considering the self-attention mechanism inherently functions as a low-pass
filter, we prove that long sequences increase the attenuation rate of the
low-pass filter effect of the self-attention mechanism. With layers going
deeper, excessive low-pass filtering causes the token signals to retain only
their Direct-Current (DC) component, which means the input token feature maps
will collapse into a narrow space, especially in long texts. Based on the above
analysis, we propose to mitigate the undesirable length collapse limitation by
introducing a temperature in softmax(), which achieves a higher low-filter
attenuation rate. The tuning-free method, called TempScale, can be plugged into
multiple transformer-based embedding models. Empirically, we demonstrate that
TempScale can improve existing embedding models, especially on long text
inputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text
Embedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from
LongEmbed, which specifically focuses on long context retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elgaar, Hadi Amiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to paraphrase generation that enables precise
control and fine-tuning of 40 linguistic attributes for English. Our model is
an encoder-decoder architecture that takes as input a source sentence and
desired linguistic attributes, and produces paraphrases of the source that
satisfy the desired attributes. To guarantee high-quality outputs at inference
time, our method is equipped with a quality control mechanism that gradually
adjusts the embedding of linguistic attributes to find the nearest and most
attainable configuration of desired attributes for paraphrase generation. We
evaluate the effectiveness of our method by comparing it to recent controllable
generation models. Experimental results demonstrate that the proposed model
outperforms baselines in generating paraphrases that satisfy desired linguistic
attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfCodeAlign: Self-Alignment for Code Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning is a supervised fine-tuning approach that significantly
improves the ability of large language models (LLMs) to follow human
instructions. We propose SelfCodeAlign, the first fully transparent and
permissive pipeline for self-aligning code LLMs without extensive human
annotations or distillation. SelfCodeAlign employs the same base model for
inference throughout the data generation process. It first extracts diverse
coding concepts from high-quality seed snippets to generate new tasks. It then
samples multiple responses per task, pairs each with test cases, and validates
them in a sandbox environment. Finally, passing examples are selected for
instruction tuning. In our primary experiments, we use SelfCodeAlign with
CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.
Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on
HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.
Across all benchmarks, this finetuned model consistently outperforms the
original version trained with OctoPack, the previous state-of-the-art method
for instruction tuning without human annotations or distillation. Additionally,
we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B
to 33B, and that the base models can benefit more from alignment with their own
data distribution. We further validate each component's effectiveness in our
pipeline, showing that SelfCodeAlign outperforms both direct distillation from
GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and
Evol-Instruct. SelfCodeAlign has also led to the creation of
StarCoder2-Instruct, the first fully transparent, permissively licensed, and
self-aligned code LLM that achieves state-of-the-art coding performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How could LLMs influence our democracy? We investigate LLMs' political
leanings and the potential influence of LLMs on voters by conducting multiple
experiments in a U.S. presidential election context. Through a voting
simulation, we first demonstrate 18 open- and closed-weight LLMs' political
preference for a Democratic nominee over a Republican nominee. We show how this
leaning towards the Democratic nominee becomes more pronounced in
instruction-tuned models compared to their base versions by analyzing their
responses to candidate-policy related questions. We further explore the
potential impact of LLMs on voter choice by conducting an experiment with 935
U.S. registered voters. During the experiments, participants interacted with
LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results
show a shift in voter choices towards the Democratic nominee following LLM
interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs
were not asked to persuade users to support the Democratic nominee during the
discourse. This effect is larger than many previous studies on the
persuasiveness of political campaigns, which have shown minimal effects in
presidential elections. Many users also expressed a desire for further
political interaction with LLMs. Which aspects of LLM interactions drove these
shifts in voter choice requires further study. Lastly, we explore how a safety
method can make LLMs more politically neutral, while leaving some open
questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng-Jui Chang, Hongyu Gong, Changhan Wang, James Glass, Yu-An Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken language models (SLMs) have gained increasing attention with
advancements in text-based, decoder-only language models. SLMs process text and
speech, enabling simultaneous speech understanding and generation. This paper
presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to
improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin
extracts speaker-invariant tokens rich in phonetic information and resilient to
input variations, enhancing zero-shot SLM tasks and speech resynthesis. We
propose a chunk-wise approach to enable streamable DC-Spin without retraining
and degradation. Comparisons of tokenization methods (self-supervised and
neural audio codecs), model scalability, and downstream task proxies show that
tokens easily modeled by an n-gram LM or aligned with phonemes offer strong
performance, providing insights for designing speech tokenizers for SLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constraint Back-translation Improves Complex Instruction Following of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) struggle to follow instructions with complex
constraints in format, length, etc. Following the conventional
instruction-tuning practice, previous works conduct post-training on complex
instruction-response pairs generated by feeding complex instructions to
advanced LLMs. However, even advanced LLMs cannot follow complex instructions
well, thus limiting the quality of generated data. In this work, we find that
existing datasets inherently contain implicit complex constraints and propose a
novel data generation technique, constraint back-translation. Specifically, we
take the high-quality instruction-response pairs in existing datasets and only
adopt advanced LLMs to add complex constraints already met by the responses to
the instructions, which naturally reduces costs and data noise. In the
experiments, we adopt Llama3-70B-Instruct to back-translate constraints and
create a high-quality complex instruction-response dataset, named CRAB. We
present that post-training on CRAB improves multiple backbone LLMs' complex
instruction-following ability, evaluated on extensive instruction-following
benchmarks. We further find that constraint back-translation also serves as a
useful auxiliary training objective in post-training. Our code, data, and
models will be released to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel Architecture for Distributed Travel Data Integration and Service
  Provision Using Microservices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biman Barua, M. Shamim Kaiser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a microservices architecture for the purpose of
enhancing the flexibility and performance of an airline reservation system. The
architectural design incorporates Redis cache technologies, two different
messaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and
PostgreSQL). It also introduces authorization techniques, including secure
communication through OAuth2 and JWT which is essential with the management of
high-demand travel services. According to selected indicators, the architecture
provides an impressive level of data consistency at 99.5% and a latency of data
propagation of less than 75 ms allowing rapid and reliable intercommunication
between microservices. A system throughput of 1050 events per second was
achieved so that the acceptability level was maintained even during peak time.
Redis caching reduced a 92% cache hit ratio on the database thereby lowering
the burden on the database and increasing the speed of response. Further
improvement of the systems scalability was done through the use of Docker and
Kubernetes which enabled services to be expanded horizontally to cope with the
changes in demand. The error rates were very low, at 0.2% further enhancing the
efficiency of the system in handling real-time data integration. This approach
is suggested to meet the specific needs of the airline reservation system. It
is secure, fast, scalable, all serving to improve the user experience as well
as the efficiency of operations. The low latency and high data integration
levels and prevaiing efficient usage of the resources demonstrates the
architecture ability to offer continued support in the ever growing high demand
situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Redefining <Creative> in Dictionary: Towards a Enhanced Semantic
  Understanding of Creative Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Feng, Yucheng Xie, Jing Wang, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creativity, both in human and diffusion models, remains an inherently
abstract concept; thus, simply adding "creative" to a prompt does not yield
reliable semantic recognition by the model. In this work, we concretize the
abstract notion of "creative" through the TP2O task, which aims to merge two
unrelated concepts, and introduce CreTok, redefining "creative" as the token
$\texttt{<CreTok>}$. This redefinition offers a more concrete and universally
adaptable representation for concept blending. This redefinition occurs
continuously, involving the repeated random sampling of text pairs with
different concepts and optimizing cosine similarity between target and constant
prompts. This approach enables $\texttt{<CreTok>}$ to learn a method for
creative concept fusion. Extensive experiments demonstrate that the creative
capability enabled by $\texttt{<CreTok>}$ substantially surpasses recent SOTA
diffusion models and achieves superior creative generation. CreTok exhibits
greater flexibility and reduced time overhead, as $\texttt{<CreTok>}$ can
function as a universal token for any concept, facilitating creative generation
without retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span> or <span class="highlight-title">BERT</span>: why not both? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Georges Gabriel Charpentier, David Samuel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple way to merge masked language modeling with causal
language modeling. This hybrid training objective results in a model that
combines the strengths of both modeling paradigms within a single transformer
stack: GPT-BERT can be transparently used like any standard causal or masked
language model. We test the pretraining process that enables this flexible
behavior on the BabyLM Challenge 2024. The results show that the hybrid
pretraining outperforms masked-only or causal-only models. We openly release
the models, training corpora and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages; submission to the BabyLM Challenge 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thought Space Explorer: Navigating and Expanding Thought Space for Large
  Language Model Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghan Zhang, Fengran Mo, Xiting Wang, Kunpeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have demonstrated their
potential in handling complex reasoning tasks, which are usually achieved by
constructing a thought chain to guide the model to solve the problem with
multi-step thinking. However, existing methods often remain confined to
previously explored solution spaces and thus overlook the critical blind spot
within LLMs' cognitive range. To address these issues, we design the Thought
Space Explorer (TSE), a novel framework to expand and optimize thought
structures to guide LLMs to explore their blind spots of thinking. By
generating new reasoning steps and branches based on the original thought
structure with various designed strategies, TSE broadens the thought space and
alleviates the impact of blind spots for LLM reasoning. Experimental results on
multiple levels of reasoning tasks demonstrate the efficacy of TSE. We also
conduct extensive analysis to understand how structured and expansive thought
can contribute to unleashing the potential of LLM reasoning capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Concept With Text-Guided Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Huang, Susan Liang, Yunlong Tang, Yapeng Tian, Anurag Kumar, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided diffusion models have revolutionized generative tasks by
producing high-fidelity content from text descriptions. They have also enabled
an editing paradigm where concepts can be replaced through text conditioning
(e.g., a dog to a tiger). In this work, we explore a novel approach: instead of
replacing a concept, can we enhance or suppress the concept itself? Through an
empirical study, we identify a trend where concepts can be decomposed in
text-guided diffusion models. Leveraging this insight, we introduce
ScalingConcept, a simple yet effective method to scale decomposed concepts up
or down in real input without introducing new elements. To systematically
evaluate our approach, we present the WeakConcept-10 dataset, where concepts
are imperfect and need to be enhanced. More importantly, ScalingConcept enables
a variety of novel zero-shot applications across image and audio domains,
including tasks such as canonical pose generation and generative sound
highlighting or removal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://wikichao.github.io/ScalingConcept/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Touch My Diacritics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Gorman, Yuval Pinter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The common practice of preprocessing text before feeding it into NLP models
introduces many decision points which have unintended consequences on model
performance. In this opinion piece, we focus on the handling of diacritics in
texts originating in many languages and scripts. We demonstrate, through
several case studies, the adverse effects of inconsistent encoding of
diacritized characters and of removing diacritics altogether. We call on the
community to adopt simple but necessary steps across all models and toolkits in
order to improve handling of diacritized text and, by extension, increase
equity in multilingual NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-environment Topic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Sobhani, Amir Feder, David Blei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic topic models are a powerful tool for extracting latent themes
from large text datasets. In many text datasets, we also observe per-document
covariates (e.g., source, style, political affiliation) that act as
environments that modulate a "global" (environment-agnostic) topic
representation. Accurately learning these representations is important for
prediction on new documents in unseen environments and for estimating the
causal effect of topics on real-world outcomes. To this end, we introduce the
Multi-environment Topic Model (MTM), an unsupervised probabilistic model that
separates global and environment-specific terms. Through experimentation on
various political content, from ads to tweets and speeches, we show that the
MTM produces interpretable global topics with distinct environment-specific
words. On multi-environment data, the MTM outperforms strong baselines in and
out-of-distribution. It also enables the discovery of accurate causal effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nearest Neighbor Normalization Improves Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Chowdhury, Franklin Wang, Sumedh Shenoy, Douwe Kiela, Sarah Schwettmann, Tristan Thrush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal models leverage large-scale pre-training to achieve strong but
still imperfect performance on tasks such as image captioning, visual question
answering, and cross-modal retrieval. In this paper, we present a simple and
efficient method for correcting errors in trained contrastive image-text
retrieval models with no additional training, called Nearest Neighbor
Normalization (NNN). We show an improvement on retrieval metrics in both text
retrieval and image retrieval for all of the contrastive models that we tested
(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used
(MS-COCO and Flickr30k). NNN requires a reference database, but does not
require any training on this database, and can even increase the retrieval
accuracy of a model after finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Fine-Tuning for Time-Series Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhimanyu Das, Matthew Faw, Rajat Sen, Yichen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the recent success of time-series foundation models for
zero-shot forecasting, we present a methodology for $\textit{in-context
fine-tuning}$ of a time-series foundation model. In particular, we design a
pretrained foundation model that can be prompted (at inference time) with
multiple time-series examples, in order to forecast a target time-series into
the future. Our foundation model is specifically trained to utilize examples
from multiple related time-series in its context window (in addition to the
history of the target time-series) to help it adapt to the specific
distribution of the target domain at inference time. We show that such a
foundation model that uses in-context examples at inference time can obtain
much better performance on popular forecasting benchmarks compared to
supervised deep learning methods, statistical models, as well as other
time-series foundation models. Interestingly, our in-context fine-tuning
approach even rivals the performance of a foundation model that is explicitly
fine-tuned on the target domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Saeed, Elgizouli Mohamed, Mukhtar Mohamed, Shaina Raza, Shady Shehata, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are widely used but raise ethical concerns due
to embedded social biases. This study examines LLM biases against Arabs versus
Westerners across eight domains, including women's rights, terrorism, and
anti-Semitism and assesses model resistance to perpetuating these biases. To
this end, we create two datasets: one to evaluate LLM bias toward Arabs versus
Westerners and another to test model safety against prompts that exaggerate
negative traits ("jailbreaks"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA
3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases
displaying negative biases toward Arabs, with LlaMA 3.1-405B being the most
biased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being
an optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except
Claude exhibit attack success rates above 87% in three categories. We also find
Claude 3.5 Sonnet the safest, but it still displays biases in seven of eight
categories. Despite being an optimized version of GPT4, We find GPT-4o to be
more prone to biases and jailbreaks, suggesting optimization flaws. Our
findings underscore the pressing need for more robust bias mitigation
strategies and strengthened security measures in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Unknown: A Chat-Based Collaborative Interface for
  Personalized Exploratory Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingzhe Peng, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Xu Yang, Dongmei Zhang, Saravan Rajmohan, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has revolutionized user interactions
with knowledge-based systems, enabling chatbots to synthesize vast amounts of
information and assist with complex, exploratory tasks. However, LLM-based
chatbots often struggle to provide personalized support, particularly when
users start with vague queries or lack sufficient contextual information. This
paper introduces the Collaborative Assistant for Personalized Exploration
(CARE), a system designed to enhance personalization in exploratory tasks by
combining a multi-agent LLM framework with a structured user interface. CARE's
interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling
iterative query refinement and dynamic solution generation. The multi-agent
framework collaborates to identify both explicit and implicit user needs,
delivering tailored, actionable solutions. In a within-subject user study with
22 participants, CARE was consistently preferred over a baseline LLM chatbot,
with users praising its ability to reduce cognitive load, inspire creativity,
and provide more tailored solutions. Our findings highlight CARE's potential to
transform LLM-based systems from passive information retrievers to proactive
partners in personalized problem-solving and exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Training for Selective Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Li, Rebecca J. Passonneau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifier models are prevalent in natural language processing (NLP), often
with high accuracy. Yet in real world settings, human-in-the-loop systems can
foster trust in model outputs and even higher performance. Selective Prediction
(SP) methods determine when to adopt a classifier's output versus defer to a
human. Previous SP approaches have addressed how to improve softmax as a
measure of model confidence, or have developed separate confidence estimators.
One previous method involves learning a deferral model based on engineered
features. We introduce a novel joint-training approach that simultaneously
optimizes learned representations used by the classifier module and a learned
deferral policy. Our results on four classification tasks demonstrate that
joint training not only leads to better SP outcomes over two strong baselines,
but also improves the performance of both modules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting text level intellectual influence with knowledge graph
  embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucian Li, Eryclis Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introduction: Tracing the spread of ideas and the presence of influence is a
question of special importance across a wide range of disciplines, ranging from
intellectual history to cultural analytics, computational social science, and
the science of science.
  Method: We collect a corpus of open source journal articles, generate
Knowledge Graph representations using the Gemini LLM, and attempt to predict
the existence of citations between sampled pairs of articles using previously
published methods and a novel Graph Neural Network based embedding model.
  Results: We demonstrate that our knowledge graph embedding method is superior
at distinguishing pairs of articles with and without citation. Once trained, it
runs efficiently and can be fine-tuned on specific corpora to suit individual
researcher needs.
  Conclusion(s): This experiment demonstrates that the relationships encoded in
a knowledge graph, especially the types of concepts brought together by
specific relations can encode information capable of revealing intellectual
influence. This suggests that further work in analyzing document level
knowledge graphs to understand latent structures could provide valuable
insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech is More Than Words: Do Speech-to-Text Translation Systems
  Leverage Prosody? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Tsiamas, Matthias Sperber, Andrew Finch, Sarthak Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prosody of a spoken utterance, including features like stress, intonation
and rhythm, can significantly affect the underlying semantics, and as a
consequence can also affect its textual translation. Nevertheless, prosody is
rarely studied within the context of speech-to-text translation (S2TT) systems.
In particular, end-to-end (E2E) systems have been proposed as well-suited for
prosody-aware translation because they have direct access to the speech signal
when making translation decisions, but the understanding of whether this is
successful in practice is still limited. A main challenge is the difficulty of
evaluating prosody awareness in translation. To address this challenge, we
introduce an evaluation methodology and a focused benchmark (named ContraProST)
aimed at capturing a wide range of prosodic phenomena. Our methodology uses
large language models and controllable text-to-speech (TTS) to generate
contrastive examples. Through experiments in translating English speech into
German, Spanish, and Japanese, we find that (a) S2TT models possess some
internal representation of prosody, but the prosody signal is often not strong
enough to affect the translations, (b) E2E systems outperform cascades of
speech recognition and text translation systems, confirming their theoretical
advantage in this regard, and (c) certain cascaded systems also capture
prosodic information in the translation, but only to a lesser extent that
depends on the particulars of the transcript's surface form.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WMT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual <span class="highlight-title">Pretrain</span>ing Using a Large Corpus Machine-Translated from a
  Single Source Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Wang, Yao Lu, Maurice Weber, Max Ryabinin, Yihong Chen, Raphael Tang, Pontus Stenetorp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English, as a very high-resource language, enables the pretraining of
high-quality large language models (LLMs). The same cannot be said for most
other languages, as leading LLMs still underperform for non-English languages,
likely due to a gap in the quality and diversity of the available multilingual
pretraining corpora. In this work, we find that machine-translated text from a
single high-quality source language can contribute significantly to the
pretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality
English web dataset, into French, German, and Spanish, resulting in a final
300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter
model, CuatroLLM, from scratch on this dataset. Across five non-English
reasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art
multilingual models trained using closed data, such as Llama3.2 and Gemma2,
despite using an order of magnitude less data, such as about 6% of the tokens
used for Llama3.2's training. We further demonstrate that with additional
domain-specific pretraining, amounting to less than 1% of TransWeb-Edu,
CuatroLLM surpasses the state of the art in multilingual reasoning. To promote
reproducibility, we release our corpus, models, and training pipeline under
open licenses at hf.co/britllm/CuatroLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representative Social Choice: From Learning Theory to AI Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social choice theory is the study of preference aggregation across a
population, used both in mechanism design for human agents and in the
democratic alignment of language models. In this study, we propose the
representative social choice framework for the modeling of democratic
representation in collective decisions, where the number of issues and
individuals are too large for mechanisms to consider all preferences directly.
These scenarios are widespread in real-world decision-making processes, such as
jury trials, indirect elections, legislation processes, corporate governance,
and, more recently, language model alignment. In representative social choice,
the population is represented by a finite sample of individual-issue pairs
based on which social choice decisions are made. We show that many of the
deepest questions in representative social choice can be naturally formulated
as statistical learning problems, and prove the generalization properties of
social choice mechanisms using the theory of machine learning. We further
formulate axioms for representative social choice, and prove Arrow-like
impossibility theorems with new combinatorial tools of analysis. Our framework
introduces the representative approach to social choice, opening up research
directions at the intersection of social choice, learning theory, and AI
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full version (20 pages). Under review. An excerpt was previously
  accepted to NeurIPS 2024 Pluralistic Alignment Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models can Self-Lengthen to Generate Long Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have significantly
enhanced their ability to process long contexts, yet a notable gap remains in
generating long, aligned outputs. This limitation stems from a training gap
where pre-training lacks effective instructions for long-text generation, and
post-training data primarily consists of short query-response pairs. Current
approaches, such as instruction backtranslation and behavior imitation, face
challenges including data quality, copyright issues, and constraints on
proprietary model usage. In this paper, we introduce an innovative iterative
training framework called Self-Lengthen that leverages only the intrinsic
knowledge and skills of LLMs without the need for auxiliary data or proprietary
models. The framework consists of two roles: the Generator and the Extender.
The Generator produces the initial response, which is then split and expanded
by the Extender. This process results in a new, longer response, which is used
to train both the Generator and the Extender iteratively. Through this process,
the models are progressively trained to handle increasingly longer responses.
Experiments on benchmarks and human evaluations show that Self-Lengthen
outperforms existing methods in long-text generation, when applied to top
open-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at
https://github.com/QwenLM/Self-Lengthen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BitStack: Fine-Grained Size Control for Compressed Large Language Models
  in Variable Memory Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized numerous applications, yet
their deployment remains challenged by memory constraints on local devices.
While scaling laws have enhanced LLM capabilities, the primary bottleneck has
shifted from \textit{capability} to \textit{availability}, emphasizing the need
for efficient memory management. Traditional compression methods, such as
quantization, often require predefined compression ratios and separate
compression processes for each setting, complicating deployment in variable
memory environments. In this paper, we introduce \textbf{BitStack}, a novel,
training-free weight compression approach that enables megabyte-level
trade-offs between memory usage and model performance. By leveraging weight
decomposition, BitStack can dynamically adjust the model size with minimal
transmission between running memory and storage devices. Our approach
iteratively decomposes weight matrices while considering the significance of
each parameter, resulting in an approximately 1-bit per parameter residual
block in each decomposition iteration. These blocks are sorted and stacked in
storage as basic transmission units, with different quantities loaded based on
current memory availability. Extensive experiments across a wide range of tasks
demonstrate that, despite offering fine-grained size control, BitStack
consistently matches or surpasses strong quantization baselines, particularly
at extreme compression ratios. To the best of our knowledge, this is the first
decomposition-based method that effectively bridges the gap to practical
compression techniques like quantization. Code is available at
https://github.com/xinghaow99/BitStack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Responsible Retrieval Augmented Generation for Climate Decision Making
  from Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matyas Juhasz, Kalyan Dutia, Henry Franks, Conor Delahunty, Patrick Fawbert Mills, Harrison Pim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate decision making is constrained by the complexity and inaccessibility
of key information within lengthy, technical, and multi-lingual documents.
Generative AI technologies offer a promising route for improving the
accessibility of information contained within these documents, but suffer from
limitations. These include (1) a tendency to hallucinate or mis-represent
information, (2) difficulty in steering or guaranteeing properties of generated
output, and (3) reduced performance in specific technical domains. To address
these challenges, we introduce a novel evaluation framework with
domain-specific dimensions tailored for climate-related documents. We then
apply this framework to evaluate Retrieval-Augmented Generation (RAG)
approaches and assess retrieval- and generation-quality within a prototype tool
that answers questions about individual climate law and policy documents. In
addition, we publish a human-annotated dataset and scalable automated
evaluation tools, with the aim of facilitating broader adoption and robust
assessment of these systems in the climate domain. Our findings highlight the
key components of responsible deployment of RAG to enhance decision-making,
while also providing insights into user experience (UX) considerations for
safely deploying such systems to build trust with users in high-risk domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource
  languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Séamus Lankford, Andy Way
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an evolving landscape of crisis communication, the need for robust and
adaptable Machine Translation (MT) systems is more pressing than ever,
particularly for low-resource languages. This study presents a comprehensive
exploration of leveraging Large Language Models (LLMs) and Multilingual LLMs
(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique
challenges posed by crisis situations where speed, accuracy, and the ability to
handle a wide range of languages are paramount, this research outlines a novel
approach that combines the cutting-edge capabilities of LLMs with fine-tuning
techniques and community-driven corpus development strategies. At the core of
this study is the development and empirical evaluation of MT systems tailored
for two low-resource language pairs, illustrating the process from initial
model selection and fine-tuning through to deployment. Bespoke systems are
developed and modelled on the recent Covid-19 pandemic. The research highlights
the importance of community involvement in creating highly specialised,
crisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM
models. It identifies fine-tuned MLLM models as offering superior performance
compared with their LLM counterparts. A scalable and replicable model for rapid
MT system development in crisis scenarios is outlined. Our approach enhances
the field of humanitarian technology by offering a blueprint for developing
multilingual communication systems during emergencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2403.02370,
  arXiv:2403.01580</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Failure Modes of LLMs for Causal Reasoning on Narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khurram Yamin, Shantanu Gupta, Gaurav R. Ghosal, Zachary C. Lipton, Bryan Wilder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the causal reasoning abilities of large language
models (LLMs) through the representative problem of inferring causal
relationships from narratives. We find that even state-of-the-art language
models rely on unreliable shortcuts, both in terms of the narrative
presentation and their parametric knowledge. For example, LLMs tend to
determine causal relationships based on the topological ordering of events
(i.e., earlier events cause later ones), resulting in lower performance
whenever events are not narrated in their exact causal order. Similarly, we
demonstrate that LLMs struggle with long-term causal reasoning and often fail
when the narratives are long and contain many events. Additionally, we show
LLMs appear to rely heavily on their parametric knowledge at the expense of
reasoning over the provided narrative. This degrades their abilities whenever
the narrative opposes parametric knowledge. We extensively validate these
failure modes through carefully controlled synthetic experiments, as well as
evaluations on real-world narratives. Finally, we observe that explicitly
generating a causal graph generally improves performance while naive
chain-of-thought is ineffective. Collectively, our results distill precise
failure modes of current state-of-the-art models and can pave the way for
future techniques to enhance causal reasoning in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 'No' Matters: Out-of-Distribution Detection in Multimodality Long
  Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rena Gao, Xuetong Wu, Siwen Luo, Caren Han, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection in multimodal contexts is essential for
identifying deviations in combined inputs from different modalities,
particularly in applications like open-domain dialogue systems or real-life
dialogue interactions. This paper aims to improve the user experience that
involves multi-round long dialogues by efficiently detecting OOD dialogues and
images. We introduce a novel scoring framework named Dialogue Image Aligning
and Enhancing Framework (DIAEF) that integrates the visual language models with
the novel proposed scores that detect OOD in two key scenarios (1) mismatches
between the dialogue and image input pair and (2) input pairs with previously
unseen labels. Our experimental results, derived from various benchmarks,
demonstrate that integrating image and multi-round dialogue OOD detection is
more effective with previously unseen labels than using either modality
independently. In the presence of mismatched pairs, our proposed score
effectively identifies these mismatches and demonstrates strong robustness in
long dialogues. This approach enhances domain-aware, adaptive conversational
agents and establishes baselines for future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have demonstrated the ability to interact with
humans under real-world conditions by combining Large Language Models (LLMs)
and modality encoders to align multimodal information (visual and auditory)
with text. However, such models raise new safety challenges of whether models
that are safety-aligned on text also exhibit consistent safeguards for
multimodal inputs. Despite recent safety-alignment research on vision LMMs, the
safety of audio LMMs remains under-explored. In this work, we comprehensively
red team the safety of five advanced audio LMMs under three settings: (i)
harmful questions in both audio and text formats, (ii) harmful questions in
text format accompanied by distracting non-speech audio, and (iii)
speech-specific jailbreaks. Our results under these settings demonstrate that
open-source audio LMMs suffer an average attack success rate of 69.14% on
harmful audio questions, and exhibit safety vulnerabilities when distracted
with non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro
achieve an attack success rate of 70.67% on the harmful query benchmark. We
provide insights on what could cause these reported safety-misalignments.
Warning: this paper contains offensive examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Models Perform Robust Reasoning in Chain-of-thought
  <span class="highlight-title">Prompt</span>ing with Noisy Rationales? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanke Zhou, Rong Tao, Jianing Zhu, Yiwen Luo, Zengmao Wang, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates an under-explored challenge in large language models
(LLMs): chain-of-thought prompting with noisy rationales, which include
irrelevant or inaccurate reasoning thoughts within examples used for in-context
learning. We construct NoRa dataset that is tailored to evaluate the robustness
of reasoning in the presence of noisy rationales. Our findings on NoRa dataset
reveal a prevalent vulnerability to such noise among current LLMs, with
existing robust methods like self-correction and self-consistency showing
limited efficacy. Notably, compared to prompting with clean rationales, base
LLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more
drastically by 2.2%-40.4% with inaccurate thoughts.
  Addressing this challenge necessitates external supervision that should be
accessible in practice. Here, we propose the method of contrastive denoising
with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning
capabilities by contrasting noisy rationales with only one clean rationale,
which can be the minimal requirement for denoising-purpose prompting. This
method follows a principle of exploration and exploitation: (1) rephrasing and
selecting rationales in the input space to achieve explicit denoising and (2)
exploring diverse reasoning paths and voting on answers in the output space.
Empirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy
over the base model and shows significantly stronger denoising capabilities
than baseline methods. The source code is publicly available at:
https://github.com/tmlr-group/NoisyRationales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Automated Verification of Textual Claims (AVeriTeC) Shared Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Schlichtkrull, Yulong Chen, Chenxi Whitehouse, Zhenyun Deng, Mubashara Akhtar, Rami Aly, Zhijiang Guo, Christos Christodoulopoulos, Oana Cocarascu, Arpit Mittal, James Thorne, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Automated Verification of Textual Claims (AVeriTeC) shared task asks
participants to retrieve evidence and predict veracity for real-world claims
checked by fact-checkers. Evidence can be found either via a search engine, or
via a knowledge store provided by the organisers. Submissions are evaluated
using AVeriTeC score, which considers a claim to be accurately verified if and
only if both the verdict is correct and retrieved evidence is considered to
meet a certain quality threshold. The shared task received 21 submissions, 18
of which surpassed our baseline. The winning team was TUDA_MAI with an AVeriTeC
score of 63%. In this paper we describe the shared task, present the full
results, and highlight key takeaways from the shared task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Commonsense Knowledge Editing Based on Free-Text in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiusheng Huang, Yequan Wang, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasons and Solutions for the Decline in Model Performance after Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiusheng Huang, Jiaxiang Liu, Yequan Wang, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge editing technology has received widespread attention for low-cost
updates of incorrect or outdated knowledge in large-scale language models.
However, recent research has found that edited models often exhibit varying
degrees of performance degradation. The reasons behind this phenomenon and
potential solutions have not yet been provided. In order to investigate the
reasons for the performance decline of the edited model and optimize the
editing method, this work explores the underlying reasons from both data and
model perspectives. Specifically, 1) from a data perspective, to clarify the
impact of data on the performance of editing models, this paper first
constructs a Multi-Question Dataset (MQD) to evaluate the impact of different
types of editing data on model performance. The performance of the editing
model is mainly affected by the diversity of editing targets and sequence
length, as determined through experiments. 2) From a model perspective, this
article explores the factors that affect the performance of editing models. The
results indicate a strong correlation between the L1-norm of the editing model
layer and the editing accuracy, and clarify that this is an important factor
leading to the bottleneck of editing performance. Finally, in order to improve
the performance of the editing model, this paper further proposes a Dump for
Sequence (D4S) method, which successfully overcomes the previous editing
bottleneck by reducing the L1-norm of the editing layer, allowing users to
perform multiple effective edits and minimizing model damage. Our code is
available at https://github.com/nlpkeg/D4S.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for
  Minority Languages <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Hossein Kargaran, François Yvon, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need for large text corpora has increased with the advent of pretrained
language models and, in particular, the discovery of scaling laws for these
models. Most available corpora have sufficient data only for languages with
large dominant communities. However, there is no corpus available that (i)
covers a wide range of minority languages; (ii) is generated by an open-source
reproducible pipeline; and (iii) is rigorously cleaned from noise, making it
trustworthy to use. We present GlotCC, a clean, document-level, 2TB general
domain corpus derived from CommonCrawl, covering more than 1000 languages. We
make GlotCC and the system used to generate it - including the pipeline,
language identification model, and filters - available to the research
community. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1,
Pipeline v. 3.0 https://github.com/cisnlp/GlotCC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What is Wrong with Perplexity for Long-context Language Modeling? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling long-context inputs is crucial for large language models (LLMs) in
tasks such as extended conversations, document summarization, and many-shot
in-context learning. While recent approaches have extended the context windows
of LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has
proven unreliable for assessing long-context capabilities. The underlying cause
of this limitation has remained unclear. In this work, we provide a
comprehensive explanation for this issue. We find that PPL overlooks key
tokens, which are essential for long-context understanding, by averaging across
all tokens and thereby obscuring the true performance of models in long-context
scenarios. To address this, we propose \textbf{LongPPL}, a novel metric that
focuses on key tokens by employing a long-short context contrastive method to
identify them. Our experiments demonstrate that LongPPL strongly correlates
with performance on various long-context benchmarks (e.g., Pearson correlation
of -0.96), significantly outperforming traditional PPL in predictive accuracy.
Additionally, we introduce \textbf{LongCE} (Long-context Cross-Entropy) loss, a
re-weighting strategy for fine-tuning that prioritizes key tokens, leading to
consistent improvements across diverse benchmarks. In summary, these
contributions offer deeper insights into the limitations of PPL and present
effective solutions for accurately evaluating and enhancing the long-context
capabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Potential of LLMs in Medical Education: Generating Questions and
  Answers for Qualification Exams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqi Zhu, Wen Tang, Ying Sun, Xuebing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on large language models (LLMs) has primarily focused on
their adaptation and application in specialized domains. The application of
LLMs in the medical field is mainly concentrated on tasks such as the
automation of medical report generation, summarization, diagnostic reasoning,
and question-and-answer interactions between doctors and patients. The
challenge of becoming a good teacher is more formidable than that of becoming a
good student, and this study pioneers the application of LLMs in the field of
medical education. In this work, we investigate the extent to which LLMs can
generate medical qualification exam questions and corresponding answers based
on few-shot prompts. Utilizing a real-world Chinese dataset of elderly chronic
diseases, we tasked the LLMs with generating open-ended questions and answers
based on a subset of sampled admission reports across eight widely used LLMs,
including ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, and
Mistral. Furthermore, we engaged medical experts to manually evaluate these
open-ended questions and answers across multiple dimensions. The study found
that LLMs, after using few-shot prompts, can effectively mimic real-world
medical qualification exam questions, whereas there is room for improvement in
the correctness, evidence-based statements, and professionalism of the
generated answers. Moreover, LLMs also demonstrate a decent level of ability to
correct and rectify reference answers. Given the immense potential of
artificial intelligence in the medical field, the task of generating questions
and answers for medical qualification exams aimed at medical students, interns
and residents can be a significant focus of future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DetectRL: Benchmarking LLM-Generated Text Detection in Real-World
  Scenarios <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xinyi Yang, Yulin Yuan, Lidia S. Chao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting text generated by large language models (LLMs) is of great recent
interest. With zero-shot methods like DetectGPT, detection capabilities have
reached impressive levels. However, the reliability of existing detectors in
real-world applications remains underexplored. In this study, we present a new
benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection
techniques still underperformed in this task. We collected human-written
datasets from domains where LLMs are particularly prone to misuse. Using
popular LLMs, we generated data that better aligns with real-world
applications. Unlike previous studies, we employed heuristic rules to create
adversarial LLM-generated text, simulating advanced prompt usages, human
revisions like word substitutions, and writing errors. Our development of
DetectRL reveals the strengths and limitations of current SOTA detectors. More
importantly, we analyzed the potential impact of writing styles, model types,
attack methods, the text lengths, and real-world human writing factors on
different types of detectors. We believe DetectRL could serve as an effective
benchmark for assessing detectors in real-world scenarios, evolving with
advanced attack methods, thus providing more stressful evaluation to drive the
development of more efficient detectors. Data and code are publicly available
at: https://github.com/NLP2CT/DetectRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Dataset & Benchmarking Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A
  Gradient Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Yanhong Li, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What makes a difference in the post-training of LLMs? We investigate the
training patterns of different layers in large language models (LLMs), through
the lens of gradient, when training with different responses and initial
models. We are specifically interested in how fast vs. slow thinking affects
the layer-wise gradients, given the recent popularity of training LLMs on
reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our
study, fast thinking without CoT leads to larger gradients and larger
differences of gradients across layers than slow thinking (Detailed CoT),
indicating the learning stability brought by the latter. Moreover, pre-trained
LLMs are less affected by the instability of fast thinking than
instruction-tuned LLMs. Additionally, we study whether the gradient patterns
can reflect the correctness of responses when training different LLMs using
slow vs. fast thinking paths. The results show that the gradients of slow
thinking can distinguish correct and irrelevant reasoning paths. As a
comparison, we conduct similar gradient analyses on non-reasoning knowledge
learning tasks, on which, however, trivially increasing the response length
does not lead to similar behaviors of slow thinking. Our study strengthens
fundamental understandings of LLM training and sheds novel insights on its
efficiency and stability, which pave the way towards building a generalizable
System-2 agent. Our code, data, and gradient statistics can be found in:
https://github.com/MingLiiii/Layer_Gradient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GigaCheck: Detecting LLM-generated Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irina Tolstykh, Aleksandra Tsybina, Sergey Yakubson, Aleksandr Gordeev, Vladimir Dokholyan, Maksim Kuprashevich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing quality and spread of LLM-based assistants, the amount of
artificially generated content is growing rapidly. In many cases and tasks,
such texts are already indistinguishable from those written by humans, and the
quality of generation tends to only increase. At the same time, detection
methods are developing more slowly, making it challenging to prevent misuse of
these technologies.
  In this work, we investigate the task of generated text detection by
proposing the GigaCheck. Our research explores two approaches: (i)
distinguishing human-written texts from LLM-generated ones, and (ii) detecting
LLM-generated intervals in Human-Machine collaborative texts. For the first
task, our approach utilizes a general-purpose LLM, leveraging its extensive
language abilities to fine-tune efficiently for the downstream task of
LLM-generated text detection, achieving high performance even with limited
data. For the second task, we propose a novel approach that combines computer
vision and natural language processing techniques. Specifically, we use a
fine-tuned general-purpose LLM in conjunction with a DETR-like detection model,
adapted from computer vision, to localize artificially generated intervals
within text.
  We evaluate the GigaCheck on five classification datasets with English texts
and three datasets designed for Human-Machine collaborative text analysis. Our
results demonstrate that GigaCheck outperforms previous methods, even in
out-of-distribution settings, establishing a strong baseline across all
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial intelligence to improve clinical coding practice in
  Scandinavia: a crossover randomized controlled trial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taridzo Chomutare, Therese Olsen Svenning, Miguel Ángel Tejedor Hernández, Phuong Dinh Ngo, Andrius Budrionis, Kaisa Markljung, Lill Irene Hind, Torbjørn Torsvik, Karl Øyvind Mikalsen, Aleksandar Babic, Hercules Dalianis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \textbf{Trial design} Crossover randomized controlled trial. \textbf{Methods}
An AI tool, Easy-ICD, was developed to assist clinical coders and was tested
for improving both accuracy and time in a user study in Norway and Sweden.
Participants were randomly assigned to two groups, and crossed over between
coding complex (longer) texts versus simple (shorter) texts, while using our
tool versus not using our tool. \textbf{Results} Based on Mann-Whitney U test,
the median coding time difference for complex clinical text sequences was 123
seconds (\emph{P}\textless.001, 95\% CI: 81 to 164), representing a 46\%
reduction in median coding time when our tool is used. There was no significant
time difference for simpler text sequences. For coding accuracy, the
improvement we noted for both complex and simple texts was not significant.
\textbf{Conclusions} This study demonstrates the potential of AI to transform
common tasks in clinical workflows, with ostensible positive impacts on work
efficiencies for complex clinical coding tasks. Further studies within hospital
workflows are required before these presumed impacts can be more clearly
understood.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wu, Xintong Li, Ruoyu Wang, Yu Xia, Yuxin Xiong, Jianing Wang, Tong Yu, Xiang Chen, Branislav Kveton, Lina Yao, Jingbo Shang, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline evaluation of LLMs is crucial in understanding their capacities,
though current methods remain underexplored in existing research. In this work,
we focus on the offline evaluation of the chain-of-thought capabilities and
show how to optimize LLMs based on the proposed evaluation method. To enable
offline feedback with rich knowledge and reasoning paths, we use knowledge
graphs (e.g., Wikidata5m) to provide feedback on the generated chain of
thoughts. Due to the heterogeneity between LLM reasoning and KG structures,
direct interaction and feedback from KGs on LLM behavior are challenging, as
they require accurate entity linking and grounding of LLM-generated chains of
thought in the KG. To address the above challenge, we propose an offline
chain-of-thought evaluation framework, OCEAN, which models chain-of-thought
reasoning in LLMs as an MDP and evaluate the policy's alignment with KG
preference modeling. To overcome the reasoning heterogeneity and grounding
problems, we leverage on-policy KG exploration and RL to model a KG policy that
generates token-level likelihood distributions for LLM-generated
chain-of-thought reasoning paths, simulating KG reasoning preference. Then we
incorporate the knowledge-graph feedback on the validity and alignment of the
generated reasoning paths into inverse propensity scores and propose KG-IPS
estimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS
estimator and provide a lower bound on its variance. With the off-policy
evaluated value function, we can directly enable off-policy optimization to
further enhance chain-of-thought alignment. Our empirical study shows that
OCEAN can be efficiently optimized for generating chain-of-thought reasoning
paths with higher estimated values without affecting LLMs' general abilities in
downstream tasks or their internal knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhi Tang, Chuang Yang, Tong Xing, Xiaohang Xu, Renhe Jiang, Kaoru Sezaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mobility prediction plays a critical role in applications such as
disaster response, urban planning, and epidemic forecasting. Traditional
methods often rely on designing crafted, domain-specific models, and typically
focus on short-term predictions, which struggle to generalize across diverse
urban environments. In this study, we introduce Llama-3-8B-Mob, a large
language model fine-tuned with instruction tuning, for long-term citywide
mobility prediction -- in a Q&A manner. We validate our approach using
large-scale human mobility data from four metropolitan areas in Japan, focusing
on predicting individual trajectories over the next 15 days. The results
demonstrate that Llama-3-8B-Mob excels in modeling long-term human mobility --
surpassing the state-of-the-art on multiple prediction metrics. It also
displays strong zero-shot generalization capabilities -- effectively
generalizing to other cities even when fine-tuned only on limited samples from
a single city. Source codes are available at
https://github.com/TANGHULU6/Llama3-8B-Mob.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in
  Byte-Level Tokenizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugene Jang, Kimin Lee, Jin-Woo Chung, Keuntae Park, Seungwon Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is a crucial step that bridges human-readable text with
model-readable discrete tokens. However, recent studies have revealed that
tokenizers can be exploited to elicit unwanted model behaviors. In this work,
we investigate incomplete tokens, i.e., undecodable tokens with stray bytes
resulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize
that such tokens are heavily reliant on their adjacent tokens and are fragile
when paired with unfamiliar tokens. To demonstrate this vulnerability, we
introduce improbable bigrams: out-of-distribution combinations of incomplete
tokens designed to exploit their dependency. Our experiments show that
improbable bigrams are significantly prone to hallucinatory behaviors.
Surprisingly, alternative tokenizations of the same phrases result in
drastically lower rates of hallucination (93% reduction in Llama3.1). We
caution against the potential vulnerabilities introduced by byte-level BPE
tokenizers, which may impede the development of trustworthy language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo-Conversation Injection for LLM Goal Hijacking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chen, Buhui Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal hijacking is a type of adversarial attack on Large Language Models
(LLMs) where the objective is to manipulate the model into producing a
specific, predetermined output, regardless of the user's original input. In
goal hijacking, an attacker typically appends a carefully crafted malicious
suffix to the user's prompt, which coerces the model into ignoring the user's
original input and generating the target response. In this paper, we introduce
a novel goal hijacking attack method called Pseudo-Conversation Injection,
which leverages the weaknesses of LLMs in role identification within
conversation contexts. Specifically, we construct the suffix by fabricating
responses from the LLM to the user's initial prompt, followed by a prompt for a
malicious new task. This leads the model to perceive the initial prompt and
fabricated response as a completed conversation, thereby executing the new,
falsified prompt. Following this approach, we propose three Pseudo-Conversation
construction strategies: Targeted Pseudo-Conversation, Universal
Pseudo-Conversation, and Robust Pseudo-Conversation. These strategies are
designed to achieve effective goal hijacking across various scenarios. Our
experiments, conducted on two mainstream LLM platforms including ChatGPT and
Qwen, demonstrate that our proposed method significantly outperforms existing
approaches in terms of attack effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel Looping: Eliminating Synchronization Boundaries for Peak
  Inference Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Koeplinger, Darshan Gandhi, Pushkar Nandkar, Nathan Sheeley, Matheen Musaddiq, Leon Zhang, Reid Goodbar, Matthew Shaffer, Han Wang, Angela Wang, Mingran Wang, Raghu Prabhakar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Token generation speed is critical to power the next wave of AI inference
applications. GPUs significantly underperform during token generation due to
synchronization overheads at kernel boundaries, utilizing only 21% of their
peak memory bandwidth. While recent dataflow architectures mitigate these
overheads by enabling aggressive fusion of decoder layers into a single kernel,
they too leave performance on the table due to synchronization penalties at
layer boundaries.
  This paper presents kernel looping, a specialized global optimization
technique which exploits an optimization opportunity brought by combining the
unique layer-level fusion possible in modern dataflow architectures with the
repeated layer structure found in language models. Kernel looping eliminates
synchronization costs between consecutive calls to the same kernel by
transforming these calls into a single call to a modified kernel containing a
pipelined outer loop. We evaluate kernel looping on the SambaNova SN40L
Reconfigurable Dataflow Unit (RDU), a commercial dataflow accelerator for AI.
Experiments demonstrate that kernel looping speeds up the decode phase of a
wide array of powerful open-source models by up to 2.2$\times$ on SN40L. Kernel
looping allows scaling of decode performance over multiple SN40L sockets,
achieving speedups of up to 2.5$\times$. Finally, kernel looping enables SN40L
to achieve over 90% of peak performance on 8 and 16 sockets and achieve a
speedup of up to 3.7$\times$ over DGX H100. Kernel looping, as well as the
models evaluated in this paper, are deployed in production in a commercial AI
inference cloud.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Morphological Typology in BPE Subword Productivity and Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iñigo Parra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the impact of morphological typology on tokenization
and language modeling performance. We focus on languages with synthetic and
analytical morphological structures and examine their productivity when
tokenized using the byte-pair encoding (BPE) algorithm. We compare the
performance of models trained with similar amounts of data in different
languages. Our experiments reveal that languages with synthetic features
exhibit greater subword regularity and productivity with BPE tokenization and
achieve better results in language modeling tasks. We also observe that the
typological continuum from linguistic theory is reflected in several
experiments. These findings suggest a correlation between morphological
typology and BPE tokenization efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Positional Bias of Faithfulness for Long-form Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Wan, Jesse Vig, Mohit Bansal, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often exhibit positional bias in long-context
settings, under-attending to information in the middle of inputs. We
investigate the presence of this bias in long-form summarization, its impact on
faithfulness, and various techniques to mitigate this bias. To consistently
evaluate faithfulness, we first compile a benchmark of eight human-annotated
long-form summarization datasets and perform a meta-evaluation of faithfulness
metrics. We show that LLM-based faithfulness metrics, though effective with
full-context inputs, remain sensitive to document order, indicating positional
bias. Analyzing LLM-generated summaries across six datasets, we find a
"U-shaped" trend in faithfulness, where LLMs faithfully summarize the beginning
and end of documents but neglect middle content. Perturbing document order
similarly reveals models are less faithful when important documents are placed
in the middle of the input. We find that this behavior is partly due to
shifting focus with context length: as context increases, summaries become less
faithful, but beyond a certain length, faithfulness improves as the model
focuses on the end. Finally, we experiment with different generation techniques
to reduce positional bias and find that prompting techniques effectively direct
model attention to specific positions, whereas more sophisticated approaches
offer limited improvements. Our data and code are available in
https://github.com/meetdavidwan/longformfact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Uncertainty Ranking: Enhancing In-Context Learning for Long-Tail
  Knowledge in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyang Yu, Runxue Bao, Parminder Bhatia, Taha Kass-Hout, Jiayu Zhou, Cao Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can learn vast amounts of knowledge from diverse
domains during pre-training. However, long-tail knowledge from specialized
domains is often scarce and underrepresented, rarely appearing in the models'
memorization. Prior work has shown that in-context learning (ICL) with
retriever augmentation can help LLMs better capture long-tail knowledge,
reducing their reliance on pre-trained data. Despite these advances, we observe
that LLM predictions for long-tail questions remain uncertain to variations in
retrieved samples. To take advantage of the uncertainty in ICL for guiding LLM
predictions toward correct answers on long-tail samples, we propose a
reinforcement learning-based dynamic uncertainty ranking method for ICL that
accounts for the varying impact of each retrieved sample on LLM predictions.
Our approach prioritizes more informative and stable samples while demoting
misleading ones, updating rankings based on the feedback from the LLM w.r.t.
each retrieved sample. To enhance training efficiency and reduce query costs,
we introduce a learnable dynamic ranking threshold, adjusted when the model
encounters negative prediction shifts. Experimental results on various
question-answering datasets from different domains show that our method
outperforms the best baseline by $2.76\%$, with a notable $5.96\%$ boost in
accuracy on long-tail questions that elude zero-shot inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Multimodal Deep Neural Networks to Disentangle Language from
  Visual Aesthetics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Conwell, Christopher Hamblin, Chelsea Boccagno, David Mayo, Jesse Cummings, Leyla Isik, Andrei Barbu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When we experience a visual stimulus as beautiful, how much of that
experience derives from perceptual computations we cannot describe versus
conceptual knowledge we can readily translate into natural language?
Disentangling perception from language in visually-evoked affective and
aesthetic experiences through behavioral paradigms or neuroimaging is often
empirically intractable. Here, we circumnavigate this challenge by using linear
decoding over the learned representations of unimodal vision, unimodal
language, and multimodal (language-aligned) deep neural network (DNN) models to
predict human beauty ratings of naturalistic images. We show that unimodal
vision models (e.g. SimCLR) account for the vast majority of explainable
variance in these ratings. Language-aligned vision models (e.g. SLIP) yield
small gains relative to unimodal vision. Unimodal language models (e.g. GPT2)
conditioned on visual embeddings to generate captions (via CLIPCap) yield no
further gains. Caption embeddings alone yield less accurate predictions than
image and caption embeddings combined (concatenated). Taken together, these
results suggest that whatever words we may eventually find to describe our
experience of beauty, the ineffable computations of feedforward perception may
provide sufficient foundation for that experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Ontology Learning with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Lo, Albert Q. Jiang, Wenda Li, Mateja Jamnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontologies are useful for automatic machine processing of domain knowledge as
they represent it in a structured format. Yet, constructing ontologies requires
substantial manual effort. To automate part of this process, large language
models (LLMs) have been applied to solve various subtasks of ontology learning.
However, this partial ontology learning does not capture the interactions
between subtasks. We address this gap by introducing OLLM, a general and
scalable method for building the taxonomic backbone of an ontology from
scratch. Rather than focusing on subtasks, like individual relations between
entities, we model entire subcomponents of the target ontology by finetuning an
LLM with a custom regulariser that reduces overfitting on high-frequency
concepts. We introduce a novel suite of metrics for evaluating the quality of
the generated ontology by measuring its semantic and structural similarity to
the ground truth. In contrast to standard metrics, our metrics use deep
learning techniques to define more robust distance measures between graphs.
Both our quantitative and qualitative results on Wikipedia show that OLLM
outperforms subtask composition methods, producing more semantically accurate
ontologies while maintaining structural integrity. We further demonstrate that
our model can be effectively adapted to new domains, like arXiv, needing only a
small number of training examples. Our source code and datasets are available
at https://github.com/andylolu2/ollm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioNCERE: Non-Contrastive Enhancement For Relation Extraction In
  Biomedical Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farshad Noravesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models for relation extraction (RE) in the biomedical domain
consider finetuning BioBERT using classification, but they may suffer from the
anisotropy problem. Contrastive learning methods can reduce this anisotropy
phenomena, and also help to avoid class collapse in any classification problem.
In the present paper, a new training method called biological non-contrastive
relation extraction (BioNCERE) is introduced for relation extraction without
using any named entity labels for training to reduce annotation costs. BioNCERE
uses transfer learning and non-contrastive learning to avoid full or
dimensional collapse as well as bypass overfitting. It resolves RE in three
stages by leveraging transfer learning two times. By freezing the weights
learned in previous stages in the proposed pipeline and by leveraging
non-contrastive learning in the second stage, the model predicts relations
without any knowledge of named entities. Experiments have been done on SemMedDB
that are almost similar to State-of-the-art performance on RE without using the
information of named entities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures, 2 tables, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Context to Action: Analysis of the Impact of State Representation
  and Context on the Generalization of Multi-Turn Web Navigation Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nalin Tiwary, Vardhan Dongre, Sanil Arun Chawla, Ashwin Lamani, Dilek Hakkani-Tür
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Model (LLM)-based frameworks have
extended their capabilities to complex real-world applications, such as
interactive web navigation. These systems, driven by user commands, navigate
web browsers to complete tasks through multi-turn dialogues, offering both
innovative opportunities and significant challenges. Despite the introduction
of benchmarks for conversational web navigation, a detailed understanding of
the key contextual components that influence the performance of these agents
remains elusive. This study aims to fill this gap by analyzing the various
contextual elements crucial to the functioning of web navigation agents. We
investigate the optimization of context management, focusing on the influence
of interaction history and web page representation. Our work highlights
improved agent performance across out-of-distribution scenarios, including
unseen websites, categories, and geographic locations through effective context
management. These findings provide insights into the design and optimization of
LLM-based agents, enabling more accurate and effective web navigation in
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating User Agents for Embodied Conversational-AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Philipov, Vardhan Dongre, Gokhan Tur, Dilek Hakkani-Tür
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied agents designed to assist users with tasks must engage in natural
language interactions, interpret instructions, execute actions, and communicate
effectively to resolve issues. However, collecting large-scale, diverse
datasets of situated human-robot dialogues to train and evaluate such agents is
expensive, labor-intensive, and time-consuming. To address this challenge, we
propose building a large language model (LLM)-based user agent that can
simulate user behavior during interactions with an embodied agent in a virtual
environment. Given a user goal (e.g., make breakfast), at each time step, the
user agent may observe" the robot actions or speak" to either intervene with
the robot or answer questions. Such a user agent assists in improving the
scalability and efficiency of embodied dialogues dataset generation and is
critical for enhancing and evaluating the robot's interaction and task
completion ability, as well as for research in reinforcement learning using AI
feedback. We evaluate our user agent's ability to generate human-like behaviors
by comparing its simulated dialogues with the TEACh dataset. We perform three
experiments: zero-shot prompting to predict dialogue acts, few-shot prompting,
and fine-tuning on the TEACh training subset. Results show the LLM-based user
agent achieves an F-measure of 42% with zero-shot prompting and 43.4% with
few-shot prompting in mimicking human speaking behavior. Through fine-tuning,
performance in deciding when to speak remained stable, while deciding what to
say improved from 51.1% to 62.5%. These findings showcase the feasibility of
the proposed approach for assessing and enhancing the effectiveness of robot
task completion through natural language communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Patient Comments Multi-Label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hajar Sakai, Sarah S. Lam, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patient experience and care quality are crucial for a hospital's
sustainability and reputation. The analysis of patient feedback offers valuable
insight into patient satisfaction and outcomes. However, the unstructured
nature of these comments poses challenges for traditional machine learning
methods following a supervised learning paradigm. This is due to the
unavailability of labeled data and the nuances these texts encompass. This
research explores leveraging Large Language Models (LLMs) in conducting
Multi-label Text Classification (MLTC) of inpatient comments shared after a
stay in the hospital. GPT-4o-Turbo was leveraged to conduct the classification.
However, given the sensitive nature of patients' comments, a security layer is
introduced before feeding the data to the LLM through a Protected Health
Information (PHI) detection framework, which ensures patients'
de-identification. Additionally, using the prompt engineering framework,
zero-shot learning, in-context learning, and chain-of-thought prompting were
experimented with. Results demonstrate that GPT-4o-Turbo, whether following a
zero-shot or few-shot setting, outperforms traditional methods and Pre-trained
Language Models (PLMs) and achieves the highest overall performance with an
F1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the
few-shot learning results. Subsequently, the results' association with other
patient experience structured variables (e.g., rating) was conducted. The study
enhances MLTC through the application of LLMs, offering healthcare
practitioners an efficient method to gain deeper insights into patient feedback
and deliver prompt, appropriate responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve
  Factualness in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hieu Tran, Junda Wang, Yujan Ting, Weijing Huang, Terrence Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable capabilities in various
natural language processing tasks, yet they often struggle with maintaining
factual accuracy, particularly in knowledge-intensive domains like healthcare.
This study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,
a novel approach designed to enhance the factual reliability of LLMs, with a
focus on medical question answering (QA). LEAF utilizes a dual strategy to
enhance the factual accuracy of responses from models such as Llama 3 70B
Instruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,
improves Retrieval-Augmented Generation (RAG) by incorporating fact-checking
results to guide the retrieval process without updating model parameters. The
second strategy, Learning from Fact-Checks via Self-Training, involves
supervised fine-tuning (SFT) on fact-checked responses or applying Simple
Preference Optimization (SimPO) with fact-checking as a ranking mechanism, both
updating LLM parameters from supervision. These findings suggest that
integrating fact-checked responses whether through RAG enhancement or
self-training enhances the reliability and factual correctness of LLM outputs,
offering a promising solution for applications where information accuracy is
crucial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Models Can and Should Embrace the Communicative Nature of
  Human-Generated Math 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasha Boguraev, Ben Lipkin, Leonie Weissweiler, Kyle Mahowald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Math is constructed by people for people: just as natural language corpora
reflect not just propositions but the communicative goals of language users,
the math data that models are trained on reflects not just idealized
mathematical entities but rich communicative intentions. While there are
important advantages to treating math in a purely symbolic manner, we here
hypothesize that there are benefits to treating math as situated linguistic
communication and that language models are well suited for this goal, in ways
that are not fully appreciated. We illustrate these points with two case
studies. First, we ran an experiment in which we found that language models
interpret the equals sign in a humanlike way -- generating systematically
different word problems for the same underlying equation arranged in different
ways. Second, we found that language models prefer proofs to be ordered in
naturalistic ways, even though other orders would be logically equivalent. We
advocate for AI systems that learn from and represent the communicative
intentions latent in human-generated math.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venkata S Govindarajan, Matianyu Zang, Kyle Mahowald, David Beaver, Junyi Jessy Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The variations between in-group and out-group speech (intergroup bias) are
subtle and could underlie many social phenomena like stereotype perpetuation
and implicit bias. In this paper, we model the intergroup bias as a tagging
task on English sports comments from forums dedicated to fandom for NFL teams.
We curate a unique dataset of over 6 million game-time comments from opposing
perspectives (the teams in the game), each comment grounded in a non-linguistic
description of the events that precipitated these comments (live win
probabilities for each team). Expert and crowd annotations justify modeling the
bias through tagging of implicit and explicit referring expressions and reveal
the rich, contextual understanding of language and the world required for this
task. For large-scale analysis of intergroup variation, we use LLMs for
automated tagging, and discover that some LLMs perform best when prompted with
linguistic descriptions of the win probability at the time of the comment,
rather than numerical probability. Further, large-scale tagging of comments
using LLMs uncovers linear variations in the form of referent across win
probabilities that distinguish in-group and out-group utterances. Code and data
are available at https://github.com/venkatasg/intergroup-nfl .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings@EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Optimization Bias of Next-Token Prediction in Linear Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Thrampoulidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate an investigation into the optimization properties of next-token
prediction (NTP), the dominant training paradigm for modern language models.
Specifically, we study the structural properties of the solutions selected by
gradient-based optimizers among the many possible minimizers of the NTP
objective. By framing NTP as cross-entropy minimization across distinct
contexts, each tied with a sparse conditional probability distribution across a
finite vocabulary of tokens, we introduce "NTP-separability conditions" that
enable reaching the data-entropy lower bound. With this setup, and focusing on
linear models with fixed context embeddings, we characterize the optimization
bias of gradient descent (GD): Within the data subspace defined by the sparsity
patterns of distinct contexts, GD selects parameters that equate the logits'
differences of in-support tokens to their log-odds. In the orthogonal subspace,
the GD parameters diverge in norm and select the direction that maximizes a
margin specific to NTP. These findings extend previous research on implicit
bias in one-hot classification to the NTP setting, highlighting key differences
and prompting further research into the optimization and generalization
properties of NTP, irrespective of the specific architecture used to generate
the context embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: fixed typos and writing in various parts; updated figures and
  future-work section</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking LLMs via Uncertainty Quantification <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12794v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12794v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F. Wong, Emine Yilmaz, Shuming Shi, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of open-source Large Language Models (LLMs) from various
institutions has highlighted the urgent need for comprehensive evaluation
methods. However, current evaluation platforms, such as the widely recognized
HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,
which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce
a new benchmarking approach for LLMs that integrates uncertainty
quantification. Our examination involves nine LLMs (LLM series) spanning five
representative natural language processing tasks. Our findings reveal that: I)
LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs
may display greater uncertainty compared to their smaller counterparts; and
III) Instruction-finetuning tends to increase the uncertainty of LLMs. These
results underscore the significance of incorporating uncertainty in the
evaluation of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">BERT</span>s are Generative In-Context Learners <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Samuel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While in-context learning is commonly associated with causal language models,
such as GPT, we demonstrate that this capability also 'emerges' in masked
language models. Through an embarrassingly simple inference technique, we
enable an existing masked model, DeBERTa, to perform generative tasks without
additional training or architectural changes. Our evaluation reveals that the
masked and causal language models behave very differently, as they clearly
outperform each other on different categories of tasks. These complementary
strengths suggest that the field's focus on causal models for in-context
learning may be limiting - both architectures can develop these capabilities,
but with distinct advantages; pointing toward promising hybrid approaches that
combine the strengths of both objectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaskBench: Benchmarking Large Language Models for Task Automation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the remarkable progress of large language models (LLMs) has
sparked interest in task automation, which involves decomposing complex tasks
described by user instructions into sub-tasks and invoking external tools to
execute them, playing a central role in autonomous agents. However, there is a
lack of systematic and standardized benchmarks to promote the development of
LLMs in task automation. To address this, we introduce TaskBench, a
comprehensive framework to evaluate the capability of LLMs in task automation.
Specifically, task automation can be divided into three critical stages: task
decomposition, tool selection, and parameter prediction. To tackle the
complexities inherent in these stages, we introduce the concept of Tool Graph
to represent decomposed tasks and adopt a back-instruct method to generate
high-quality user instructions. We propose TaskEval, a multi-faceted evaluation
methodology that assesses LLM performance across these three stages. Our
approach combines automated construction with rigorous human verification,
ensuring high consistency with human evaluation. Experimental results
demonstrate that TaskBench effectively reflects the capabilities of various
LLMs in task automation. It provides insights into model performance across
different task complexities and domains, pushing the boundaries of what current
models can achieve. TaskBench offers a scalable, adaptable, and reliable
benchmark for advancing LLM-based autonomous agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaCour!: Enabling Research on Argumentation in Hearings of the European
  Court of Human Rights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05061v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05061v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lena Held, Ivan Habernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Why does an argument end up in the final court decision? Was it deliberated
or questioned during the oral hearings? Was there something in the hearings
that triggered a particular judge to write a dissenting opinion? Despite the
availability of the final judgments of the European Court of Human Rights
(ECHR), none of these legal research questions can currently be answered as the
ECHR's multilingual oral hearings are not transcribed, structured, or
speaker-attributed. We address this fundamental gap by presenting LaCour!, the
first corpus of textual oral arguments of the ECHR, consisting of 154 full
hearings (2.1 million tokens from over 267 hours of video footage) in English,
French, and other court languages, each linked to the corresponding final
judgment documents. In addition to the transcribed and partially manually
corrected text from the video, we provide sentence-level timestamps and
manually annotated role and language labels. We also showcase LaCour! in a set
of preliminary experiments that explore the interplay between questions and
dissenting opinions. Apart from the use cases in legal NLP, we hope that law
students or other interested parties will also use LaCour! as a learning
resource, as it is freely available in various formats at
https://huggingface.co/datasets/TrustHLT/LaCour.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revealing Fine-Grained Values and Opinions in Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering latent values and opinions embedded in large language models
(LLMs) can help identify biases and mitigate potential harm. Recently, this has
been approached by prompting LLMs with survey questions and quantifying the
stances in the outputs towards morally and politically charged statements.
However, the stances generated by LLMs can vary greatly depending on how they
are prompted, and there are many ways to argue for or against a given position.
In this work, we propose to address this by analysing a large and robust
dataset of 156k LLM responses to the 62 propositions of the Political Compass
Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform
coarse-grained analysis of their generated stances and fine-grained analysis of
the plain text justifications for those stances. For fine-grained analysis, we
propose to identify tropes in the responses: semantically similar phrases that
are recurrent and consistent across different prompts, revealing natural
patterns in the text that a given LLM is prone to produce. We find that
demographic features added to prompts significantly affect outcomes on the PCT,
reflecting bias, as well as disparities between the results of tests when
eliciting closed-form vs. open domain responses. Additionally, patterns in the
plain text rationales via tropes show that similar justifications are
repeatedly generated across models and prompts even with disparate stances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mathematical Formalized Problem Solving and Theorem Proving in Different
  Fields in Lean 4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xichen Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using computerized verifiable formal languages like Lean 4 to prove
mathematical theorems has a significant impact on mathematical formalization.
Lean 4 offers prominent potential for advancing mathematical reasoning.
However, existing efforts are limited to mathematical formalization languages
in substantial online corpora and are dedicated to keeping pace with rapidly
evolving languages. To bridge the gap between the traditional and computerized
proof, my approach to formalizing theorem proving involves generating formal
steps and complete proofs using Large Language Models (LLMs) based on Natural
Language (NL) proofs. The method is to introduce the basic structure and
tactics in general, determine how AI can assist the mathematical formalization
process to improve its performance, and give examples of solving problems in
Lean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract
algebra.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree of Attacks: Jailbreaking Black-Box LLMs Automatically <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02119v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02119v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, Amin Karbasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) display versatile functionality, they
continue to generate harmful, biased, and toxic content, as demonstrated by the
prevalence of human-designed jailbreaks. In this work, we present Tree of
Attacks with Pruning (TAP), an automated method for generating jailbreaks that
only requires black-box access to the target LLM. TAP utilizes an attacker LLM
to iteratively refine candidate (attack) prompts until one of the refined
prompts jailbreaks the target. In addition, before sending prompts to the
target, TAP assesses them and prunes the ones unlikely to result in jailbreaks,
reducing the number of queries sent to the target LLM. In empirical
evaluations, we observe that TAP generates prompts that jailbreak
state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the
prompts. This significantly improves upon the previous state-of-the-art
black-box methods for generating jailbreaks while using a smaller number of
queries than them. Furthermore, TAP is also capable of jailbreaking LLMs
protected by state-of-the-art guardrails, e.g., LlamaGuard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at NeurIPS 2024. Code:
  https://github.com/RICommunity/TAP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Need a Small Specialized Language Model? Plan Early! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are versatile tools but are not suitable for small
inference budgets. Small models have more efficient inference, but their lower
capacity means that their performance can be good only if one limits their
scope to a specialized domain. This paper explores how to get good specialized
small language models using a large, generic, pretraining set and a limited
amount of specialized data. We consider two scenarios, depending on whether (i)
one can afford pretraining a model for each specialization task, or (ii) one
wants to cheaply adapt a single pretrained model for each task. In the first
scenario, we propose an effective solution based on importance sampling: we
resample the pretraining set to imitate the specialization data and train a
small model on it. In the second scenario, we propose a novel architecture,
projected networks (PN). PN is a large network whose parameters can be linearly
projected into a small network for specialization. For both scenarios, we
demonstrate the empirical effectiveness of our solutions across various
domains, training set sizes, and training budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIO: A Foundation Model on Multimodal Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. Codes and models are available in
  https://github.com/MIO-Team/MIO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Learning Algorithms Do Not Learn Preference Rankings <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19534v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19534v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelica Chen, Sadhika Malladi, Lily H. Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, Kyunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference learning algorithms (e.g., RLHF and DPO) are frequently used to
steer LLMs to produce generations that are more preferred by humans, but our
understanding of their inner workings is still limited. In this work, we study
the conventional wisdom that preference learning trains models to assign higher
likelihoods to more preferred outputs than less preferred outputs, measured via
ranking accuracy. Surprisingly, we find that most state-of-the-art
preference-tuned models achieve a ranking accuracy of less than 60% on common
preference datasets. We furthermore derive the idealized ranking accuracy that
a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective
perfectly. We demonstrate that existing models exhibit a significant alignment
gap -- i.e., a gap between the observed and idealized ranking accuracies. We
attribute this discrepancy to the DPO objective, which is empirically and
theoretically ill-suited to fix even mild ranking errors in the reference
model, and derive a simple and efficient formula for quantifying the difficulty
of learning a given preference datapoint. Finally, we demonstrate that ranking
accuracy strongly correlates with the empirically popular win rate metric when
the model is close to the reference model used in the objective, shedding
further light on the differences between on-policy (e.g., RLHF) and off-policy
(e.g., DPO) preference learning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniJARVIS: Unified Vision-Language-Action Tokenization Enables
  Open-World Instruction Following Agents <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, Yitao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model
for open-world instruction-following agents in Minecraft. Compared to prior
works that either emit textual goals to separate controllers or produce the
control command directly, OmniJARVIS seeks a different path to ensure both
strong reasoning and efficient decision-making capabilities via unified
tokenization of multimodal interaction data. First, we introduce a
self-supervised approach to learn a behavior encoder that produces discretized
tokens for behavior trajectories $\tau = \{o_0, a_0, \dots\}$ and an imitation
learning policy decoder conditioned on these tokens. These additional behavior
tokens will be augmented to the vocabulary of pretrained Multimodal Language
Models. With this encoder, we then pack long-term multimodal interactions
involving task instructions, memories, thoughts, observations, textual
responses, behavior trajectories, etc into unified token sequences and model
them with autoregressive transformers. Thanks to the semantically meaningful
behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing
chain-of-thoughts), plan, answer questions, and act (by producing behavior
tokens for the imitation learning policy decoder). OmniJARVIS demonstrates
excellent performances on a comprehensive collection of atomic, programmatic,
and open-ended tasks in open-world Minecraft. Our analysis further unveils the
crucial design principles in interaction data formation, unified tokenization,
and its scaling potentials. The dataset, models, and code will be released at
https://craftjarvis.org/OmniJARVIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted on NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Personalization in Language Models: A Systematic Study <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Nils Heil, Jiarui Liu, Shehzaad Dhuliawala, Yahang Qi, Bernhard Schölkopf, Rada Mihalcea, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Personalization (IP) is a phenomenon of language models inferring a
user's background from the implicit cues in the input prompts and tailoring the
response based on this inference. While previous work has touched upon various
instances of this problem, there lacks a unified framework to study this
behavior. This work systematically studies IP through a rigorous mathematical
formulation, a multi-perspective moral reasoning framework, and a set of case
studies. Our theoretical foundation for IP relies on a structural causal model
and introduces a novel method, indirect intervention, to estimate the causal
effect of a mediator variable that cannot be directly intervened upon. Beyond
the technical approach, we also introduce a set of moral reasoning principles
based on three schools of moral philosophy to study when IP may or may not be
ethically appropriate. Equipped with both mathematical and ethical insights, we
present three diverse case studies illustrating the varied nature of the IP
problem and offer recommendations for future research. Our code is at
https://github.com/jiarui-liu/IP, and our data is at
https://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Will LLMs Replace the Encoder-Only Models in Temporal Relation
  Classification? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Roccabruna, Massimo Rizzoli, Giuseppe Riccardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic detection of temporal relations among events has been mainly
investigated with encoder-only models such as RoBERTa. Large Language Models
(LLM) have recently shown promising performance in temporal reasoning tasks
such as temporal question answering. Nevertheless, recent studies have tested
the LLMs' performance in detecting temporal relations of closed-source models
only, limiting the interpretability of those results. In this work, we
investigate LLMs' performance and decision process in the Temporal Relation
Classification task. First, we assess the performance of seven open and
closed-sourced LLMs experimenting with in-context learning and lightweight
fine-tuning approaches. Results show that LLMs with in-context learning
significantly underperform smaller encoder-only models based on RoBERTa. Then,
we delve into the possible reasons for this gap by applying explainable
methods. The outcome suggests a limitation of LLMs in this task due to their
autoregressive nature, which causes them to focus only on the last part of the
sequence. Additionally, we evaluate the word embeddings of these two models to
better understand their pre-training differences. The code and the fine-tuned
models can be found respectively on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure-aware Domain Knowledge Injection for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Liu, Ze Chen, Zhihang Fu, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue Wu, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a pioneering methodology, termed StructTuning, to
efficiently transform foundation Large Language Models (LLMs) into domain
specialists. It significantly reduces the training corpus requirement to a mere
0.3%, while achieving an impressive 50% of traditional knowledge injection
performance. Our method is inspired by the educational processes of human
students, particularly how structured domain knowledge from textbooks is
assimilated and subsequently applied to tackle real-world challenges through
specific exercises. Based on this, we propose a novel two-stage strategy for
knowledge injection and alignment: Structure-aware Continual Pre-Training
(SCPT) and Structure-aware Supervised Fine-Tuning (SSFT). In the SCPT phase, we
automatically extract the domain knowledge taxonomy and reorganize the training
corpora, enabling LLMs to effectively link textual segments to targeted
knowledge points within the taxonomy. In the SSFT phase, we explicitly prompt
models to elucidate the underlying knowledge structure in their outputs,
leveraging the structured domain insight to address practical problems. Our
ultimate method has undergone extensive evaluations across model architectures
and scales, using closed-book question-answering tasks on LongBench and
MMedBench datasets. Remarkably, our method demonstrates the potential of
comparable improvement against the state-of-the-art MMedLM2 on MMedBench, while
significantly reducing the training costs to 5%. This breakthrough paves the
way for scaling up our StructTuning for stronger domain-specific LLMs with
comprehensive data utilization. Code is available at
https://github.com/alibaba/struxgpt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code is available at https://github.com/alibaba/struxgpt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database
  Queries <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irina Saparina, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical semantic parsers are expected to understand user utterances and map
them to executable programs, even when these are ambiguous. We introduce a new
benchmark, AMBROSIA, which we hope will inform and inspire the development of
text-to-SQL parsers capable of recognizing and interpreting ambiguous requests.
Our dataset contains questions showcasing three different types of ambiguity
(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,
and corresponding SQL queries. In each case, the ambiguity persists even when
the database context is provided. This is achieved through a novel approach
that involves controlled generation of databases from scratch. We benchmark
various LLMs on AMBROSIA, revealing that even the most advanced models struggle
to identify and interpret ambiguity in questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 D&B Track Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gated Slot Attention for Efficient Linear-Time Sequence Modeling <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, Peng Zhou, Guohong Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear attention Transformers and their gated variants, celebrated for
enabling parallel training and efficient recurrent inference, still fall short
in recall-intensive tasks compared to traditional Transformers and demand
significant resources for training from scratch. This paper introduces Gated
Slot Attention (GSA), which enhances Attention with Bounded-memory-Control
(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention
(GLA). Essentially, GSA comprises a two-layer GLA linked via
$\operatorname{softmax}$, utilizing context-aware memory reading and adaptive
forgetting to improve memory capacity while maintaining compact recurrent state
size. This design greatly enhances both training and inference efficiency
through GLA's hardware-efficient training algorithm and reduced state size.
Additionally, retaining the $\operatorname{softmax}$ operation is particularly
beneficial in "finetuning pretrained Transformers to RNNs" (T2R) settings,
reducing the need for extensive training from scratch. Extensive experiments
confirm GSA's superior performance in scenarios requiring in-context recall and
in T2R settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearning as multi-task optimization: A normalized gradient difference
  approach with an adaptive learning rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Bu, Xiaomeng Jin, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Mingyi Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning has been used to remove unwanted knowledge acquired by
large language models (LLMs). In this paper, we examine machine unlearning from
an optimization perspective, framing it as a regularized multi-task
optimization problem, where one task optimizes a forgetting objective and
another optimizes the model performance. In particular, we introduce a
normalized gradient difference (NGDiff) algorithm, enabling us to have better
control over the trade-off between the objectives, while integrating a new,
automatic learning rate scheduler. We provide a theoretical analysis and
empirically demonstrate the superior performance of NGDiff among
state-of-the-art unlearning methods on the TOFU and MUSE datasets while
exhibiting stable training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProgressGym: Alignment with a Millennium of Moral Progress <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Qiu, Yang Zhang, Xuchuan Huang, Jasmine Xinze Li, Jiaming Ji, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frontier AI systems, including large language models (LLMs), hold increasing
influence over the epistemology of human users. Such influence can reinforce
prevailing societal values, potentially contributing to the lock-in of
misguided moral beliefs and, consequently, the perpetuation of problematic
moral practices on a broad scale. We introduce progress alignment as a
technical solution to mitigate this imminent risk. Progress alignment
algorithms learn to emulate the mechanics of human moral progress, thereby
addressing the susceptibility of existing alignment methods to contemporary
moral blindspots. To empower research in progress alignment, we introduce
ProgressGym, an experimental framework allowing the learning of moral progress
mechanics from history, in order to facilitate future progress in real-world
moral decisions. Leveraging 9 centuries of historical text and 18 historical
LLMs, ProgressGym enables codification of real-world progress alignment
challenges into concrete benchmarks. Specifically, we introduce three core
challenges: tracking evolving values (PG-Follow), preemptively anticipating
moral progress (PG-Predict), and regulating the feedback loop between human and
AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension
are inapplicable to these tasks. In response, we present lifelong and
extrapolative algorithms as baseline methods of progress alignment, and build
an open leaderboard soliciting novel algorithms and challenges. The framework
and the leaderboard are available at
https://github.com/PKU-Alignment/ProgressGym and
https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing LLM's Cognition via Structurization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue Wu, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When reading long-form text, human cognition is complex and structurized.
While large language models (LLMs) process input contexts through a causal and
sequential perspective, this approach can potentially limit their ability to
handle intricate and complex inputs effectively. To enhance LLM's cognition
capability, this paper presents a novel concept of context structurization.
Specifically, we transform the plain, unordered contextual sentences into
well-ordered and hierarchically structurized elements. By doing so, LLMs can
better grasp intricate and extended contexts through precise attention and
information-seeking along the organized structures. Extensive evaluations are
conducted across various model architectures and sizes (including a series of
auto-regressive LLMs as well as BERT-like masking models) on a diverse set of
NLP tasks (e.g., context-based question-answering, exhaustive hallucination
evaluation, and passage-level dense retrieval). Empirical results show
consistent and significant performance gains afforded by a single-round
structurization. In particular, we boost the open-sourced LLaMA2-70B model to
achieve comparable performance against GPT-3.5-Turbo as the hallucination
evaluator. Besides, we show the feasibility of distilling advanced LLMs'
language processing abilities to a smaller yet effective StruXGPT-7B to execute
structurization, addressing the practicality of our approach. Code is available
at https://github.com/alibaba/struxgpt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NeurIPS 2024. Code is available at
  https://github.com/alibaba/struxgpt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Model Agents Simulate Human Trust Behavior? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, James Evans, Philip Torr, Bernard Ghanem, Guohao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) agents have been increasingly adopted as
simulation tools to model humans in social science and role-playing
applications. However, one fundamental question remains: can LLM agents really
simulate human behavior? In this paper, we focus on one elemental behavior in
human interactions, trust, and investigate whether LLM agents can simulate
human trust behavior. We first find that LLM agents generally exhibit trust
behavior, referred to as agent trust, under the framework of Trust Games, which
are widely recognized in behavioral economics. Then, we discover that GPT-4
agents manifest high behavioral alignment with humans in terms of trust
behavior, indicating the feasibility of simulating human trust behavior with
LLM agents. In addition, we probe the biases of agent trust and differences in
agent trust towards other LLM agents and humans. We also explore the intrinsic
properties of agent trust under conditions including external manipulations and
advanced reasoning strategies. Our study provides new insights into the
behaviors of LLM agents and the fundamental analogy between LLMs and humans
beyond value alignment. We further illustrate broader implications of our
discoveries for applications where trust is paramount.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Proceedings of NeurIPS 2024. The first two authors
  contributed equally. 10 pages for main paper, 56 pages including appendix.
  Project website: https://agent-trust.camel-ai.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Proper Treatment of Tokenization in Psycholinguistics <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Giulianelli, Luca Malagutti, Juan Luis Gastaldi, Brian DuSell, Tim Vieira, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are widely used in computational psycholinguistics to test
theories that relate the negative log probability (the surprisal) of a region
of interest (a substring of characters) under a language model to its cognitive
cost experienced by readers, as operationalized, for example, by gaze duration
on the region. However, the application of modern language models to
psycholinguistic studies is complicated by the practice of using tokenization
as an intermediate step in training a model. Doing so results in a language
model over token strings rather than one over character strings. Vexingly,
regions of interest are generally misaligned with these token strings. The
paper argues that token-level language models should be (approximately)
marginalized into character-level language models before they are used in
psycholinguistic studies to compute the surprisal of a region of interest;
then, the marginalized character-level language model can be used to compute
the surprisal of an arbitrary character substring, which we term a focal area,
that the experimenter may wish to use as a predictor. Our proposal of
marginalizing a token-level model into a character-level one solves this
misalignment issue independently of the tokenization scheme. Empirically, we
discover various focal areas whose surprisal is a better psychometric predictor
than the surprisal of the region of interest itself.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main conference long paper at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes
  the Emoji Potential in LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navya Jain, Zekun Wu, Cristian Munoz, Airlie Hilliard, Adriano Koshiyama, Emre Kazim, Philip Treleaven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the demand for human-like interactions with LLMs continues to grow, so
does the interest in manipulating their personality traits, which has emerged
as a key area of research. Methods like prompt-based In-Context Knowledge
Editing (IKE) and gradient-based Model Editor Networks (MEND) have been
explored but show irregularity and variability. IKE depends on the prompt,
leading to variability and sensitivity, while MEND yields inconsistent and
gibberish outputs. To address this, we employed Opinion QA Based
Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank
Adaptation (QLoRA), to manipulate the Big Five personality traits: Openness,
Conscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,
models such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,
despite their absence in the PEFT data. For instance, Llama-2-7B-chat generated
emojis in 99.5\% of extraversion-related test instances, while
Mistral-7B-Instruct did so in 92.5\% of openness-related test instances.
Explainability analysis indicated that the LLMs used emojis intentionally to
express these traits. This paper provides a number of novel contributions.
First, introducing an Opinion QA dataset for PEFT-driven personality
manipulation; second, developing metric models to benchmark LLM personality
traits; third, demonstrating PEFT's superiority over IKE in personality
manipulation; and finally, analysing and validating emoji usage through
explainability methods such as mechanistic interpretability and in-context
learning explainability methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Behavioral Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GITA: Graph to Visual and Textual Integration for Vision-Language Graph
  Reasoning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02130v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02130v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbin Wei, Shuai Fu, Weisen Jiang, Zejian Zhang, Zhixiong Zeng, Qi Wu, James T. Kwok, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used for various tasks with
graph structures. Though LLMs can process graph information in a textual
format, they overlook the rich vision modality, which is an intuitive way for
humans to comprehend structural information and conduct general graph
reasoning. The potential benefits and capabilities of representing graph
structures as visual images (i.e., $\textit{visual graph}$) are still
unexplored. To fill the gap, we innovatively propose an end-to-end framework,
called $\textbf{G}$raph to v$\textbf{I}$sual and $\textbf{T}$extual
Integr$\textbf{A}$tion (GITA), which firstly incorporates visual graphs into
general graph reasoning. Besides, we establish $\textbf{G}$raph-based
$\textbf{V}$ision-$\textbf{L}$anguage $\textbf{Q}$uestion $\textbf{A}$nswering
(GVLQA) dataset from existing graph data, which is the first vision-language
dataset for general graph reasoning purposes. Extensive experiments on the
GVLQA dataset and five real-world datasets show that GITA outperforms
mainstream LLMs in terms of general graph reasoning capabilities. Moreover, We
highlight the effectiveness of the layout augmentation on visual graphs and
pretraining on the GVLQA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; Project Page: v-graph.github.io; Code:
  https://github.com/WEIYanbin1999/GITA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fight Back Against Jailbreaking via <span class="highlight-title">Prompt</span> Adversarial Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06255v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06255v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Mo, Yuji Wang, Zeming Wei, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have achieved tremendous success in
various applications, they are also susceptible to jailbreaking attacks.
Several primary defense strategies have been proposed to protect LLMs from
producing harmful information, mostly focusing on model fine-tuning or
heuristical defense designs. However, how to achieve intrinsic robustness
through prompt optimization remains an open problem. In this paper, motivated
by adversarial training paradigms for achieving reliable robustness, we propose
an approach named Prompt Adversarial Tuning (PAT) that trains a prompt control
attached to the user prompt as a guard prefix. To achieve our defense goal
whilst maintaining natural performance, we optimize the control prompt with
both adversarial and benign prompts. Comprehensive experiments show that our
method is effective against both grey-box and black-box attacks, reducing the
success rate of advanced attacks to nearly 0%, while maintaining the model's
utility on the benign task and incurring only negligible computational
overhead, charting a new perspective for future explorations in LLM security.
Our code is available at https://github.com/PKU-ML/PAT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The FineWeb <span class="highlight-title">Dataset</span>s: Decanting the Web for the Finest Text Data at
  Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of a large language model (LLM) depends heavily on the
quality and size of its pretraining dataset. However, the pretraining datasets
for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly
available and very little is known about how they were created. In this work,
we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl
snapshots that produces better-performing LLMs than other open pretraining
datasets. To advance the understanding of how best to curate high-quality
pretraining datasets, we carefully document and ablate all of the design
choices used in FineWeb, including in-depth investigations of deduplication and
filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion
token collection of educational text filtered from FineWeb. LLMs pretrained on
FineWeb-Edu exhibit dramatically better performance on knowledge- and
reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we
publicly release our data curation codebase and all of the models trained
during our ablation experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoTimes: Autoregressive Time Series Forecasters via Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02370v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02370v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models of time series have not been fully developed due to the
limited availability of time series corpora and the underexploration of
scalable pre-training. Based on the similar sequential formulation of time
series and natural language, increasing research demonstrates the feasibility
of leveraging large language models (LLM) for time series. Nevertheless, the
inherent autoregressive property and decoder-only architecture of LLMs have not
been fully considered, resulting in insufficient utilization of LLM abilities.
To fully revitalize the general-purpose token transition and multi-step
generation capability of large language models, we propose AutoTimes to
repurpose LLMs as autoregressive time series forecasters, which projects time
series into the embedding space of language tokens and autoregressively
generates future predictions with arbitrary lengths. Compatible with any
decoder-only LLMs, the consequent forecaster exhibits the flexibility of the
lookback length and scalability with larger LLMs. Further, we formulate time
series as prompts, extending the context for prediction beyond the lookback
window, termed in-context forecasting. By introducing LLM-embedded textual
timestamps, AutoTimes can utilize chronological information to align
multivariate time series. Empirically, AutoTimes achieves state-of-the-art with
0.1% trainable parameters and over $5\times$ training/inference speedup
compared to advanced LLM-based forecasters. Code is available at this
repository: https://github.com/thuml/AutoTimes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mars: Situated Inductive Reasoning in an Open-World Environment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojuan Tang, Jiaqi Li, Yitao Liang, Song-chun Zhu, Muhan Zhang, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) trained on massive corpora have shown remarkable
success in knowledge-intensive tasks. Yet, most of them rely on pre-stored
knowledge. Inducing new general knowledge from a specific environment and
performing reasoning with the acquired knowledge -- \textit{situated inductive
reasoning}, is crucial and challenging for machine intelligence. In this paper,
we design Mars, an interactive environment devised for situated inductive
reasoning. It introduces counter-commonsense game mechanisms by modifying
terrain, survival setting and task dependency while adhering to certain
principles. In Mars, agents need to actively interact with their surroundings,
derive useful rules and perform decision-making tasks in specific contexts. We
conduct experiments on various RL-based and LLM-based methods, finding that
they all struggle on this challenging situated inductive reasoning benchmark.
Furthermore, we explore \textit{Induction from Reflection}, where we instruct
agents to perform inductive reasoning from history trajectory. The superior
performance underscores the importance of inductive reasoning in Mars. Through
Mars, we aim to galvanize advancements in situated inductive reasoning and set
the stage for developing the next generation of AI systems that can reason in
an adaptive and context-sensitive way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 Track Datasets and Benchmarks. Project page:
  https://marscrafter.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larissa Pusch, Tim O. F. Conrad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in natural language processing have revolutionized the way we
can interact with digital information systems, such as databases, making them
more accessible. However, challenges persist, especially when accuracy is
critical, as in the biomedical domain. A key issue is the hallucination
problem, where models generate information unsupported by the underlying data,
potentially leading to dangerous misinformation. This paper presents a novel
approach designed to bridge this gap by combining Large Language Models (LLM)
and Knowledge Graphs (KG) to improve the accuracy and reliability of
question-answering systems, on the example of a biomedical KG. Built on the
LangChain framework, our method incorporates a query checker that ensures the
syntactical and semantic validity of LLM-generated queries, which are then used
to extract information from a Knowledge Graph, substantially reducing errors
like hallucinations. We evaluated the overall performance using a new benchmark
dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo
and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other
models in generating accurate queries, open-source models like llama3:70b show
promise with appropriate prompt engineering. To make this approach accessible,
a user-friendly web-based interface has been developed, allowing users to input
natural language queries, view generated and corrected Cypher queries, and
verify the resulting paths for accuracy. Overall, this hybrid approach
effectively addresses common issues such as data gaps and hallucinations,
offering a reliable and intuitive solution for question answering systems. The
source code for generating the results of this paper and for the user-interface
can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11200v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11200v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) agents have demonstrated impressive capabilities
in utilizing external tools and knowledge to boost accuracy and reduce
hallucinations. However, developing prompting techniques that enable LLM agents
to effectively use these tools and knowledge remains a heuristic and
labor-intensive task. Here, we introduce AvaTaR, a novel and automated
framework that optimizes an LLM agent to effectively leverage provided tools,
improving performance on a given task. During optimization, we design a
comparator module to iteratively deliver insightful and comprehensive prompts
to the LLM agent by contrastively reasoning between positive and negative
examples sampled from training data. We demonstrate AvaTaR on four complex
multimodal retrieval datasets featuring textual, visual, and relational
information, and three general question-answering (QA) datasets. We find AvaTaR
consistently outperforms state-of-the-art approaches across all seven tasks,
exhibiting strong generalization ability when applied to novel cases and
achieving an average relative improvement of 14% on the Hit@1 metric for the
retrieval datasets and 13% for the QA datasets. Code and dataset are available
at https://github.com/zou-group/avatar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in
  Low-Resource and Extinct Languages <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06196v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06196v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew M. Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan A. Chi, Ryan Chi, Scott A. Hale, Hannah Rose Kirk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the LingOly benchmark, a novel benchmark for
advanced reasoning abilities in large language models. Using challenging
Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context
identification and generalisation of linguistic patterns in very low-resource
or extinct languages, and (ii) abilities to follow complex task instructions.
The LingOly benchmark covers more than 90 mostly low-resource languages,
minimising issues of data contamination, and contains 1,133 problems across 6
formats and 5 levels of human difficulty. We assess performance with both
direct accuracy and comparison to a no-context baseline to penalise
memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to
be challenging, and models perform poorly on the higher difficulty problems. On
harder problems, even the top model only achieved 38.7% accuracy, a 24.7%
improvement over the no-context baseline. Large closed models typically
outperform open models, and in general, the higher resource the language, the
better the scores. These results indicate, in absence of memorisation, true
multi-step out-of-domain reasoning remains a challenge for current language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral presentation at NeurIPS 2024 Datasets and Benchmarks Track. 10
  pages, 5 figures, 22 pages supplemental materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERBench: An Entity-Relationship based Automatically Verifiable
  Hallucination Benchmark for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jio Oh, Soyeon Kim, Junseok Seo, Jindong Wang, Ruochen Xu, Xing Xie, Steven Euijong Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved unprecedented performances in
various applications, yet evaluating them is still challenging. Existing
benchmarks are either manually constructed or are automatic, but lack the
ability to evaluate the thought process of LLMs with arbitrary complexity. We
contend that utilizing existing relational databases based on the
entity-relationship (ER) model is a promising approach for constructing
benchmarks as they contain structured knowledge that can be used to question
LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational
databases have integrity constraints that can be used to better construct
complex in-depth questions and verify answers: (1) functional dependencies can
be used to pinpoint critical keywords that an LLM must know to properly answer
a given question containing certain attribute values; and (2) foreign key
constraints can be used to join relations and construct multi-hop questions,
which can be arbitrarily long and used to debug intermediate answers. We thus
propose ERBench, which uses these integrity constraints to convert any database
into an LLM benchmark. ERBench supports continuous evaluation as databases
change, multimodal questions, and various prompt engineering techniques. In our
experiments, we construct LLM benchmarks using databases of multiple domains
and make an extensive comparison of contemporary LLMs. We show how ERBench can
properly evaluate any LLM by not only checking for answer correctness, but also
effectively verifying the rationales by looking for the right keywords.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attend First, Consolidate Later: On the Importance of Attention in
  Different LLM Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Ben-Artzy, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In decoder-based LLMs, the representation of a given layer serves two
purposes: as input to the next layer during the computation of the current
token; and as input to the attention mechanism of future tokens. In this work,
we show that the importance of the latter role might be overestimated. To show
that, we start by manipulating the representations of previous tokens; e.g. by
replacing the hidden states at some layer k with random vectors. Our
experimenting with four LLMs and four tasks show that this operation often
leads to small to negligible drop in performance. Importantly, this happens if
the manipulation occurs in the top part of the model-k is in the final 30-50%
of the layers. In contrast, doing the same manipulation in earlier layers might
lead to chance level performance. We continue by switching the hidden state of
certain tokens with hidden states of other tokens from another prompt; e.g.,
replacing the word "Italy" with "France" in "What is the capital of Italy?". We
find that when applying this switch in the top 1/3 of the model, the model
ignores it (answering "Rome"). However if we apply it before, the model
conforms to the switch ("Paris"). Our results hint at a two stage process in
transformer-based LLMs: the first part gathers input from previous tokens,
while the second mainly processes that information internally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of
  Language Models for Fact Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denitsa Saynova, Lovisa Hagström, Moa Johansson, Richard Johansson, Marco Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous interpretations of language models (LMs) miss important distinctions
in how these models process factual information. For example, given the query
"Astrid Lindgren was born in" with the corresponding completion "Sweden", no
difference is made between whether the prediction was based on having the exact
knowledge of the birthplace of the Swedish author or assuming that a person
with a Swedish-sounding name was born in Sweden. In this paper, we investigate
four different prediction scenarios for which the LM can be expected to show
distinct behaviors. These scenarios correspond to different levels of model
reliability and types of information being processed - some being less
desirable for factual predictions. To facilitate precise interpretations of LMs
for fact completion, we propose a model-specific recipe called PrISM for
constructing datasets with examples of each scenario based on a set of
diagnostic criteria. We apply a popular interpretability method, causal tracing
(CT), to the four prediction scenarios and find that while CT produces
different results for each scenario, aggregations over a set of mixed examples
may only represent the results from the scenario with the strongest measured
signal. In summary, we contribute tools for a more granular study of fact
completion in language models and analyses that provide a more nuanced
understanding of how LMs process fact-related queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00072v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00072v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Yukai Miao, Li Chen, Dawei Wang, Dan Li, Yanyu Ren, Hongtao Xie, Ce Yang, Xuhui Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RAG systems face limitations when semantic relevance alone does not guarantee
improved generation quality. This issue becomes particularly evident due to the
sensitivity of large language models (LLMs) to the ordering of few-shot
prompts, which can affect model performance. To address this challenge,
aligning LLM outputs with human preferences using structured feedback, such as
options to copy, regenerate, or dislike, offers a promising method for
improvement. This feedback is applied to the entire list of inputs rather than
giving specific ratings for individual documents, making it a Listwide Labels
Learning-to-Rank task.
  To address this task, we propose Pistis-RAG, a new RAG framework designed
with a content-centric approach to better align LLMs with human preferences.
Pistis-RAG effectively utilizes human feedback, enhancing content ranking and
generation quality. To validate our framework, we use public datasets to
simulate human feedback, allowing us to evaluate and refine our method
effectively. Experimental results indicate that Pistis-RAG improves alignment
with human preferences relative to the baseline RAG system, showing a 6.06%
increase in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy
metrics. These results highlight Pistis-RAG's effectiveness in overcoming the
limitations associated with traditional RAG approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Complex Instruction-Following with Multiple Constraints
  Composition <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, Yiming Liu, Jie Tang, Hongning Wang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction following is one of the fundamental capabilities of large
language models (LLMs). As the ability of LLMs is constantly improving, they
have been increasingly applied to deal with complex human instructions in
real-world scenarios. Therefore, how to evaluate the ability of complex
instruction-following of LLMs has become a critical research problem. Existing
benchmarks mainly focus on modeling different types of constraints in human
instructions while neglecting the composition of different constraints, which
is an indispensable constituent in complex instructions. To this end, we
propose ComplexBench, a benchmark for comprehensively evaluating the ability of
LLMs to follow complex instructions composed of multiple constraints. We
propose a hierarchical taxonomy for complex instructions, including 4
constraint types, 19 constraint dimensions, and 4 composition types, and
manually collect a high-quality dataset accordingly. To make the evaluation
reliable, we augment LLM-based evaluators with rules to effectively verify
whether generated texts can satisfy each constraint and composition.
Furthermore, we obtain the final evaluation score based on the dependency
structure determined by different composition types. ComplexBench identifies
significant deficiencies in existing LLMs when dealing with complex
instructions with multiple constraints composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Unlearning via Embedding-Corrupted <span class="highlight-title">Prompt</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Yuhao Liu, Yaxuan Wang, Jeffrey Flanigan, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have advanced to encompass extensive knowledge
across diverse domains. Yet controlling what a large language model should not
know is important for ensuring alignment and thus safe use. However, accurately
and efficiently unlearning knowledge from an LLM remains challenging due to the
potential collateral damage caused by the fuzzy boundary between retention and
forgetting, and the large computational requirements for optimization across
state-of-the-art models with hundreds of billions of parameters. In this work,
we present \textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning
framework for large language models to address both the challenges of knowledge
entanglement and unlearning efficiency. Instead of relying on the LLM itself to
unlearn, we enforce an unlearned state during inference by employing a prompt
classifier to identify and safeguard prompts to forget. We learn corruptions
added to prompt embeddings via zeroth order optimization toward the unlearning
objective offline and corrupt prompts flagged by the classifier during
inference. We find that these embedding-corrupted prompts not only lead to
desirable outputs that satisfy the unlearning objective but also closely
approximate the output from a model that has never been trained on the data
intended for forgetting. Through extensive experiments on unlearning, we
demonstrate the superiority of our method in achieving promising unlearning at
\textit{nearly zero side effects} in general domains and domains closely
related to the unlearned ones. Additionally, we highlight the scalability of
our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no
additional cost as the number of parameters increases. We have made our code
publicly available at \url{https://github.com/chrisliu298/llm-unlearn-eco}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Language Models Replace Programmers? REPOCOD Says 'Not Yet' 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved high accuracy, i.e., more than 90
pass@1, in solving Python coding problems in HumanEval and MBPP. Thus, a
natural question is, whether LLMs achieve comparable code completion
performance compared to human developers? Unfortunately, one cannot answer this
question using existing manual crafted or simple (e.g., single-line) code
generation benchmarks, since such tasks fail to represent real-world software
development tasks. In addition, existing benchmarks often use poor code
correctness metrics, providing misleading conclusions.
  To address these challenges, we create REPOCOD, a code generation benchmark
with 980 problems collected from 11 popular real-world projects, with more than
58% of them requiring file-level or repository-level context information. In
addition, REPOCOD has the longest average canonical solution length (331.6
tokens) and the highest average cyclomatic complexity (9.00) compared to
existing benchmarks. Each task in REPOCOD includes 313.5 developerwritten test
cases on average for better correctness evaluation. In our evaluations of ten
LLMs, none of the models achieve more than 30 pass@1 on REPOCOD, indicating the
necessity of building stronger LLMs that can help developers in real-world
software development. REPOCOD is available at
https://github.com/ltasset/REPOCOD
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Generative Information Extraction: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17617v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17617v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information extraction (IE) aims to extract structural knowledge from plain
natural language texts. Recently, generative Large Language Models (LLMs) have
demonstrated remarkable capabilities in text understanding and generation. As a
result, numerous works have been proposed to integrate LLMs for IE tasks based
on a generative paradigm. To conduct a comprehensive systematic review and
exploration of LLM efforts for IE tasks, in this study, we survey the most
recent advancements in this field. We first present an extensive overview by
categorizing these works in terms of various IE subtasks and techniques, and
then we empirically analyze the most advanced methods and discover the emerging
trend of IE tasks with LLMs. Based on a thorough review conducted, we identify
several insights in technique and promising research directions that deserve
further exploration in future studies. We maintain a public repository and
consistently update related works and resources on GitHub
(\href{https://github.com/quqxui/Awesome-LLM4IE-Papers}{LLM4IE repository})
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted by Frontiers of Computer Science (FCS),
  with the DOI: {10.1007/s11704-024-40555-y}. You can cite the FCS version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Longitudinal Analysis of Racial and Gender Bias in New York Times and
  Fox News Images and Articles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazem Ibrahim, Nouar AlDahoul, Syed Mustafa Ali Abbasi, Fareed Zaffar, Talal Rahwan, Yasir Zaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The manner in which different racial and gender groups are portrayed in news
coverage plays a large role in shaping public opinion. As such, understanding
how such groups are portrayed in news media is of notable societal value, and
has thus been a significant endeavour in both the computer and social sciences.
Yet, the literature still lacks a longitudinal study examining both the
frequency of appearance of different racial and gender groups in online news
articles, as well as the context in which such groups are discussed. To fill
this gap, we propose two machine learning classifiers to detect the race and
age of a given subject. Next, we compile a dataset of 123,337 images and
441,321 online news articles from New York Times (NYT) and Fox News (Fox), and
examine representation through two computational approaches. Firstly, we
examine the frequency and prominence of appearance of racial and gender groups
in images embedded in news articles, revealing that racial and gender
minorities are largely under-represented, and when they do appear, they are
featured less prominently compared to majority groups. Furthermore, we find
that NYT largely features more images of racial minority groups compared to
Fox. Secondly, we examine both the frequency and context with which racial
minority groups are presented in article text. This reveals the narrow scope in
which certain racial groups are covered and the frequency with which different
groups are presented as victims and/or perpetrators in a given conflict. Taken
together, our analysis contributes to the literature by providing two novel
open-source classifiers to detect race and age from images, and shedding light
on the racial and gender biases in news articles from venues on opposite ends
of the American political spectrum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, and 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain of Preference Optimization: Improving Chain-of-Thought Reasoning
  in LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development of chain-of-thought (CoT) decoding has enabled large
language models (LLMs) to generate explicit logical reasoning paths for complex
problem-solving. However, research indicates that these paths are not always
deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching
to extensively explore the reasoning space and find better reasoning paths that
CoT decoding might overlook. This deliberation, however, comes at the cost of
significantly increased inference complexity. In this work, we demonstrate that
fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to
achieve similar or better performance, thereby avoiding the substantial
inference burden. This is achieved through Chain of Preference Optimization
(CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths
with those of ToT using the inherent preference information in the tree-search
process. Extensive experimental results show that CPO significantly improves
LLM performance in solving a variety of complex problems, including question
answering, fact verification, and arithmetic reasoning, demonstrating its
effectiveness. Our code is available at https://github.com/sail-sg/CPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Narrative Feature or Structured Feature? A Study of Large Language
  Models to Identify Cancer Patients at Risk of Heart Failure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chen, Mengyuan Zhang, Mustafa Mohammed Ahmed, Yi Guo, Thomas J. George, Jiang Bian, Yonghui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer treatments are known to introduce cardiotoxicity, negatively impacting
outcomes and survivorship. Identifying cancer patients at risk of heart failure
(HF) is critical to improving cancer treatment outcomes and safety. This study
examined machine learning (ML) models to identify cancer patients at risk of HF
using electronic health records (EHRs), including traditional ML, Time-Aware
long short-term memory (T-LSTM), and large language models (LLMs) using novel
narrative features derived from the structured medical codes. We identified a
cancer cohort of 12,806 patients from the University of Florida Health,
diagnosed with lung, breast, and colorectal cancers, among which 1,602
individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the
best F1 scores, outperforming the traditional support vector machines by 0.397,
the T-LSTM deep learning model by 0.077, and a widely used transformer model,
BERT, by 0.056. The analysis shows that the proposed narrative features
remarkably increased feature density and improved performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How does Architecture Influence the Base Capabilities of <span class="highlight-title">Pre-train</span>ed
  Language Models? A Case Study Based on FFN-Wider and MoE <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02436v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02436v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Lu, Yanyan Zhao, Bing Qin, Liangyu Huo, Qing Yang, Dongliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models have been proven to possess strong base
capabilities, which not only excel in in-distribution language modeling but
also show powerful abilities in out-of-distribution language modeling, transfer
learning and few-shot learning. Unlike existing work focusing on the influence
of scale on base capabilities, our work examines the influence of architecture
on those. Specifically, our concern is: How does architecture influence the
base capabilities of pre-trained language models? In this work, we attempt to
explain and reverse the decline in base capabilities caused by the architecture
of FFN-Wider Transformers, seeking to provide some insights. Through analysis,
we found the contribution ratio of Multi-Head Attention (a combination
function) to pre-trained language modeling is a key factor affecting base
capabilities. FFN-Wider Transformers reduce the contribution ratio of this
combination function, leading to a decline in base capabilities. We confirmed
this by experiments and proposed Combination Enhanced Architecture (CEA) to
address the decline in base capabilities of such models. Significantly, we
extended our explanation and CEA to Mixture of Experts (MoE) Transformers. We
successfully achieved significant improvements in base capabilities on a 14B
parameter MoE model, demonstrating the practical application value of our work.
This also indicates that our analysis has a certain guiding significance for
architecture analysis, architecture improvement and architecture design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMCBench: Benchmarking Large Language Model Compression for Efficient
  Deployment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Yang, Changyi He, Jinyang Guo, Jianyu Wu, Yifu Ding, Aishan Liu, Haotong Qin, Pengliang Ji, Xianglong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have demonstrated their strong
intelligence ability, the high demand for computation and storage hinders their
practical application. To this end, many model compression techniques are
proposed to increase the efficiency of LLMs. However, current researches only
validate their methods on limited models, datasets, metrics, etc, and still
lack a comprehensive evaluation under more general scenarios. So it is still a
question of which model compression approach we should use under a specific
case. To mitigate this gap, we present the Large Language Model Compression
Benchmark (LLMCBench), a rigorously designed benchmark with an in-depth
analysis for LLM compression algorithms. We first analyze the actual model
production requirements and carefully design evaluation tracks and metrics.
Then, we conduct extensive experiments and comparison using multiple mainstream
LLM compression approaches. Finally, we perform an in-depth analysis based on
the evaluation and provide useful insight for LLM compression design. We hope
our LLMCBench can contribute insightful suggestions for LLM compression
algorithm design and serve as a foundation for future research. Our code is
available at https://github.com/AboveParadise/LLMCBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Unified View of Preference Learning for Large Language Models:
  A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02795v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02795v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bofei Gao, Feifan Song, Yibo Miao, Zefan Cai, Zhe Yang, Liang Chen, Helan Hu, Runxin Xu, Qingxiu Dong, Ce Zheng, Shanghaoran Quan, Wen Xiao, Ge Zhang, Daoguang Zan, Keming Lu, Bowen Yu, Dayiheng Liu, Zeyu Cui, Jian Yang, Lei Sha, Houfeng Wang, Zhifang Sui, Peiyi Wang, Tianyu Liu, Baobao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of
the crucial factors to achieve success is aligning the LLM's output with human
preferences. This alignment process often requires only a small amount of data
to efficiently enhance the LLM's performance. While effective, research in this
area spans multiple domains, and the methods involved are relatively complex to
understand. The relationships between different methods have been
under-explored, limiting the development of the preference alignment. In light
of this, we break down the existing popular alignment strategies into different
components and provide a unified framework to study the current alignment
strategies, thereby establishing connections among them. In this survey, we
decompose all the strategies in preference learning into four components:
model, data, feedback, and algorithm. This unified view offers an in-depth
understanding of existing alignment algorithms and also opens up possibilities
to synergize the strengths of different strategies. Furthermore, we present
detailed working examples of prevalent existing algorithms to facilitate a
comprehensive understanding for the readers. Finally, based on our unified
perspective, we explore the challenges and future research directions for
aligning large language models with human preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Microstructures and Accuracy of Graph Recall by Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11821v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11821v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbang Wang, Hejie Cui, Jon Kleinberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs data is crucial for many applications, and much of it exists in the
relations described in textual format. As a result, being able to accurately
recall and encode a graph described in earlier text is a basic yet pivotal
ability that LLMs need to demonstrate if they are to perform reasoning tasks
that involve graph-structured information. Human performance at graph recall
has been studied by cognitive scientists for decades, and has been found to
often exhibit certain structural patterns of bias that align with human
handling of social relationships. To date, however, we know little about how
LLMs behave in analogous graph recall tasks: do their recalled graphs also
exhibit certain biased patterns, and if so, how do they compare with humans and
affect other graph reasoning tasks? In this work, we perform the first
systematical study of graph recall by LLMs, investigating the accuracy and
biased microstructures (local structural patterns) in their recall. We find
that LLMs not only underperform often in graph recall, but also tend to favor
more triangles and alternating 2-paths. Moreover, we find that more advanced
LLMs have a striking dependence on the domain that a real-world graph comes
from -- by yielding the best recall accuracy when the graph is narrated in a
language style consistent with its original domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024; Code available at:
  https://github.com/Abel0828/llm-graph-recall</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Factuality of Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02420v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02420v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Georgiev, Rocktim Jyoti Das, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), especially when instruction-tuned for chat,
have become part of our daily lives, freeing people from the process of
searching, extracting, and integrating information from multiple sources by
offering a straightforward answer to a variety of questions in a single place.
Unfortunately, in many cases, LLM responses are factually incorrect, which
limits their applicability in real-world scenarios. As a result, research on
evaluating and improving the factuality of LLMs has attracted a lot of
attention recently. In this survey, we critically analyze existing work with
the aim to identify the major challenges and their associated causes, pointing
out to potential solutions for improving the factuality of LLMs, and analyzing
the obstacles to automated factuality evaluation for open-ended text
generation. We further offer an outlook on where future research should go.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure and 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Machines Resonate with Humans? Evaluating the Emotional and Empathic
  Comprehension of LMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Arslan Manzoor, Yuxia Wang, Minghan Wang, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathy plays a pivotal role in fostering prosocial behavior, often triggered
by the sharing of personal experiences through narratives. However, modeling
empathy using NLP approaches remains challenging due to its deep
interconnection with human interaction dynamics. Previous approaches, which
involve fine-tuning language models (LMs) on human-annotated empathic datasets,
have had limited success. In our pursuit of improving empathy understanding in
LMs, we propose several strategies, including contrastive learning with masked
LMs and supervised fine-tuning with large language models. While these methods
show improvements over previous methods, the overall results remain
unsatisfactory. To better understand this trend, we performed an analysis which
reveals a low agreement among annotators. This lack of consensus hinders
training and highlights the subjective nature of the task. We also explore the
cultural impact on annotations. To study this, we meticulously collected story
pairs in Urdu language and find that subjectivity in interpreting empathy among
annotators appears to be independent of cultural background. Our systematic
exploration of LMs' understanding of empathy reveals substantial opportunities
for further investigation in both task formulation and modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do not think about pink elephant! <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyomin Hwang, Suyoung Kim, JunHoo Lee, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Models (LMs) have heightened expectations for the potential of general
AI as they are akin to human intelligence. This paper shows that recent large
models such as Stable Diffusion and DALL-E3 also share the vulnerability of
human intelligence, namely the "white bear phenomenon". We investigate the
causes of the white bear phenomenon by analyzing their representation space.
Based on this analysis, we propose a simple prompt-based attack method, which
generates figures prohibited by the LM provider's policy. To counter these
attacks, we introduce prompt-based defense strategies inspired by cognitive
therapy techniques, successfully mitigating attacks by up to 48.22\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in CVPR 2024 Responsible Generative AI
  Workshop (ReGenAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs
  Without Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10862v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10862v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adib Hasan, Ileana Rugina, Alex Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the impact of model compression on the way Large
Language Models (LLMs) process prompts, particularly concerning jailbreak
resistance. We show that moderate WANDA pruning can enhance resistance to
jailbreaking attacks without fine-tuning, while maintaining performance on
standard benchmarks. To systematically evaluate this safety enhancement, we
introduce a dataset of 225 harmful tasks across five categories. Our analysis
of LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 reveals that pruning
benefits correlate with initial model safety levels. We interpret these results
by examining changes in attention patterns and perplexity shifts, demonstrating
that pruned models exhibit sharper attention and increased sensitivity to
artificial jailbreak constructs. We extend our evaluation to the AdvBench
harmful behavior tasks and the GCG attack method. We find that LLaMA-2 is much
safer on AdvBench prompts than on our dataset when evaluated with manual
jailbreak attempts, and that pruning is effective against both automated
attacks and manual jailbreaking on Advbench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 7th BlackboxNLP Workshop: Analyzing and
  Interpreting Neural Networks for NLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linking In-context Learning in <span class="highlight-title">Transformer</span>s to Human Episodic Memory <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Ji-An, Corey Y. Zhou, Marcus K. Benna, Marcelo G. Mattar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding connections between artificial and biological intelligent
systems can reveal fundamental principles of general intelligence. While many
artificial intelligence models have a neuroscience counterpart, such
connections are largely missing in Transformer models and the self-attention
mechanism. Here, we examine the relationship between interacting attention
heads and human episodic memory. We focus on induction heads, which contribute
to in-context learning in Transformer-based large language models (LLMs). We
demonstrate that induction heads are behaviorally, functionally, and
mechanistically similar to the contextual maintenance and retrieval (CMR) model
of human episodic memory. Our analyses of LLMs pre-trained on extensive text
data show that CMR-like heads often emerge in the intermediate and late layers,
qualitatively mirroring human memory biases. The ablation of CMR-like heads
suggests their causal role in in-context learning. Our findings uncover a
parallel between the computational mechanisms of LLMs and human memory,
offering valuable insights into both research fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Baichuan Alignment Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingan Lin, Fan Yang, Yanjun Shen, Haoze Sun, Tianpeng Li, Tao Zhang, Chenzheng Zhu, Tao Zhang, Miao Zheng, Xu Li, Yijie Zhou, Mingyang Chen, Yanzhao Qin, Youquan Li, Hao Liang, Fei Li, Yadong Li, Mang Wang, Guosheng Dong, Kun Fang, Jianhua Xu, Bin Cui, Wentao Zhang, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Baichuan Alignment, a detailed analysis of the alignment
techniques employed in the Baichuan series of models. This represents the
industry's first comprehensive account of alignment methodologies, offering
valuable insights for advancing AI research. We investigate the critical
components that enhance model performance during the alignment process,
including optimization methods, data strategies, capability enhancements, and
evaluation processes. The process spans three key stages: Prompt Augmentation
System (PAS), Supervised Fine-Tuning (SFT), and Preference Alignment. The
problems encountered, the solutions applied, and the improvements made are
thoroughly recorded.
  Through comparisons across well-established benchmarks, we highlight the
technological advancements enabled by Baichuan Alignment. Baichuan-Instruct is
an internal model, while Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct
versions of the Qwen2-72B and Llama-3-70B base models, optimized through
Baichuan Alignment. Baichuan-Instruct demonstrates significant improvements in
core capabilities, with user experience gains ranging from 17% to 28%, and
performs exceptionally well on specialized benchmarks. In open-source benchmark
evaluations, both Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently
outperform their respective official instruct versions across nearly all
datasets. This report aims to clarify the key technologies behind the alignment
process, fostering a deeper understanding within the community.
Llama3-PBM-Nova-70B model is available at
https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization
  <span class="highlight-title">Dataset</span> <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Roush, Yusuf Shabazz, Arvind Balaji, Peter Zhang, Stefano Mezza, Markus Zhang, Sanjay Basu, Sriram Vishwanath, Mehdi Fatemi, Ravid Shwartz-Ziv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce OpenDebateEvidence, a comprehensive dataset for argument mining
and summarization sourced from the American Competitive Debate community. This
dataset includes over 3.5 million documents with rich metadata, making it one
of the most extensive collections of debate evidence. OpenDebateEvidence
captures the complexity of arguments in high school and college debates,
providing valuable resources for training and evaluation. Our extensive
experiments demonstrate the efficacy of fine-tuning state-of-the-art large
language models for argumentative abstractive summarization across various
methods, models, and datasets. By providing this comprehensive resource, we aim
to advance computational argumentation and support practical applications for
debaters, educators, and researchers. OpenDebateEvidence is publicly available
to support further research and innovation in computational argumentation.
Access it here: https://huggingface.co/datasets/Yusuf5/OpenCaselist
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published to the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-adaptive Zero-shot Chain-of-Thought <span class="highlight-title">Prompt</span>ing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20441v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20441v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosong Yuan, Chen Shen, Shaotian Yan, Xiaofeng Zhang, Liang Xie, Wenxiao Wang, Renchu Guan, Ying Wang, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective
strategy for enhancing the performance of large language models (LLMs) in
real-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level
prompt uniformly applied across the whole of instances is inherently limited
since one prompt cannot be a good partner for all, a more appropriate approach
should consider the interaction between the prompt and each instance
meticulously. This work introduces an instance-adaptive prompting algorithm as
an alternative zero-shot CoT reasoning scheme by adaptively differentiating
good and bad prompts. Concretely, we first employ analysis on LLMs through the
lens of information flow to detect the mechanism under zero-shot CoT reasoning,
in which we discover that information flows from question to prompt and
question to rationale jointly influence the reasoning results most. We notice
that a better zero-shot CoT reasoning needs the prompt to obtain semantic
information from the question then the rationale aggregates sufficient
information from the question directly and via the prompt indirectly. On the
contrary, lacking any of those would probably lead to a bad one. Stem from
that, we further propose an instance-adaptive prompting strategy (IAP) for
zero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen
on math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal
Judgement) obtain consistent improvement, demonstrating that the
instance-adaptive zero-shot CoT prompting performs better than other task-level
methods with some curated prompts or sophisticated procedures, showing the
significance of our findings in the zero-shot CoT reasoning mechanism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ uOttawa at LegalLens-2024: <span class="highlight-title">Transformer</span>-based Classification Experiments <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nima Meghdadi, Diana Inkpen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the methods used for LegalLens-2024 shared task, which
focused on detecting legal violations within unstructured textual data and
associating these violations with potentially affected individuals. The shared
task included two subtasks: A) Legal Named Entity Recognition (L-NER) and B)
Legal Natural Language Inference (L-NLI). For subtask A, we utilized the spaCy
library, while for subtask B, we employed a combined model incorporating
RoBERTa and CNN. Our results were 86.3% in the L-NER subtask and 88.25% in the
L-NLI subtask. Overall, our paper demonstrates the effectiveness of transformer
models in addressing complex tasks in the legal domain. The source code for our
implementation is publicly available at
https://github.com/NimaMeghdadi/uOttawa-at-LegalLens-2024-Transformer-based-Classification
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Just accepted at the the EMNLP conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient multi-<span class="highlight-title">prompt</span> evaluation of LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe Maia Polo, Ronald Xu, Lucas Weber, Mírian Silva, Onkar Bhardwaj, Leshem Choshen, Allysson Flavio Melo de Oliveira, Yuekai Sun, Mikhail Yurochkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most popular benchmarks for comparing LLMs rely on a limited set of prompt
templates, which may not fully capture the LLMs' abilities and can affect the
reproducibility of results on leaderboards. Many recent works empirically
verify prompt sensitivity and advocate for changes in LLM evaluation. In this
paper, we consider the problem of estimating the performance distribution
across many prompt variants instead of finding a single prompt to evaluate
with. We introduce PromptEval, a method for estimating performance across a
large set of prompts borrowing strength across prompts and examples to produce
accurate estimates under practical evaluation budgets. The resulting
distribution can be used to obtain performance quantiles to construct various
robust performance metrics (e.g., top 95% quantile or median). We prove that
PromptEval consistently estimates the performance distribution and demonstrate
its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench
Hard, and LMentry; for example, PromptEval can accurately estimate performance
quantiles across 100 prompt templates on MMLU with a budget equivalent to two
single-prompt evaluations. Moreover, we show how PromptEval can be useful in
LLM-as-a-judge and best prompt identification applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HuRef: HUman-REadable Fingerprint for Large Language Models <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04828v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04828v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyi Zeng, Lizheng Wang, Yuncong Hu, Yi Xu, Chenghu Zhou, Xinbing Wang, Yu Yu, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protecting the copyright of large language models (LLMs) has become crucial
due to their resource-intensive training and accompanying carefully designed
licenses. However, identifying the original base model of an LLM is challenging
due to potential parameter alterations. In this study, we introduce HuRef, a
human-readable fingerprint for LLMs that uniquely identifies the base model
without interfering with training or exposing model parameters to the public.
We first observe that the vector direction of LLM parameters remains stable
after the model has converged during pretraining, with negligible perturbations
through subsequent training steps, including continued pretraining, supervised
fine-tuning, and RLHF, which makes it a sufficient condition to identify the
base model. The necessity is validated by continuing to train an LLM with an
extra term to drive away the model parameters' direction and the model becomes
damaged. However, this direction is vulnerable to simple attacks like dimension
permutation or matrix rotation, which significantly change it without affecting
performance. To address this, leveraging the Transformer structure, we
systematically analyze potential attacks and define three invariant terms that
identify an LLM's base model. Due to the potential risk of information leakage,
we cannot publish invariant terms directly. Instead, we map them to a Gaussian
vector using an encoder, then convert it into a natural image using StyleGAN2,
and finally publish the image. In our black-box setting, all fingerprinting
steps are internally conducted by the LLMs owners. To ensure the published
fingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP).
Experimental results across various LLMs demonstrate the effectiveness of our
method. The code is available at https://github.com/LUMIA-Group/HuRef.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StreamBench: Towards Benchmarking Continuous Improvement of Language
  Agents <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that large language model (LLM) agents are able to
improve themselves from experience, which is an important ability for
continuous enhancement post-deployment. However, existing benchmarks primarily
evaluate their innate capabilities and do not assess their ability to improve
over time. To address this gap, we introduce StreamBench, a pioneering
benchmark designed to evaluate the continuous improvement of LLM agents over an
input-feedback sequence. StreamBench simulates an online learning environment
where LLMs receive a continuous flow of feedback stream and iteratively enhance
their performance. In addition, we propose several simple yet effective
baselines for improving LLMs on StreamBench, and provide a comprehensive
analysis to identify critical components that contribute to successful
streaming strategies. Our work serves as a stepping stone towards developing
effective online learning strategies for LLMs, paving the way for more adaptive
AI systems in streaming scenarios. Source code:
https://github.com/stream-bench/stream-bench. Benchmark website:
https://stream-bench.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransVIP: Speech to Speech Translation System with Voice and Isochrony
  Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17809v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17809v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Le, Yao Qian, Dongmei Wang, Long Zhou, Shujie Liu, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Sheng Zhao, Michael Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a rising interest and trend in research towards directly translating
speech from one language to another, known as end-to-end speech-to-speech
translation. However, most end-to-end models struggle to outperform cascade
models, i.e., a pipeline framework by concatenating speech recognition, machine
translation and text-to-speech models. The primary challenges stem from the
inherent complexities involved in direct translation tasks and the scarcity of
data. In this study, we introduce a novel model framework TransVIP that
leverages diverse datasets in a cascade fashion yet facilitates end-to-end
inference through joint probability. Furthermore, we propose two separated
encoders to preserve the speaker's voice characteristics and isochrony from the
source speech during the translation process, making it highly suitable for
scenarios such as video dubbing. Our experiments on the French-English language
pair demonstrate that our model outperforms the current state-of-the-art
speech-to-speech translation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neural Information Processing Systems, poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented
  Generation with Key Point Recall <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23000v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23000v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is a promising approach to address the
limitations of fixed knowledge in large language models (LLMs). However,
current benchmarks for evaluating RAG systems suffer from two key deficiencies:
(1) they fail to adequately measure LLMs' capability in handling long-context
retrieval due to a lack of datasets that reflect the characteristics of
retrieved documents, and (2) they lack a comprehensive evaluation method for
assessing LLMs' ability to generate long-form responses that effectively
exploits retrieved information. To address these shortcomings, we introduce the
Long$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG
comprises 280 questions spanning 10 domains and across 8 question categories,
each associated with 5 retrieved documents with an average length of 2,444
words. KPR evaluates the extent to which LLMs incorporate key points extracted
from the retrieved documents into their generated responses, providing a more
nuanced assessment of their ability to exploit retrieved information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP'24 (Findings). Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lever LM: Configuring In-Context Sequence to Lever Large Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10104v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10104v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Yang, Yingzhe Peng, Haoxuan Ma, Shuo Xu, Chi Zhang, Yucheng Han, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Archimedes famously said, ``Give me a lever long enough and a fulcrum on
which to place it, and I shall move the world'', in this study, we propose to
use a tiny Language Model (LM), \eg, a Transformer with 67M parameters, to
lever much larger Vision-Language Models (LVLMs) with 9B parameters.
Specifically, we use this tiny \textbf{Lever-LM} to configure effective
in-context demonstration (ICD) sequences to improve the In-Context Learinng
(ICL) performance of LVLMs. Previous studies show that diverse ICD
configurations like the selection and ordering of the demonstrations heavily
affect the ICL performance, highlighting the significance of configuring
effective ICD sequences. Motivated by this and by re-considering the the
process of configuring ICD sequence, we find this is a mirror process of human
sentence composition and further assume that effective ICD configurations may
contain internal statistical patterns that can be captured by Lever-LM. Then a
dataset with effective ICD sequences is constructed to train Lever-LM. After
training, given novel queries, new ICD sequences are configured by the trained
Lever-LM to solve vision-language tasks through ICL. Experiments show that
these ICD sequences can improve the ICL performance of two LVLMs compared with
some strong baselines in Visual Question Answering and Image Captioning,
validating that Lever-LM can really capture the statistical patterns for
levering LVLMs. The code is available at
\url{https://github.com/ForJadeForest/Lever-LM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaterialBENCH: Evaluating College-Level Materials Science
  Problem-Solving Abilities of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michiko Yoshitake, Yuta Suzuki, Ryo Igarashi, Yoshitaka Ushiku, Keisuke Nagato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A college-level benchmark dataset for large language models (LLMs) in the
materials science field, MaterialBENCH, is constructed. This dataset consists
of problem-answer pairs, based on university textbooks. There are two types of
problems: one is the free-response answer type, and the other is the
multiple-choice type. Multiple-choice problems are constructed by adding three
incorrect answers as choices to a correct answer, so that LLMs can choose one
of the four as a response. Most of the problems for free-response answer and
multiple-choice types overlap except for the format of the answers. We also
conduct experiments using the MaterialBENCH on LLMs, including ChatGPT-3.5,
ChatGPT-4, Bard (at the time of the experiments), and GPT-3.5 and GPT-4 with
the OpenAI API. The differences and similarities in the performance of LLMs
measured by the MaterialBENCH are analyzed and discussed. Performance
differences between the free-response type and multiple-choice type in the same
models and the influence of using system massages on multiple-choice problems
are also studied. We anticipate that MaterialBENCH will encourage further
developments of LLMs in reasoning abilities to solve more complicated problems
and eventually contribute to materials research and discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LIVE: Learnable In-Context Vector for Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13185v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13185v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingzhe Peng, Chenduo Hao, Xu Yang, Jiawei Peng, Xinting Hu, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models continue to scale, Large Language Models (LLMs) have
exhibited emerging capabilities in In-Context Learning (ICL), enabling them to
solve language tasks by prefixing a few in-context demonstrations (ICDs) as
context. Inspired by these advancements, researchers have extended these
techniques to develop Large Multimodal Models (LMMs) with ICL capabilities.
However, applying ICL usually faces two major challenges: 1) using more ICDs
will largely increase the inference time and 2) the performance is sensitive to
the selection of ICDs. These challenges are further exacerbated in LMMs due to
the integration of multiple data types and the combinational complexity of
multimodal ICDs. Recently, to address these challenges, some NLP studies
introduce non-learnable In-Context Vectors (ICVs) which extract useful task
information from ICDs into a single vector and then insert it into the LLM to
help solve the corresponding task. However, although useful in simple NLP
tasks, these non-learnable methods fail to handle complex multimodal tasks like
Visual Question Answering (VQA). In this study, we propose Learnable In-Context
VEctor (LIVE) to distill essential task information from demonstrations,
improving ICL performance in LMMs. Experiments show that LIVE can significantly
reduce computational costs while enhancing accuracy in VQA tasks compared to
traditional ICL and other non-learnable ICV methods. The code is available at
\url{https://github.com/ForJadeForest/LIVE-Learnable-In-Context-Vector}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Machine Translation with a BiLSTM-Attention Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxu Wu, Yiren Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Natural Language Processing (NLP) technology,
the accuracy and efficiency of machine translation have become hot topics of
research. This paper proposes a novel Seq2Seq model aimed at improving
translation quality while reducing the storage space required by the model. The
model employs a Bidirectional Long Short-Term Memory network (Bi-LSTM) as the
encoder to capture the context information of the input sequence; the decoder
incorporates an attention mechanism, enhancing the model's ability to focus on
key information during the translation process. Compared to the current
mainstream Transformer model, our model achieves superior performance on the
WMT14 machine translation dataset while maintaining a smaller size.
  The study first introduces the design principles and innovative points of the
model architecture, followed by a series of experiments to verify the
effectiveness of the model. The experimental includes an assessment of the
model's performance on different language pairs, as well as comparative
analysis with traditional Seq2Seq models. The results show that while
maintaining translation accuracy, our model significantly reduces the storage
requirements, which is of great significance for translation applications in
resource-constrained scenarios. our code are available at
https://github.com/mindspore-lab/models/tree/master/research/arxiv_papers/miniformer.
Thanks for the support provided by MindSpore Community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large
  Language Models Attentive Readers? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05197v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05197v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeladri Bhuiya, Viktor Schlegel, Stefan Winkler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art Large Language Models (LLMs) are accredited with an
increasing number of different capabilities, ranging from reading
comprehension, over advanced mathematical and reasoning skills to possessing
scientific knowledge. In this paper we focus on their multi-hop reasoning
capability: the ability to identify and integrate information from multiple
textual sources.
  Given the concerns with the presence of simplifying cues in existing
multi-hop reasoning benchmarks, which allow models to circumvent the reasoning
requirement, we set out to investigate, whether LLMs are prone to exploiting
such simplifying cues. We find evidence that they indeed circumvent the
requirement to perform multi-hop reasoning, but they do so in more subtle ways
than what was reported about their fine-tuned pre-trained language model (PLM)
predecessors. Motivated by this finding, we propose a challenging multi-hop
reasoning benchmark, by generating seemingly plausible multi-hop reasoning
chains, which ultimately lead to incorrect answers. We evaluate multiple open
and proprietary state-of-the-art LLMs, and find that their performance to
perform multi-hop reasoning is affected, as indicated by up to 45% relative
decrease in F1 score when presented with such seemingly plausible alternatives.
We conduct a deeper analysis and find evidence that while LLMs tend to ignore
misleading lexical cues, misleading reasoning paths indeed present a
significant challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoFiT: Localized Fine-tuning on LLM Representations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangcong Yin, Xi Ye, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in interpretability shows that large language models (LLMs) can
be adapted for new tasks in a learning-free way: it is possible to intervene on
LLM representations to elicit desired behaviors for alignment. For instance,
adding certain bias vectors to the outputs of certain attention heads is
reported to boost the truthfulness of models. In this work, we show that
localized fine-tuning serves as an effective alternative to such representation
intervention methods. We introduce a framework called Localized Fine-Tuning on
LLM Representations (LoFiT), which identifies a subset of attention heads that
are most important for learning a specific task, then trains offset vectors to
add to the model's hidden representations at those selected heads. LoFiT
localizes to a sparse set of heads (3%-10%) and learns the offset vectors from
limited training data, comparable to the settings used for representation
intervention. For truthfulness and reasoning tasks, we find that LoFiT's
intervention vectors are more effective for LLM adaptation than vectors from
representation intervention methods such as Inference-time Intervention. We
also find that the localization step is important: selecting a task-specific
set of attention heads can lead to higher performance than intervening on heads
selected for a different task. Finally, across 7 tasks we study, LoFiT achieves
comparable performance to other parameter-efficient fine-tuning methods such as
LoRA, despite modifying 20x-200x fewer parameters than these methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coding Reliable LLM-based Integrated Task and Knowledge Agents with
  GenieWorksheets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshit Joshi, Shicheng Liu, James Chen, Robert Weigle, Monica S. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) present an opportunity to create automated
assistants that can help users navigate complex tasks. However, existing
approaches have limitations in handling conditional logic, integrating
knowledge sources, and consistently following instructions. Researchers and
industry professionals often employ ad hoc pipelines to construct
conversational agents. These pipelines aim to maintain context, address failure
cases, and minimize hallucinations, yet frequently fail to achieve these
objectives. To this end, we present Genie - a programmable framework for
creating task-oriented conversational agents that are designed to handle
complex user interactions and knowledge queries. Unlike LLMs, Genie provides
reliable grounded responses, with controllable agent policies through its
expressive specification, Genie Worksheet. In contrast to dialog trees, it is
resilient to diverse user queries, helpful with knowledge sources, and offers
ease of programming policies through its declarative paradigm. The agents built
using Genie outperforms the state-of-the-art method on complex logic domains in
STARV2 dataset by up to 20.5%. Additionally, through a real-user study
involving 62 participants, we show that Genie beats the GPT-4 with function
calling baseline by 21.1%, 20.1%, and 61% on execution accuracy, dialogue act
accuracy, and goal completion rate, respectively, on three diverse real-world
domains
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who Evaluates the Evaluations? Objectively Scoring Text-to-Image <span class="highlight-title">Prompt</span>
  Coherence Metrics with T2IScoreScore (TS2) <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04251v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04251v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With advances in the quality of text-to-image (T2I) models has come interest
in benchmarking their prompt faithfulness -- the semantic coherence of
generated images to the prompts they were conditioned on. A variety of T2I
faithfulness metrics have been proposed, leveraging advances in cross-modal
embeddings and vision-language models (VLMs). However, these metrics are not
rigorously compared and benchmarked, instead presented with correlation to
human Likert scores over a set of easy-to-discriminate images against seemingly
weak baselines.
  We introduce T2IScoreScore, a curated set of semantic error graphs containing
a prompt and a set of increasingly erroneous images. These allow us to
rigorously judge whether a given prompt faithfulness metric can correctly order
images with respect to their objective error count and significantly
discriminate between different error nodes, using meta-metric scores derived
from established statistical tests. Surprisingly, we find that the
state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we
tested fail to significantly outperform simple (and supposedly worse)
feature-based metrics like CLIPScore, particularly on a hard subset of
naturally-occurring T2I model errors. TS2 will enable the development of better
T2I prompt faithfulness metrics through more rigorous comparison of their
conformity to expected orderings and separations under objective criteria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ URAvatar: Universal Relightable Gaussian Codec Avatars <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxuan Li, Chen Cao, Gabriel Schwartz, Rawal Khirodkar, Christian Richardt, Tomas Simon, Yaser Sheikh, Shunsuke Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new approach to creating photorealistic and relightable head
avatars from a phone scan with unknown illumination. The reconstructed avatars
can be animated and relit in real time with the global illumination of diverse
environments. Unlike existing approaches that estimate parametric reflectance
parameters via inverse rendering, our approach directly models learnable
radiance transfer that incorporates global light transport in an efficient
manner for real-time rendering. However, learning such a complex light
transport that can generalize across identities is non-trivial. A phone scan in
a single environment lacks sufficient information to infer how the head would
appear in general environments. To address this, we build a universal
relightable avatar model represented by 3D Gaussians. We train on hundreds of
high-quality multi-view human scans with controllable point lights.
High-resolution geometric guidance further enhances the reconstruction accuracy
and generalization. Once trained, we finetune the pretrained model on a phone
scan using inverse rendering to obtain a personalized relightable avatar. Our
experiments establish the efficacy of our design, outperforming existing
approaches while retaining real-time rendering capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2024. Website:
  https://junxuan-li.github.io/urgca-website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoMimic: Scaling Imitation Learning via Egocentric Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simar Kareer, Dhruv Patel, Ryan Punamiya, Pranay Mathur, Shuo Cheng, Chen Wang, Judy Hoffman, Danfei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scale and diversity of demonstration data required for imitation learning
is a significant challenge. We present EgoMimic, a full-stack framework which
scales manipulation via human embodiment data, specifically egocentric human
videos paired with 3D hand tracking. EgoMimic achieves this through: (1) a
system to capture human embodiment data using the ergonomic Project Aria
glasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap
to human data, (3) cross-domain data alignment techniques, and (4) an imitation
learning architecture that co-trains on human and robot data. Compared to prior
works that only extract high-level intent from human videos, our approach
treats human and robot data equally as embodied demonstration data and learns a
unified policy from both data sources. EgoMimic achieves significant
improvement on a diverse set of long-horizon, single-arm and bimanual
manipulation tasks over state-of-the-art imitation learning methods and enables
generalization to entirely new scenes. Finally, we show a favorable scaling
trend for EgoMimic, where adding 1 hour of additional hand data is
significantly more valuable than 1 hour of additional robot data. Videos and
additional information can be found at https://egomimic.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Motion in Text-to-Video Generation with Decomposed Encoding
  and Conditioning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghui Ruan, Pichao Wang, Divya Saxena, Jiannong Cao, Yuhui Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in Text-to-Video (T2V) generation, producing videos with
realistic motion remains challenging. Current models often yield static or
minimally dynamic outputs, failing to capture complex motions described by
text. This issue stems from the internal biases in text encoding, which
overlooks motions, and inadequate conditioning mechanisms in T2V generation
models. To address this, we propose a novel framework called DEcomposed MOtion
(DEMO), which enhances motion synthesis in T2V generation by decomposing both
text encoding and conditioning into content and motion components. Our method
includes a content encoder for static elements and a motion encoder for
temporal dynamics, alongside separate content and motion conditioning
mechanisms. Crucially, we introduce text-motion and video-motion supervision to
improve the model's understanding and generation of motion. Evaluations on
benchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench
demonstrate DEMO's superior ability to produce videos with enhanced motion
dynamics while maintaining high visual quality. Our approach significantly
advances T2V generation by integrating comprehensive motion understanding
directly from textual descriptions. Project page:
https://PR-Ryan.github.io/DEMO-project/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024, code available at
  https://github.com/PR-Ryan/DEMO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Embodied Reinforcement Learning Agents: Informativeness and
  Diversity of Language Use <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Xi, Yinong He, Jianing Yang, Yinpei Dai, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, it is desirable for embodied agents to have the
ability to leverage human language to gain explicit or implicit knowledge for
learning tasks. Despite recent progress, most previous approaches adopt simple
low-level instructions as language inputs, which may not reflect natural human
communication. It's not clear how to incorporate rich language use to
facilitate task learning. To address this question, this paper studies
different types of language inputs in facilitating reinforcement learning (RL)
embodied agents. More specifically, we examine how different levels of language
informativeness (i.e., feedback on past behaviors and future guidance) and
diversity (i.e., variation of language expressions) impact agent learning and
inference. Our empirical results based on four RL benchmarks demonstrate that
agents trained with diverse and informative language feedback can achieve
enhanced generalization and fast adaptation to new tasks. These findings
highlight the pivotal role of language use in teaching embodied agents new
tasks in an open world. Project website:
https://github.com/sled-group/Teachable_RL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main. Project website:
  https://github.com/sled-group/Teachable_RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARQ: A Mixed-Precision Quantization Framework for Accurate and
  Certifiably Robust DNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Yang, Shubham Ugare, Yifan Zhao, Gagandeep Singh, Sasa Misailovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed precision quantization has become an important technique for enabling
the execution of deep neural networks (DNNs) on limited resource computing
platforms. Traditional quantization methods have primarily concentrated on
maintaining neural network accuracy, either ignoring the impact of quantization
on the robustness of the network, or using only empirical techniques for
improving robustness. In contrast, techniques for robustness certification,
which can provide strong guarantees about the robustness of DNNs have not been
used during quantization due to their high computation cost.
  This paper introduces ARQ, an innovative mixed-precision quantization method
that not only preserves the clean accuracy of the smoothed classifiers but also
maintains their certified robustness. ARQ uses reinforcement learning to find
accurate and robust DNN quantization, while efficiently leveraging randomized
smoothing, a popular class of statistical DNN verification algorithms, to guide
the search process.
  We compare ARQ with multiple state-of-the-art quantization techniques on
several DNN architectures commonly used in quantization studies: ResNet-20 on
CIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. We demonstrate
that ARQ consistently performs better than these baselines across all the
benchmarks and the input perturbation levels. In many cases, the performance of
ARQ quantized networks can reach that of the original DNN with floating-point
weights, but with only 1.5% instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Video Representations without Natural Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyang Yu, Xinlei Chen, Yossi Gandelsman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we show that useful video representations can be learned from
synthetic videos and natural images, without incorporating natural videos in
the training. We propose a progression of video datasets synthesized by simple
generative processes, that model a growing set of natural video properties
(e.g. motion, acceleration, and shape transformations). The downstream
performance of video models pre-trained on these generated datasets gradually
increases with the dataset progression. A VideoMAE model pre-trained on our
synthetic videos closes 97.2% of the performance gap on UCF101 action
classification between training from scratch and self-supervised pre-training
from natural videos, and outperforms the pre-trained model on HMDB51.
Introducing crops of static images to the pre-training stage results in similar
performance to UCF101 pre-training and outperforms the UCF101 pre-trained model
on 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the
low-level properties of the datasets, we identify correlations between frame
diversity, frame similarity to natural data, and downstream performance. Our
approach provides a more controllable and transparent alternative to video data
curation processes for pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://unicorn53547.github.io/video_syn_rep/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELTA: Dense Efficient Long-range 3D Tracking for any video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, Chaoyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking dense 3D motion from monocular videos remains challenging,
particularly when aiming for pixel-level precision over long sequences. We
introduce \Approach, a novel method that efficiently tracks every pixel in 3D
space, enabling accurate motion estimation across entire videos. Our approach
leverages a joint global-local attention mechanism for reduced-resolution
tracking, followed by a transformer-based upsampler to achieve high-resolution
predictions. Unlike existing methods, which are limited by computational
inefficiency or sparse tracking, \Approach delivers dense 3D tracking at scale,
running over 8x faster than previous methods while achieving state-of-the-art
accuracy. Furthermore, we explore the impact of depth representation on
tracking performance and identify log-depth as the optimal choice. Extensive
experiments demonstrate the superiority of \Approach on multiple benchmarks,
achieving new state-of-the-art results in both 2D and 3D dense tracking tasks.
Our method provides a robust solution for applications requiring fine-grained,
long-term motion tracking in 3D space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://snap-research.github.io/DELTA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse
  Unposed Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, Songyou Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D
scenes parameterized by 3D Gaussians from \textit{unposed} sparse multi-view
images. Our model, trained exclusively with photometric loss, achieves
real-time 3D Gaussian reconstruction during inference. To eliminate the need
for accurate pose input during reconstruction, we anchor one input view's local
camera coordinates as the canonical space and train the network to predict
Gaussian primitives for all views within this space. This approach obviates the
need to transform Gaussian primitives from local coordinates into a global
coordinate system, thus avoiding errors associated with per-frame Gaussians and
pose estimation. To resolve scale ambiguity, we design and compare various
intrinsic embedding methods, ultimately opting to convert camera intrinsics
into a token embedding and concatenate it with image tokens as input to the
model, enabling accurate scene scale prediction. We utilize the reconstructed
3D Gaussians for novel view synthesis and pose estimation tasks and propose a
two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental
results demonstrate that our pose-free approach can achieve superior novel view
synthesis quality compared to pose-required methods, particularly in scenarios
with limited input image overlap. For pose estimation, our method, trained
without ground truth depth or explicit matching loss, significantly outperforms
the state-of-the-art methods with substantial improvements. This work makes
significant advances in pose-free generalizable 3D reconstruction and
demonstrates its applicability to real-world scenarios. Code and trained models
are available at https://noposplat.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://noposplat.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoSplatting: Towards Geometry Guided Gaussian Splatting for
  Physically-based Inverse Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Ye, Chong Gao, Guanbin Li, Wenzheng Chen, Baoquan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of physically-based inverse rendering using 3D
Gaussian Splatting (3DGS) representations. While recent 3DGS methods have
achieved remarkable results in novel view synthesis (NVS), accurately capturing
high-fidelity geometry, physically interpretable materials and lighting remains
challenging, as it requires precise geometry modeling to provide accurate
surface normals, along with physically-based rendering (PBR) techniques to
ensure correct material and lighting disentanglement. Previous 3DGS methods
resort to approximating surface normals, but often struggle with noisy local
geometry, leading to inaccurate normal estimation and suboptimal
material-lighting decomposition. In this paper, we introduce GeoSplatting, a
novel hybrid representation that augments 3DGS with explicit geometric guidance
and differentiable PBR equations. Specifically, we bridge isosurface and 3DGS
together, where we first extract isosurface mesh from a scalar field, then
convert it into 3DGS points and formulate PBR equations for them in a fully
differentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,
enabling precise surface normal modeling, which facilitates the use of PBR
frameworks for material decomposition. This approach further maintains the
efficiency and quality of NVS from 3DGS while ensuring accurate geometry from
the isosurface. Comprehensive evaluations across diverse datasets demonstrate
the superiority of GeoSplatting, consistently outperforming existing methods
both quantitatively and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffPano: Scalable and Consistent Text to Panorama Generation with
  Spherical Epipolar-Aware Diffusion <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weicai Ye, Chenhao Ji, Zheng Chen, Junyao Gao, Xiaoshui Huang, Song-Hai Zhang, Wanli Ouyang, Tong He, Cairong Zhao, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based methods have achieved remarkable achievements in 2D image or
3D object generation, however, the generation of 3D scenes and even
$360^{\circ}$ images remains constrained, due to the limited number of scene
datasets, the complexity of 3D scenes themselves, and the difficulty of
generating consistent multi-view images. To address these issues, we first
establish a large-scale panoramic video-text dataset containing millions of
consecutive panoramic keyframes with corresponding panoramic depths, camera
poses, and text descriptions. Then, we propose a novel text-driven panoramic
generation framework, termed DiffPano, to achieve scalable, consistent, and
diverse panoramic scene generation. Specifically, benefiting from the powerful
generative capabilities of stable diffusion, we fine-tune a single-view
text-to-panorama diffusion model with LoRA on the established panoramic
video-text dataset. We further design a spherical epipolar-aware multi-view
diffusion model to ensure the multi-view consistency of the generated panoramic
images. Extensive experiments demonstrate that DiffPano can generate scalable,
consistent, and diverse panoramic images with given unseen text descriptions
and camera poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024, Project: https://github.com/zju3dv/DiffPano; Code:
  https://github.com/zju3dv/DiffPano</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chasing Better Deep Image Priors between Over- and
  Under-parameterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Wu, Xiaohan Chen, Yifan Jiang, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) are well-known to act as over-parameterized deep
image priors (DIP) that regularize various image inverse problems. Meanwhile,
researchers also proposed extremely compact, under-parameterized image priors
(e.g., deep decoder) that are strikingly competent for image restoration too,
despite a loss of accuracy. These two extremes push us to think whether there
exists a better solution in the middle: between over- and under-parameterized
image priors, can one identify "intermediate" parameterized image priors that
achieve better trade-offs between performance, efficiency, and even preserving
strong transferability? Drawing inspirations from the lottery ticket hypothesis
(LTH), we conjecture and study a novel "lottery image prior" (LIP) by
exploiting DNN inherent sparsity, stated as: given an over-parameterized
DNN-based image prior, it will contain a sparse subnetwork that can be trained
in isolation, to match the original DNN's performance when being applied as a
prior to various image inverse problems. Our results validate the superiority
of LIPs: we can successfully locate the LIP subnetworks from over-parameterized
DIPs at substantial sparsity ranges. Those LIP subnetworks significantly
outperform deep decoders under comparably compact model sizes (by often fully
preserving the effectiveness of their over-parameterized counterparts), and
they also possess high transferability across different images as well as
restoration task types. Besides, we also extend LIP to compressive sensing
image reconstruction, where a pre-trained GAN generator is used as the prior
(in contrast to untrained DIP or deep decoder), and confirm its validity in
this setting too. To our best knowledge, this is the first time that LTH is
demonstrated to be relevant in the context of inverse problems or image priors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes are available at
  https://github.com/VITA-Group/Chasing-Better-DIPs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DexMimicGen: Automated Data Generation for Bimanual Dexterous
  Manipulation via Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from human demonstrations is an effective means to teach
robots manipulation skills. But data acquisition is a major bottleneck in
applying this paradigm more broadly, due to the amount of cost and human effort
involved. There has been significant interest in imitation learning for
bimanual dexterous robots, like humanoids. Unfortunately, data collection is
even more challenging here due to the challenges of simultaneously controlling
multiple arms and multi-fingered hands. Automated data generation in simulation
is a compelling, scalable alternative to fuel this need for data. To this end,
we introduce DexMimicGen, a large-scale automated data generation system that
synthesizes trajectories from a handful of human demonstrations for humanoid
robots with dexterous hands. We present a collection of simulation environments
in the setting of bimanual dexterous manipulation, spanning a range of
manipulation behaviors and different requirements for coordination among the
two arms. We generate 21K demos across these tasks from just 60 source human
demos and study the effect of several data generation and policy learning
decisions on agent performance. Finally, we present a real-to-sim-to-real
pipeline and deploy it on a real-world humanoid can sorting task. Videos and
more are at https://dexmimicgen.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://dexmimicgen.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended Object Tracking and Classification based on Linear Splines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Tesori, Giorgio Battistelli, Luigi Chisci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a framework based on linear splines for 2-dimensional
extended object tracking and classification. Unlike state of the art models,
linear splines allow to represent extended objects whose contour is an
arbitrarily complex curve. An exact likelihood is derived for the case in which
noisy measurements can be scattered from any point on the contour of the
extended object, while an approximate Monte Carlo likelihood is provided for
the case wherein scattering points can be anywhere, i.e. inside or on the
contour, on the object surface. Exploiting such likelihood to measure how well
the observed data fit a given shape, a suitable estimator is developed. The
proposed estimator models the extended object in terms of a kinematic state,
providing object position and orientation, along with a shape vector,
characterizing object contour and surface. The kinematic state is estimated via
a nonlinear Kalman filter, while the shape vector is estimated via a Bayesian
classifier so that classification is implicitly solved during shape estimation.
Numerical experiments are provided to assess, compared to state of the art
extended object estimators, the effectiveness of the proposed one.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Black-Box Adaptation for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jay N. Paranjape, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a form of distributed learning that allows
multiple institutions or clients to collaboratively learn a global model to
solve a task. This allows the model to utilize the information from every
institute while preserving data privacy. However, recent studies show that the
promise of protecting the privacy of data is not upheld by existing methods and
that it is possible to recreate the training data from the different
institutions. This is done by utilizing gradients transferred between the
clients and the global server during training or by knowing the model
architecture at the client end. In this paper, we propose a federated learning
framework for semantic segmentation without knowing the model architecture nor
transferring gradients between the client and the server, thus enabling better
privacy preservation. We propose BlackFed - a black-box adaptation of neural
networks that utilizes zero order optimization (ZOO) to update the client model
weights and first order optimization (FOO) to update the server weights. We
evaluate our approach on several computer vision and medical imaging datasets
to demonstrate its effectiveness. To the best of our knowledge, this work is
one of the first works in employing federated learning for segmentation, devoid
of gradients or model information exchange. Code:
https://github.com/JayParanjape/blackfed/tree/master
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NEURIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Redefining <Creative> in Dictionary: Towards a Enhanced Semantic
  Understanding of Creative Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Feng, Yucheng Xie, Jing Wang, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creativity, both in human and diffusion models, remains an inherently
abstract concept; thus, simply adding "creative" to a prompt does not yield
reliable semantic recognition by the model. In this work, we concretize the
abstract notion of "creative" through the TP2O task, which aims to merge two
unrelated concepts, and introduce CreTok, redefining "creative" as the token
$\texttt{<CreTok>}$. This redefinition offers a more concrete and universally
adaptable representation for concept blending. This redefinition occurs
continuously, involving the repeated random sampling of text pairs with
different concepts and optimizing cosine similarity between target and constant
prompts. This approach enables $\texttt{<CreTok>}$ to learn a method for
creative concept fusion. Extensive experiments demonstrate that the creative
capability enabled by $\texttt{<CreTok>}$ substantially surpasses recent SOTA
diffusion models and achieves superior creative generation. CreTok exhibits
greater flexibility and reduced time overhead, as $\texttt{<CreTok>}$ can
function as a universal token for any concept, facilitating creative generation
without retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Concept With Text-Guided Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Huang, Susan Liang, Yunlong Tang, Yapeng Tian, Anurag Kumar, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided diffusion models have revolutionized generative tasks by
producing high-fidelity content from text descriptions. They have also enabled
an editing paradigm where concepts can be replaced through text conditioning
(e.g., a dog to a tiger). In this work, we explore a novel approach: instead of
replacing a concept, can we enhance or suppress the concept itself? Through an
empirical study, we identify a trend where concepts can be decomposed in
text-guided diffusion models. Leveraging this insight, we introduce
ScalingConcept, a simple yet effective method to scale decomposed concepts up
or down in real input without introducing new elements. To systematically
evaluate our approach, we present the WeakConcept-10 dataset, where concepts
are imperfect and need to be enhanced. More importantly, ScalingConcept enables
a variety of novel zero-shot applications across image and audio domains,
including tasks such as canonical pose generation and generative sound
highlighting or removal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://wikichao.github.io/ScalingConcept/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Vision Language Models for Facial Attribute Recognition:
  Emotion, Race, Gender, and Age 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nouar AlDahoul, Myles Joshua Toledo Tan, Harishwar Reddy Kasireddy, Yasir Zaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technologies for recognizing facial attributes like race, gender, age, and
emotion have several applications, such as surveillance, advertising content,
sentiment analysis, and the study of demographic trends and social behaviors.
Analyzing demographic characteristics based on images and analyzing facial
expressions have several challenges due to the complexity of humans' facial
attributes. Traditional approaches have employed CNNs and various other deep
learning techniques, trained on extensive collections of labeled images. While
these methods demonstrated effective performance, there remains potential for
further enhancements. In this paper, we propose to utilize vision language
models (VLMs) such as generative pre-trained transformer (GPT), GEMINI, large
language and vision assistant (LLAVA), PaliGemma, and Microsoft Florence2 to
recognize facial attributes such as race, gender, age, and emotion from images
with human faces. Various datasets like FairFace, AffectNet, and UTKFace have
been utilized to evaluate the solutions. The results show that VLMs are
competitive if not superior to traditional techniques. Additionally, we propose
"FaceScanPaliGemma"--a fine-tuned PaliGemma model--for race, gender, age, and
emotion recognition. The results show an accuracy of 81.1%, 95.8%, 80%, and
59.4% for race, gender, age group, and emotion classification, respectively,
outperforming pre-trained version of PaliGemma, other VLMs, and SotA methods.
Finally, we propose "FaceScanGPT", which is a GPT-4o model to recognize the
above attributes when several individuals are present in the image using a
prompt engineered for a person with specific facial and/or physical attributes.
The results underscore the superior multitasking capability of FaceScanGPT to
detect the individual's attributes like hair cut, clothing color, postures,
etc., using only a prompt to drive the detection and recognition tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HoloChrome: Polychromatic Illumination for Speckle Reduction in
  Holographic Near-Eye Displays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Schiffers, Grace Kuo, Nathan Matsuda, Douglas Lanman, Oliver Cossairt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Holographic displays hold the promise of providing authentic depth cues,
resulting in enhanced immersive visual experiences for near-eye applications.
However, current holographic displays are hindered by speckle noise, which
limits accurate reproduction of color and texture in displayed images. We
present HoloChrome, a polychromatic holographic display framework designed to
mitigate these limitations. HoloChrome utilizes an ultrafast,
wavelength-adjustable laser and a dual-Spatial Light Modulator (SLM)
architecture, enabling the multiplexing of a large set of discrete wavelengths
across the visible spectrum. By leveraging spatial separation in our dual-SLM
setup, we independently manipulate speckle patterns across multiple
wavelengths. This novel approach effectively reduces speckle noise through
incoherent averaging achieved by wavelength multiplexing. Our method is
complementary to existing speckle reduction techniques, offering a new pathway
to address this challenge. Furthermore, the use of polychromatic illumination
broadens the achievable color gamut compared to traditional three-color primary
holographic displays.
  Our simulations and tabletop experiments validate that HoloChrome
significantly reduces speckle noise and expands the color gamut. These
advancements enhance the performance of holographic near-eye displays, moving
us closer to practical, immersive next-generation visual experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COSNet: A Novel Semantic Segmentation Network using Enhanced Boundaries
  in Cluttered Scenes <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ali, Mamoona Javaid, Mubashir Noman, Mustansar Fiaz, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated waste recycling aims to efficiently separate the recyclable objects
from the waste by employing vision-based systems. However, the presence of
varying shaped objects having different material types makes it a challenging
problem, especially in cluttered environments. Existing segmentation methods
perform reasonably on many semantic segmentation datasets by employing
multi-contextual representations, however, their performance is degraded when
utilized for waste object segmentation in cluttered scenarios. In addition,
plastic objects further increase the complexity of the problem due to their
translucent nature. To address these limitations, we introduce an efficacious
segmentation network, named COSNet, that uses boundary cues along with
multi-contextual information to accurately segment the objects in cluttered
scenes. COSNet introduces novel components including feature sharpening block
(FSB) and boundary enhancement module (BEM) for enhancing the features and
highlighting the boundary information of irregular waste objects in cluttered
environment. Extensive experiments on three challenging datasets including
ZeroWaste-f, SpectralWaste, and ADE20K demonstrate the effectiveness of the
proposed method. Our COSNet achieves a significant gain of 1.8% on ZeroWaste-f
and 2.1% on SpectralWaste datasets respectively in terms of mIoU metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIDOVECL: AI-generated <span class="highlight-title">Dataset</span> of Outpainted Vehicles for Eye-level
  Classification and Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Kazemi, Qurat ul ain Fatima, Volodymyr Kindratenko, Christopher Tessum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image labeling is a critical bottleneck in the development of computer vision
technologies, often constraining the potential of machine learning models due
to the time-intensive nature of manual annotations. This work introduces a
novel approach that leverages outpainting to address the problem of annotated
data scarcity by generating artificial contexts and annotations, significantly
reducing manual labeling efforts. We apply this technique to a particularly
acute challenge in autonomous driving, urban planning, and environmental
monitoring: the lack of diverse, eye-level vehicle images in desired classes.
Our dataset comprises AI-generated vehicle images obtained by detecting and
cropping vehicles from manually selected seed images, which are then outpainted
onto larger canvases to simulate varied real-world conditions. The outpainted
images include detailed annotations, providing high-quality ground truth data.
Advanced outpainting techniques and image quality assessments ensure visual
fidelity and contextual relevance. Augmentation with outpainted vehicles
improves overall performance metrics by up to 8\% and enhances prediction of
underrepresented classes by up to 20\%. This approach, exemplifying outpainting
as a self-annotating paradigm, presents a solution that enhances dataset
versatility across multiple domains of machine learning. The code and links to
datasets used in this study are available for further research and replication
at https://github.com/amir-kazemi/aidovecl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nearest Neighbor Normalization Improves Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Chowdhury, Franklin Wang, Sumedh Shenoy, Douwe Kiela, Sarah Schwettmann, Tristan Thrush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal models leverage large-scale pre-training to achieve strong but
still imperfect performance on tasks such as image captioning, visual question
answering, and cross-modal retrieval. In this paper, we present a simple and
efficient method for correcting errors in trained contrastive image-text
retrieval models with no additional training, called Nearest Neighbor
Normalization (NNN). We show an improvement on retrieval metrics in both text
retrieval and image retrieval for all of the contrastive models that we tested
(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used
(MS-COCO and Flickr30k). NNN requires a reference database, but does not
require any training on this database, and can even increase the retrieval
accuracy of a model after finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter choices in HaarPSI for IQA with medical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clemens Karner, Janek Gröhl, Ian Selby, Judith Babar, Jake Beckford, Thomas R Else, Timothy J Sadler, Shahab Shahipasand, Arthikkaa Thavakumar, Michael Roberts, James H. F. Rudd, Carola-Bibiane Schönlieb, Jonathan R Weir-McCall, Anna Breger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When developing machine learning models, image quality assessment (IQA)
measures are a crucial component for evaluation. However, commonly used IQA
measures have been primarily developed and optimized for natural images. In
many specialized settings, such as medical images, this poses an
often-overlooked problem regarding suitability. In previous studies, the IQA
measure HaarPSI showed promising behavior for natural and medical images.
HaarPSI is based on Haar wavelet representations and the framework allows
optimization of two parameters. So far, these parameters have been aligned for
natural images. Here, we optimize these parameters for two annotated medical
data sets, a photoacoustic and a chest X-Ray data set. We observe that they are
more sensitive to the parameter choices than the employed natural images, and
on the other hand both medical data sets lead to similar parameter values when
optimized. We denote the optimized setting, which improves the performance for
the medical images notably, by HaarPSI$_{MED}$. The results suggest that
adapting common IQA measures within their frameworks for medical images can
provide a valuable, generalizable addition to the employment of more specific
task-based measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Spatio-Temporal Drivers of Extreme Events <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Hakam Shams Eddin, Juergen Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spatio-temporal relations of impacts of extreme events and their drivers
in climate data are not fully understood and there is a need of machine
learning approaches to identify such spatio-temporal relations from data. The
task, however, is very challenging since there are time delays between extremes
and their drivers, and the spatial response of such drivers is inhomogeneous.
In this work, we propose a first approach and benchmarks to tackle this
challenge. Our approach is trained end-to-end to predict spatio-temporally
extremes and spatio-temporally drivers in the physical input variables jointly.
By enforcing the network to predict extremes from spatio-temporal binary masks
of identified drivers, the network successfully identifies drivers that are
correlated with extremes. We evaluate our approach on three newly created
synthetic benchmarks, where two of them are based on remote sensing or
reanalysis climate data, and on two real-world reanalysis datasets. The source
code and datasets are publicly available at the project page
https://hakamshams.github.io/IDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Generalizability of Diffusion Models Requires Rethinking
  the Hidden Gaussian Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Yixiang Dai, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the generalizability of diffusion models by looking
into the hidden properties of the learned score functions, which are
essentially a series of deep denoisers trained on various noise levels. We
observe that as diffusion models transition from memorization to
generalization, their corresponding nonlinear diffusion denoisers exhibit
increasing linearity. This discovery leads us to investigate the linear
counterparts of the nonlinear diffusion models, which are a series of linear
models trained to match the function mappings of the nonlinear diffusion
denoisers. Surprisingly, these linear denoisers are approximately the optimal
denoisers for a multivariate Gaussian distribution characterized by the
empirical mean and covariance of the training dataset. This finding implies
that diffusion models have the inductive bias towards capturing and utilizing
the Gaussian structure (covariance information) of the training dataset for
data generation. We empirically demonstrate that this inductive bias is a
unique property of diffusion models in the generalization regime, which becomes
increasingly evident when the model's capacity is relatively small compared to
the training dataset size. In the case that the model is highly
overparameterized, this inductive bias emerges during the initial training
phases before the model fully memorizes its training data. Our study provides
crucial insights into understanding the notable strong generalization
phenomenon recently observed in real-world diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Predictive Quality Assessment for Ultrasonic Additive
  Manufacturing with Deep Learning Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lokendra Poudel, Sushant Jha, Ryan Meeker, Duy-Nhat Phan, Rahul Bhowmik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasonic Additive Manufacturing (UAM) employs ultrasonic welding to bond
similar or dissimilar metal foils to a substrate, resulting in solid,
consolidated metal components. However, certain processing conditions can lead
to inter-layer defects, affecting the final product's quality. This study
develops a method to monitor in-process quality using deep learning-based
convolutional neural networks (CNNs). The CNN models were evaluated on their
ability to classify samples with and without embedded thermocouples across five
power levels (300W, 600W, 900W, 1200W, 1500W) using thermal images with
supervised labeling. Four distinct CNN classification models were created for
different scenarios including without (baseline) and with thermocouples, only
without thermocouples across power levels, only with thermocouples across power
levels, and combined without and with thermocouples across power levels. The
models achieved 98.29% accuracy on combined baseline and thermocouple images,
97.10% for baseline images across power levels, 97.43% for thermocouple images,
and 97.27% for both types across power levels. The high accuracy, above 97%,
demonstrates the system's effectiveness in identifying and classifying
conditions within the UAM process, providing a reliable tool for quality
assurance and process control in manufacturing environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junliang Du, Yiru Cang, Tong Zhou, Jiacheng Hu, Weijie He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces the Hybrid Multi-modal VGG (HM-VGG) model, a
cutting-edge deep learning approach for the early diagnosis of glaucoma. The
HM-VGG model utilizes an attention mechanism to process Visual Field (VF) data,
enabling the extraction of key features that are vital for identifying early
signs of glaucoma. Despite the common reliance on large annotated datasets, the
HM-VGG model excels in scenarios with limited data, achieving remarkable
results with small sample sizes. The model's performance is underscored by its
high metrics in Precision, Accuracy, and F1-Score, indicating its potential for
real-world application in glaucoma detection. The paper also discusses the
challenges associated with ophthalmic image analysis, particularly the
difficulty of obtaining large volumes of annotated data. It highlights the
importance of moving beyond single-modality data, such as VF or Optical
Coherence Tomography (OCT) images alone, to a multimodal approach that can
provide a richer, more comprehensive dataset. This integration of different
data types is shown to significantly enhance diagnostic accuracy. The HM- VGG
model offers a promising tool for doctors, streamlining the diagnostic process
and improving patient outcomes. Furthermore, its applicability extends to
telemedicine and mobile healthcare, making diagnostic services more accessible.
The research presented in this paper is a significant step forward in the field
of medical image processing and has profound implications for clinical
ophthalmology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TPC: Test-time Procrustes Calibration for Diffusion-based Human Image
  Animation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunjae Yoon, Gwanhyeong Koo, Younghwan Lee, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human image animation aims to generate a human motion video from the inputs
of a reference human image and a target motion video. Current diffusion-based
image animation systems exhibit high precision in transferring human identity
into targeted motion, yet they still exhibit irregular quality in their
outputs. Their optimal precision is achieved only when the physical
compositions (i.e., scale and rotation) of the human shapes in the reference
image and target pose frame are aligned. In the absence of such alignment,
there is a noticeable decline in fidelity and consistency. Especially, in
real-world environments, this compositional misalignment commonly occurs,
posing significant challenges to the practical usage of current systems. To
this end, we propose Test-time Procrustes Calibration (TPC), which enhances the
robustness of diffusion-based image animation systems by maintaining optimal
performance even when faced with compositional misalignment, effectively
addressing real-world scenarios. The TPC provides a calibrated reference image
for the diffusion model, enhancing its capability to understand the
correspondence between human shapes in the reference and target images. Our
method is simple and can be applied to any diffusion-based image animation
system in a model-agnostic manner, improving the effectiveness at test time
without additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 16 figures, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handwriting Recognition in Historical Documents with Multimodal LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an immense quantity of historical and cultural documentation that
exists only as handwritten manuscripts. At the same time, performing OCR across
scripts and different handwriting styles has proven to be an enormously
difficult problem relative to the process of digitizing print. While recent
Transformer based models have achieved relatively strong performance, they rely
heavily on manually transcribed training data and have difficulty generalizing
across writers. Multimodal LLM, such as GPT-4v and Gemini, have demonstrated
effectiveness in performing OCR and computer vision tasks with few shot
prompting. In this paper, I evaluate the accuracy of handwritten document
transcriptions generated by Gemini against the current state of the art
Transformer based methods.
  Keywords: Optical Character Recognition, Multimodal Language Models, Cultural
Preservation, Mass digitization, Handwriting Recognitio
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems
  using Disparity Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Larey, Eyal Rond, Omer Achrack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition technologies are increasingly used in various applications,
yet they are vulnerable to face spoofing attacks. These spoofing attacks often
involve unique 3D structures, such as printed papers or mobile device screens.
Although stereo-depth cameras can detect such attacks effectively, their
high-cost limits their widespread adoption. Conversely, two-sensor systems
without extrinsic calibration offer a cost-effective alternative but are unable
to calculate depth using stereo techniques. In this work, we propose a method
to overcome this challenge by leveraging facial attributes to derive disparity
information and estimate relative depth for anti-spoofing purposes, using
non-calibrated systems. We introduce a multi-modal anti-spoofing model, coined
Disparity Model, that incorporates created disparity maps as a third modality
alongside the two original sensor modalities. We demonstrate the effectiveness
of the Disparity Model in countering various spoof attacks using a
comprehensive dataset collected from the Intel RealSense ID Solution F455. Our
method outperformed existing methods in the literature, achieving an Equal
Error Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False
Positive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the
errors of the best comparison method, respectively. Additionally, we introduce
a model ensemble that addresses 3D spoof attacks as well, achieving an EER of
2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a
state-of-the-art solution for the challenging task of anti-spoofing in
non-calibrated systems that lack depth information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian-guided Label Mapping for Visual Reprogramming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained
vision models by adapting their input or output interfaces to solve downstream
tasks whose labels (i.e., downstream labels) might be totally different from
the labels associated with the pretrained models (i.e., pretrained labels).
When adapting the output interface, label mapping methods transform the
pretrained labels to downstream labels by establishing a gradient-free
one-to-one correspondence between the two sets of labels. However, in this
paper, we reveal that one-to-one mappings may overlook the complex relationship
between pretrained and downstream labels. Motivated by this observation, we
propose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an
iteratively-updated probabilistic label mapping matrix, with each element
quantifying a pairwise relationship between pretrained and downstream labels.
The assignment of values to the constructed matrix is guided by Bayesian
conditional probability, considering the joint distribution of the downstream
labels and the labels predicted by the pretrained model on downstream samples.
Experiments conducted on both pretrained vision models (e.g., ResNeXt) and
vision-language models (e.g., CLIP) demonstrate the superior performance of BLM
over existing label mapping methods. The success of BLM also offers a
probabilistic lens through which to understand and analyze the effectiveness of
VR. Our code is available at https://github.com/tmlr-group/BayesianLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Synthetic Faces: How Synthetic <span class="highlight-title">Dataset</span>s Can Expose Real
  Identities <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hatef Otroshi Shahreza, Sébastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation is gaining increasing popularity in different
computer vision applications. Existing state-of-the-art face recognition models
are trained using large-scale face datasets, which are crawled from the
Internet and raise privacy and ethical concerns. To address such concerns,
several works have proposed generating synthetic face datasets to train face
recognition models. However, these methods depend on generative models, which
are trained on real face images. In this work, we design a simple yet effective
membership inference attack to systematically study if any of the existing
synthetic face recognition datasets leak any information from the real data
used to train the generator model. We provide an extensive study on 6
state-of-the-art synthetic face recognition datasets, and show that in all
these synthetic datasets, several samples from the original real dataset are
leaked. To our knowledge, this paper is the first work which shows the leakage
from training data of generator models into the generated synthetic face
recognition datasets. Our study demonstrates privacy pitfalls in synthetic face
recognition datasets and paves the way for future studies on generating
responsible synthetic face datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024 Workshop on New Frontiers in Adversarial
  Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-assembling the past: The RePAIR <span class="highlight-title">dataset</span> and benchmark for real world
  2D and 3D puzzle solving <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodore Tsesmelis, Luca Palmieri, Marina Khoroshiltseva, Adeela Islam, Gur Elkin, Ofir Itzhak Shahar, Gianluca Scarpellini, Stefano Fiorini, Yaniv Ohayon, Nadav Alali, Sinem Aslan, Pietro Morerio, Sebastiano Vascon, Elena Gravina, Maria Cristina Napolitano, Giuseppe Scarpati, Gabriel Zuchtriegel, Alexandra Spühler, Michel E. Fuchs, Stuart James, Ohad Ben-Shahar, Marcello Pelillo, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the RePAIR dataset that represents a challenging
benchmark to test modern computational and data driven methods for
puzzle-solving and reassembly tasks. Our dataset has unique properties that are
uncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and
fractures are realistic, caused by a collapse of a fresco during a World War II
bombing at the Pompeii archaeological park. The fragments are also eroded and
have missing pieces with irregular shapes and different dimensions, challenging
further the reassembly algorithms. The dataset is multi-modal providing high
resolution images with characteristic pictorial elements, detailed 3D scans of
the fragments and meta-data annotated by the archaeologists. Ground truth has
been generated through several years of unceasing fieldwork, including the
excavation and cleaning of each fragment, followed by manual puzzle solving by
archaeologists of a subset of approx. 1000 pieces among the 16000 available.
After digitizing all the fragments in 3D, a benchmark was prepared to challenge
current reassembly and puzzle-solving methods that often solve more simplistic
synthetic scenarios. The tested baselines show that there clearly exists a gap
to fill in solving this computationally complex problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Track Datasets and Benchmarks, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffPAD: Denoising Diffusion-based Adversarial Patch Decontamination <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Fu, Xiao Zhang, Sepideh Pashami, Fatemeh Rahimian, Anders Holst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving adversarial machine learning landscape, developing
effective defenses against patch attacks has become a critical challenge,
necessitating reliable solutions to safeguard real-world AI systems. Although
diffusion models have shown remarkable capacity in image synthesis and have
been recently utilized to counter $\ell_p$-norm bounded attacks, their
potential in mitigating localized patch attacks remains largely underexplored.
In this work, we propose DiffPAD, a novel framework that harnesses the power of
diffusion models for adversarial patch decontamination. DiffPAD first performs
super-resolution restoration on downsampled input images, then adopts
binarization, dynamic thresholding scheme and sliding window for effective
localization of adversarial patches. Such a design is inspired by the
theoretically derived correlation between patch size and diffusion restoration
error that is generalized across diverse patch attack scenarios. Finally,
DiffPAD applies inpainting techniques to the original input images with the
estimated patch region being masked. By integrating closed-form solutions for
super-resolution restoration and image inpainting into the conditional reverse
sampling process of a pre-trained diffusion model, DiffPAD obviates the need
for text guidance or fine-tuning. Through comprehensive experiments, we
demonstrate that DiffPAD not only achieves state-of-the-art adversarial
robustness against patch attacks but also excels in recovering naturalistic
images without patch remnants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Efficacy of Classical and Deep Neuroimaging Biomarkers in
  Early Alzheimer's Disease Diagnosis <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milla E. Nielsen, Mads Nielsen, Mostafa Mehdipour Ghazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) is the leading cause of dementia, and its early
detection is crucial for effective intervention, yet current diagnostic methods
often fall short in sensitivity and specificity. This study aims to detect
significant indicators of early AD by extracting and integrating various
imaging biomarkers, including radiomics, hippocampal texture descriptors,
cortical thickness measurements, and deep learning features. We analyze
structural magnetic resonance imaging (MRI) scans from the Alzheimer's Disease
Neuroimaging Initiative (ADNI) cohorts, utilizing comprehensive image analysis
and machine learning techniques. Our results show that combining multiple
biomarkers significantly improves detection accuracy. Radiomics and texture
features emerged as the most effective predictors for early AD, achieving AUCs
of 0.88 and 0.72 for AD and MCI detection, respectively. Although deep learning
features proved to be less effective than traditional approaches, incorporating
age with other biomarkers notably enhanced MCI detection performance.
Additionally, our findings emphasize the continued importance of classical
imaging biomarkers in the face of modern deep-learning approaches, providing a
robust framework for early AD diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SPIE Medical Imaging (MI25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImOV3D: Learning Open-Vocabulary Point Clouds 3D Object Detection from
  Only 2D Images <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timing Yang, Yuanliang Ju, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary 3D object detection (OV-3Det) aims to generalize beyond the
limited number of base categories labeled during the training phase. The
biggest bottleneck is the scarcity of annotated 3D data, whereas 2D image
datasets are abundant and richly annotated. Consequently, it is intuitive to
leverage the wealth of annotations in 2D images to alleviate the inherent data
scarcity in OV-3Det. In this paper, we push the task setup to its limits by
exploring the potential of using solely 2D images to learn OV-3Det. The major
challenges for this setup is the modality gap between training images and
testing point clouds, which prevents effective integration of 2D knowledge into
OV-3Det. To address this challenge, we propose a novel framework ImOV3D to
leverage pseudo multimodal representation containing both images and point
clouds (PC) to close the modality gap. The key of ImOV3D lies in flexible
modality conversion where 2D images can be lifted into 3D using monocular depth
estimation and can also be derived from 3D scenes through rendering. This
allows unifying both training images and testing point clouds into a common
image-PC representation, encompassing a wealth of 2D semantic information and
also incorporating the depth and structural characteristics of 3D spatial data.
We carefully conduct such conversion to minimize the domain gap between
training and test cases. Extensive experiments on two benchmark datasets,
SUNRGBD and ScanNet, show that ImOV3D significantly outperforms existing
methods, even in the absence of ground truth 3D training data. With the
inclusion of a minimal amount of real 3D data for fine-tuning, the performance
also significantly surpasses previous state-of-the-art. Codes and pre-trained
models are released on the https://github.com/yangtiming/ImOV3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Code link
  https://github.com/yangtiming/ImOV3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localization, balance and affinity: a stronger multifaceted
  collaborative salient object detector in remote sensing images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yakun Xie, Suning Liu, Hongyu Chen, Shaohan Cao, Huixin Zhang, Dejun Feng, Qian Wan, Jun Zhu, Qing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in salient object detection(SOD) in optical
remote sensing images(ORSI), challenges persist due to the intricate edge
structures of ORSIs and the complexity of their contextual relationships.
Current deep learning approaches encounter difficulties in accurately
identifying boundary features and lack efficiency in collaboratively modeling
the foreground and background by leveraging contextual features. To address
these challenges, we propose a stronger multifaceted collaborative salient
object detector in ORSIs, termed LBA-MCNet, which incorporates aspects of
localization, balance, and affinity. The network focuses on accurately locating
targets, balancing detailed features, and modeling image-level global context
information. Specifically, we design the Edge Feature Adaptive Balancing and
Adjusting(EFABA) module for precise edge localization, using edge features to
guide attention to boundaries and preserve spatial details. Moreover, we design
the Global Distributed Affinity Learning(GDAL) module to model global context.
It captures global context by generating an affinity map from the encoders
final layer, ensuring effective modeling of global patterns. Additionally, deep
supervision during deconvolution further enhances feature representation.
Finally, we compared with 28 state of the art approaches on three publicly
available datasets. The results clearly demonstrate the superiority of our
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JEMA: A Joint Embedding Framework for Scalable Co-Learning with
  Multimodal Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Sousa, Roya Darabi, Armando Sousa, Frank Brueckner, Luís Paulo Reis, Ana Reis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces JEMA (Joint Embedding with Multimodal Alignment), a
novel co-learning framework tailored for laser metal deposition (LMD), a
pivotal process in metal additive manufacturing. As Industry 5.0 gains traction
in industrial applications, efficient process monitoring becomes increasingly
crucial. However, limited data and the opaque nature of AI present challenges
for its application in an industrial setting. JEMA addresses this challenges by
leveraging multimodal data, including multi-view images and metadata such as
process parameters, to learn transferable semantic representations. By applying
a supervised contrastive loss function, JEMA enables robust learning and
subsequent process monitoring using only the primary modality, simplifying
hardware requirements and computational overhead. We investigate the
effectiveness of JEMA in LMD process monitoring, focusing specifically on its
generalization to downstream tasks such as melt pool geometry prediction,
achieved without extensive fine-tuning. Our empirical evaluation demonstrates
the high scalability and performance of JEMA, particularly when combined with
Vision Transformer models. We report an 8% increase in performance in
multimodal settings and a 1% improvement in unimodal settings compared to
supervised contrastive learning. Additionally, the learned embedding
representation enables the prediction of metadata, enhancing interpretability
and making possible the assessment of the added metadata's contributions. Our
framework lays the foundation for integrating multisensor data with metadata,
enabling diverse downstream tasks within the LMD domain and beyond.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrAct: Making First-layer Pre-Activations Trainable <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Petersen, Christian Borgelt, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the training of the first layer of vision models and notice the
clear relationship between pixel values and gradient update magnitudes: the
gradients arriving at the weights of a first layer are by definition directly
proportional to (normalized) input pixel values. Thus, an image with low
contrast has a smaller impact on learning than an image with higher contrast,
and a very bright or very dark image has a stronger impact on the weights than
an image with moderate brightness. In this work, we propose performing gradient
descent on the embeddings produced by the first layer of the model. However,
switching to discrete inputs with an embedding layer is not a reasonable option
for vision models. Thus, we propose the conceptual procedure of (i) a gradient
descent step on first layer activations to construct an activation proposal,
and (ii) finding the optimal weights of the first layer, i.e., those weights
which minimize the squared distance to the activation proposal. We provide a
closed form solution of the procedure and adjust it for robust stochastic
training while computing everything efficiently. Empirically, we find that
TrAct (Training Activations) speeds up training by factors between 1.25x and 4x
while requiring only a small computational overhead. We demonstrate the utility
of TrAct with different optimizers for a range of different vision models
including convolutional and transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Synthesis with Class-Aware Semantic Diffusion Models for Surgical
  Scene Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihang Zhou, Rebecca Towning, Zaid Awad, Stamatia Giannarou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical scene segmentation is essential for enhancing surgical precision,
yet it is frequently compromised by the scarcity and imbalance of available
data. To address these challenges, semantic image synthesis methods based on
generative adversarial networks and diffusion models have been developed.
However, these models often yield non-diverse images and fail to capture small,
critical tissue classes, limiting their effectiveness. In response, we propose
the Class-Aware Semantic Diffusion Model (CASDM), a novel approach which
utilizes segmentation maps as conditions for image synthesis to tackle data
scarcity and imbalance. Novel class-aware mean squared error and class-aware
self-perceptual loss functions have been defined to prioritize critical, less
visible classes, thereby enhancing image quality and relevance. Furthermore, to
our knowledge, we are the first to generate multi-class segmentation maps using
text prompts in a novel fashion to specify their contents. These maps are then
used by CASDM to generate surgical scene images, enhancing datasets for
training and validating segmentation models. Our evaluation, which assesses
both image quality and downstream segmentation performance, demonstrates the
strong effectiveness and generalisability of CASDM in producing realistic
image-map pairs, significantly advancing surgical scene segmentation across
diverse and challenging datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MV-CC: Mask Enhanced Video Model for Remote Sensing Change Caption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixun Liu, Kaiyu Li, Jiayi Song, Dongwei Sun, Xiangyong Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing image change caption (RSICC) aims to provide natural language
descriptions for bi-temporal remote sensing images. Since Change Caption (CC)
task requires both spatial and temporal features, previous works follow an
encoder-fusion-decoder architecture. They use an image encoder to extract
spatial features and the fusion module to integrate spatial features and
extract temporal features, which leads to increasingly complex manual design of
the fusion module. In this paper, we introduce a novel video model-based
paradigm without design of the fusion module and propose a Mask-enhanced Video
model for Change Caption (MV-CC). Specifically, we use the off-the-shelf video
encoder to simultaneously extract the temporal and spatial features of
bi-temporal images. Furthermore, the types of changes in the CC are set based
on specific task requirements, and to enable the model to better focus on the
regions of interest, we employ masks obtained from the Change Detection (CD)
method to explicitly guide the CC model. Experimental results demonstrate that
our proposed method can obtain better performance compared with other
state-of-the-art RSICC methods. The code is available at
https://github.com/liuruixun/MV-CC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manipulating Vehicle 3D Shapes through Latent Space Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JiangDong Miao, Tatsuya Ikeda, Bisser Raytchev, Ryota Mizoguchi, Takenori Hiraoka, Takuji Nakashima, Keigo Shimizu, Toru Higaki, Kazufumi Kaneda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although 3D object editing has the potential to significantly influence
various industries, recent research in 3D generation and editing has primarily
focused on converting text and images into 3D models, often overlooking the
need for fine-grained control over the editing of existing 3D objects. This
paper introduces a framework that employs a pre-trained regressor, enabling
continuous, precise, attribute-specific modifications to both the stylistic and
geometric attributes of vehicle 3D models. Our method not only preserves the
inherent identity of vehicle 3D objects, but also supports multi-attribute
editing, allowing for extensive customization without compromising the model's
structural integrity. Experimental results demonstrate the efficacy of our
approach in achieving detailed edits on various vehicle 3D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BitStack: Fine-Grained Size Control for Compressed Large Language Models
  in Variable Memory Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized numerous applications, yet
their deployment remains challenged by memory constraints on local devices.
While scaling laws have enhanced LLM capabilities, the primary bottleneck has
shifted from \textit{capability} to \textit{availability}, emphasizing the need
for efficient memory management. Traditional compression methods, such as
quantization, often require predefined compression ratios and separate
compression processes for each setting, complicating deployment in variable
memory environments. In this paper, we introduce \textbf{BitStack}, a novel,
training-free weight compression approach that enables megabyte-level
trade-offs between memory usage and model performance. By leveraging weight
decomposition, BitStack can dynamically adjust the model size with minimal
transmission between running memory and storage devices. Our approach
iteratively decomposes weight matrices while considering the significance of
each parameter, resulting in an approximately 1-bit per parameter residual
block in each decomposition iteration. These blocks are sorted and stacked in
storage as basic transmission units, with different quantities loaded based on
current memory availability. Extensive experiments across a wide range of tasks
demonstrate that, despite offering fine-grained size control, BitStack
consistently matches or surpasses strong quantization baselines, particularly
at extreme compression ratios. To the best of our knowledge, this is the first
decomposition-based method that effectively bridges the gap to practical
compression techniques like quantization. Code is available at
https://github.com/xinghaow99/BitStack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Estimation for 3D Object Detection via Evidential Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Durasov, Rafid Mahmood, Jiwoong Choi, Marc T. Law, James Lucas, Pascal Fua, Jose M. Alvarez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection is an essential task for computer vision applications in
autonomous vehicles and robotics. However, models often struggle to quantify
detection reliability, leading to poor performance on unfamiliar scenes. We
introduce a framework for quantifying uncertainty in 3D object detection by
leveraging an evidential learning loss on Bird's Eye View representations in
the 3D detector. These uncertainty estimates require minimal computational
overhead and are generalizable across different architectures. We demonstrate
both the efficacy and importance of these uncertainty estimates on identifying
out-of-distribution scenes, poorly localized objects, and missing (false
negative) detections; our framework consistently improves over baselines by
10-20% on average. Finally, we integrate this suite of tasks into a system
where a 3D object detector auto-labels driving scenes and our uncertainty
estimates verify label correctness before the labels are used to train a second
model. Here, our uncertainty-driven verification results in a 1% improvement in
mAP and a 1-2% improvement in NDS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Web Data to Real Fields: Low-Cost Unsupervised Domain Adaptation
  for Agricultural Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasileios Tzouras, Lazaros Nalpantidis, Ronja Güldenring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In precision agriculture, vision models often struggle with new, unseen
fields where crops and weeds have been influenced by external factors,
resulting in compositions and appearances that differ from the learned
distribution. This paper aims to adapt to specific fields at low cost using
Unsupervised Domain Adaptation (UDA). We explore a novel domain shift from a
diverse, large pool of internet-sourced data to a small set of data collected
by a robot at specific locations, minimizing the need for extensive on-field
data collection. Additionally, we introduce a novel module -- the Multi-level
Attention-based Adversarial Discriminator (MAAD) -- which can be integrated at
the feature extractor level of any detection model. In this study, we
incorporate MAAD with CenterNet to simultaneously detect leaf, stem, and vein
instances. Our results show significant performance improvements in the
unlabeled target domain compared to baseline models, with a 7.5% increase in
object detection accuracy and a 5.1% improvement in keypoint detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on
  Text-modulated Diffusion Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Lei Cao, Jiayi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multi-modal image fusion methods fail to address the compound
degradations presented in source images, resulting in fusion images plagued by
noise, color bias, improper exposure, \textit{etc}. Additionally, these methods
often overlook the specificity of foreground objects, weakening the salience of
the objects of interest within the fused images. To address these challenges,
this study proposes a novel interactive multi-modal image fusion framework
based on the text-modulated diffusion model, called Text-DiFuse. First, this
framework integrates feature-level information integration into the diffusion
process, allowing adaptive degradation removal and multi-modal information
fusion. This is the first attempt to deeply and explicitly embed information
fusion within the diffusion process, effectively addressing compound
degradation in image fusion. Second, by embedding the combination of the text
and zero-shot location model into the diffusion fusion process, a
text-controlled fusion re-modulation strategy is developed. This enables
user-customized text control to improve fusion performance and highlight
foreground objects in the fused images. Extensive experiments on diverse public
datasets show that our Text-DiFuse achieves state-of-the-art fusion performance
across various scenarios with complex degradation. Moreover, the semantic
segmentation experiment validates the significant enhancement in semantic
performance achieved by our text-controlled fusion re-modulation strategy. The
code is publicly available at https://github.com/Leiii-Cao/Text-DiFuse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EZ-HOI: VLM Adaptation via Guided <span class="highlight-title">Prompt</span> Learning for Zero-Shot HOI
  Detection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinqian Lei, Bo Wang, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting Human-Object Interactions (HOI) in zero-shot settings, where models
must handle unseen classes, poses significant challenges. Existing methods that
rely on aligning visual encoders with large Vision-Language Models (VLMs) to
tap into the extensive knowledge of VLMs, require large, computationally
expensive models and encounter training difficulties. Adapting VLMs with prompt
learning offers an alternative to direct alignment. However, fine-tuning on
task-specific datasets often leads to overfitting to seen classes and
suboptimal performance on unseen classes, due to the absence of unseen class
labels. To address these challenges, we introduce a novel prompt learning-based
framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce
Large Language Model (LLM) and VLM guidance for learnable prompts, integrating
detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.
However, because training datasets contain seen-class labels alone, fine-tuning
VLMs on such datasets tends to optimize learnable prompts for seen classes
instead of unseen ones. Therefore, we design prompt learning for unseen classes
using information from related seen classes, with LLMs utilized to highlight
the differences between unseen and related seen classes. Quantitative
evaluations on benchmark datasets demonstrate that our EZ-HOI achieves
state-of-the-art performance across various zero-shot settings with only 10.35%
to 33.95% of the trainable parameters compared to existing methods. Code is
available at https://github.com/ChelsieLei/EZ-HOI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AllClear: A Comprehensive <span class="highlight-title">Dataset</span> and Benchmark for Cloud Removal in
  Satellite Imagery <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangyu Zhou, Chia-Hsiang Kao, Cheng Perng Phoo, Utkarsh Mall, Bharath Hariharan, Kavita Bala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clouds in satellite imagery pose a significant challenge for downstream
applications. A major challenge in current cloud removal research is the
absence of a comprehensive benchmark and a sufficiently large and diverse
training dataset. To address this problem, we introduce the largest public
dataset -- $\textit{AllClear}$ for cloud removal, featuring 23,742 globally
distributed regions of interest (ROIs) with diverse land-use patterns,
comprising 4 million images in total. Each ROI includes complete temporal
captures from the year 2022, with (1) multi-spectral optical imagery from
Sentinel-2 and Landsat 8/9, (2) synthetic aperture radar (SAR) imagery from
Sentinel-1, and (3) auxiliary remote sensing products such as cloud masks and
land cover maps. We validate the effectiveness of our dataset by benchmarking
performance, demonstrating the scaling law -- the PSNR rises from $28.47$ to
$33.87$ with $30\times$ more data, and conducting ablation studies on the
temporal length and the importance of individual modalities. This dataset aims
to provide comprehensive coverage of the Earth's surface and promote better
cloud removal results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Datasets and Benchmarks Track. Code and data
  available at https://allclear.cs.cornell.edu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Airway Labeling Meets Clinical Applications: Reflecting Topology
  Consistency and Outliers via Learnable Attentions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Li, Minghui Zhang, Chuyan Zhang, Yun Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate airway anatomical labeling is crucial for clinicians to identify and
navigate complex bronchial structures during bronchoscopy. Automatic airway
anatomical labeling is challenging due to significant individual variability
and anatomical variations. Previous methods are prone to generate inconsistent
predictions, which is harmful for preoperative planning and intraoperative
navigation. This paper aims to address these challenges by proposing a novel
method that enhances topological consistency and improves the detection of
abnormal airway branches.
  We propose a novel approach incorporating two modules: the Soft Subtree
Consistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module
constructs a soft subtree to capture clinically relevant topological
relationships, allowing for flexible feature aggregation within and across
subtrees. The ABS module facilitates the interaction between node features and
prototypes to distinguish abnormal branches, preventing the erroneous
aggregation of features between normal and abnormal nodes.
  Evaluated on a challenging dataset characterized by severe airway distortion
and atrophy, our method achieves superior performance compared to
state-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the
segmental level and an 83.7% accuracy at the subsegmental level, representing a
1.4% increase in subsegmental accuracy and a 3.1% increase in topological
consistency. Notably, the method demonstrates reliable performance in cases
with disease-induced airway deformities, ensuring consistent and accurate
labeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided
  Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Deng, Youxin Pang, Xiaochen Zhao, Chao Xu, Lizhen Wang, Hongjiang Xiao, Shi Yan, Hongwen Zhang, Yebin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Stereo-Talker, a novel one-shot audio-driven human
video synthesis system that generates 3D talking videos with precise lip
synchronization, expressive body gestures, temporally consistent
photo-realistic quality, and continuous viewpoint control. The process follows
a two-stage approach. In the first stage, the system maps audio input to
high-fidelity motion sequences, encompassing upper-body gestures and facial
expressions. To enrich motion diversity and authenticity, large language model
(LLM) priors are integrated with text-aligned semantic audio features,
leveraging LLMs' cross-modal generalization power to enhance motion quality. In
the second stage, we improve diffusion-based video generation models by
incorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided
MoE focuses on view-specific attributes, while a mask-guided MoE enhances
region-based rendering stability. Additionally, a mask prediction module is
devised to derive human masks from motion data, enhancing the stability and
accuracy of masks and enabling mask guiding during inference. We also introduce
a comprehensive human video dataset with 2,203 identities, covering diverse
body gestures and detailed annotations, facilitating broad generalization. The
code, data, and pre-trained models will be released for research purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual MRI Data Augmentation using Conditional Denoising
  Diffusion Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Morão, Joao Santinha, Yasna Forghani, Nuno Loução, Pedro Gouveia, Mario A. T. Figueiredo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models in medical imaging face challenges in
generalizability and robustness due to variations in image acquisition
parameters (IAP). In this work, we introduce a novel method using conditional
denoising diffusion generative models (cDDGMs) to generate counterfactual
magnetic resonance (MR) images that simulate different IAP without altering
patient anatomy. We demonstrate that using these counterfactual images for data
augmentation can improve segmentation accuracy, particularly in
out-of-distribution settings, enhancing the overall generalizability and
robustness of DL models across diverse imaging conditions. Our approach shows
promise in addressing domain and covariate shifts in medical imaging. The code
is publicly available at https:
//github.com/pedromorao/Counterfactual-MRI-Data-Augmentation
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Diffusion Models for Anomaly Localization in Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cosmin I. Bercea, Philippe C. Cattin, Julia A. Schnabel, Julia Wolleb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This chapter explores anomaly localization in medical images using denoising
diffusion models. After providing a brief methodological background of these
models, including their application to image reconstruction and their
conditioning using guidance mechanisms, we provide an overview of available
datasets and evaluation metrics suitable for their application to anomaly
localization in medical images. In this context, we discuss supervision schemes
ranging from fully supervised segmentation to semi-supervised, weakly
supervised, self-supervised, and unsupervised methods, and provide insights
into the effectiveness and limitations of these approaches. Furthermore, we
highlight open challenges in anomaly localization, including detection bias,
domain shift, computational cost, and model interpretability. Our goal is to
provide an overview of the current state of the art in the field, outline
research gaps, and highlight the potential of diffusion models for robust
anomaly localization in medical images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRoundation: Are Foundation Models Ready for Face Recognition? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahar Chettaoui, Naser Damer, Fadi Boutros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are predominantly trained in an unsupervised or
self-supervised manner on highly diverse and large-scale datasets, making them
broadly applicable to various downstream tasks. In this work, we investigate
for the first time whether such models are suitable for the specific domain of
face recognition. We further propose and demonstrate the adaptation of these
models for face recognition across different levels of data availability.
Extensive experiments are conducted on multiple foundation models and datasets
of varying scales for training and fine-tuning, with evaluation on a wide range
of benchmarks. Our results indicate that, despite their versatility,
pre-trained foundation models underperform in face recognition compared to
similar architectures trained specifically for this task. However, fine-tuning
foundation models yields promising results, often surpassing models trained
from scratch when training data is limited. Even with access to large-scale
face recognition training datasets, fine-tuned foundation models perform
comparably to models trained from scratch, but with lower training
computational costs and without relying on the assumption of extensive data
availability. Our analysis also explores bias in face recognition, with
slightly higher bias observed in some settings when using foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Show Me What and Where has Changed? Question Answering and Grounding for
  Remote Sensing Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Li, Fuyu Dong, Di Wang, Shaofeng Li, Quan Wang, Xinbo Gao, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing change detection aims to perceive changes occurring on the
Earth's surface from remote sensing data in different periods, and feed these
changes back to humans. However, most existing methods only focus on detecting
change regions, lacking the ability to interact with users to identify changes
that the users expect. In this paper, we introduce a new task named Change
Detection Question Answering and Grounding (CDQAG), which extends the
traditional change detection task by providing interpretable textual answers
and intuitive visual evidence. To this end, we construct the first CDQAG
benchmark dataset, termed QAG-360K, comprising over 360K triplets of questions,
textual answers, and corresponding high-quality visual masks. It encompasses 10
essential land-cover categories and 8 comprehensive question types, which
provides a large-scale and diverse dataset for remote sensing applications.
Based on this, we present VisTA, a simple yet effective baseline method that
unifies the tasks of question answering and grounding by delivering both visual
and textual answers. Our method achieves state-of-the-art results on both the
classic CDVQA and the proposed CDQAG datasets. Extensive qualitative and
quantitative experimental results provide useful insights for the development
of better CDQAG models, and we hope that our work can inspire further research
in this important yet underexplored direction. The proposed benchmark dataset
and method are available at https://github.com/like413/VisTA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models
  for Medical Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong He, Pengfei Li, Gang Liu, Shenjun Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) inherit the superior text
understanding capabilities of LLMs and extend these capabilities to multimodal
scenarios. These models achieve excellent results in the general domain of
multimodal tasks. However, in the medical domain, the substantial training
costs and the requirement for extensive medical data pose challenges to the
development of medical MLLMs. Furthermore, due to the free-text form of
answers, tasks such as visual grounding that need to produce output in a
prescribed form become difficult for MLLMs. So far, there have been no medical
MLLMs works in medical visual grounding area. For the medical vision grounding
task, which involves identifying locations in medical images based on short
text descriptions, we propose Parameter-efficient Fine-tuning medical
multimodal large language models for Medcial Visual Grounding (PFMVG). To
validate the performance of the model, we evaluate it on a public benchmark
dataset for medical visual grounding, where it achieves competitive results,
and significantly outperforming GPT-4v. Our code will be open sourced after
peer review.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Disentangled Representations: Towards Improved Latent
  Units via Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngjun Jun, Jiwoo Park, Kyobin Choo, Tae Eun Choi, Seong Jae Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disentangled representation learning (DRL) aims to break down observed data
into core intrinsic factors for a profound understanding of the data. In
real-world scenarios, manually defining and labeling these factors are
non-trivial, making unsupervised methods attractive. Recently, there have been
limited explorations of utilizing diffusion models (DMs), which are already
mainstream in generative modeling, for unsupervised DRL. They implement their
own inductive bias to ensure that each latent unit input to the DM expresses
only one distinct factor. In this context, we design Dynamic Gaussian Anchoring
to enforce attribute-separated latent units for more interpretable DRL. This
unconventional inductive bias explicitly delineates the decision boundaries
between attributes while also promoting the independence among latent units.
Additionally, we also propose Skip Dropout technique, which easily modifies the
denoising U-Net to be more DRL-friendly, addressing its uncooperative nature
with the disentangling feature extractor. Our methods, which carefully consider
the latent unit semantics and the distinct DM structure, enhance the
practicality of DM-based disentangled representations, demonstrating
state-of-the-art disentanglement performance on both synthetic and real data,
as well as advantages in downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Action Recognition (HAR) Using Skeleton-based Quantum Spatial
  Temporal Relative <span class="highlight-title">Transformer</span> Network: ST-RTR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faisal Mehmood, Enqing Chen, Touqeer Abbas, Samah M. Alzanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Human Action Recognition (HAR) is an interesting research area in
human-computer interaction used to monitor the activities of elderly and
disabled individuals affected by physical and mental health. In the recent era,
skeleton-based HAR has received much attention because skeleton data has shown
that it can handle changes in striking, body size, camera views, and complex
backgrounds. One key characteristic of ST-GCN is automatically learning spatial
and temporal patterns from skeleton sequences. It has some limitations, as this
method only works for short-range correlation due to its limited receptive
field. Consequently, understanding human action requires long-range
interconnection. To address this issue, we developed a quantum spatial-temporal
relative transformer ST-RTR model. The ST-RTR includes joint and relay nodes,
which allow efficient communication and data transmission within the network.
These nodes help to break the inherent spatial and temporal skeleton
topologies, which enables the model to understand long-range human action
better. Furthermore, we combine quantum ST-RTR with a fusion model for further
performance improvements. To assess the performance of the quantum ST-RTR
method, we conducted experiments on three skeleton-based HAR benchmarks: NTU
RGB+D 60, NTU RGB+D 120, and UAV-Human. It boosted CS and CV by 2.11 % and
1.45% on NTU RGB+D 60, 1.25% and 1.05% on NTU RGB+D 120. On UAV-Human datasets,
accuracy improved by 2.54%. The experimental outcomes explain that the proposed
ST-RTR model significantly improves action recognition associated with the
standard ST-GCN method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyang Pan, Angjoo Kanazawa, Hang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-occlusion is common when capturing people in the wild, where the
performer do not follow predefined motion scripts. This challenges existing
monocular human reconstruction systems that assume full body visibility. We
introduce Self-Occluded Avatar Recovery (SOAR), a method for complete human
reconstruction from partial observations where parts of the body are entirely
unobserved. SOAR leverages structural normal prior and generative diffusion
prior to address such an ill-posed reconstruction problem. For structural
normal prior, we model human with an reposable surfel model with well-defined
and easily readable shapes. For generative diffusion prior, we perform an
initial reconstruction and refine it using score distillation. On various
benchmarks, we show that SOAR performs favorably than state-of-the-art
reconstruction and generation methods, and on-par comparing to concurrent
works. Additional video results and code are available at
https://soar-avatar.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDT: An Efficient Diffusion <span class="highlight-title">Transformer</span> Framework Inspired by Human-like
  Sketching <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinwang Chen, Ning Liu, Yichen Zhu, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based Diffusion Probabilistic Models (DPMs) have shown more
potential than CNN-based DPMs, yet their extensive computational requirements
hinder widespread practical applications. To reduce the computation budget of
transformer-based DPMs, this work proposes the Efficient Diffusion Transformer
(EDT) framework. The framework includes a lightweight-design diffusion model
architecture, and a training-free Attention Modulation Matrix and its
alternation arrangement in EDT inspired by human-like sketching. Additionally,
we propose a token relation-enhanced masking training strategy tailored
explicitly for EDT to augment its token relation learning capability. Our
extensive experiments demonstrate the efficacy of EDT. The EDT framework
reduces training and inference costs and surpasses existing transformer-based
diffusion models in image synthesis performance, thereby achieving a
significant overall enhancement. With lower FID, EDT-S, EDT-B, and EDT-XL
attained speed-ups of 3.93x, 2.84x, and 1.92x respectively in the training
phase, and 2.29x, 2.29x, and 2.22x respectively in inference, compared to the
corresponding sizes of MDTv2. The source code is released at
https://github.com/xinwangChen/EDT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Xinwang Chen and Ning Liu are with equal contributions. This paper
  has been accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Token Merging for Long-form Video Understanding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seon-Ho Lee, Jue Wang, Zhikang Zhang, David Fan, Xinyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the scale of data and models for video understanding rapidly expand,
handling long-form video input in transformer-based models presents a practical
challenge. Rather than resorting to input sampling or token dropping, which may
result in information loss, token merging shows promising results when used in
collaboration with transformers. However, the application of token merging for
long-form video processing is not trivial. We begin with the premise that token
merging should not rely solely on the similarity of video tokens; the saliency
of tokens should also be considered. To address this, we explore various video
token merging strategies for long-form video classification, starting with a
simple extension of image token merging, moving to region-concentrated merging,
and finally proposing a learnable video token merging (VTM) algorithm that
dynamically merges tokens based on their saliency. Extensive experimental
results show that we achieve better or comparable performances on the LVU,
COIN, and Breakfast datasets. Moreover, our approach significantly reduces
memory costs by 84% and boosts throughput by approximately 6.89 times compared
to baseline algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Driving by the Rules: A Benchmark for Integrating Traffic Sign
  Regulations into Vectorized HD Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Chang, Maixuan Xue, Xinran Liu, Zheng Pan, Xing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring adherence to traffic sign regulations is essential for both human
and autonomous vehicle navigation. While current benchmark datasets concentrate
on lane perception or basic traffic sign recognition, they often overlook the
intricate task of integrating these regulations into lane operations.
Addressing this gap, we introduce MapDR, a novel dataset designed for the
extraction of Driving Rules from traffic signs and their association with
vectorized, locally perceived HD Maps. MapDR features over 10,000 annotated
video clips that capture the intricate correlation between traffic sign
regulations and lanes. We define two pivotal sub-tasks: 1) Rule Extraction from
Traffic Sign, which accurately deciphers regulatory instructions, and 2)
Rule-Lane Correspondence Reasoning, which aligns these rules with their
respective lanes. Built upon this benchmark, we provide a multimodal solution
that offers a strong baseline for advancing autonomous driving technologies. It
fills a critical gap in the integration of traffic sign rules, contributing to
the development of reliable autonomous navigation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context LoRA for Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research arXiv:2410.15027 has explored the use of diffusion
transformers (DiTs) for task-agnostic image generation by simply concatenating
attention tokens across images. However, despite substantial computational
resources, the fidelity of the generated images remains suboptimal. In this
study, we reevaluate and streamline this framework by hypothesizing that
text-to-image DiTs inherently possess in-context generation capabilities,
requiring only minimal tuning to activate them. Through diverse task
experiments, we qualitatively demonstrate that existing text-to-image DiTs can
effectively perform in-context generation without any tuning. Building on this
insight, we propose a remarkably simple pipeline to leverage the in-context
abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint
captioning of multiple images, and (3) apply task-specific LoRA tuning using
small datasets (e.g., $20\sim 100$ samples) instead of full-parameter tuning
with large datasets. We name our models In-Context LoRA (IC-LoRA). This
approach requires no modifications to the original DiT models, only changes to
the training data. Remarkably, our pipeline generates high-fidelity image sets
that better adhere to prompts. While task-specific in terms of tuning data, our
framework remains task-agnostic in architecture and pipeline, offering a
powerful tool for the community and providing valuable insights for further
research on product-level task-agnostic generation systems. We release our
code, data, and models at https://github.com/ali-vilab/In-Context-LoRA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ali-vilab.github.io/In-Context-Lora-Page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Set 3D object detection in LiDAR data as an Out-of-Distribution
  problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Soum-Fontez, Jean-Emmanuel Deschaud, François Goulette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Object Detection from LiDAR data has achieved industry-ready performance
in controlled environments through advanced deep learning methods. However,
these neural network models are limited by a finite set of inlier object
categories. Our work redefines the open-set 3D Object Detection problem in
LiDAR data as an Out-Of-Distribution (OOD) problem to detect outlier objects.
This approach brings additional information in comparison with traditional
object detection. We establish a comparative benchmark and show that two-stage
OOD methods, notably autolabelling, show promising results for 3D OOD Object
Detection. Our contributions include setting a rigorous evaluation protocol by
examining the evaluation of hyperparameters and evaluating strategies for
generating additional data to train an OOD-aware 3D object detector. This
comprehensive analysis is essential for developing robust 3D object detection
systems that can perform reliably in diverse and unpredictable real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse Attitude Statistics Based Star Map Identification Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunmei Dong, Qinglong Wang, Haiqing Wang, Qianqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The star tracker is generally affected by the atmospheric background light
and the aerodynamic environment when working in near space, which results in
missing stars or false stars. Moreover, high-speed maneuvering may cause star
trailing, which reduces the accuracy of the star position. To address the
challenges for starmap identification, a reverse attitude statistics based
method is proposed to handle position noise, false stars, and missing stars.
Conversely to existing methods which match before solving for attitude, this
method introduces attitude solving into the matching process, and obtains the
final match and the correct attitude simultaneously by frequency statistics.
Firstly, based on stable angular distance features, the initial matching is
obtained by utilizing spatial hash indexing. Then, the dual-vector attitude
determination is introduced to calculate potential attitude. Finally, the star
pairs are accurately matched by applying a frequency statistics filtering
method. In addition, Bayesian optimization is employed to find optimal
parameters under the impact of noises, which is able to enhance the algorithm
performance further. In this work, the proposed method is validated in
simulation, field test and on-orbit experiment. Compared with the
state-of-the-art, the identification rate is improved by more than 14.3%, and
the solving time is reduced by over 28.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17figures, 4 tables, 4663 words, submitted to IEEE Sensors
  Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EXACFS -- A CIL Method to mitigate Catastrophic Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S Balasubramanian, M Sai Subramaniam, Sai Sriram Talasu, P Yedu Krishna, Manepalli Pranav Phanindra Sai, Ravi Mukkamala, Darshan Gera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNS) excel at learning from static datasets but
struggle with continual learning, where data arrives sequentially. Catastrophic
forgetting, the phenomenon of forgetting previously learned knowledge, is a
primary challenge. This paper introduces EXponentially Averaged Class-wise
Feature Significance (EXACFS) to mitigate this issue in the class incremental
learning (CIL) setting. By estimating the significance of model features for
each learned class using loss gradients, gradually aging the significance
through the incremental tasks and preserving the significant features through a
distillation loss, EXACFS effectively balances remembering old knowledge
(stability) and learning new knowledge (plasticity). Extensive experiments on
CIFAR-100 and ImageNet-100 demonstrate EXACFS's superior performance in
preserving stability while acquiring plasticity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EchoNarrator: Generating natural text explanations for ejection fraction
  predictions <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarina Thomas, Qing Cao, Anna Novikova, Daria Kulikova, Guy Ben-Yosef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ejection fraction (EF) of the left ventricle (LV) is considered as one of the
most important measurements for diagnosing acute heart failure and can be
estimated during cardiac ultrasound acquisition. While recent successes in deep
learning research successfully estimate EF values, the proposed models often
lack an explanation for the prediction. However, providing clear and intuitive
explanations for clinical measurement predictions would increase the trust of
cardiologists in these models. In this paper, we explore predicting EF
measurements with Natural Language Explanation (NLE). We propose a model that
in a single forward pass combines estimation of the LV contour over multiple
frames, together with a set of modules and routines for computing various
motion and shape attributes that are associated with ejection fraction. It then
feeds the attributes into a large language model to generate text that helps to
explain the network's outcome in a human-like manner. We provide experimental
evaluation of our explanatory output, as well as EF prediction, and show that
our model can provide EF comparable to state-of-the-art together with
meaningful and accurate natural language explanation to the prediction. The
project page can be found at https://github.com/guybenyosef/EchoNarrator .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaled Inverse Graphics: Efficiently Learning Large Sets of 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Kassab, Antoine Schnepf, Jean-Yves Franceschi, Laurent Caraffa, Flavian Vasile, Jeremie Mary, Andrew Comport, Valérie Gouet-Brunet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the field of inverse graphics has been witnessing continuous growth,
techniques devised thus far predominantly focus on learning individual scene
representations. In contrast, learning large sets of scenes has been a
considerable bottleneck in NeRF developments, as repeatedly applying inverse
graphics on a sequence of scenes, though essential for various applications,
remains largely prohibitive in terms of resource costs. We introduce a
framework termed "scaled inverse graphics", aimed at efficiently learning large
sets of scene representations, and propose a novel method to this end. It
operates in two stages: (i) training a compression model on a subset of scenes,
then (ii) training NeRF models on the resulting smaller representations,
thereby reducing the optimization space per new scene. In practice, we compact
the representation of scenes by learning NeRFs in a latent space to reduce the
image resolution, and sharing information across scenes to reduce NeRF
representation complexity. We experimentally show that our method presents both
the lowest training time and memory footprint in scaled inverse graphics
compared to other methods applied independently on each scene. Our codebase is
publicly available as open-source. Our project page can be found at
https://scaled-ig.github.io .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLLA-UNet: Mamba-like Linear Attention in an Efficient U-Shape Model for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufeng Jiang, Zongxi Li, Xiangyan Chen, Haoran Xie, Jing Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in medical imaging have resulted in more complex and
diverse images, with challenges such as high anatomical variability, blurred
tissue boundaries, low organ contrast, and noise. Traditional segmentation
methods struggle to address these challenges, making deep learning approaches,
particularly U-shaped architectures, increasingly prominent. However, the
quadratic complexity of standard self-attention makes Transformers
computationally prohibitive for high-resolution images. To address these
challenges, we propose MLLA-UNet (Mamba-Like Linear Attention UNet), a novel
architecture that achieves linear computational complexity while maintaining
high segmentation accuracy through its innovative combination of linear
attention and Mamba-inspired adaptive mechanisms, complemented by an efficient
symmetric sampling structure for enhanced feature processing. Our architecture
effectively preserves essential spatial features while capturing long-range
dependencies at reduced computational complexity. Additionally, we introduce a
novel sampling strategy for multi-scale feature fusion. Experiments demonstrate
that MLLA-UNet achieves state-of-the-art performance on six challenging
datasets with 24 different segmentation tasks, including but not limited to
FLARE22, AMOS CT, and ACDC, with an average DSC of 88.32%. These results
underscore the superiority of MLLA-UNet over existing methods. Our
contributions include the novel 2D segmentation architecture and its empirical
validation. The code is available via https://github.com/csyfjiang/MLLA-UNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoTaDual: Modality-Task Dual Alignment for Enhanced Zero-shot Composed
  Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwen Li, Fei Su, Zhicheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed Image Retrieval (CIR) is a challenging vision-language task,
utilizing bi-modal (image+text) queries to retrieve target images. Despite the
impressive performance of supervised CIR, the dependence on costly,
manually-labeled triplets limits its scalability and zero-shot capability. To
address this issue, zero-shot composed image retrieval (ZS-CIR) is presented
along with projection-based approaches. However, such methods face two major
problems, i.e., task discrepancy between pre-training (image $\leftrightarrow$
text) and inference (image+text $\rightarrow$ image), and modality discrepancy.
The latter pertains to approaches based on text-only projection training due to
the necessity of feature extraction from the reference image during inference.
In this paper, we propose a two-stage framework to tackle both discrepancies.
First, to ensure efficiency and scalability, a textual inversion network is
pre-trained on large-scale caption datasets. Subsequently, we put forward
Modality-Task Dual Alignment (MoTaDual) as the second stage, where
large-language models (LLMs) generate triplet data for fine-tuning, and
additionally, prompt learning is introduced in a multi-modal context to
effectively alleviate both modality and task discrepancies. The experimental
results show that our MoTaDual achieves the state-of-the-art performance across
four widely used ZS-CIR benchmarks, while maintaining low training time and
computational cost. The code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Analysis of <span class="highlight-title">GPT</span>-4V's Performance on Fashion Aesthetic
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Hirakawa, Takashi Wada, Kazuya Morishita, Ryotaro Shimizu, Takuya Furusawa, Sai Htaung Kham, Yuki Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fashion aesthetic evaluation is the task of estimating how well the outfits
worn by individuals in images suit them. In this work, we examine the zero-shot
performance of GPT-4V on this task for the first time. We show that its
predictions align fairly well with human judgments on our datasets, and also
find that it struggles with ranking outfits in similar colors. The code is
available at https://github.com/st-tech/gpt4v-fashion-aesthetic-evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiufeng Huang, Ruiqi Li, Yiu-ming Cheung, Ka Chun Cheung, Simon See, Renjie Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has become a crucial method for acquiring 3D
assets. To protect the copyright of these assets, digital watermarking
techniques can be applied to embed ownership information discreetly within 3DGS
models. However, existing watermarking methods for meshes, point clouds, and
implicit radiance fields cannot be directly applied to 3DGS models, as 3DGS
models use explicit 3D Gaussians with distinct structures and do not rely on
neural networks. Naively embedding the watermark on a pre-trained 3DGS can
cause obvious distortion in rendered images. In our work, we propose an
uncertainty-based method that constrains the perturbation of model parameters
to achieve invisible watermarking for 3DGS. At the message decoding stage, the
copyright messages can be reliably extracted from both 3D Gaussians and 2D
rendered images even under various forms of 3D and 2D distortions. We conduct
extensive experiments on the Blender, LLFF and MipNeRF-360 datasets to validate
the effectiveness of our proposed method, demonstrating state-of-the-art
performance on both message decoding accuracy and view synthesis quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aggregate-and-Adapt Natural Language <span class="highlight-title">Prompt</span>s for Downstream
  Generalization of CLIP <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Huang, Skyler Seto, Samira Abnar, David Grangier, Navdeep Jaitly, Josh Susskind
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained vision-language models like CLIP have shown promising
generalization capability, but may struggle in specialized domains (e.g.,
satellite imagery) or fine-grained classification (e.g., car models) where the
visual concepts are unseen or under-represented during pretraining. Prompt
learning offers a parameter-efficient finetuning framework that can adapt CLIP
to downstream tasks even when limited annotation data are available. In this
paper, we improve prompt learning by distilling the textual knowledge from
natural language prompts (either human- or LLM-generated) to provide rich
priors for those under-represented concepts. We first obtain a prompt
``summary'' aligned to each input image via a learned prompt aggregator. Then
we jointly train a prompt generator, optimized to produce a prompt embedding
that stays close to the aggregated summary while minimizing task loss at the
same time. We dub such prompt embedding as Aggregate-and-Adapted Prompt
Embedding (AAPE). AAPE is shown to be able to generalize to different
downstream data distributions and tasks, including vision-language
understanding tasks (e.g., few-shot classification, VQA) and generation tasks
(image captioning) where AAPE achieves competitive performance. We also show
AAPE is particularly helpful to handle non-canonical and OOD examples.
Furthermore, AAPE learning eliminates LLM-based inference cost as required by
baselines, and scales better with data and LLM model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XRDSLAM: A Flexible and Modular Framework for Deep Learning based SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Wang, Nan Wang, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a flexible SLAM framework, XRDSLAM. It adopts a
modular code design and a multi-process running mechanism, providing highly
reusable foundational modules such as unified dataset management, 3d
visualization, algorithm configuration, and metrics evaluation. It can help
developers quickly build a complete SLAM system, flexibly combine different
algorithm modules, and conduct standardized benchmarking for accuracy and
efficiency comparison. Within this framework, we integrate several
state-of-the-art SLAM algorithms with different types, including NeRF and 3DGS
based SLAM, and even odometry or reconstruction algorithms, which demonstrates
the flexibility and extensibility. We also conduct a comprehensive comparison
and evaluation of these integrated algorithms, analyzing the characteristics of
each. Finally, we contribute all the code, configuration and data to the
open-source community, which aims to promote the widespread research and
development of SLAM technology within the open-source ecosystem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Attacks of Vision Tasks in the Past 10 Years: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiyu Zhang, Xiaogang Xu, Jiafei Wu, Zhe Liu, Lu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks, which manipulate input data to undermine model
availability and integrity, pose significant security threats during machine
learning inference. With the advent of Large Vision-Language Models (LVLMs),
new attack vectors, such as cognitive bias, prompt injection, and jailbreak
techniques, have emerged. Understanding these attacks is crucial for developing
more robust systems and demystifying the inner workings of neural networks.
However, existing reviews often focus on attack classifications and lack
comprehensive, in-depth analysis. The research community currently needs: 1)
unified insights into adversariality, transferability, and generalization; 2)
detailed evaluations of existing methods; 3) motivation-driven attack
categorizations; and 4) an integrated perspective on both traditional and LVLM
attacks. This article addresses these gaps by offering a thorough summary of
traditional and LVLM adversarial attacks, emphasizing their connections and
distinctions, and providing actionable insights for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wide Two-Layer Networks can Learn from Adversarial Perturbations <span class="chip">NeurIPS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples have raised several open questions, such as why they can
deceive classifiers and transfer between different models. A prevailing
hypothesis to explain these phenomena suggests that adversarial perturbations
appear as random noise but contain class-specific features. This hypothesis is
supported by the success of perturbation learning, where classifiers trained
solely on adversarial examples and the corresponding incorrect labels
generalize well to correctly labeled test data. Although this hypothesis and
perturbation learning are effective in explaining intriguing properties of
adversarial examples, their solid theoretical foundation is limited. In this
study, we theoretically explain the counterintuitive success of perturbation
learning. We assume wide two-layer networks and the results hold for any data
distribution. We prove that adversarial perturbations contain sufficient
class-specific features for networks to generalize from them. Moreover, the
predictions of classifiers trained on mislabeled adversarial examples coincide
with those of classifiers trained on correctly labeled clean samples. The code
is available at https://github.com/s-kumano/perturbation-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathilde Caron, Alireza Fathi, Cordelia Schmid, Ahmet Iscen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web-scale visual entity recognition, the task of associating images with
their corresponding entities within vast knowledge bases like Wikipedia,
presents significant challenges due to the lack of clean, large-scale training
data. In this paper, we propose a novel methodology to curate such a dataset,
leveraging a multimodal large language model (LLM) for label verification,
metadata generation, and rationale explanation. Instead of relying on the
multimodal LLM to directly annotate data, which we found to be suboptimal, we
prompt it to reason about potential candidate entity labels by accessing
additional contextually relevant information (such as Wikipedia), resulting in
more accurate annotations. We further use the multimodal LLM to enrich the
dataset by generating question-answer pairs and a grounded finegrained textual
description (referred to as "rationale") that explains the connection between
images and their assigned entities. Experiments demonstrate that models trained
on this automatically curated data achieve state-of-the-art performance on
web-scale visual entity recognition tasks (e.g. +6.9% improvement in OVEN
entity task), underscoring the importance of high-quality training data in this
domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Nie, Jiangqun Ni, Jian Zhang, Bin Zhang, Weizhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of deepfake generation techniques, the importance of
deepfake detection in protecting multimedia content integrity has become
increasingly obvious. Recently, temporal inconsistency clues have been explored
to improve the generalizability of deepfake video detection. According to our
observation, the temporal artifacts of forged videos in terms of motion
information usually exhibits quite distinct inconsistency patterns along
horizontal and vertical directions, which could be leveraged to improve the
generalizability of detectors. In this paper, a transformer-based framework for
Diffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits
directional inconsistencies for deepfake video detection. Specifically, DIP
begins with a spatiotemporal encoder to represent spatiotemporal information. A
directional inconsistency decoder is adopted accordingly, where direction-aware
attention and inconsistency diffusion are incorporated to explore potential
inconsistency patterns and jointly learn the inherent relationships. In
addition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to
contrast spatiotemporally augmented sample pairs and prevent the model from
overfitting nonessential forgery artifacts. Extensive experiments on several
public datasets demonstrate that our method could effectively identify
directional forgery clues and achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, accepted with IEEE Trans. on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-Blur: A 3D Scene-Based <span class="highlight-title">Dataset</span> for Realistic Image Deblurring <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwoo Lee, Joonkyu Park, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To train a deblurring network, an appropriate dataset with paired blurry and
sharp images is essential. Existing datasets collect blurry images either
synthetically by aggregating consecutive sharp frames or using sophisticated
camera systems to capture real blur. However, these methods offer limited
diversity in blur types (blur trajectories) or require extensive human effort
to reconstruct large-scale datasets, failing to fully reflect real-world blur
scenarios. To address this, we propose GS-Blur, a dataset of synthesized
realistic blurry images created using a novel approach. To this end, we first
reconstruct 3D scenes from multi-view images using 3D Gaussian Splatting
(3DGS), then render blurry images by moving the camera view along the randomly
generated motion trajectories. By adopting various camera trajectories in
reconstructing our GS-Blur, our dataset contains realistic and diverse types of
blur, offering a large-scale dataset that generalizes well to real-world blur.
Using GS-Blur with various deblurring methods, we demonstrate its ability to
generalize effectively compared to previous synthetic or real blur datasets,
showing significant improvements in deblurring performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Datasets & Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel Clinical-Grade Prostate Cancer Detection and Grading Model:
  Development and Prospective Validation Using Real World Data, with
  Performance Assessment on IHC Requested Cases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramin Nateghi, Ruoji Zhou, Madeline Saft, Marina Schnauss, Clayton Neill, Ridwan Alam, Nicole Handa, Mitchell Huang, Eric V Li, Jeffery A Goldstein, Edward M Schaeffer, Menatalla Nadim, Fattaneh Pourakpour, Bogdan Isaila, Christopher Felicelli, Vikas Mehta, Behtash G Nezami, Ashley Ross, Ximing Yang, Lee AD Cooper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence may assist healthcare systems in meeting increasing
demand for pathology services while maintaining diagnostic quality and reducing
turnaround time and costs. We aimed to investigate the performance of an
institutionally developed system for prostate cancer detection, grading, and
workflow optimization and to contrast this with commercial alternatives. From
August 2021 to March 2023, we scanned 21,396 slides from 1,147 patients with
positive biopsies. We developed models for cancer detection, grading, and
screening of equivocal cases for IHC ordering. We compared a task-specific
model trained using the PANDA dataset of prostate cancer biopsies with one
built using features extracted by the general-purpose histology foundation
model, UNI and compare their performance in an unfiltered prospectively
collected dataset that reflects our patient population (1737 slides,95
patients). We evaluated the contributions of a bespoke model designed to
improve sensitivity in detecting small cancer foci and scoring of broader
patterns observed at lower resolution. We found high concordance between the
developed systems and pathologist reference in detection (AUC 98.5, sensitivity
95.0, and specificity 97.8), ISUP grading (quadratic Cohen's kappa 0.869),
grade group 3 or higher (AUC 97.5, sensitivity 94.9, specificity 96.6) and
comparable to published data from commercial systems. Screening could reduce
IHC ordering for equivocal cases by 44.5% with an overall error rate of 1.8%
(1.4% false positive, 0.4% false negative rates). Institutions like academic
medical centers that have high scanning volumes and report abstraction
capabilities can develop accurate computational pathology models for internal
use. These models have the potential to aid in quality control role and to
improve workflow in the pathology lab to help meet future challenges in
prostate cancer diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recovering Complete Actions for Cross-<span class="highlight-title">dataset</span> Skeleton Action
  Recognition <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanchao Liu, Yujiang Li, Tai-Jiang Mu, Shi-Min Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite huge progress in skeleton-based action recognition, its
generalizability to different domains remains a challenging issue. In this
paper, to solve the skeleton action generalization problem, we present a
recover-and-resample augmentation framework based on a novel complete action
prior. We observe that human daily actions are confronted with temporal
mismatch across different datasets, as they are usually partial observations of
their complete action sequences. By recovering complete actions and resampling
from these full sequences, we can generate strong augmentations for unseen
domains. At the same time, we discover the nature of general action
completeness within large datasets, indicated by the per-frame diversity over
time. This allows us to exploit two assets of transferable knowledge that can
be shared across action samples and be helpful for action completion: boundary
poses for determining the action start, and linear temporal transforms for
capturing global action patterns. Therefore, we formulate the recovering stage
as a two-step stochastic action completion with boundary pose-conditioned
extrapolation followed by smooth linear transforms. Both the boundary poses and
linear transforms can be efficiently learned from the whole dataset via
clustering. We validate our approach on a cross-dataset setting with three
skeleton action datasets, outperforming other domain generalization approaches
by a considerable margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Posture-Informed Muscular Force Learning for Robust Hand Pressure
  Estimation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungjin Seo, Junghoon Seo, Hanseok Jeong, Sangpil Kim, Sang Ho Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PiMForce, a novel framework that enhances hand pressure estimation
by leveraging 3D hand posture information to augment forearm surface
electromyography (sEMG) signals. Our approach utilizes detailed spatial
information from 3D hand poses in conjunction with dynamic muscle activity from
sEMG to enable accurate and robust whole-hand pressure measurements under
diverse hand-object interactions. We also developed a multimodal data
collection system that combines a pressure glove, an sEMG armband, and a
markerless finger-tracking module. We created a comprehensive dataset from 21
participants, capturing synchronized data of hand posture, sEMG signals, and
exerted hand pressure across various hand postures and hand-object interaction
scenarios using our collection system. Our framework enables precise hand
pressure estimation in complex and natural interaction scenarios. Our approach
substantially mitigates the limitations of traditional sEMG-based or
vision-based methods by integrating 3D hand posture information with sEMG
signals. Video demos, data, and code are available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cycle-Constrained Adversarial Denoising Convolutional Network for PET
  Image Denoising: Multi-Dimensional Validation on Large <span class="highlight-title">Dataset</span>s with Reader
  Study and Real Low-Dose Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucun Hou, Fenglin Zhan, Xin Cheng, Chenxi Li, Ziquan Yuan, Runze Liao, Haihao Wang, Jianlang Hua, Jing Wu, Jianyong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positron emission tomography (PET) is a critical tool for diagnosing tumors
and neurological disorders but poses radiation risks to patients, particularly
to sensitive populations. While reducing injected radiation dose mitigates this
risk, it often compromises image quality. To reconstruct full-dose-quality
images from low-dose scans, we propose a Cycle-constrained Adversarial
Denoising Convolutional Network (Cycle-DCN). This model integrates a noise
predictor, two discriminators, and a consistency network, and is optimized
using a combination of supervised loss, adversarial loss, cycle consistency
loss, identity loss, and neighboring Structural Similarity Index (SSIM) loss.
Experiments were conducted on a large dataset consisting of raw PET brain data
from 1,224 patients, acquired using a Siemens Biograph Vision PET/CT scanner.
Each patient underwent a 120-seconds brain scan. To simulate low-dose PET
conditions, images were reconstructed from shortened scan durations of 30, 12,
and 5 seconds, corresponding to 1/4, 1/10, and 1/24 of the full-dose
acquisition, respectively, using a custom-developed GPU-based image
reconstruction software. The results show that Cycle-DCN significantly improves
average Peak Signal-to-Noise Ratio (PSNR), SSIM, and Normalized Root Mean
Square Error (NRMSE) across three dose levels, with improvements of up to 56%,
35%, and 71%, respectively. Additionally, it achieves contrast-to-noise ratio
(CNR) and Edge Preservation Index (EPI) values that closely align with
full-dose images, effectively preserving image details, tumor shape, and
contrast, while resolving issues with blurred edges. The results of reader
studies indicated that the images restored by Cycle-DCN consistently received
the highest ratings from nuclear medicine physicians, highlighting their strong
clinical relevance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Learning Multi-Modal Forgery Representation for Diffusion Generated
  Video Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiufeng Song, Xiao Guo, Jiache Zhang, Qirui Li, Lei Bai, Xiaoming Liu, Guangtao Zhai, Xiaohong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large numbers of synthesized videos from diffusion models pose threats to
information security and authenticity, leading to an increasing demand for
generated content detection. However, existing video-level detection algorithms
primarily focus on detecting facial forgeries and often fail to identify
diffusion-generated content with a diverse range of semantics. To advance the
field of video forensics, we propose an innovative algorithm named Multi-Modal
Detection(MM-Det) for detecting diffusion-generated videos. MM-Det utilizes the
profound perceptual and comprehensive abilities of Large Multi-modal Models
(LMMs) by generating a Multi-Modal Forgery Representation (MMFR) from LMM's
multi-modal space, enhancing its ability to detect unseen forgery content.
Besides, MM-Det leverages an In-and-Across Frame Attention (IAFA) mechanism for
feature augmentation in the spatio-temporal domain. A dynamic fusion strategy
helps refine forgery representations for the fusion. Moreover, we construct a
comprehensive diffusion video dataset, called Diffusion Video Forensics (DVF),
across a wide range of forgery videos. MM-Det achieves state-of-the-art
performance in DVF, demonstrating the effectiveness of our algorithm. Both
source code and DVF are available at https://github.com/SparkleXFantasy/MM-Det.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and
  Benchmarking <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarking vision-based driving policies is challenging. On one hand,
open-loop evaluation with real data is easy, but these results do not reflect
closed-loop performance. On the other, closed-loop evaluation is possible in
simulation, but is hard to scale due to its significant computational demands.
Further, the simulators available today exhibit a large domain gap to real
data. This has resulted in an inability to draw clear conclusions from the
rapidly growing body of research on end-to-end autonomous driving. In this
paper, we present NAVSIM, a middle ground between these evaluation paradigms,
where we use large datasets in combination with a non-reactive simulator to
enable large-scale real-world benchmarking. Specifically, we gather
simulation-based metrics, such as progress and time to collision, by unrolling
bird's eye view abstractions of the test scenes for a short simulation horizon.
Our simulation is non-reactive, i.e., the evaluated policy and environment do
not influence each other. As we demonstrate empirically, this decoupling allows
open-loop metric computation while being better aligned with closed-loop
evaluations than traditional displacement errors. NAVSIM enabled a new
competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting
in several new insights. On a large set of challenging scenarios, we observe
that simple methods with moderate compute requirements such as TransFuser can
match recent large-scale end-to-end driving architectures such as UniAD. Our
modular framework can potentially be extended with new datasets, data curation
strategies, and metrics, and will be continually maintained to host future
challenges. Our code is available at
https://github.com/autonomousvision/navsim.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantized neural network for complex hologram generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutaka Endo, Minoru Oikawa, Timothy D. Wilkinson, Tomoyoshi Shimobaba, Tomoyoshi Ito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-generated holography (CGH) is a promising technology for augmented
reality displays, such as head-mounted or head-up displays. However, its high
computational demand makes it impractical for implementation. Recent efforts to
integrate neural networks into CGH have successfully accelerated computing
speed, demonstrating the potential to overcome the trade-off between
computational cost and image quality. Nevertheless, deploying neural
network-based CGH algorithms on computationally limited embedded systems
requires more efficient models with lower computational cost, memory footprint,
and power consumption. In this study, we developed a lightweight model for
complex hologram generation by introducing neural network quantization.
Specifically, we built a model based on tensor holography and quantized it from
32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Our
performance evaluation shows that the proposed INT8 model achieves hologram
quality comparable to that of the FP32 model while reducing the model size by
approximately 70% and increasing the speed fourfold. Additionally, we
implemented the INT8 model on a system-on-module to demonstrate its
deployability on embedded platforms and high power efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise
  Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, Zeynep Akata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) models have made significant advancements in recent
years, but they still struggle to accurately capture intricate details
specified in complex compositional prompts. While fine-tuning T2I models with
reward objectives has shown promise, it suffers from "reward hacking" and may
not generalize well to unseen prompt distributions. In this work, we propose
Reward-based Noise Optimization (ReNO), a novel approach that enhances T2I
models at inference by optimizing the initial noise based on the signal from
one or multiple human preference reward models. Remarkably, solving this
optimization problem with gradient ascent for 50 iterations yields impressive
results on four different one-step models across two competitive benchmarks,
T2I-CompBench and GenEval. Within a computational budget of 20-50 seconds,
ReNO-enhanced one-step models consistently surpass the performance of all
current open-source Text-to-Image models. Extensive user studies demonstrate
that our model is preferred nearly twice as often compared to the popular SDXL
model and is on par with the proprietary Stable Diffusion 3 with 8B parameters.
Moreover, given the same computational resources, a ReNO-optimized one-step
model outperforms widely-used open-source models such as SDXL and
PixArt-$\alpha$, highlighting the efficiency and effectiveness of ReNO in
enhancing T2I model performance at inference time. Code is available at
https://github.com/ExplainableML/ReNO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Invisible Image Watermarks Are Provably Removable Using Generative AI <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01953v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01953v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuandong Zhao, Kexun Zhang, Zihao Su, Saastha Vasan, Ilya Grishchenko, Christopher Kruegel, Giovanni Vigna, Yu-Xiang Wang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Invisible watermarks safeguard images' copyrights by embedding hidden
messages only detectable by owners. They also prevent people from misusing
images, especially those generated by AI models. We propose a family of
regeneration attacks to remove these invisible watermarks. The proposed attack
method first adds random noise to an image to destroy the watermark and then
reconstructs the image. This approach is flexible and can be instantiated with
many existing image-denoising algorithms and pre-trained generative models such
as diffusion models. Through formal proofs and extensive empirical evaluations,
we demonstrate that pixel-level invisible watermarks are vulnerable to this
regeneration attack. Our results reveal that, across four different pixel-level
watermarking schemes, the proposed method consistently achieves superior
performance compared to existing attack techniques, with lower detection rates
and higher image quality. However, watermarks that keep the image semantically
similar can be an alternative defense against our attacks. Our finding
underscores the need for a shift in research/industry emphasis from invisible
watermarks to semantic-preserving watermarks. Code is available at
https://github.com/XuandongZhao/WatermarkAttacker
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoVA: Adapting Mixture of Vision Experts to Multimodal Context <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the key component in multimodal large language models (MLLMs), the ability
of the visual encoder greatly affects MLLM's understanding on diverse image
content. Although some large-scale pretrained vision encoders such as vision
encoders in CLIP and DINOv2 have brought promising performance, we found that
there is still no single vision encoder that can dominate various image content
understanding, e.g., the CLIP vision encoder leads to outstanding results on
general image understanding but poor performance on document or chart content.
To alleviate the bias of CLIP vision encoder, we first delve into the inherent
behavior of different pre-trained vision encoders and then propose the MoVA, a
powerful and novel MLLM, adaptively routing and fusing task-specific vision
experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design
a context-aware expert routing strategy to dynamically select the most suitable
vision experts according to the user instruction, input image, and expertise of
vision experts. This benefits from the powerful model function understanding
ability of the large language model (LLM). In the fine-grained stage, we
elaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to
extract and fuse task-specific knowledge from various experts. This
coarse-to-fine paradigm effectively leverages representations from experts
based on multimodal context and model expertise, further enhancing the
generalization ability. We conduct extensive experiments to evaluate the
effectiveness of the proposed approach. Without any bells and whistles, MoVA
can achieve significant performance gains over current state-of-the-art methods
in a wide range of challenging multimodal benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self
  Attention at the Threadblock Level <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hassani, Wen-Mei Hwu, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neighborhood attention reduces the cost of self attention by restricting each
token's attention span to its nearest neighbors. This restriction,
parameterized by a window size and dilation factor, draws a spectrum of
possible attention patterns between linear projection and self attention.
Neighborhood attention, and more generally sliding window attention patterns,
have long been bounded by infrastructure, particularly in higher-rank spaces
(2-D and 3-D), calling for the development of custom kernels, which have been
limited in either functionality, or performance, if not both. In this work, we
aim to massively improve upon existing infrastructure by providing two new
methods for implementing neighborhood attention. We first show that
neighborhood attention can be represented as a batched GEMM problem, similar to
standard attention, and implement it for 1-D and 2-D neighborhood attention.
These kernels on average provide 895% and 272% improvement in full precision
runtime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood
attention respectively. We find that aside from being heavily bound by memory
bandwidth, certain inherent inefficiencies exist in all unfused implementations
of neighborhood attention, which in most cases undo their theoretical
efficiency gain. Motivated by the progress made into fused dot-product
attention kernels, we developed fused neighborhood attention; an adaptation of
fused dot-product attention kernels that allow fine-grained control over
attention across different spatial axes. Known for reducing the quadratic time
complexity of self attention to a linear complexity, neighborhood attention can
now enjoy a reduced and constant memory footprint, and record-breaking half
precision runtime. We observe that our fused implementation successfully
circumvents some of the unavoidable inefficiencies in unfused
implementations...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in 38th Conference on Neural Information Processing Systems
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual place recognition for aerial imagery: A <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Moskalenko, Anastasiia Kornilova, Gonzalo Ferrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial imagery and its direct application to visual localization is an
essential problem for many Robotics and Computer Vision tasks. While Global
Navigation Satellite Systems (GNSS) are the standard default solution for
solving the aerial localization problem, it is subject to a number of
limitations, such as, signal instability or solution unreliability that make
this option not so desirable. Consequently, visual geolocalization is emerging
as a viable alternative. However, adapting Visual Place Recognition (VPR) task
to aerial imagery presents significant challenges, including weather variations
and repetitive patterns. Current VPR reviews largely neglect the specific
context of aerial data. This paper introduces a methodology tailored for
evaluating VPR techniques specifically in the domain of aerial imagery,
providing a comprehensive assessment of various methods and their performance.
However, we not only compare various VPR methods, but also demonstrate the
importance of selecting appropriate zoom and overlap levels when constructing
map tiles to achieve maximum efficiency of VPR algorithms in the case of aerial
imagery. The code is available on our GitHub repository --
https://github.com/prime-slam/aero-vloc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Cooperative Trajectory Representations for Motion Forecasting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhi Ruan, Haibao Yu, Wenxian Yang, Siqi Fan, Zaiqing Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion forecasting is an essential task for autonomous driving, and utilizing
information from infrastructure and other vehicles can enhance forecasting
capabilities. Existing research mainly focuses on leveraging single-frame
cooperative information to enhance the limited perception capability of the ego
vehicle, while underutilizing the motion and interaction context of traffic
participants observed from cooperative devices. In this paper, we propose a
forecasting-oriented representation paradigm to utilize motion and interaction
features from cooperative information. Specifically, we present V2X-Graph, a
representative framework to achieve interpretable and end-to-end trajectory
feature fusion for cooperative motion forecasting. V2X-Graph is evaluated on
V2X-Seq in vehicle-to-infrastructure (V2I) scenarios. To further evaluate on
vehicle-to-everything (V2X) scenario, we construct the first real-world V2X
motion forecasting dataset V2X-Traj, which contains multiple autonomous
vehicles and infrastructure in every scenario. Experimental results on both
V2X-Seq and V2X-Traj show the advantage of our method. We hope both V2X-Graph
and V2X-Traj will benefit the further development of cooperative motion
forecasting. Find the project at https://github.com/AIR-THU/V2X-Graph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ De-Confusing Pseudo-Labels in Source-Free Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idit Diamant, Amir Rosenfeld, Idan Achituve, Jacob Goldberger, Arnon Netzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation aims to adapt a source-trained model to an
unlabeled target domain without access to the source data. It has attracted
growing attention in recent years, where existing approaches focus on
self-training that usually includes pseudo-labeling techniques. In this paper,
we introduce a novel noise-learning approach tailored to address noise
distribution in domain adaptation settings and learn to de-confuse the
pseudo-labels. More specifically, we learn a noise transition matrix of the
pseudo-labels to capture the label corruption of each class and learn the
underlying true label distribution. Estimating the noise transition matrix
enables a better true class-posterior estimation, resulting in better
prediction accuracy. We demonstrate the effectiveness of our approach when
combined with several source-free domain adaptation methods: SHOT, SHOT++, and
AaD. We obtain state-of-the-art results on three domain adaptation datasets:
VisDA, DomainNet, and OfficeHome.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Aware Diffusion for Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calvin Luo, Mandy He, Zilai Zeng, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training an agent to achieve particular goals or perform desired behaviors is
often accomplished through reinforcement learning, especially in the absence of
expert demonstrations. However, supporting novel goals or behaviors through
reinforcement learning requires the ad-hoc design of appropriate reward
functions, which quickly becomes intractable. To address this challenge, we
propose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a
pretrained, frozen text-conditioned diffusion model to compute dense zero-shot
reward signals for text-aligned policy learning. We hypothesize that
large-scale pretrained generative models encode rich priors that can supervise
a policy to behave not only in a text-aligned manner, but also in alignment
with a notion of naturalness summarized from internet-scale training data. In
our experiments, we demonstrate that TADPoLe is able to learn policies for
novel goal-achievement and continuous locomotion behaviors specified by natural
language, in both Humanoid and Dog environments. The behaviors are learned
zero-shot without ground-truth rewards or expert demonstrations, and are
qualitatively more natural according to human evaluation. We further show that
TADPoLe performs competitively when applied to robotic manipulation tasks in
the Meta-World environment, without having access to any in-domain
demonstrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Score identity Distillation: Rapidly Surpassing the Teacher
  in One Step 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, Hai Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score identity Distillation (SiD) is a data-free method that has achieved
state-of-the-art performance in image generation by leveraging only a
pretrained diffusion model, without requiring any training data. However, the
ultimate performance of SiD is constrained by the accuracy with which the
pretrained model captures the true data scores at different stages of the
diffusion process. In this paper, we introduce SiDA (SiD with Adversarial
Loss), which not only enhances generation quality but also improves
distillation efficiency by incorporating real images and adversarial loss. SiDA
utilizes the encoder from the generator's score network as a discriminator,
boosting its ability to distinguish between real images and those generated by
SiD. The adversarial loss is batch-normalized within each GPU and then combined
with the original SiD loss. This integration effectively incorporates the
average "fakeness" per GPU batch into the pixel-based SiD loss, enabling SiDA
to distill a single-step generator either from scratch or by fine-tuning an
existing one. SiDA converges significantly faster than its predecessor when
trained from scratch, and swiftly improves upon the original model's
performance after an initial warmup period during fine-tuning from a
pre-distilled SiD generator. This one-step adversarial distillation method
establishes new benchmarks in generation performance when distilling EDM
diffusion models pretrained on CIFAR-10 (32x32) and ImageNet (64x64), achieving
FID score of 1.110 on ImageNet 64x64. It sets record-low FID scores when
distilling EDM2 models trained on ImageNet (512x512), surpassing even the
largest teacher model, EDM2-XXL. Our SiDA's results record FID scores of 2.156
for EDM2-XS, 1.669 for EDM2-S, 1.488 for EDM2-M, and 1.465 for EDM2-L,
demonstrating significant improvements across all model sizes. Our open-source
code will be integrated into the SiD codebase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EraW-Net: Enhance-Refine-Align W-Net for Scene-Associated Driver
  Attention Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Zhou, Chunsheng Liu, Faliang Chang, Wenqian Wang, Penghui Hao, Yiming Huang, Zhiqiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Associating driver attention with driving scene across two fields of views
(FOVs) is a hard cross-domain perception problem, which requires comprehensive
consideration of cross-view mapping, dynamic driving scene analysis, and driver
status tracking. Previous methods typically focus on a single view or map
attention to the scene via estimated gaze, failing to exploit the implicit
connection between them. Moreover, simple fusion modules are insufficient for
modeling the complex relationships between the two views, making information
integration challenging. To address these issues, we propose a novel method for
end-to-end scene-associated driver attention estimation, called EraW-Net. This
method enhances the most discriminative dynamic cues, refines feature
representations, and facilitates semantically aligned cross-domain integration
through a W-shaped architecture, termed W-Net. Specifically, a Dynamic Adaptive
Filter Module (DAF-Module) is proposed to address the challenges of frequently
changing driving environments by extracting vital regions. It suppresses the
indiscriminately recorded dynamics and highlights crucial ones by innovative
joint frequency-spatial analysis, enhancing the model's ability to parse
complex dynamics. Additionally, to track driver states during non-fixed facial
poses, we propose a Global Context Sharing Module (GCS-Module) to construct
refined feature representations by capturing hierarchical features that adapt
to various scales of head and eye movements. Finally, W-Net achieves systematic
cross-view information integration through its "Encoding-Independent Partial
Decoding-Fusion Decoding" structure, addressing semantic misalignment in
heterogeneous data integration. Experiments demonstrate that the proposed
method robustly and accurately estimates the mapping of driver attention in
scene on large public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FairSkin: Fair Diffusion for Skin Disease Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Zhang, Yuguang Yao, Zhen Tan, Zhiming Li, Pan Wang, Huan Liu, Jingtong Hu, Sijia Liu, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generation is a prevailing technique for clinical data augmentation for
advancing diagnostic accuracy and reducing healthcare disparities. Diffusion
Model (DM) has become a leading method in generating synthetic medical images,
but it suffers from a critical twofold bias: (1) The quality of images
generated for Caucasian individuals is significantly higher, as measured by the
Frechet Inception Distance (FID). (2) The ability of the downstream-task
learner to learn critical features from disease images varies across different
skin tones. These biases pose significant risks, particularly in skin disease
detection, where underrepresentation of certain skin tones can lead to
misdiagnosis or neglect of specific conditions. To address these challenges, we
propose FairSkin, a novel DM framework that mitigates these biases through a
three-level resampling mechanism, ensuring fairer representation across racial
and disease categories. Our approach significantly improves the diversity and
quality of generated images, contributing to more equitable skin disease
detection in clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-light Pedestrian Detection in Visible and Infrared Image Feeds:
  Issues and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thangarajah Akilan, Hrishikesh Vachhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrian detection has become a cornerstone for several high-level tasks,
including autonomous driving, intelligent transportation, and traffic
surveillance. There are several works focussed on pedestrian detection using
visible images, mainly in the daytime. However, this task is very intriguing
when the environmental conditions change to poor lighting or nighttime.
Recently, new ideas have been spurred to use alternative sources, such as Far
InfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light
conditions. This study reviews recent developments in low-light pedestrian
detection approaches. It systematically categorizes and analyses various
algorithms from region-based to non-region-based and graph-based learning
methodologies by highlighting their methodologies, implementation issues, and
challenges. It also outlines the key benchmark datasets that can be used for
research and development of advanced pedestrian detection algorithms,
particularly in low-light situations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UDHF2-Net: Uncertainty-diffusion-model-based High-Frequency <span class="highlight-title">TransFormer</span>
  Network for Remotely Sensed Imagery Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Zhang, Chang Li, Yongjun Zhang, Rongjun Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remotely sensed imagery interpretation (RSII) faces the three major problems:
(1) objective representation of spatial distribution patterns; (2) edge
uncertainty problem caused by downsampling encoder and intrinsic edge noises
(e.g., mixed pixel and edge occlusion etc.); and (3) false detection problem
caused by geometric registration error in change detection. To solve the
aforementioned problems, uncertainty-diffusion-model-based high-Frequency
TransFormer network (UDHF2-Net) is the first to be proposed, whose
superiorities are as follows: (1) a spatially-stationary-and-non-stationary
high-frequency connection paradigm (SHCP) is proposed to enhance the
interaction of spatially frequency-wise stationary and non-stationary features
to yield high-fidelity edge extraction result. Inspired by HRFormer, SHCP
proposes high-frequency-wise stream to replace high-resolution-wise stream in
HRFormer through the whole encoder-decoder process with parallel frequency-wise
high-to-low streams, so it improves the edge extraction accuracy by
continuously remaining high-frequency information; (2) a
mask-and-geo-knowledge-based uncertainty diffusion module (MUDM), which is a
self-supervised learning strategy, is proposed to improve the edge accuracy of
extraction and change detection by gradually removing the simulated spectrum
noises based on geo-knowledge and the generated diffused spectrum noises; (3) a
frequency-wise semi-pseudo-Siamese UDHF2-Net is the first to be proposed to
balance accuracy and complexity for change detection. Besides the
aforementioned spectrum noises in semantic segmentation, MUDM is also a
self-supervised learning strategy to effectively reduce the edge false change
detection from the generated imagery with geometric registration error.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SR-CACO-2: A <span class="highlight-title">Dataset</span> for Confocal Fluorescence Microscopy Image
  Super-Resolution <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soufiane Belharbi, Mara KM Whitford, Phuong Hoang, Shakeeb Murtaza, Luke McCaffrey, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confocal fluorescence microscopy is one of the most accessible and widely
used imaging techniques for the study of biological processes at the cellular
and subcellular levels. Scanning confocal microscopy allows the capture of
high-quality images from thick three-dimensional (3D) samples, yet suffers from
well-known limitations such as photobleaching and phototoxicity of specimens
caused by intense light exposure, limiting its applications. Cellular damage
can be alleviated by changing imaging parameters to reduce light exposure,
often at the expense of image quality. Machine/deep learning methods for
single-image super-resolution (SISR) can be applied to restore image quality by
upscaling lower-resolution (LR) images to yield high-resolution images (HR).
These SISR methods have been successfully applied to photo-realistic images due
partly to the abundance of publicly available data. In contrast, the lack of
publicly available data partly limits their application and success in scanning
confocal microscopy. In this paper, we introduce a large scanning confocal
microscopy dataset named SR-CACO-2 that is comprised of low- and
high-resolution image pairs marked for three different fluorescent markers. It
allows the evaluation of performance of SISR methods on three different
upscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell
line Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured
with four resolutions and three markers, forming 9,937 image patches for SISR
methods. We provide benchmarking results for 16 state-of-the-art methods of the
main SISR families. Results show that these methods have limited success in
producing high-resolution textures. The dataset is freely accessible under a
Creative Commons license (CC BY-NC-SA 4.0). Our dataset, code and pretrained
weights for SISR methods are available: https://github.com/sbelharbi/sr-caco-2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 15 figures, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eddeep: Fast eddy-current distortion correction for diffusion MRI with
  deep learning <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Legouhy, Ross Callaghan, Whitney Stee, Philippe Peigneux, Hojjat Azadbakht, Hui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern diffusion MRI sequences commonly acquire a large number of volumes
with diffusion sensitization gradients of differing strengths or directions.
Such sequences rely on echo-planar imaging (EPI) to achieve reasonable scan
duration. However, EPI is vulnerable to off-resonance effects, leading to
tissue susceptibility and eddy-current induced distortions. The latter is
particularly problematic because it causes misalignment between volumes,
disrupting downstream modelling and analysis. The essential correction of eddy
distortions is typically done post-acquisition, with image registration.
However, this is non-trivial because correspondence between volumes can be
severely disrupted due to volume-specific signal attenuations induced by
varying directions and strengths of the applied gradients. This challenge has
been successfully addressed by the popular FSL~Eddy tool but at considerable
computational cost. We propose an alternative approach, leveraging recent
advances in image processing enabled by deep learning (DL). It consists of two
convolutional neural networks: 1) An image translator to restore correspondence
between images; 2) A registration model to align the translated images. Results
demonstrate comparable distortion estimates to FSL~Eddy, while requiring only
modest training sample sizes. This work, to the best of our knowledge, is the
first to tackle this problem with deep learning. Together with recently
developed DL-based susceptibility correction techniques, they pave the way for
real-time preprocessing of diffusion MRI, facilitating its wider uptake in the
clinic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in MICCAI 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map
  Generation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15656v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15656v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Weihao Gu, Xianjing Zhang, Jintao Xu, Rui Ai, Huimin Lu, Juho Kannala, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) semantic map generation of the environment is an
essential component of autonomous driving. Existing methods have achieved good
performance in this task by fusing different sensor modalities, such as LiDAR
and camera. However, current works are based on raw data or network
feature-level fusion and only consider short-range HD map generation, limiting
their deployment to realistic autonomous driving applications. In this paper,
we focus on the task of building the HD maps in both short ranges, i.e., within
30 m, and also predicting long-range HD maps up to 90 m, which is required by
downstream path planning and control tasks to improve the smoothness and safety
of autonomous driving. To this end, we propose a novel network named
SuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.
We use LiDAR depth to improve image depth estimation and use image features to
guide long-range LiDAR feature prediction. We benchmark our SuperFusion on the
nuScenes dataset and a self-recorded dataset and show that it outperforms the
state-of-the-art baseline methods with large margins on all intervals.
Additionally, we apply the generated HD map to a downstream path planning task,
demonstrating that the long-range HD maps predicted by our method can lead to
better path planning for autonomous vehicles. Our code has been released at
https://github.com/haomo-ai/SuperFusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Vivoli, Marco Bertini, Dimosthenis Karatzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The comic domain is rapidly advancing with the development of single-page
analysis and synthesis models. However, evaluation metrics and datasets lag
behind, often limited to small-scale or single-style test sets. We introduce a
novel benchmark, CoMix, designed to evaluate the multi-task capabilities of
models in comic analysis. Unlike existing benchmarks that focus on isolated
tasks such as object detection or text recognition, CoMix addresses a broader
range of tasks including object detection, speaker identification, character
re-identification, reading order, and multi-modal reasoning tasks like
character naming and dialogue generation. Our benchmark comprises three
existing datasets with expanded annotations to support multi-task evaluation.
To mitigate the over-representation of manga-style data, we have incorporated a
new dataset of carefully selected American comic-style books, thereby enriching
the diversity of comic styles. CoMix is designed to assess pre-trained models
in zero-shot and limited fine-tuning settings, probing their transfer
capabilities across different comic styles and tasks. The validation split of
the benchmark is publicly available for research purposes, and an evaluation
server for the held-out test split is also provided. Comparative results
between human performance and state-of-the-art models reveal a significant
performance gap, highlighting substantial opportunities for advancements in
comic understanding. The dataset, baseline models, and code are accessible at
https://github.com/emanuelevivoli/CoMix-dataset. This initiative sets a new
standard for comprehensive comic analysis, providing the community with a
common benchmark for evaluation on a large and varied set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 (D&B)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaichen Zhou, Lanqing Hong, Enze Xie, Yongxin Yang, Zhenguo Li, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although significant progress has been made in the field of 2D-based
interactive editing, fine-grained 3D-based interactive editing remains
relatively unexplored. This limitation can be attributed to two main
challenges: the lack of an efficient 3D representation robust to different
modifications and the absence of an effective 3D interactive segmentation
method. In this paper, we introduce a novel fine-grained interactive 3D
segmentation and editing algorithm with radiance fields, which we refer to as
SERF. Our method entails creating a neural mesh representation by integrating
multi-view algorithms with pre-trained 2D models. Building upon this
representation, we introduce a novel surface rendering technique that preserves
local information and is robust to deformation. Moreover, this representation
forms the basis for achieving accurate and interactive 3D segmentation without
requiring 3D supervision. Harnessing this representation facilitates a range of
interactive 3D editing operations, encompassing tasks such as interactive
geometry editing and texture painting. Extensive experiments and visualization
examples of editing on both real and synthetic data demonstrate the superiority
of our method on representation quality and editing ability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable
  Remote Sensing Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Gong, Zhixiang Wei, Di Wang, Xianzheng Ma, Hongruixuan Chen, Yuru Jia, Yupeng Deng, Zhenming Ji, Xiangwei Zhu, Naoto Yokoya, Jing Zhang, Bo Du, Liangpei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Remote Sensing Domain Generalization (RSDG) has emerged as a
critical and valuable research frontier, focusing on developing models that
generalize effectively across diverse scenarios. Despite the substantial domain
gaps in RS images that are characterized by variabilities such as location,
wavelength, and sensor type, research in this area remains underexplored: (1)
Current cross-domain methods primarily focus on Domain Adaptation (DA), which
adapts models to predefined domains rather than to unseen ones; (2) Few studies
targeting the RSDG issue, especially for semantic segmentation tasks, where
existing models are developed for specific unknown domains, struggling with
issues of underfitting on other unknown scenarios; (3) Existing RS foundation
models tend to prioritize in-domain performance over cross-domain
generalization. To this end, we introduce the first vision foundation model for
RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong
cross-domain generalization through a specially designed data-level Earth-Style
Injection pipeline and a model-level Multi-Task Training pipeline. In addition,
for the semantic segmentation task, we have curated an RSDG benchmark
comprising 28 cross-domain settings across various regions, spectral bands,
platforms, and climates, providing a comprehensive framework for testing the
generalizability of future RSDG models. Extensive experiments on this benchmark
demonstrate the superiority of CrossEarth over existing state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The codes and models will be available at
  https://github.com/Cuzyoung/CrossEarth</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaptainCook4D: A <span class="highlight-title">Dataset</span> for Understanding Errors in Procedural
  Activities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohith Peddi, Shivvrat Arya, Bharath Challa, Likhitha Pallapothula, Akshay Vyas, Bhavya Gouripeddi, Jikai Wang, Qifan Zhang, Vasundhara Komaragiri, Eric Ragan, Nicholas Ruozzi, Yu Xiang, Vibhav Gogate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following step-by-step procedures is an essential component of various
activities carried out by individuals in their daily lives. These procedures
serve as a guiding framework that helps to achieve goals efficiently, whether
it is assembling furniture or preparing a recipe. However, the complexity and
duration of procedural activities inherently increase the likelihood of making
errors. Understanding such procedural activities from a sequence of frames is a
challenging task that demands an accurate interpretation of visual information
and the ability to reason about the structure of the activity. To this end, we
collect a new egocentric 4D dataset, CaptainCook4D, comprising 384 recordings
(94.5 hours) of people performing recipes in real kitchen environments. This
dataset consists of two distinct types of activity: one in which participants
adhere to the provided recipe instructions and another in which they deviate
and induce errors. We provide 5.3K step annotations and 10K fine-grained action
annotations and benchmark the dataset for the following tasks: supervised error
recognition, multistep localization, and procedure learning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 Neural Information Processing Systems Datasets
  and Benchmarks Track, Project Page:
  https://captaincook4d.github.io/captain-cook/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embracing Events and Frames with Hierarchical Feature Refinement Network
  for Object Detection <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Cao, Zehua Zhang, Yan Xia, Xinyi Li, Jiahao Xia, Guang Chen, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In frame-based vision, object detection faces substantial performance
degradation under challenging conditions due to the limited sensing capability
of conventional cameras. Event cameras output sparse and asynchronous events,
providing a potential solution to solve these problems. However, effectively
fusing two heterogeneous modalities remains an open issue. In this work, we
propose a novel hierarchical feature refinement network for event-frame fusion.
The core concept is the design of the coarse-to-fine fusion module, denoted as
the cross-modality adaptive feature refinement (CAFR) module. In the initial
phase, the bidirectional cross-modality interaction (BCI) part facilitates
information bridging from two distinct sources. Subsequently, the features are
further refined by aligning the channel-level mean and variance in the two-fold
adaptive feature refinement (TAFR) part. We conducted extensive experiments on
two benchmarks: the low-resolution PKU-DDD17-Car dataset and the
high-resolution DSEC dataset. Experimental results show that our method
surpasses the state-of-the-art by an impressive margin of $\textbf{8.0}\%$ on
the DSEC dataset. Besides, our method exhibits significantly better robustness
(\textbf{69.5}\% versus \textbf{38.7}\%) when introducing 15 different
corruption types to the frame images. The code can be found at the link
(https://github.com/HuCaoFighting/FRN).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency Diffusion Bridge Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guande He, Kaiwen Zheng, Jianfei Chen, Fan Bao, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have become the dominant paradigm of generative
modeling in a variety of domains by learning stochastic processes from noise to
data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of
generative modeling that builds stochastic processes between fixed data
endpoints based on a reference diffusion process, have achieved empirical
success across tasks with coupled data distribution, such as image-to-image
translation. However, DDBM's sampling process typically requires hundreds of
network evaluations to achieve decent performance, which may impede their
practical deployment due to high computational demands. In this work, inspired
by the recent advance of consistency models in DMs, we tackle this problem by
learning the consistency function of the probability-flow ordinary differential
equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting
step given any point on the ODE trajectory. Based on a dedicated general-form
ODE solver, we propose two paradigms: consistency bridge distillation and
consistency bridge training, which is flexible to apply on DDBMs with broad
design choices. Experimental results show that our proposed method could sample
$4\times$ to $50\times$ faster than the base DDBM and produce better visual
quality given the same step in various tasks with pixel resolution ranging from
$64 \times 64$ to $256 \times 256$, as well as supporting downstream tasks such
as semantic interpolation in the data space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dessie: Disentanglement for Articulated 3D Horse Shape and Pose
  Estimation from Images <span class="chip">ACCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ci Li, Yi Yang, Zehang Weng, Elin Hernlund, Silvia Zuffi, Hedvig Kjellström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, 3D parametric animal models have been developed to aid in
estimating 3D shape and pose from images and video. While progress has been
made for humans, it's more challenging for animals due to limited annotated
data. To address this, we introduce the first method using synthetic data
generation and disentanglement to learn to regress 3D shape and pose. Focusing
on horses, we use text-based texture generation and a synthetic data pipeline
to create varied shapes, poses, and appearances, learning disentangled spaces.
Our method, Dessie, surpasses existing 3D horse reconstruction methods and
generalizes to other large animals like zebras, cows, and deer. See the project
website at: \url{https://celiali.github.io/Dessie/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M3LEO: A Multi-Modal, Multi-Label Earth Observation <span class="highlight-title">Dataset</span> Integrating
  Interferometric SAR and Multispectral Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew J Allen, Francisco Dorr, Joseph Alejandro Gallego Mejia, Laura Martínez-Ferrer, Anna Jungbluth, Freddie Kalaitzis, Raúl Ramos-Pollán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Satellite-based remote sensing has revolutionised the way we address global
challenges. Huge quantities of Earth Observation (EO) data are generated by
satellite sensors daily, but processing these large datasets for use in ML
pipelines is technically and computationally challenging. While some
preprocessed Earth observation datasets exist, their content is often limited
to optical or near-optical wavelength data, which is ineffective at night or in
adverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing
technique based on microwave length radiation, offers a viable alternative.
However, the application of machine learning to SAR has been limited due to a
lack of ML-ready data and pipelines, particularly for the full diversity of SAR
data, including polarimetry, coherence and interferometry. In this work, we
introduce M3LEO, a multi-modal, multi-label Earth observation dataset that
includes polarimetric, interferometric, and coherence SAR data derived from
Sentinel-1, alongside multispectral Sentinel-2 imagery and auxiliary data
describing terrain properties such as land use. M3LEO spans approximately 17M
4x4 km data chips from six diverse geographic regions. The dataset is
complemented by a flexible PyTorch Lightning framework configured using Hydra
to accommodate its use across diverse ML applications in Earth observation. We
provide tools to process any dataset available on popular platforms such as
Google Earth Engine for seamless integration with our framework. We show that
the distribution shift in self-supervised embeddings is substantial across
geographic regions, even when controlling for terrain properties. Data:
huggingface.co/M3LEO, Code: github.com/spaceml-org/M3LEO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blind Inpainting with Object-aware Discrimination for Artificial Marker
  Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuechen Guo, Wenhao Hu, Chiming Ni, Wenhao Chai, Shiyan Li, Gaoang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images often incorporate doctor-added markers that can hinder
AI-based diagnosis. This issue highlights the need of inpainting techniques to
restore the corrupted visual contents. However, existing methods require manual
mask annotation as input, limiting the application scenarios. In this paper, we
propose a novel blind inpainting method that automatically reconstructs visual
contents within the corrupted regions without mask input as guidance. Our model
includes a blind reconstruction network and an object-aware discriminator for
adversarial training. The reconstruction network contains two branches that
predict corrupted regions in images and simultaneously restore the missing
visual contents. Leveraging the potent recognition capability of a dense object
detector, the object-aware discriminator ensures markers undetectable after
inpainting. Thus, the restored images closely resemble the clean ones. We
evaluate our method on three datasets of various medical imaging modalities,
confirming better performance over other state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Video Anomaly Detection: A Concise <span class="highlight-title">Review</span> and a New <span class="highlight-title">Dataset</span> <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04857v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04857v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyun Zhu, Lei Wang, Arjun Raj, Tom Gedeon, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Anomaly Detection (VAD) finds widespread applications in security
surveillance, traffic monitoring, industrial monitoring, and healthcare.
Despite extensive research efforts, there remains a lack of concise reviews
that provide insightful guidance for researchers. Such reviews would serve as
quick references to grasp current challenges, research trends, and future
directions. In this paper, we present such a review, examining models and
datasets from various perspectives. We emphasize the critical relationship
between model and dataset, where the quality and diversity of datasets
profoundly influence model performance, and dataset development adapts to the
evolving needs of emerging approaches. Our review identifies practical issues,
including the absence of comprehensive datasets with diverse scenarios. To
address this, we introduce a new dataset, Multi-Scenario Anomaly Detection
(MSAD), comprising 14 distinct scenarios captured from various camera views.
Our dataset has diverse motion patterns and challenging variations, such as
different lighting and weather conditions, providing a robust foundation for
training superior models. We conduct an in-depth analysis of recent
representative models using MSAD and highlight its potential in addressing the
challenges of detecting anomalies across diverse and evolving surveillance
scenarios. [Project website: https://msad-dataset.github.io/]
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WildGaussians: 3D Gaussian Splatting in the Wild <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, Torsten Sattler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the field of 3D scene reconstruction is dominated by NeRFs due to their
photorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,
offering similar quality with real-time rendering speeds. However, both methods
primarily excel with well-controlled 3D scenes, while in-the-wild data -
characterized by occlusions, dynamic objects, and varying illumination -
remains challenging. NeRFs can adapt to such conditions easily through
per-image embedding vectors, but 3DGS struggles due to its explicit
representation and lack of shared parameters. To address this, we introduce
WildGaussians, a novel approach to handle occlusions and appearance changes
with 3DGS. By leveraging robust DINO features and integrating an appearance
modeling module within 3DGS, our method achieves state-of-the-art results. We
demonstrate that WildGaussians matches the real-time rendering speed of 3DGS
while surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all
within a simple architectural framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; Project page: https://wild-gaussians.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FasterDiT: Towards Faster Diffusion <span class="highlight-title">Transformer</span>s Training without
  Architecture Modification <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfeng Yao, Wang Cheng, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiT) have attracted significant attention in
research. However, they suffer from a slow convergence rate. In this paper, we
aim to accelerate DiT training without any architectural modification. We
identify the following issues in the training process: firstly, certain
training strategies do not consistently perform well across different data.
Secondly, the effectiveness of supervision at specific timesteps is limited. In
response, we propose the following contributions: (1) We introduce a new
perspective for interpreting the failure of the strategies. Specifically, we
slightly extend the definition of Signal-to-Noise Ratio (SNR) and suggest
observing the Probability Density Function (PDF) of SNR to understand the
essence of the data robustness of the strategy. (2) We conduct numerous
experiments and report over one hundred experimental results to empirically
summarize a unified accelerating strategy from the perspective of PDF. (3) We
develop a new supervision method that further accelerates the training process
of DiT. Based on them, we propose FasterDiT, an exceedingly simple and
practicable design strategy. With few lines of code modifications, it achieves
2.30 FID on ImageNet 256 resolution at 1000k iterations, which is comparable to
DiT (2.27 FID) but 7 times faster in training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (poster); update to camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Liu, Zhihang Fu, Sheng Jin, Chao Chen, Ze Chen, Rongxin Jiang, Fan Zhou, Yaowu Chen, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and rejecting unknown out-of-distribution (OOD) samples is critical
for deployed neural networks to void unreliable predictions. In real-world
scenarios, however, the efficacy of existing OOD detection methods is often
impeded by the inherent imbalance of in-distribution (ID) data, which causes
significant performance decline. Through statistical observations, we have
identified two common challenges faced by different OOD detectors:
misidentifying tail class ID samples as OOD, while erroneously predicting OOD
samples as head class from ID. To explain this phenomenon, we introduce a
generalized statistical framework, termed ImOOD, to formulate the OOD detection
problem on imbalanced data distribution. Consequently, the theoretical analysis
reveals that there exists a class-aware bias item between balanced and
imbalanced OOD detection, which contributes to the performance gap. Building
upon this finding, we present a unified training-time regularization technique
to mitigate the bias and boost imbalanced OOD detectors across architecture
designs. Our theoretically grounded method translates into consistent
improvements on the representative CIFAR10-LT, CIFAR100-LT, and ImageNet-LT
benchmarks against several state-of-the-art OOD detection approaches. Code is
available at https://github.com/alibaba/imood.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NeurIPS 2024. Code is available at
  https://github.com/alibaba/imood</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subsurface Scattering for 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Niklas Dihlmann, Arjun Majumdar, Andreas Engelhardt, Raphael Braun, Hendrik P. A. Lensch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction and relighting of objects made from scattering materials
present a significant challenge due to the complex light transport beneath the
surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at
real-time speeds. While 3D Gaussians efficiently approximate an object's
surface, they fail to capture the volumetric properties of subsurface
scattering. We propose a framework for optimizing an object's shape together
with the radiance transfer field given multi-view OLAT (one light at a time)
data. Our method decomposes the scene into an explicit surface represented as
3D Gaussians, with a spatially varying BRDF, and an implicit volumetric
representation of the scattering component. A learned incident light field
accounts for shadowing. We optimize all parameters jointly via ray-traced
differentiable rendering. Our approach enables material editing, relighting and
novel view synthesis at interactive rates. We show successful application on
synthetic data and introduce a newly acquired multi-view multi-light dataset of
objects in a light-stage setup. Compared to previous work we achieve comparable
or better results at a fraction of optimization and rendering time while
enabling detailed control over material attributes. Project page
https://sss.jdihlmann.com/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://sss.jdihlmann.com/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNION: Unsupervised 3D Object Detection using Object Appearance-based
  Pseudo-Classes <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ted Lentsch, Holger Caesar, Dariu M. Gavrila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised 3D object detection methods have emerged to leverage vast
amounts of data without requiring manual labels for training. Recent approaches
rely on dynamic objects for learning to detect mobile objects but penalize the
detections of static instances during training. Multiple rounds of (self)
training are used to add detected static instances to the set of training
targets; this procedure to improve performance is computationally expensive. To
address this, we propose the method UNION. We use spatial clustering and
self-supervised scene flow to obtain a set of static and dynamic object
proposals from LiDAR. Subsequently, object proposals' visual appearances are
encoded to distinguish static objects in the foreground and background by
selecting static instances that are visually similar to dynamic objects. As a
result, static and dynamic mobile objects are obtained together, and existing
detectors can be trained with a single training. In addition, we extend 3D
object discovery to detection by using object appearance-based cluster labels
as pseudo-class labels for training object classification. We conduct extensive
experiments on the nuScenes dataset and increase the state-of-the-art
performance for unsupervised 3D object discovery, i.e. UNION more than doubles
the average precision to 38.4. The code is available at
github.com/TedLentsch/UNION.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NM-FlowGAN: Modeling sRGB Noise without Paired Images using a Hybrid
  Approach of Normalizing Flows and GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10112v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10112v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young Joo Han, Ha-Jin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling and synthesizing real sRGB noise is crucial for various low-level
vision tasks, such as building datasets for training image denoising systems.
The distribution of real sRGB noise is highly complex and affected by a
multitude of factors, making its accurate modeling extremely challenging.
Therefore, recent studies have proposed methods that employ data-driven
generative models, such as Generative Adversarial Networks (GAN) and
Normalizing Flows. These studies achieve more accurate modeling of sRGB noise
compared to traditional noise modeling methods. However, there are performance
limitations due to the inherent characteristics of each generative model. To
address this issue, we propose NM-FlowGAN, a hybrid approach that exploits the
strengths of both GAN and Normalizing Flows. We combine pixel-wise noise
modeling networks based on Normalizing Flows and spatial correlation modeling
networks based on GAN. Specifically, the pixel-wise noise modeling network
leverages the high training stability of Normalizing Flows to capture noise
characteristics that are affected by a multitude of factors, and the spatial
correlation networks efficiently model pixel-to-pixel relationships. In
particular, unlike recent methods that rely on paired noisy images, our method
synthesizes noise using clean images and factors that affect noise
characteristics, such as easily obtainable parameters like camera type and ISO
settings, making it applicable to various fields where obtaining noisy-clean
image pairs is not feasible. In our experiments, our NM-FlowGAN outperforms
other baselines in the sRGB noise synthesis task. Moreover, the denoising
neural network trained with synthesized image pairs from our model shows
superior performance compared to other baselines. Our code is available at:
\url{https://github.com/YoungJooHan/NM-FlowGAN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PuLID: Pure and Lightning ID Customization via Contrastive Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Peng Zhang, Qian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Pure and Lightning ID customization (PuLID), a novel tuning-free
ID customization method for text-to-image generation. By incorporating a
Lightning T2I branch with a standard diffusion one, PuLID introduces both
contrastive alignment loss and accurate ID loss, minimizing disruption to the
original model and ensuring high ID fidelity. Experiments show that PuLID
achieves superior performance in both ID fidelity and editability. Another
attractive property of PuLID is that the image elements (e.g., background,
lighting, composition, and style) before and after the ID insertion are kept as
consistent as possible. Codes and models are available at
https://github.com/ToTheBeginning/PuLID
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Codes and models are available at
  https://github.com/ToTheBeginning/PuLID</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN
  Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiang Han, Xinyu Li, Jiang Shang, Yiman Liu, Keyan Chen, Shugong Xu, Qiaohong Liu, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting anatomical structures and lesions from ultrasound images
contributes to disease assessment. Weakly supervised learning (WSL) based on
sparse annotation has achieved encouraging performance and demonstrated the
potential to reduce annotation costs. This study attempts to introduce
scribble-based WSL into ultrasound image segmentation tasks. However,
ultrasound images often suffer from poor contrast and unclear edges, coupled
with insufficient supervison signals for edges, posing challenges to edge
prediction. Uncertainty modeling has been proven to facilitate models in
dealing with these issues. Nevertheless, existing uncertainty estimation
paradigms are not robust enough and often filter out predictions near decision
boundaries, resulting in unstable edge predictions. Therefore, we propose
leveraging predictions near decision boundaries effectively. Specifically, we
introduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided
Consistency strategy. This strategy utilizes high-evidence predictions, which
are more likely to occur near high-density regions, to guide the optimization
of low-evidence predictions that may appear near decision boundaries.
Furthermore, the diverse sizes and locations of lesions in ultrasound images
pose a challenge for CNNs with local receptive fields, as they struggle to
model global information. Therefore, we introduce Visual Mamba based on
structured state space sequence models, which achieves long-range dependency
with linear computational complexity, and we construct a novel hybrid CNN-Mamba
framework. During training, the collaboration between the CNN branch and the
Mamba branch in the proposed framework draws inspiration from each other based
on the EGC strategy. Experiments demonstrate the competitiveness of the
proposed method. Dataset and code will be available on
https://github.com/GtLinyer/MambaEviScrib.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stabilize the Latent Space for Image Autoregressive Modeling: A Unified
  Perspective <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent-based image generative models, such as Latent Diffusion Models (LDMs)
and Mask Image Models (MIMs), have achieved notable success in image generation
tasks. These models typically leverage reconstructive autoencoders like VQGAN
or VAE to encode pixels into a more compact latent space and learn the data
distribution in the latent space instead of directly from pixels. However, this
practice raises a pertinent question: Is it truly the optimal choice? In
response, we begin with an intriguing observation: despite sharing the same
latent space, autoregressive models significantly lag behind LDMs and MIMs in
image generation. This finding contrasts sharply with the field of NLP, where
the autoregressive model GPT has established a commanding presence. To address
this discrepancy, we introduce a unified perspective on the relationship
between latent space and generative models, emphasizing the stability of latent
space in image generative modeling. Furthermore, we propose a simple but
effective discrete image tokenizer to stabilize the latent space for image
generative modeling by applying K-Means on the latent features of
self-supervised learning models. Experimental results show that image
autoregressive modeling with our tokenizer (DiGIT) benefits both image
understanding and image generation with the next token prediction principle,
which is inherently straightforward for GPT models but challenging for other
generative models. Remarkably, for the first time, a GPT-style autoregressive
model for images outperforms LDMs, which also exhibits substantial improvement
akin to GPT when scaling up model size. Our findings underscore the potential
of an optimized latent space and the integration of discrete tokenization in
advancing the capabilities of image generative models. The code is available at
\url{https://github.com/DAMO-NLP-SG/DiGIT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIC: Gaussian-Informed Continuum for Physical Property Identification
  and Simulation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14927v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14927v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Cai, Yuji Yang, Weihao Yuan, Yisheng He, Zilong Dong, Liefeng Bo, Hui Cheng, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of estimating physical properties (system
identification) through visual observations. To facilitate geometry-aware
guidance in physical property estimation, we introduce a novel hybrid framework
that leverages 3D Gaussian representation to not only capture explicit shapes
but also enable the simulated continuum to render object masks as 2D shape
surrogates during training. We propose a new dynamic 3D Gaussian framework
based on motion factorization to recover the object as 3D Gaussian point sets
across different time states. Furthermore, we develop a coarse-to-fine filling
strategy to generate the density fields of the object from the Gaussian
reconstruction, allowing for the extraction of object continuums along with
their surfaces and the integration of Gaussian attributes into these continuum.
In addition to the extracted object surfaces, the Gaussian-informed continuum
also enables the rendering of object masks during simulations, serving as
2D-shape guidance for physical property estimation. Extensive experimental
evaluations demonstrate that our pipeline achieves state-of-the-art performance
across multiple benchmarks and metrics. Additionally, we illustrate the
effectiveness of the proposed method through real-world demonstrations,
showcasing its practical utility. Our project page is at
https://jukgei.github.io/project/gic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D
  Facial Animation Synthesis Using VQ-VAE <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07966v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07966v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sichun Wu, Kazi Injamamul Haque, Zerrin Yumak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven 3D facial animation synthesis has been an active field of
research with attention from both academia and industry. While there are
promising results in this area, recent approaches largely focus on lip-sync and
identity control, neglecting the role of emotions and emotion control in the
generative process. That is mainly due to the lack of emotionally rich facial
animation data and algorithms that can synthesize speech animations with
emotional expressions at the same time. In addition, majority of the models are
deterministic, meaning given the same audio input, they produce the same output
motion. We argue that emotions and non-determinism are crucial to generate
diverse and emotionally-rich facial animations. In this paper, we propose
ProbTalk3D a non-deterministic neural network approach for emotion controllable
speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and
an emotionally rich facial animation dataset 3DMEAD. We provide an extensive
comparative analysis of our model against the recent 3D facial animation
synthesis approaches, by evaluating the results objectively, qualitatively, and
with a perceptual user study. We highlight several objective metrics that are
more suitable for evaluating stochastic outputs and use both in-the-wild and
ground truth data for subjective evaluation. To our knowledge, that is the
first non-deterministic 3D facial animation synthesis method incorporating a
rich emotion dataset and emotion control with emotion labels and intensity
levels. Our evaluation demonstrates that the proposed model achieves superior
performance compared to state-of-the-art emotion-controlled, deterministic and
non-deterministic models. We recommend watching the supplementary video for
quality judgement. The entire codebase is publicly available
(https://github.com/uuembodiedsocialai/ProbTalk3D/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM
  SIGGRAPH MIG 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid Plug-in Defenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01762v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01762v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wu, Yujian Betterest Li, Jian Lou, Xiaoyu Zhang, Handing Wang, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of daily services, the deployment of deep neural networks
underscores the paramount importance of their reliability. However, the
vulnerability of these networks to adversarial attacks, primarily
evasion-based, poses a concerning threat to their functionality. Common methods
for enhancing robustness involve heavy adversarial training or leveraging
learned knowledge from clean data, both necessitating substantial computational
resources. This inherent time-intensive nature severely limits the agility of
large foundational models to swiftly counter adversarial perturbations. To
address this challenge, this paper focuses on the Rapid Plug-in Defender
(RaPiD) problem, aiming to rapidly counter adversarial perturbations without
altering the deployed model. Drawing inspiration from the generalization and
the universal computation ability of pre-trained transformer models, we propose
a novel method termed CeTaD (Considering Pre-trained Transformers as Defenders)
for RaPiD, optimized for efficient computation. CeTaD strategically fine-tunes
the normalization layer parameters within the defender using a limited set of
clean and adversarial examples. Our evaluation centers on assessing CeTaD's
effectiveness, transferability, and the impact of different components in
scenarios involving one-shot adversarial examples. The proposed method is
capable of rapidly adapting to various attacks and different application
scenarios without altering the target model and clean training data. We also
explore the influence of varying training data conditions on CeTaD's
performance. Notably, CeTaD exhibits adaptability across differentiable service
models and proves the potential of continuous learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAT: Coordinating Anatomical-Textual <span class="highlight-title">Prompt</span>s for Multi-Organ and Tumor
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongzhen Huang, Yankai Jiang, Rongzhao Zhang, Shaoting Zhang, Xiaofan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing promptable segmentation methods in the medical imaging field
primarily consider either textual or visual prompts to segment relevant
objects, yet they often fall short when addressing anomalies in medical images,
like tumors, which may vary greatly in shape, size, and appearance. Recognizing
the complexity of medical scenarios and the limitations of textual or visual
prompts, we propose a novel dual-prompt schema that leverages the complementary
strengths of visual and textual prompts for segmenting various organs and
tumors. Specifically, we introduce CAT, an innovative model that Coordinates
Anatomical prompts derived from 3D cropped images with Textual prompts enriched
by medical domain knowledge. The model architecture adopts a general
query-based design, where prompt queries facilitate segmentation queries for
mask prediction. To synergize two types of prompts within a unified framework,
we implement a ShareRefiner, which refines both segmentation and prompt queries
while disentangling the two types of prompts. Trained on a consortium of 10
public CT datasets, CAT demonstrates superior performance in multiple
segmentation tasks. Further validation on a specialized in-house dataset
reveals the remarkable capacity of segmenting tumors across multiple cancer
stages. This approach confirms that coordinating multimodal prompts is a
promising avenue for addressing complex scenarios in the medical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optical Diffusion Models for Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilker Oguz, Niyazi Ulas Dinc, Mustafa Yildirim, Junjie Ke, Innfarn Yoo, Qifei Wang, Feng Yang, Christophe Moser, Demetri Psaltis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models generate new samples by progressively decreasing the noise
from the initially provided random distribution. This inference procedure
generally utilizes a trained neural network numerous times to obtain the final
output, creating significant latency and energy consumption on digital
electronic hardware such as GPUs. In this study, we demonstrate that the
propagation of a light beam through a semi-transparent medium can be programmed
to implement a denoising diffusion model on image samples. This framework
projects noisy image patterns through passive diffractive optical layers, which
collectively only transmit the predicted noise term in the image. The optical
transparent layers, which are trained with an online training approach,
backpropagating the error to the analytical model of the system, are passive
and kept the same across different steps of denoising. Hence this method
enables high-speed image generation with minimal power consumption, benefiting
from the bandwidth and energy efficiency of optical information processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSL-NBV: A <span class="highlight-title">Self-Supervised</span>-Learning-Based Next-Best-View algorithm for
  Efficient 3D Plant Reconstruction by a Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchao Ci, Eldert J. van Henten, Xin Wang, Akshay K. Burusa, Gert Kootstra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 3D reconstruction of plants is challenging due to their complex shape
causing many occlusions. Next-Best-View (NBV) methods address this by
iteratively selecting new viewpoints to maximize information gain (IG).
Deep-learning-based NBV (DL-NBV) methods demonstrate higher computational
efficiency over classic voxel-based NBV approaches but current methods require
extensive training using ground-truth plant models, making them impractical for
real-world plants. These methods, moreover, rely on offline training with
pre-collected data, limiting adaptability in changing agricultural
environments. This paper proposes a self-supervised learning-based NBV method
(SSL-NBV) that uses a deep neural network to predict the IG for candidate
viewpoints. The method allows the robot to gather its own training data during
task execution by comparing new 3D sensor data to the earlier gathered data and
by employing weakly-supervised learning and experience replay for efficient
online learning. Comprehensive evaluations were conducted in simulation and
real-world environments using cross-validation. The results showed that SSL-NBV
required fewer views for plant reconstruction than non-NBV methods and was over
800 times faster than a voxel-based method. SSL-NBV reduced training
annotations by over 90% compared to a baseline DL-NBV. Furthermore, SSL-NBV
could adapt to novel scenarios through online fine-tuning. Also using real
plants, the results showed that the proposed method can learn to effectively
plan new viewpoints for 3D plant reconstruction. Most importantly, SSL-NBV
automated the entire network training and uses continuous online learning,
allowing it to operate in changing agricultural environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 11 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConDiSR: Contrastive Disentanglement and Style Regularization for Single
  Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09400v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09400v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Matsun, Numan Saeed, Fadillah Adamsyah Maani, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical data often exhibits distribution shifts, which cause test-time
performance degradation for deep learning models trained using standard
supervised learning pipelines. This challenge is addressed in the field of
Domain Generalization (DG) with the sub-field of Single Domain Generalization
(SDG) being specifically interesting due to the privacy- or logistics-related
issues often associated with medical data. Existing disentanglement-based SDG
methods heavily rely on structural information embedded in segmentation masks,
however classification labels do not provide such dense information. This work
introduces a novel SDG method aimed at medical image classification that
leverages channel-wise contrastive disentanglement. It is further enhanced with
reconstruction-based style regularization to ensure extraction of distinct
style and structure feature representations. We evaluate our method on the
complex task of multicenter histopathology image classification, comparing it
against state-of-the-art (SOTA) SDG baselines. Results demonstrate that our
method surpasses the SOTA by a margin of 1% in average accuracy while also
showing more stable performance. This study highlights the importance and
challenges of exploring SDG frameworks in the context of the classification
task. The code is publicly available at
https://github.com/BioMedIA-MBZUAI/ConDiSR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A flaw was found in the results acquisition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DF40: Toward Next-Generation Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Yan, Taiping Yao, Shen Chen, Yandan Zhao, Xinghe Fu, Junwei Zhu, Donghao Luo, Chengjie Wang, Shouhong Ding, Yunsheng Wu, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new comprehensive benchmark to revolutionize the current
deepfake detection field to the next generation. Predominantly, existing works
identify top-notch detection algorithms and models by adhering to the common
practice: training detectors on one specific dataset (e.g., FF++) and testing
them on other prevalent deepfake datasets. This protocol is often regarded as a
"golden compass" for navigating SoTA detectors. But can these stand-out
"winners" be truly applied to tackle the myriad of realistic and diverse
deepfakes lurking in the real world? If not, what underlying factors contribute
to this gap? In this work, we found the dataset (both train and test) can be
the "primary culprit" due to: (1) forgery diversity: Deepfake techniques are
commonly referred to as both face forgery and entire image synthesis. Most
existing datasets only contain partial types of them, with limited forgery
methods implemented; (2) forgery realism: The dominated training dataset, FF++,
contains out-of-date forgery techniques from the past four years. "Honing
skills" on these forgeries makes it difficult to guarantee effective detection
generalization toward nowadays' SoTA deepfakes; (3) evaluation protocol: Most
detection works perform evaluations on one type, which hinders the development
of universal deepfake detectors. To address this dilemma, we construct a highly
diverse deepfake detection dataset called DF40, which comprises 40 distinct
deepfake techniques. We then conduct comprehensive evaluations using 4 standard
evaluation protocols and 8 representative detection methods, resulting in over
2,000 evaluations. Through these evaluations, we provide an extensive analysis
from various perspectives, leading to 7 new insightful findings. We also open
up 4 valuable yet previously underexplored research questions to inspire future
works. Our project page is https://github.com/YZY-stack/DF40.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2108.05080 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G3: An Effective and Adaptive Framework for Worldwide Geolocalization
  Using Large Multi-Modality Models <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyue Jia, Yiding Liu, Xiaopeng Li, Yuhao Wang, Yantong Du, Xiao Han, Xuetao Wei, Shuaiqiang Wang, Dawei Yin, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Worldwide geolocalization aims to locate the precise location at the
coordinate level of photos taken anywhere on the Earth. It is very challenging
due to 1) the difficulty of capturing subtle location-aware visual semantics,
and 2) the heterogeneous geographical distribution of image data. As a result,
existing studies have clear limitations when scaled to a worldwide context.
They may easily confuse distant images with similar visual contents, or cannot
adapt to various locations worldwide with different amounts of relevant data.
To resolve these limitations, we propose G3, a novel framework based on
Retrieval-Augmented Generation (RAG). In particular, G3 consists of three
steps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to
optimize both retrieval and generation phases of worldwide geolocalization.
During Geo-alignment, our solution jointly learns expressive multi-modal
representations for images, GPS and textual descriptions, which allows us to
capture location-aware semantics for retrieving nearby images for a given
query. During Geo-diversification, we leverage a prompt ensembling method that
is robust to inconsistent retrieval performance for different image queries.
Finally, we combine both retrieved and generated GPS candidates in
Geo-verification for location prediction. Experiments on two well-established
datasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other
state-of-the-art methods. Our code and data are available online for
reproduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Adversarial Robust Fairness via Anti-Bias Soft Label
  Distillation <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiji Zhao, Ranjie Duan, Xizhe Wang, Xingxing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Training (AT) has been widely proved to be an effective method to
improve the adversarial robustness against adversarial examples for Deep Neural
Networks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD)
has demonstrated its superior performance in improving the robustness of small
student models with the guidance of large teacher models. However, both AT and
ARD encounter the robust fairness problem: these models exhibit strong
robustness when facing part of classes (easy class), but weak robustness when
facing others (hard class). In this paper, we give an in-depth analysis of the
potential factors and argue that the smoothness degree of samples' soft labels
for different classes (i.e., hard class or easy class) will affect the robust
fairness of DNNs from both empirical observation and theoretical analysis.
Based on the above finding, we propose an Anti-Bias Soft Label Distillation
(ABSLD) method to mitigate the adversarial robust fairness problem within the
framework of Knowledge Distillation (KD). Specifically, ABSLD adaptively
reduces the student's error risk gap between different classes to achieve
fairness by adjusting the class-wise smoothness degree of samples' soft labels
during the training process, and the smoothness degree of soft labels is
controlled by assigning different temperatures in KD to different classes.
Extensive experiments demonstrate that ABSLD outperforms state-of-the-art AT,
ARD, and robust fairness methods in the comprehensive metric (Normalized
Standard Deviation) of robustness and fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iVideo<span class="highlight-title">GPT</span>: Interactive Video<span class="highlight-title">GPT</span>s are Scalable World Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15223v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15223v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, Mingsheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models empower model-based agents to interactively explore, reason, and
plan within imagined environments for real-world decision-making. However, the
high demand for interactivity poses challenges in harnessing recent
advancements in video generative models for developing world models at scale.
This work introduces Interactive VideoGPT (iVideoGPT), a scalable
autoregressive transformer framework that integrates multimodal signals--visual
observations, actions, and rewards--into a sequence of tokens, facilitating an
interactive experience of agents via next-token prediction. iVideoGPT features
a novel compressive tokenization technique that efficiently discretizes
high-dimensional visual observations. Leveraging its scalable architecture, we
are able to pre-train iVideoGPT on millions of human and robotic manipulation
trajectories, establishing a versatile foundation that is adaptable to serve as
interactive world models for a wide range of downstream tasks. These include
action-conditioned video prediction, visual planning, and model-based
reinforcement learning, where iVideoGPT achieves competitive performance
compared with state-of-the-art methods. Our work advances the development of
interactive general world models, bridging the gap between generative video
models and practical model-based reinforcement learning applications. Code and
pre-trained models are available at https://thuml.github.io/iVideoGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code is available at project website:
  https://thuml.github.io/iVideoGPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing
  Reliability,Reproducibility, and Practicality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08845v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08845v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Zhang, Langtian Ma, Yuchen Yan, Yuchen Zhang, Kai Wang, Yue Yang, Ziyao Guo, Wenqi Shao, Yang You, Yu Qiao, Ping Luo, Kaipeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-video (T2V) technology advancements, as demonstrated by models
such as Gen2, Pika, and Sora, have significantly broadened its applicability
and popularity. Despite these strides, evaluating these models poses
substantial challenges. Primarily, due to the limitations inherent in automatic
metrics, manual evaluation is often considered a superior method for assessing
T2V generation. However, existing manual evaluation protocols face
reproducibility, reliability, and practicality issues. To address these
challenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE)
protocol, a comprehensive and standardized protocol for T2V models. The T2VHE
protocol includes well-defined metrics, thorough annotator training, and an
effective dynamic evaluation module. Experimental results demonstrate that this
protocol not only ensures high-quality annotations but can also reduce
evaluation costs by nearly 50\%. We will open-source the entire setup of the
T2VHE protocol, including the complete protocol workflow, the dynamic
evaluation component details, and the annotation interface code. This will help
communities establish more sophisticated human assessment protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhyRecon: Physically Plausible Neural Scene Reconstruction <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16666v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16666v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Ni, Yixin Chen, Bohan Jing, Nan Jiang, Bin Wang, Bo Dai, Puhao Li, Yixin Zhu, Song-Chun Zhu, Siyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the issue of physical implausibility in multi-view neural
reconstruction. While implicit representations have gained popularity in
multi-view 3D reconstruction, previous work struggles to yield physically
plausible results, limiting their utility in domains requiring rigorous
physical accuracy. This lack of plausibility stems from the absence of physics
modeling in existing methods and their inability to recover intricate
geometrical structures. In this paper, we introduce PHYRECON, the first
approach to leverage both differentiable rendering and differentiable physics
simulation to learn implicit surface representations. PHYRECON features a novel
differentiable particle-based physical simulator built on neural implicit
representations. Central to this design is an efficient transformation between
SDF-based implicit representations and explicit surface points via our proposed
Surface Points Marching Cubes (SP-MC), enabling differentiable learning with
both rendering and physical losses. Additionally, PHYRECON models both
rendering and physical uncertainty to identify and compensate for inconsistent
and inaccurate monocular geometric priors. The physical uncertainty further
facilitates physics-guided pixel sampling to enhance the learning of slender
structures. By integrating these techniques, our model supports differentiable
joint modeling of appearance, geometry, and physics. Extensive experiments
demonstrate that PHYRECON significantly improves the reconstruction quality.
Our results also exhibit superior physical stability in physical simulators,
with at least a 40% improvement across all datasets, paving the way for future
physics-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS'24. Project page: https://phyrecon.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model LEGO: Creating Models Like Disassembling and Assembling Building
  Blocks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacong Hu, Jing Gao, Jingwen Ye, Yang Gao, Xingen Wang, Zunlei Feng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of deep learning, the increasing complexity and
scale of parameters make training a new model increasingly resource-intensive.
In this paper, we start from the classic convolutional neural network (CNN) and
explore a paradigm that does not require training to obtain new models. Similar
to the birth of CNN inspired by receptive fields in the biological visual
system, we draw inspiration from the information subsystem pathways in the
biological visual system and propose Model Disassembling and Assembling (MDA).
During model disassembling, we introduce the concept of relative contribution
and propose a component locating technique to extract task-aware components
from trained CNN classifiers. For model assembling, we present the alignment
padding strategy and parameter scaling strategy to construct a new model
tailored for a specific task, utilizing the disassembled task-aware components.
The entire process is akin to playing with LEGO bricks, enabling arbitrary
assembly of new models, and providing a novel perspective for model creation
and reuse. Extensive experiments showcase that task-aware components
disassembled from CNN classifiers or new models assembled using these
components closely match or even surpass the performance of the baseline,
demonstrating its promising results for model reuse. Furthermore, MDA exhibits
diverse potential applications, with comprehensive experiments exploring model
decision route analysis, model compression, knowledge distillation, and more.
The code is available at https://github.com/jiaconghu/Model-LEGO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETO:Efficient <span class="highlight-title">Transformer</span>-based Local Feature Matching by Organizing
  Multiple Homography Hypotheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Ni, Guofeng Zhang, Guanglin Li, Yijin Li, Xinyang Liu, Zhaoyang Huang, Hujun Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the efficiency problem of learning local feature matching. Recent
advancements have given rise to purely CNN-based and transformer-based
approaches, each augmented with deep learning techniques. While CNN-based
methods often excel in matching speed, transformer-based methods tend to
provide more accurate matches. We propose an efficient transformer-based
network architecture for local feature matching. This technique is built on
constructing multiple homography hypotheses to approximate the continuous
correspondence in the real world and uni-directional cross-attention to
accelerate the refinement. On the YFCC100M dataset, our matching accuracy is
competitive with LoFTR, a state-of-the-art transformer-based architecture,
while the inference speed is boosted to 4 times, even outperforming the
CNN-based methods. Comprehensive evaluations on other open datasets such as
Megadepth, ScanNet, and HPatches demonstrate our method's efficacy,
highlighting its potential to significantly enhance a wide array of downstream
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhao Liu, Ke Wang, Jieqi Shi, Zhijian Qiao, Shaojie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic mapping based on the supervised object detectors is sensitive to
image distribution. In real-world environments, the object detection and
segmentation performance can lead to a major drop, preventing the use of
semantic mapping in a wider domain. On the other hand, the development of
vision-language foundation models demonstrates a strong zero-shot
transferability across data distribution. It provides an opportunity to
construct generalizable instance-aware semantic maps. Hence, this work explores
how to boost instance-aware semantic mapping from object detection generated
from foundation models. We propose a probabilistic label fusion method to
predict close-set semantic classes from open-set label measurements. An
instance refinement module merges the over-segmented instances caused by
inconsistent segmentation. We integrate all the modules into a unified semantic
mapping system. Reading a sequence of RGB-D input, our work incrementally
reconstructs an instance-aware semantic map. We evaluate the zero-shot
performance of our method in ScanNet and SceneNN datasets. Our method achieves
40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation
task. It outperforms the traditional semantic mapping method significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE RAL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniAR: A Unified model for predicting human Attention and Responses on
  visual content <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10175v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10175v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhao Li, Junfeng He, Gang Li, Rachit Bhargava, Shaolei Shen, Nachiappan Valliappan, Youwei Liang, Hongxiang Gu, Venky Ramachandran, Golnaz Farhadi, Yang Li, Kai J Kohlhoff, Vidhya Navalpakkam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in human behavior modeling involves understanding both implicit,
early-stage perceptual behavior, such as human attention, and explicit,
later-stage behavior, such as subjective preferences or likes. Yet most prior
research has focused on modeling implicit and explicit human behavior in
isolation; and often limited to a specific type of visual content. We propose
UniAR -- a unified model of human attention and preference behavior across
diverse visual content. UniAR leverages a multimodal transformer to predict
subjective feedback, such as satisfaction or aesthetic quality, along with the
underlying human attention or interaction heatmaps and viewing order. We train
UniAR on diverse public datasets spanning natural images, webpages, and graphic
designs, and achieve SOTA performance on multiple benchmarks across various
image domains and behavior modeling tasks. Potential applications include
providing instant feedback on the effectiveness of UIs/visual content, and
enabling designers and content-creation models to optimize their creation for
human-centric improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One <span class="highlight-title">Prompt</span> to Verify Your Models: Black-Box Text-to-Image Models
  Verification via Non-Transferable Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Guo, Wenbo Jiang, Rui Zhang, Guoming Lu, Hongwei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the success of Text-to-Image (T2I) models has led to the rise of
numerous third-party platforms, which claim to provide cheaper API services and
more flexibility in model options. However, this also raises a new security
concern: Are these third-party services truly offering the models they claim?
To address this problem, we propose the first T2I model verification method
named Text-to-Image Model Verification via Non-Transferable Adversarial Attacks
(TVN). The non-transferability of adversarial examples means that these
examples are only effective on a target model and ineffective on other models,
thereby allowing for the verification of the target model. TVN utilizes the
Non-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize the cosine
similarity of a prompt's text encoding, generating non-transferable adversarial
prompts. By calculating the CLIP-text scores between the non-transferable
adversarial prompts without perturbations and the images, we can verify if the
model matches the claimed target model, based on a 3-sigma threshold. The
experiments showed that TVN performed well in both closed-set and open-set
scenarios, achieving a verification accuracy of over 90\%. Moreover, the
adversarial prompts generated by TVN significantly reduced the CLIP-text scores
of the target model, while having little effect on other models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection of adrenal anomalous findings in spinal CT images using multi
  model graph aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20568v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20568v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shabalin Carmel, Shenkman Israel, Shelef Ilan, Ben-Arie Gal, Alex Geftler, Shahar Yuval
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low back pain is the symptom that is the second most frequently reported to
primary care physicians, effecting 50 to 80 percent of the population in a
lifetime, resulting in multiple referrals of patients suffering from back
problems, to CT and MRI scans, which are then examined by radiologists. The
radiologists examining these spinal scans naturally focus on spinal pathologies
and might miss other types of abnormalities, and in particular, abdominal ones,
such as malignancies. Nevertheless, the patients whose spine was scanned might
as well have malignant and other abdominal pathologies. Thus, clinicians have
suggested the need for computerized assistance and decision support in
screening spinal scans for additional abnormalities. In the current study, We
have addressed the important case of detecting suspicious lesions in the
adrenal glands as an example for the overall methodology we have developed. A
patient CT scan is integrated from multiple slices with an axial orientation.
Our method determines whether a patient has an abnormal adrenal gland, and
localises the abnormality if it exists. Our method is composed of three deep
learning models; each model has a different task for achieving the final goal.
We call our compound method the Multi Model Graph Aggregation MMGA method. The
novelty in this study is twofold. First, the use, for an important screening
task, of CT scans that are originally focused and tuned for imaging the spine,
which were acquired from patients with potential spinal disorders, for
detection of a totally different set of abnormalities such as abdominal Adrenal
glands pathologies. Second, we have built a complex pipeline architecture
composed from three deep learning models that can be utilized for other organs
(such as the pancreas or the kidney), or for similar applications, but using
other types of imaging, such as MRI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VASA, a framework for generating lifelike talking faces with
appealing visual affective skills (VAS) given a single static image and a
speech audio clip. Our premiere model, VASA-1, is capable of not only
generating lip movements that are exquisitely synchronized with the audio, but
also producing a large spectrum of facial nuances and natural head motions that
contribute to the perception of authenticity and liveliness. The core
innovations include a holistic facial dynamics and head movement generation
model that works in a face latent space, and the development of such an
expressive and disentangled face latent space using videos. Through extensive
experiments including evaluation on a set of new metrics, we show that our
method significantly outperforms previous methods along various dimensions
comprehensively. Our method not only delivers high video quality with realistic
facial and head dynamics but also supports the online generation of 512x512
videos at up to 40 FPS with negligible starting latency. It paves the way for
real-time engagements with lifelike avatars that emulate human conversational
behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (Oral) Camera ready. Project webpage:
  https://www.microsoft.com/en-us/research/project/vasa-1/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizing Alignment Paradigm of Text-to-Image Generation with
  Preferences through $f$-divergence Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Sun, Bo Xia, Yongzhe Chang, Xueqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has recently expanded its successful
application from aligning large language models (LLMs) to aligning
text-to-image models with human preferences, which has generated considerable
interest within the community. However, we have observed that these approaches
rely solely on minimizing the reverse Kullback-Leibler divergence during
alignment process between the fine-tuned model and the reference model,
neglecting the incorporation of other divergence constraints. In this study, we
focus on extending reverse Kullback-Leibler divergence in the alignment
paradigm of text-to-image models to $f$-divergence, which aims to garner better
alignment performance as well as good generation diversity. We provide the
generalized formula of the alignment paradigm under the $f$-divergence
condition and thoroughly analyze the impact of different divergence constraints
on alignment process from the perspective of gradient fields. We conduct
comprehensive evaluation on image-text alignment performance, human value
alignment performance and generation diversity performance under different
divergence constraints, and the results indicate that alignment based on
Jensen-Shannon divergence achieves the best trade-off among them. The option of
divergence employed for aligning text-to-image models significantly impacts the
trade-off between alignment performance (especially human value alignment) and
generation diversity, which highlights the necessity of selecting an
appropriate divergence for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Explicit Rules to Implicit Reasoning in an Interpretable Violence
  Monitoring System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Dong Jiang, Chih-Yung Chang, Hsiang-Chuan Chang, Diptendu Sinha Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, research based on pre-trained models has demonstrated outstanding
performance in violence surveillance tasks. However, these black-box systems
face challenges regarding explainability during training and inference
processes. An important question is how to incorporate explicit knowledge into
these implicit models, thereby designing expert-driven and interpretable
violence surveillance systems. This paper proposes a new paradigm for weakly
supervised violence monitoring (WSVM) called Rule base Violence monitoring
(RuleVM). The proposed RuleVM uses a dual-branch structure for different
designs for images and text. One of the branches is called the implicit branch,
which uses only visual features for coarse-grained binary classification. In
this branch, image feature extraction is divided into two channels: one
responsible for extracting scene frames and the other focusing on extracting
actions. The other branch is called the explicit branch, which utilizes
language-image alignment to perform fine-grained classification. For the
language channel design in the explicit branch, the proposed RuleCLIP uses the
state-of-the-art YOLO-World model to detect objects and actions in video
frames, and association rules are identified through data mining methods as
descriptions of the video. Leveraging the dual-branch architecture, RuleVM
achieves interpretable coarse-grained and fine-grained violence surveillance.
Extensive experiments were conducted on two commonly used benchmarks, and the
results show that RuleCLIP achieved the best performance in both coarse-grained
and fine-grained detection, significantly outperforming existing
state-of-the-art methods. Moreover, interpretability experiments uncovered some
interesting rules, such as the observation that as the number of people
increases, the risk level of violent behavior also rises.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Longitudinal Analysis of Racial and Gender Bias in New York Times and
  Fox News Images and Articles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazem Ibrahim, Nouar AlDahoul, Syed Mustafa Ali Abbasi, Fareed Zaffar, Talal Rahwan, Yasir Zaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The manner in which different racial and gender groups are portrayed in news
coverage plays a large role in shaping public opinion. As such, understanding
how such groups are portrayed in news media is of notable societal value, and
has thus been a significant endeavour in both the computer and social sciences.
Yet, the literature still lacks a longitudinal study examining both the
frequency of appearance of different racial and gender groups in online news
articles, as well as the context in which such groups are discussed. To fill
this gap, we propose two machine learning classifiers to detect the race and
age of a given subject. Next, we compile a dataset of 123,337 images and
441,321 online news articles from New York Times (NYT) and Fox News (Fox), and
examine representation through two computational approaches. Firstly, we
examine the frequency and prominence of appearance of racial and gender groups
in images embedded in news articles, revealing that racial and gender
minorities are largely under-represented, and when they do appear, they are
featured less prominently compared to majority groups. Furthermore, we find
that NYT largely features more images of racial minority groups compared to
Fox. Secondly, we examine both the frequency and context with which racial
minority groups are presented in article text. This reveals the narrow scope in
which certain racial groups are covered and the frequency with which different
groups are presented as victims and/or perpetrators in a given conflict. Taken
together, our analysis contributes to the literature by providing two novel
open-source classifiers to detect race and age from images, and shedding light
on the racial and gender biases in news articles from venues on opposite ends
of the American political spectrum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, and 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind the Context: Attention-Guided Weak-to-Strong Consistency for
  Enhanced Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Cheng, Chenxi Shao, Jie Ma, Guoliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is a pivotal step in diagnostic and therapeutic
processes, relying on high-quality annotated data that is often challenging and
costly to obtain. Semi-supervised learning offers a promising approach to
enhance model performance by leveraging unlabeled data. Although weak-to-strong
consistency is a prevalent method in semi-supervised image segmentation, there
is a scarcity of research on perturbation strategies specifically tailored for
semi-supervised medical image segmentation tasks. To address this challenge,
this paper introduces a simple yet efficient semi-supervised learning framework
named Attention-Guided weak-to-strong Consistency Match (AIGCMatch). The
AIGCMatch framework incorporates attention-guided perturbation strategies at
both the image and feature levels to achieve weak-to-strong consistency
regularization. This method not only preserves the structural information of
medical images but also enhances the model's ability to process complex
semantic information. Extensive experiments conducted on the ACDC and ISIC-2017
datasets have validated the effectiveness of AIGCMatch. Our method achieved a
90.4\% Dice score in the 7-case scenario on the ACDC dataset, surpassing the
state-of-the-art methods and demonstrating its potential and efficacy in
clinical settings. Additionally, on the ISIC-2017 dataset, we significantly
outperformed our baseline, indicating the robustness and generalizability of
AIGCMatch across different medical image segmentation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View
  Synthesis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Min, Yawei Luo, Jianwen Sun, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from
sparse-view observations in a feed-forward inference manner, eliminating the
need for scene-specific retraining required in conventional 3DGS. However,
existing methods rely heavily on epipolar priors, which can be unreliable in
complex realworld scenes, particularly in non-overlapping and occluded regions.
In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based
model for generalizable novel view synthesis that operates independently of
epipolar line constraints. To enhance multiview feature extraction with 3D
perception, we employ a selfsupervised Vision Transformer (ViT) with cross-view
completion pre-training on large-scale datasets. Additionally, we introduce an
Iterative Cross-view Gaussians Alignment method to ensure consistent depth
scales across different views. Our eFreeSplat represents an innovative approach
for generalizable novel view synthesis. Different from the existing pure
geometry-free methods, eFreeSplat focuses more on achieving epipolar-free
feature matching and encoding by providing 3D priors through cross-view
pretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks
using the RealEstate10K and ACID datasets. Extensive experiments demonstrate
that eFreeSplat surpasses state-of-the-art baselines that rely on epipolar
priors, achieving superior geometry reconstruction and novel view synthesis
quality. Project page: https://tatakai1.github.io/efreesplat/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceiving Longer Sequences With Bi-Directional Cross-Attention
  <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12138v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12138v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Hiller, Krista A. Ehinger, Tom Drummond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel bi-directional Transformer architecture (BiXT) which
scales linearly with input size in terms of computational cost and memory
consumption, but does not suffer the drop in performance or limitation to only
one input modality seen with other efficient Transformer-based approaches. BiXT
is inspired by the Perceiver architectures but replaces iterative attention
with an efficient bi-directional cross-attention module in which input tokens
and latent variables attend to each other simultaneously, leveraging a
naturally emerging attention-symmetry between the two. This approach unlocks a
key bottleneck experienced by Perceiver-like architectures and enables the
processing and interpretation of both semantics ('what') and location ('where')
to develop alongside each other over multiple layers -- allowing its direct
application to dense and instance-based tasks alike. By combining efficiency
with the generality and performance of a full Transformer architecture, BiXT
can process longer sequences like point clouds, text or images at higher
feature resolutions and achieves competitive performance across a range of
tasks like point cloud part segmentation, semantic image segmentation, image
classification, hierarchical sequence modeling and document retrieval. Our
experiments demonstrate that BiXT models outperform larger competitors by
leveraging longer sequences more efficiently on vision tasks like
classification and segmentation, and perform on par with full Transformer
variants on sequence modeling and document retrieval -- but require $28\%$
fewer FLOPs and are up to $8.4\times$ faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024; Code and models will be available at
  https://github.com/mrkshllr/BiXT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiview Scene Graph <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juexiao Zhang, Gao Zhu, Sihang Li, Xinhao Liu, Haorui Song, Xinran Tang, Chen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A proper scene representation is central to the pursuit of spatial
intelligence where agents can robustly reconstruct and efficiently understand
3D scenes. A scene representation is either metric, such as landmark maps in 3D
reconstruction, 3D bounding boxes in object detection, or voxel grids in
occupancy prediction, or topological, such as pose graphs with loop closures in
SLAM or visibility graphs in SfM. In this work, we propose to build Multiview
Scene Graphs (MSG) from unposed images, representing a scene topologically with
interconnected place and object nodes. The task of building MSG is challenging
for existing representation learning methods since it needs to jointly address
both visual place recognition, object detection, and object association from
images with limited fields of view and potentially large viewpoint changes. To
evaluate any method tackling this task, we developed an MSG dataset and
annotation based on a public 3D dataset. We also propose an evaluation metric
based on the intersection-over-union score of MSG edges. Moreover, we develop a
novel baseline method built on mainstream pretrained vision models, combining
visual place recognition and object association into one Transformer decoder
architecture. Experiments demonstrate that our method has superior performance
compared to existing relevant baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in NeurIPS 2024. Website at
  https://ai4ce.github.io/MSG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14325v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14325v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Guo, Shuai Lu, Weihang Zhang, Fang Chen, Hongen Liao, Huiqi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies highlighted a practical setting of unsupervised anomaly
detection (UAD) that builds a unified model for multi-class images, serving as
an alternative to the conventional one-class-one-model setup. Despite various
advancements addressing this challenging task, the detection performance under
the multi-class setting still lags far behind state-of-the-art class-separated
models. Our research aims to bridge this substantial performance gap. In this
paper, we introduce a minimalistic reconstruction-based anomaly detection
framework, namely Dinomaly, which leverages pure Transformer architectures
without relying on complex designs, additional modules, or specialized tricks.
Given this powerful framework consisted of only Attentions and MLPs, we found
four simple components that are essential to multi-class anomaly detection: (1)
Foundation Transformers that extracts universal and discriminative features,
(2) Noisy Bottleneck where pre-existing Dropouts do all the noise injection
tricks, (3) Linear Attention that naturally cannot focus, and (4) Loose
Reconstruction that does not force layer-to-layer and point-by-point
reconstruction. Extensive experiments are conducted across popular anomaly
detection benchmarks including MVTec-AD, VisA, and Real-IAD. Our proposed
Dinomaly achieves impressive image-level AUROC of 99.6%, 98.7%, and 89.3% on
the three datasets respectively, which is not only superior to state-of-the-art
multi-class UAD methods, but also achieves the most advanced class-separated
UAD records.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLM Agents Generate Their Own Memories: Distilling Experience into
  Embodied Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14596v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14596v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale generative language and vision-language models excel in
in-context learning for decision making. However, they require high-quality
exemplar demonstrations to be included in their context window. In this work,
we ask: Can LLMs and VLMs generate their own examples from generic, sub-optimal
demonstrations? We propose In-Context Abstraction Learning (ICAL), a method
that builds a memory of multimodal experience from sub-optimal demonstrations
and human feedback. Given a task demonstration that may contain inefficiencies
or mistakes, a VLM abstracts the trajectory into a generalized program by
correcting inefficient actions and annotating cognitive abstractions: causal
relationships, object state changes, temporal subgoals, and task-relevant
visual elements. These abstractions are iteratively improved through human
feedback while the agent attempts to execute the trajectory. The resulting
examples, when used as exemplars in the prompt, significantly improve
decision-making in retrieval-augmented LLM and VLM agents. Moreover, as the
agent's library of examples grows, it becomes more efficient, relying less on
human feedback and requiring fewer environment interactions per demonstration.
Our ICAL agent surpasses the state-of-the-art in dialogue-based instruction
following in TEACh, multimodal web agents in VisualWebArena, and action
anticipation in Ego4D. In TEACh, we achieve a 12.6% improvement in
goal-condition success. In VisualWebArena, our task success rate improves over
the SOTA from 14.3% to 22.7% using GPT4V. In Ego4D action forecasting, we
improve over few-shot GPT-4V and remain competitive with supervised models. We
show finetuning our retrieval-augmented in-context agent yields additional
improvements. Our approach significantly reduces reliance on manual prompt
engineering and consistently outperforms in-context learning from action plans
that lack such abstractions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: http://ical-learning.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLAP4CLIP: Continual Learning with Probabilistic Finetuning for
  Vision-Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurav Jha, Dong Gong, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) aims to help deep neural networks learn new knowledge
while retaining what has been learned. Owing to their powerful
generalizability, pre-trained vision-language models such as Contrastive
Language-Image Pre-training (CLIP) have lately gained traction as practical CL
candidates. However, the domain mismatch between the pre-training and the
downstream CL tasks often calls for finetuning of the CLIP on the latter. Most
existing finetuning methods exhibit deterministic nature. This makes them
overlook the many possible interactions across the input modalities and deems
them unsafe for high-risk tasks requiring reliable uncertainty estimation. To
address these, our work proposes Continual LeArning with Probabilistic
finetuning (CLAP) - a probabilistic modeling framework over visual-guided text
features per task, thus providing more calibrated CL finetuning. Unlike recent
data-hungry anti-forgetting CL techniques, CLAP alleviates forgetting by
exploiting the rich pre-trained knowledge of CLIP for weight initialization and
distribution regularization of task-specific parameters. Cooperating with the
diverse range of existing prompting methods, CLAP can surpass the predominant
deterministic finetuning approaches for CL with CLIP. We conclude with
out-of-the-box applications of superior uncertainty estimation abilities of
CLAP including novel data detection and exemplar selection within the existing
CL setups. Our code is available at
\url{https://github.com/srvCodes/clap4clip}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a poster at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PipeFusion: Patch-level Pipeline Parallelism for Diffusion <span class="highlight-title">Transformer</span>s
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14430v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14430v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Fang, Jinzhe Pan, Jiannan Wang, Aoyu Li, Xibo Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents PipeFusion, an innovative parallel methodology to tackle
the high latency issues associated with generating high-resolution images using
diffusion transformers (DiTs) models. PipeFusion partitions images into patches
and the model layers across multiple GPUs. It employs a patch-level pipeline
parallel strategy to orchestrate communication and computation efficiently. By
capitalizing on the high similarity between inputs from successive diffusion
steps, PipeFusion reuses one-step stale feature maps to provide context for the
current pipeline step. This approach notably reduces communication costs
compared to existing DiTs inference parallelism, including tensor parallel,
sequence parallel and DistriFusion. PipeFusion also exhibits superior memory
efficiency, because it can distribute model parameters across multiple devices,
making it more suitable for DiTs with large parameter sizes, such as Flux.1.
Experimental results demonstrate that PipeFusion achieves state-of-the-art
performance on 8xL40 PCIe GPUs for Pixart, Stable-Diffusion 3 and Flux.1
models.Our Source code is available at https://github.com/xdit-project/xDiT.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Length-Induced Embedding Collapse in <span class="highlight-title">Transformer</span>-based Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings enable various applications, but their performance
deteriorates on longer texts. In this paper, we find that the performance
degradation is due to a phenomenon called Length Collapse, where longer text
embeddings collapse into a narrow space. This collapse results in a
distributional inconsistency between embeddings of different text lengths,
ultimately hurting the performance of downstream tasks. Theoretically, by
considering the self-attention mechanism inherently functions as a low-pass
filter, we prove that long sequences increase the attenuation rate of the
low-pass filter effect of the self-attention mechanism. With layers going
deeper, excessive low-pass filtering causes the token signals to retain only
their Direct-Current (DC) component, which means the input token feature maps
will collapse into a narrow space, especially in long texts. Based on the above
analysis, we propose to mitigate the undesirable length collapse limitation by
introducing a temperature in softmax(), which achieves a higher low-filter
attenuation rate. The tuning-free method, called TempScale, can be plugged into
multiple transformer-based embedding models. Empirically, we demonstrate that
TempScale can improve existing embedding models, especially on long text
inputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text
Embedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from
LongEmbed, which specifically focuses on long context retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Bias in Political Search Query Suggestions by Relative
  Comparison with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Haak, Björn Engelmann, Christin Katharina Kreutz, Philipp Schaer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search query suggestions affect users' interactions with search engines,
which then influences the information they encounter. Thus, bias in search
query suggestions can lead to exposure to biased search results and can impact
opinion formation. This is especially critical in the political domain.
Detecting and quantifying bias in web search engines is difficult due to its
topic dependency, complexity, and subjectivity. The lack of context and
phrasality of query suggestions emphasizes this problem. In a multi-step
approach, we combine the benefits of large language models, pairwise
comparison, and Elo-based scoring to identify and quantify bias in English
search query suggestions. We apply our approach to the U.S. political news
domain and compare bias in Google and Bing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Medical Information Extraction and
  Query Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Peikos, Pranav Kasela, Gabriella Pasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a system that integrates large language models (LLMs)
into the clinical trial retrieval process, enhancing the effectiveness of
matching patients with eligible trials while maintaining information privacy
and allowing expert oversight. We evaluate six LLMs for query generation,
focusing on open-source and relatively small models that require minimal
computational resources. Our evaluation includes two closed-source and four
open-source models, with one specifically trained in the medical field and five
general-purpose models. We compare the retrieval effectiveness achieved by
LLM-generated queries against those created by medical experts and
state-of-the-art methods from the literature. Our findings indicate that the
evaluated models reach retrieval effectiveness on par with or greater than
expert-created queries. The LLMs consistently outperform standard baselines and
other approaches in the literature. The best performing LLMs exhibit fast
response times, ranging from 1.7 to 8 seconds, and generate a manageable number
of query terms (15-63 on average), making them suitable for practical
implementation. Our overall findings suggest that leveraging small, open-source
LLMs for clinical trials retrieval can balance performance, computational
efficiency, and real-world applicability in medical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WI-IAT '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auditing Google's Search Algorithm: Measuring News Diversity Across
  Brazil, the UK, and the US 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Hernandes, Giulio Corsi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the influence of Google's search algorithm on news
diversity by analyzing search results in Brazil, the UK, and the US. It
explores how Google's system preferentially favors a limited number of news
outlets. Utilizing algorithm auditing techniques, the research measures source
concentration with the Herfindahl-Hirschman Index (HHI) and Gini coefficient,
revealing significant concentration trends. The study underscores the
importance of conducting horizontal analyses across multiple search queries, as
focusing solely on individual results pages may obscure these patterns. Factors
such as popularity, political bias, and recency were evaluated for their impact
on news rankings. Findings indicate a slight leftward bias in search outcomes
and a preference for popular, often national outlets. This bias, combined with
a tendency to prioritize recent content, suggests that Google's algorithm may
reinforce existing media inequalities. By analyzing the largest dataset to date
-- 221,863 search results -- this research provides comprehensive, longitudinal
insights into how algorithms shape public access to diverse news sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Content Relevance: Evaluating Instruction Following in Retrieval
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqun Zhou, Yuanlei Zheng, Wei Chen, Qianqian Zheng, Zeyuan Shang, Wei Zhang, Rui Meng, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-following capabilities in large language models (LLMs) have
significantly progressed, enabling more complex user interactions through
detailed prompts. However, retrieval systems have not matched these advances,
most of them still relies on traditional lexical and semantic matching
techniques that fail to fully capture user intent. Recent efforts have
introduced instruction-aware retrieval models, but these primarily focus on
intrinsic content relevance, which neglects the importance of customized
preferences for broader document-level attributes. This study evaluates the
instruction-following capabilities of various retrieval models beyond content
relevance, including LLM-based dense retrieval and reranking models. We develop
InfoSearch, a novel retrieval evaluation benchmark spanning six document-level
attributes: Audience, Keyword, Format, Language, Length, and Source, and
introduce novel metrics -- Strict Instruction Compliance Ratio (SICR) and
Weighted Instruction Sensitivity Evaluation (WISE) to accurately assess the
models' responsiveness to instructions. Our findings reveal that while
reranking models generally surpass retrieval models in instruction following,
they still face challenges in handling certain attributes. Moreover, although
instruction fine-tuning and increased model size lead to better performance,
most models fall short of achieving comprehensive instruction compliance as
assessed by our benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identify Then Recommend: Towards Unsupervised Group Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Liu, Shihao Zhu, Tianyuan Yang, Jian Ma, Wenliang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group Recommendation (GR), which aims to recommend items to groups of users,
has become a promising and practical direction for recommendation systems. This
paper points out two issues of the state-of-the-art GR models. (1) The
pre-defined and fixed number of user groups is inadequate for real-time
industrial recommendation systems, where the group distribution can shift
dynamically. (2) The training schema of existing GR methods is supervised,
necessitating expensive user-group and group-item labels, leading to
significant annotation costs. To this end, we present a novel unsupervised
group recommendation framework named \underline{I}dentify \underline{T}hen
\underline{R}ecommend (\underline{ITR}), where it first identifies the user
groups in an unsupervised manner even without the pre-defined number of groups,
and then two pre-text tasks are designed to conduct self-supervised group
recommendation. Concretely, at the group identification stage, we first
estimate the adaptive density of each user point, where areas with higher
densities are more likely to be recognized as group centers. Then, a heuristic
merge-and-split strategy is designed to discover the user groups and decision
boundaries. Subsequently, at the self-supervised learning stage, the
pull-and-repulsion pre-text task is proposed to optimize the user-group
distribution. Besides, the pseudo group recommendation pre-text task is
designed to assist the recommendations. Extensive experiments demonstrate the
superiority and effectiveness of ITR on both user recommendation (e.g., 22.22\%
NDCG@5 $\uparrow$) and group recommendation (e.g., 22.95\% NDCG@5 $\uparrow$).
Furthermore, we deploy ITR on the industrial recommender and achieve promising
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoTaDual: Modality-Task Dual Alignment for Enhanced Zero-shot Composed
  Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwen Li, Fei Su, Zhicheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed Image Retrieval (CIR) is a challenging vision-language task,
utilizing bi-modal (image+text) queries to retrieve target images. Despite the
impressive performance of supervised CIR, the dependence on costly,
manually-labeled triplets limits its scalability and zero-shot capability. To
address this issue, zero-shot composed image retrieval (ZS-CIR) is presented
along with projection-based approaches. However, such methods face two major
problems, i.e., task discrepancy between pre-training (image $\leftrightarrow$
text) and inference (image+text $\rightarrow$ image), and modality discrepancy.
The latter pertains to approaches based on text-only projection training due to
the necessity of feature extraction from the reference image during inference.
In this paper, we propose a two-stage framework to tackle both discrepancies.
First, to ensure efficiency and scalability, a textual inversion network is
pre-trained on large-scale caption datasets. Subsequently, we put forward
Modality-Task Dual Alignment (MoTaDual) as the second stage, where
large-language models (LLMs) generate triplet data for fine-tuning, and
additionally, prompt learning is introduced in a multi-modal context to
effectively alleviate both modality and task discrepancies. The experimental
results show that our MoTaDual achieves the state-of-the-art performance across
four widely used ZS-CIR benchmarks, while maintaining low training time and
computational cost. The code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Cross-Modal Text-Molecule Retrieval with Better Modality
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Song, Wanru Zhuang, Yujie Lin, Liang Zhang, Chunyan Li, Jinsong Su, Song He, Xiaochen Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal text-molecule retrieval model aims to learn a shared feature
space of the text and molecule modalities for accurate similarity calculation,
which facilitates the rapid screening of molecules with specific properties and
activities in drug design. However, previous works have two main defects.
First, they are inadequate in capturing modality-shared features considering
the significant gap between text sequences and molecule graphs. Second, they
mainly rely on contrastive learning and adversarial training for cross-modality
alignment, both of which mainly focus on the first-order similarity, ignoring
the second-order similarity that can capture more structural information in the
embedding space. To address these issues, we propose a novel cross-modal
text-molecule retrieval model with two-fold improvements. Specifically, on the
top of two modality-specific encoders, we stack a memory bank based feature
projector that contain learnable memory vectors to extract modality-shared
features better. More importantly, during the model training, we calculate four
kinds of similarity distributions (text-to-text, text-to-molecule,
molecule-to-molecule, and molecule-to-text similarity distributions) for each
instance, and then minimize the distance between these similarity distributions
(namely second-order similarity losses) to enhance cross-modal alignment.
Experimental results and analysis strongly demonstrate the effectiveness of our
model. Particularly, our model achieves SOTA performance, outperforming the
previously-reported best result by 6.4%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BIBM 2024 regular paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling User Satisfaction and Creator Productivity Trade-Offs in
  Recommendation Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yao, Yiming Liao, Jingzhou Liu, Shaoliang Nie, Qifan Wang, Haifeng Xu, Hongning Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On User-Generated Content (UGC) platforms, recommendation algorithms
significantly impact creators' motivation to produce content as they compete
for algorithmically allocated user traffic. This phenomenon subtly shapes the
volume and diversity of the content pool, which is crucial for the platform's
sustainability. In this work, we demonstrate, both theoretically and
empirically, that a purely relevance-driven policy with low exploration
strength boosts short-term user satisfaction but undermines the long-term
richness of the content pool. In contrast, a more aggressive exploration policy
may slightly compromise user satisfaction but promote higher content creation
volume. Our findings reveal a fundamental trade-off between immediate user
satisfaction and overall content production on UGC platforms. Building on this
finding, we propose an efficient optimization method to identify the optimal
exploration strength, balancing user and creator engagement. Our model can
serve as a pre-deployment audit tool for recommendation algorithms on UGC
platforms, helping to align their immediate objectives with sustainable,
long-term goals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content Aware Analysis of Scholarly Networks: A Case Study on CORD19
  <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet Emre Akbulut, Yusuf Erdem Nacar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the relationships among key elements of scientific
research network, namely articles, researchers, and journals. We introduce a
novel approach to use semantic information through the HITS algorithm based
propagation of topic information in the network. The topic information is
derived by using the Named Entity Recognition and Entity Linkage. In our case,
MedCAT is used to extract the topics from the CORD19 Dataset, which is a corpus
of academic articles about COVID-19 and coronavirus scientific network. Our
approach focuses on the COVID-19 domain, utilizing the CORD-19 dataset to
demonstrate the efficacy of integrating topic-related information within the
citation framework. Through the application of a hybrid HITS algorithm, we show
that incorporating topic data significantly influences article rankings,
revealing deeper insights into the structure of the academic community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Multi-Agent Copilot towards Autonomous Agricultural Data
  Management and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Jianxin Sun, Hongfeng Yu, Joe Luck, Geng Bai, Nipuna Chamara, Yufeng Ge, Tala Awada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current agricultural data management and analysis paradigms are to large
extent traditional, in which data collecting, curating, integration, loading,
storing, sharing and analyzing still involve too much human effort and
know-how. The experts, researchers and the farm operators need to understand
the data and the whole process of data management pipeline to make fully use of
the data. The essential problem of the traditional paradigm is the lack of a
layer of orchestrational intelligence which can understand, organize and
coordinate the data processing utilities to maximize data management and
analysis outcome. The emerging reasoning and tool mastering abilities of large
language models (LLM) make it a potentially good fit to this position, which
helps a shift from the traditional user-driven paradigm to AI-driven paradigm.
In this paper, we propose and explore the idea of a LLM based copilot for
autonomous agricultural data management and analysis. Based on our previously
developed platform of Agricultural Data Management and Analytics (ADMA), we
build a proof-of-concept multi-agent system called ADMA Copilot, which can
understand user's intent, makes plans for data processing pipeline and
accomplishes tasks automatically, in which three agents: a LLM based
controller, an input formatter and an output formatter collaborate together.
Different from existing LLM based solutions, by defining a meta-program graph,
our work decouples control flow and data flow to enhance the predictability of
the behaviour of the agents. Experiments demonstrates the intelligence,
autonomy, efficacy, efficiency, extensibility, flexibility and privacy of our
system. Comparison is also made between ours and existing systems to show the
superiority and potential of our system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSL: Rethinking and Improving Softmax Loss from Pairwise Perspective for
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqin Yang, Jiawei Chen, Xin Xin, Sheng Zhou, Binbin Hu, Yan Feng, Chun Chen, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Softmax Loss (SL) is widely applied in recommender systems (RS) and has
demonstrated effectiveness. This work analyzes SL from a pairwise perspective,
revealing two significant limitations: 1) the relationship between SL and
conventional ranking metrics like DCG is not sufficiently tight; 2) SL is
highly sensitive to false negative instances. Our analysis indicates that these
limitations are primarily due to the use of the exponential function. To
address these issues, this work extends SL to a new family of loss functions,
termed Pairwise Softmax Loss (PSL), which replaces the exponential function in
SL with other appropriate activation functions. While the revision is minimal,
we highlight three merits of PSL: 1) it serves as a tighter surrogate for DCG
with suitable activation functions; 2) it better balances data contributions;
and 3) it acts as a specific BPR loss enhanced by Distributionally Robust
Optimization (DRO). We further validate the effectiveness and robustness of PSL
through empirical experiments. The code is available at
https://github.com/Tiny-Snow/IR-Benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Aware Query Policies in Active Learning for Efficient Autonomous
  Robotic Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sapphira Akins, Hans Mertens, Frances Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In missions constrained by finite resources, efficient data collection is
critical. Informative path planning, driven by automated decision-making,
optimizes exploration by reducing the costs associated with accurate
characterization of a target in an environment. Previous implementations of
active learning did not consider the action cost for regression problems or
only considered the action cost for classification problems. This paper
analyzes an AL algorithm for Gaussian Process regression while incorporating
action cost. The algorithm's performance is compared on various regression
problems to include terrain mapping on diverse simulated surfaces along metrics
of root mean square error, samples and distance until convergence, and model
variance upon convergence. The cost-dependent acquisition policy doesn't
organically optimize information gain over distance. Instead, the traditional
uncertainty metric with a distance constraint best minimizes root-mean-square
error over trajectory distance. This studys impact is to provide insight into
incorporating action cost with AL methods to optimize exploration under
realistic mission constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An engine not a camera: Measuring performative power of online search <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Celestine Mendler-Dünner, Gabriele Carovano, Moritz Hardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The power of digital platforms is at the center of major ongoing policy and
regulatory efforts. To advance existing debates, we designed and executed an
experiment to measure the performative power of online search providers.
Instantiated in our setting, performative power quantifies the ability of a
search engine to steer web traffic by rearranging results. To operationalize
this definition we developed a browser extension that performs unassuming
randomized experiments in the background. These randomized experiments emulate
updates to the search algorithm and identify the causal effect of different
content arrangements on clicks. Analyzing tens of thousands of clicks, we
discuss what our robust quantitative findings say about the power of online
search engines, using the Google Shopping antitrust investigation as a case
study. More broadly, we envision our work to serve as a blueprint for how the
recent definition of performative power can help integrate quantitative
insights from online experiments with future investigations into the economic
power of digital platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing
  the Upper Bound of Generative Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21488v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21488v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhirui Kuai, Zuxu Chen, Huimu Wang, Mingming Li, Dadong Miao, Binbin Wang, Xusong Chen, Li Kuang, Yuxing Han, Jiaxing Wang, Guoyu Tang, Lin Liu, Songlin Wang, Jingwei Zhuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval (GR) has emerged as a transformative paradigm in search
and recommender systems, leveraging numeric-based identifier representations to
enhance efficiency and generalization. Notably, methods like TIGER employing
Residual Quantization-based Semantic Identifiers (RQ-SID), have shown
significant promise in e-commerce scenarios by effectively managing item IDs.
However, a critical issue termed the "\textbf{Hourglass}" phenomenon, occurs in
RQ-SID, where intermediate codebook tokens become overly concentrated,
hindering the full utilization of generative retrieval methods. This paper
analyses and addresses this problem by identifying data sparsity and
long-tailed distribution as the primary causes. Through comprehensive
experiments and detailed ablation studies, we analyze the impact of these
factors on codebook utilization and data distribution. Our findings reveal that
the "Hourglass" phenomenon substantially impacts the performance of RQ-SID in
generative retrieval. We propose effective solutions to mitigate this issue,
thereby significantly enhancing the effectiveness of generative retrieval in
real-world E-commerce applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world
  Document Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulong Hui, Yao Lu, Huanchen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of Retrieval-Augmented Generation (RAG) has improved Large Language
Models (LLMs) in collaborating with external data, yet significant challenges
exist in real-world scenarios. In areas such as academic literature and finance
question answering, data are often found in raw text and tables in HTML or PDF
formats, which can be lengthy and highly unstructured. In this paper, we
introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that
involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We
revisit popular LLM- and RAG-based solutions for document analysis and evaluate
the design choices and answer qualities across multiple document domains and
diverse query types. Our evaluation yields interesting findings and highlights
the importance of data parsing and retrieval. We hope our benchmark can shed
light and better serve real-world document analysis applications. The benchmark
suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-end Learnable Clustering for Intent Learning in Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05975v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05975v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Xinwang Liu, Shengju Yu, Kejun Zhang, Wenliang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent learning, which aims to learn users' intents for user understanding
and item recommendation, has become a hot research spot in recent years.
However, existing methods suffer from complex and cumbersome alternating
optimization, limiting performance and scalability. To this end, we propose a
novel intent learning method termed \underline{ELCRec}, by unifying behavior
representation learning into an \underline{E}nd-to-end \underline{L}earnable
\underline{C}lustering framework, for effective and efficient
\underline{Rec}ommendation. Concretely, we encode user behavior sequences and
initialize the cluster centers (latent intents) as learnable neurons. Then, we
design a novel learnable clustering module to separate different cluster
centers, thus decoupling users' complex intents. Meanwhile, it guides the
network to learn intents from behaviors by forcing behavior embeddings close to
cluster centers. This allows simultaneous optimization of recommendation and
clustering via mini-batch data. Moreover, we propose intent-assisted
contrastive learning by using cluster centers as self-supervision signals,
further enhancing mutual promotion. Both experimental results and theoretical
analyses demonstrate the superiority of ELCRec from six perspectives. Compared
to the runner-up, ELCRec improves NDCG@5 by 8.9\% and reduces computational
costs by 22.5\% on the Beauty dataset. Furthermore, due to the scalability and
universal applicability, we deploy this method on the industrial recommendation
system with 130 million page views and achieve promising results. The codes are
available on GitHub (https://github.com/yueliu1999/ELCRec). A collection
(papers, codes, datasets) of deep group recommendation/intent learning methods
is available on GitHub
(https://github.com/yueliu1999/Awesome-Deep-Group-Recommendation).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00072v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00072v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Yukai Miao, Li Chen, Dawei Wang, Dan Li, Yanyu Ren, Hongtao Xie, Ce Yang, Xuhui Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RAG systems face limitations when semantic relevance alone does not guarantee
improved generation quality. This issue becomes particularly evident due to the
sensitivity of large language models (LLMs) to the ordering of few-shot
prompts, which can affect model performance. To address this challenge,
aligning LLM outputs with human preferences using structured feedback, such as
options to copy, regenerate, or dislike, offers a promising method for
improvement. This feedback is applied to the entire list of inputs rather than
giving specific ratings for individual documents, making it a Listwide Labels
Learning-to-Rank task.
  To address this task, we propose Pistis-RAG, a new RAG framework designed
with a content-centric approach to better align LLMs with human preferences.
Pistis-RAG effectively utilizes human feedback, enhancing content ranking and
generation quality. To validate our framework, we use public datasets to
simulate human feedback, allowing us to evaluate and refine our method
effectively. Experimental results indicate that Pistis-RAG improves alignment
with human preferences relative to the baseline RAG system, showing a 6.06%
increase in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy
metrics. These results highlight Pistis-RAG's effectiveness in overcoming the
limitations associated with traditional RAG approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DTN: Deep Multiple Task-specific Feature Interactions Network for
  Multi-Task Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11611v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11611v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowen Bi, Yuteng Lian, Jie Cui, Jun Liu, Peijian Wang, Guanghui Li, Xuejun Chen, Jinglin Zhao, Hao Wen, Jing Zhang, Zhaoqi Zhang, Wenzhuo Song, Yang Sun, Weiwei Zhang, Mingchen Cai, Jian Dong, Guanxing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural-based multi-task learning (MTL) has been successfully applied to many
recommendation applications. However, these MTL models (e.g., MMoE, PLE) did
not consider feature interaction during the optimization, which is crucial for
capturing complex high-order features and has been widely used in ranking
models for real-world recommender systems. Moreover, through feature importance
analysis across various tasks in MTL, we have observed an interesting
divergence phenomenon that the same feature can have significantly different
importance across different tasks in MTL. To address these issues, we propose
Deep Multiple Task-specific Feature Interactions Network (DTN) with a novel
model structure design. DTN introduces multiple diversified task-specific
feature interaction methods and task-sensitive network in MTL networks,
enabling the model to learn task-specific diversified feature interaction
representations, which improves the efficiency of joint representation learning
in a general setup. We applied DTN to our company's real-world E-commerce
recommendation dataset, which consisted of over 6.3 billion samples, the
results demonstrated that DTN significantly outperformed state-of-the-art MTL
models. Moreover, during online evaluation of DTN in a large-scale E-commerce
recommender system, we observed a 3.28% in clicks, a 3.10% increase in orders
and a 2.70% increase in GMV (Gross Merchandise Value) compared to the
state-of-the-art MTL models. Finally, extensive offline experiments conducted
on public benchmark datasets demonstrate that DTN can be applied to various
scenarios beyond recommendations, enhancing the performance of ranking models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Microstructures and Accuracy of Graph Recall by Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11821v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11821v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbang Wang, Hejie Cui, Jon Kleinberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs data is crucial for many applications, and much of it exists in the
relations described in textual format. As a result, being able to accurately
recall and encode a graph described in earlier text is a basic yet pivotal
ability that LLMs need to demonstrate if they are to perform reasoning tasks
that involve graph-structured information. Human performance at graph recall
has been studied by cognitive scientists for decades, and has been found to
often exhibit certain structural patterns of bias that align with human
handling of social relationships. To date, however, we know little about how
LLMs behave in analogous graph recall tasks: do their recalled graphs also
exhibit certain biased patterns, and if so, how do they compare with humans and
affect other graph reasoning tasks? In this work, we perform the first
systematical study of graph recall by LLMs, investigating the accuracy and
biased microstructures (local structural patterns) in their recall. We find
that LLMs not only underperform often in graph recall, but also tend to favor
more triangles and alternating 2-paths. Moreover, we find that more advanced
LLMs have a striking dependence on the domain that a real-world graph comes
from -- by yielding the best recall accuracy when the graph is narrated in a
language style consistent with its original domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024; Code available at:
  https://github.com/Abel0828/llm-graph-recall</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recommendation Unlearning via Influence Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02147v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02147v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhang, Zhiyu Hu, Yimeng Bai, Jiancan Wu, Qifan Wang, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation unlearning is an emerging task to serve users for erasing
unusable data (e.g., some historical behaviors) from a well-trained recommender
model. Existing methods process unlearning requests by fully or partially
retraining the model after removing the unusable data. However, these methods
are impractical due to the high computation cost of full retraining and the
highly possible performance damage of partial training. In this light, a
desired recommendation unlearning method should obtain a similar model as full
retraining in a more efficient manner, i.e., achieving complete, efficient and
harmless unlearning.
  In this work, we propose a new Influence Function-based Recommendation
Unlearning (IFRU) framework, which efficiently updates the model without
retraining by estimating the influence of the unusable data on the model via
the influence function. In the light that recent recommender models use
historical data for both the constructions of the optimization loss and the
computational graph (e.g., neighborhood aggregation), IFRU jointly estimates
the direct influence of unusable data on optimization loss and the spillover
influence on the computational graph to pursue complete unlearning.
Furthermore, we propose an importance-based pruning algorithm to reduce the
cost of the influence function. IFRU is harmless and applicable to mainstream
differentiable models. Extensive experiments demonstrate that IFRU achieves
more than 250 times acceleration compared to retraining-based methods with
recommendation performance comparable to full retraining. Codes are avaiable at
https://github.com/baiyimeng/IFRU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TORS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning and Tools for Human-Level Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvis Hsieh, Preston Fu, Jonathan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) trained on web-scale datasets are largely successful
due to their ability to memorize large amounts of training data, even if only
present in a few examples. These capabilities are often desirable in evaluation
on tasks such as question answering but raise questions about whether these
models can exhibit genuine reasoning or succeed only at mimicking patterns from
the training data. This distinction is particularly salient in forecasting
tasks, where the answer is not present in the training data, and the model must
reason to make logical deductions. We present Reasoning and Tools for
Forecasting (RTF), a framework of reasoning-and-acting (ReAct) agents that can
dynamically retrieve updated information and run numerical simulation with
equipped tools. We evaluate our model with questions from competitive
forecasting platforms and demonstrate that our method is competitive with and
can outperform human predictions. This suggests that LMs, with the right tools,
can indeed think and adapt like humans, offering valuable insights for
real-world decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ User-Creator Feature Polarization in Recommender Systems with Dual
  Influence <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Lin, Kun Jin, Andrew Estornell, Xiaoying Zhang, Yiling Chen, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems serve the dual purpose of presenting relevant content to
users and helping content creators reach their target audience. The dual nature
of these systems naturally influences both users and creators: users'
preferences are affected by the items they are recommended, while creators may
be incentivized to alter their content to attract more users. We define a
model, called user-creator feature dynamics, to capture the dual influence of
recommender systems. We prove that a recommender system with dual influence is
guaranteed to polarize, causing diversity loss in the system. We then
investigate, both theoretically and empirically, approaches for mitigating
polarization and promoting diversity in recommender systems. Unexpectedly, we
find that common diversity-promoting approaches do not work in the presence of
dual influence, while relevancy-optimizing methods like top-$k$ truncation can
prevent polarization and improve diversity of the system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Group Proportional Representation in Retrieval <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Oesterling, Claudio Mayrink Verdun, Carol Xuan Long, Alexander Glynn, Lucas Monteiro Paes, Sajani Vithana, Martina Cardone, Flavio P. Calmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image search and retrieval tasks can perpetuate harmful stereotypes, erase
cultural identities, and amplify social disparities. Current approaches to
mitigate these representational harms balance the number of retrieved items
across population groups defined by a small number of (often binary)
attributes. However, most existing methods overlook intersectional groups
determined by combinations of group attributes, such as gender, race, and
ethnicity. We introduce Multi-Group Proportional Representation (MPR), a novel
metric that measures representation across intersectional groups. We develop
practical methods for estimating MPR, provide theoretical guarantees, and
propose optimization algorithms to ensure MPR in retrieval. We demonstrate that
existing methods optimizing for equal and proportional representation metrics
may fail to promote MPR. Crucially, our work shows that optimizing MPR yields
more proportional representation across multiple intersectional groups
specified by a rich function class, often with minimal compromise in retrieval
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 33 figures. Accepted as poster at NeurIPS 2024. Code can be
  found at
  https://github.com/alex-oesterling/multigroup-proportional-representation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Multi-Aspect Queries for Conversational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Abbasiantaeb, Simon Lupart, Mohammad Aliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational information seeking (CIS) systems aim to model the user's
information need within the conversational context and retrieve the relevant
information. One major approach to modeling the conversational context aims to
rewrite the user utterance in the conversation to represent the information
need independently. Recent work has shown the benefit of expanding the
rewritten utterance with relevant terms. In this work, we hypothesize that
breaking down the information of an utterance into multi-aspect rewritten
queries can lead to more effective retrieval performance. This is more evident
in more complex utterances that require gathering evidence from various
information sources, where a single query rewrite or query representation
cannot capture the complexity of the utterance. To test this hypothesis, we
conduct extensive experiments on five widely used CIS datasets where we
leverage LLMs to generate multi-aspect queries to represent the information
need for each utterance in multiple query rewrites. We show that, for most of
the utterances, the same retrieval model would perform better with more than
one rewritten query by 85% in terms of nDCG@3. We further propose a
multi-aspect query generation and retrieval framework, called MQ4CS. Our
extensive experiments show that MQ4CS outperforms the state-of-the-art query
rewriting methods. We make our code and our new dataset of generated
multi-aspect queries publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven
  Question Answering Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuldeep Singh, Simerjot Kaur, Charese Smiley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial decision-making hinges on the analysis of relevant information
embedded in the enormous volume of documents in the financial domain. To
address this challenge, we developed FinQAPT, an end-to-end pipeline that
streamlines the identification of relevant financial reports based on a query,
extracts pertinent context, and leverages Large Language Models (LLMs) to
perform downstream tasks. To evaluate the pipeline, we experimented with
various techniques to optimize the performance of each module using the FinQA
dataset. We introduced a novel clustering-based negative sampling technique to
enhance context extraction and a novel prompting method called Dynamic N-shot
Prompting to boost the numerical question-answering capabilities of LLMs. At
the module level, we achieved state-of-the-art accuracy on FinQA, attaining an
accuracy of 80.6%. However, at the pipeline level, we observed decreased
performance due to challenges in extracting relevant context from financial
reports. We conducted a detailed error analysis of each module and the
end-to-end pipeline, pinpointing specific challenges that must be addressed to
develop a robust solution for handling complex financial tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICAIF 2024, 8 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Gaussian Processes via Relevance Pursuit <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Ament, Elizabeth Santorella, David Eriksson, Ben Letham, Maximilian Balandat, Eytan Bakshy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian processes (GPs) are non-parametric probabilistic regression models
that are popular due to their flexibility, data efficiency, and well-calibrated
uncertainty estimates. However, standard GP models assume homoskedastic
Gaussian noise, while many real-world applications are subject to non-Gaussian
corruptions. Variants of GPs that are more robust to alternative noise models
have been proposed, and entail significant trade-offs between accuracy and
robustness, and between computational requirements and theoretical guarantees.
In this work, we propose and study a GP model that achieves robustness against
sparse outliers by inferring data-point-specific noise levels with a sequential
selection procedure maximizing the log marginal likelihood that we refer to as
relevance pursuit. We show, surprisingly, that the model can be parameterized
such that the associated log marginal likelihood is strongly concave in the
data-point-specific noise variances, a property rarely found in either robust
regression objectives or GP marginal likelihoods. This in turn implies the weak
submodularity of the corresponding subset selection problem, and thereby proves
approximation guarantees for the proposed algorithm. We compare the model's
performance relative to other approaches on diverse regression and Bayesian
optimization tasks, including the challenging but common setting of sparse
corruptions of the labels within or close to the function range.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Geometric States via Geometric Diffusion Bridge <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Luo, Yixian Xu, Di He, Shuxin Zheng, Tie-Yan Liu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate prediction of geometric state evolution in complex systems is
critical for advancing scientific domains such as quantum chemistry and
material modeling. Traditional experimental and computational methods face
challenges in terms of environmental constraints and computational demands,
while current deep learning approaches still fall short in terms of precision
and generality. In this work, we introduce the Geometric Diffusion Bridge
(GDB), a novel generative modeling framework that accurately bridges initial
and target geometric states. GDB leverages a probabilistic approach to evolve
geometric state distributions, employing an equivariant diffusion bridge
derived by a modified version of Doob's $h$-transform for connecting geometric
states. This tailored diffusion process is anchored by initial and target
geometric states as fixed endpoints and governed by equivariant transition
kernels. Moreover, trajectory data can be seamlessly leveraged in our GDB
framework by using a chain of equivariant diffusion bridges, providing a more
detailed and accurate characterization of evolution dynamics. Theoretically, we
conduct a thorough examination to confirm our framework's ability to preserve
joint distributions of geometric states and capability to completely model the
underlying dynamics inducing trajectory distributions with negligible error.
Experimental evaluations across various real-world scenarios show that GDB
surpasses existing state-of-the-art approaches, opening up a new pathway for
accurately bridging geometric states and tackling crucial scientific challenges
with improved accuracy and applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 5 tables; NeurIPS 2024 Camera Ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Embodied Reinforcement Learning Agents: Informativeness and
  Diversity of Language Use <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Xi, Yinong He, Jianing Yang, Yinpei Dai, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, it is desirable for embodied agents to have the
ability to leverage human language to gain explicit or implicit knowledge for
learning tasks. Despite recent progress, most previous approaches adopt simple
low-level instructions as language inputs, which may not reflect natural human
communication. It's not clear how to incorporate rich language use to
facilitate task learning. To address this question, this paper studies
different types of language inputs in facilitating reinforcement learning (RL)
embodied agents. More specifically, we examine how different levels of language
informativeness (i.e., feedback on past behaviors and future guidance) and
diversity (i.e., variation of language expressions) impact agent learning and
inference. Our empirical results based on four RL benchmarks demonstrate that
agents trained with diverse and informative language feedback can achieve
enhanced generalization and fast adaptation to new tasks. These findings
highlight the pivotal role of language use in teaching embodied agents new
tasks in an open world. Project website:
https://github.com/sled-group/Teachable_RL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main. Project website:
  https://github.com/sled-group/Teachable_RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaAdam: Improving Adam optimizer using connection aware methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remi Genet, Hugo Inzirillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new method inspired by Adam that enhances convergence speed
and achieves better loss function minima. Traditional optimizers, including
Adam, apply uniform or globally adjusted learning rates across neural networks
without considering their architectural specifics. This architecture-agnostic
approach is deeply embedded in most deep learning frameworks, where optimizers
are implemented as standalone modules without direct access to the network's
structural information. For instance, in popular frameworks like Keras or
PyTorch, optimizers operate solely on gradients and parameters, without
knowledge of layer connectivity or network topology. Our algorithm, CaAdam,
explores this overlooked area by introducing connection-aware optimization
through carefully designed proxies of architectural information. We propose
multiple scaling methodologies that dynamically adjust learning rates based on
easily accessible structural properties such as layer depth, connection counts,
and gradient distributions. This approach enables more granular optimization
while working within the constraints of current deep learning frameworks.
Empirical evaluations on standard datasets (e.g., CIFAR-10, Fashion MNIST) show
that our method consistently achieves faster convergence and higher accuracy
compared to standard Adam optimizer, demonstrating the potential benefits of
incorporating architectural awareness in optimization strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARQ: A Mixed-Precision Quantization Framework for Accurate and
  Certifiably Robust DNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Yang, Shubham Ugare, Yifan Zhao, Gagandeep Singh, Sasa Misailovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed precision quantization has become an important technique for enabling
the execution of deep neural networks (DNNs) on limited resource computing
platforms. Traditional quantization methods have primarily concentrated on
maintaining neural network accuracy, either ignoring the impact of quantization
on the robustness of the network, or using only empirical techniques for
improving robustness. In contrast, techniques for robustness certification,
which can provide strong guarantees about the robustness of DNNs have not been
used during quantization due to their high computation cost.
  This paper introduces ARQ, an innovative mixed-precision quantization method
that not only preserves the clean accuracy of the smoothed classifiers but also
maintains their certified robustness. ARQ uses reinforcement learning to find
accurate and robust DNN quantization, while efficiently leveraging randomized
smoothing, a popular class of statistical DNN verification algorithms, to guide
the search process.
  We compare ARQ with multiple state-of-the-art quantization techniques on
several DNN architectures commonly used in quantization studies: ResNet-20 on
CIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. We demonstrate
that ARQ consistently performs better than these baselines across all the
benchmarks and the input perturbation levels. In many cases, the performance of
ARQ quantized networks can reach that of the original DNN with floating-point
weights, but with only 1.5% instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TabM: Advancing Tabular Deep Learning with Parameter-Efficient
  Ensembling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yury Gorishniy, Akim Kotelnikov, Artem Babenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning architectures for supervised learning on tabular data range
from simple multilayer perceptrons (MLP) to sophisticated Transformers and
retrieval-augmented methods. This study highlights a major, yet so far
overlooked opportunity for substantially improving tabular MLPs: namely,
parameter-efficient ensembling -- a paradigm for implementing an ensemble of
models as one model producing multiple predictions. We start by developing TabM
-- a simple model based on MLP and our variations of BatchEnsemble (an existing
technique). Then, we perform a large-scale evaluation of tabular DL
architectures on public benchmarks in terms of both task performance and
efficiency, which renders the landscape of tabular DL in a new light.
Generally, we show that MLPs, including TabM, form a line of stronger and more
practical models compared to attention- and retrieval-based architectures. In
particular, we find that TabM demonstrates the best performance among tabular
DL models. Lastly, we conduct an empirical analysis on the ensemble-like nature
of TabM. For example, we observe that the multiple predictions of TabM are weak
individually, but powerful collectively. Overall, our work brings an impactful
technique to tabular DL, analyses its behaviour, and advances the
performance-efficiency trade-off with TabM -- a simple and powerful baseline
for researchers and practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/yandex-research/tabm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Optimization in Deep Learning with Central Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy M. Cohen, Alex Damian, Ameet Talwalkar, Zico Kolter, Jason D. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization in deep learning remains poorly understood, even in the simple
setting of deterministic (i.e. full-batch) training. A key difficulty is that
much of an optimizer's behavior is implicitly determined by complex oscillatory
dynamics, referred to as the "edge of stability." The main contribution of this
paper is to show that an optimizer's implicit behavior can be explicitly
captured by a "central flow:" a differential equation which models the
time-averaged optimization trajectory. We show that these flows can empirically
predict long-term optimization trajectories of generic neural networks with a
high degree of numerical accuracy. By interpreting these flows, we reveal for
the first time 1) the precise sense in which RMSProp adapts to the local loss
landscape, and 2) an "acceleration via regularization" mechanism, wherein
adaptive optimizers implicitly navigate towards low-curvature regions in which
they can take larger steps. This mechanism is key to the efficacy of these
adaptive optimizers. Overall, we believe that central flows constitute a
promising tool for reasoning about optimization in deep learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>first two authors contributed equally; author order determined by
  coin flip</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfCodeAlign: Self-Alignment for Code Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning is a supervised fine-tuning approach that significantly
improves the ability of large language models (LLMs) to follow human
instructions. We propose SelfCodeAlign, the first fully transparent and
permissive pipeline for self-aligning code LLMs without extensive human
annotations or distillation. SelfCodeAlign employs the same base model for
inference throughout the data generation process. It first extracts diverse
coding concepts from high-quality seed snippets to generate new tasks. It then
samples multiple responses per task, pairs each with test cases, and validates
them in a sandbox environment. Finally, passing examples are selected for
instruction tuning. In our primary experiments, we use SelfCodeAlign with
CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.
Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on
HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.
Across all benchmarks, this finetuned model consistently outperforms the
original version trained with OctoPack, the previous state-of-the-art method
for instruction tuning without human annotations or distillation. Additionally,
we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B
to 33B, and that the base models can benefit more from alignment with their own
data distribution. We further validate each component's effectiveness in our
pipeline, showing that SelfCodeAlign outperforms both direct distillation from
GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and
Evol-Instruct. SelfCodeAlign has also led to the creation of
StarCoder2-Instruct, the first fully transparent, permissively licensed, and
self-aligned code LLM that achieves state-of-the-art coding performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DexMimicGen: Automated Data Generation for Bimanual Dexterous
  Manipulation via Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from human demonstrations is an effective means to teach
robots manipulation skills. But data acquisition is a major bottleneck in
applying this paradigm more broadly, due to the amount of cost and human effort
involved. There has been significant interest in imitation learning for
bimanual dexterous robots, like humanoids. Unfortunately, data collection is
even more challenging here due to the challenges of simultaneously controlling
multiple arms and multi-fingered hands. Automated data generation in simulation
is a compelling, scalable alternative to fuel this need for data. To this end,
we introduce DexMimicGen, a large-scale automated data generation system that
synthesizes trajectories from a handful of human demonstrations for humanoid
robots with dexterous hands. We present a collection of simulation environments
in the setting of bimanual dexterous manipulation, spanning a range of
manipulation behaviors and different requirements for coordination among the
two arms. We generate 21K demos across these tasks from just 60 source human
demos and study the effect of several data generation and policy learning
decisions on agent performance. Finally, we present a real-to-sim-to-real
pipeline and deploy it on a real-world humanoid can sorting task. Videos and
more are at https://dexmimicgen.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://dexmimicgen.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Crosscoders for Mechanistic Analysis of Symmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liv Gorton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce group crosscoders, an extension of crosscoders that
systematically discover and analyse symmetrical features in neural networks.
While neural networks often develop equivariant representations without
explicit architectural constraints, understanding these emergent symmetries has
traditionally relied on manual analysis. Group crosscoders automate this
process by performing dictionary learning across transformed versions of inputs
under a symmetry group. Applied to InceptionV1's mixed3b layer using the
dihedral group $\mathrm{D}_{32}$, our method reveals several key insights:
First, it naturally clusters features into interpretable families that
correspond to previously hypothesised feature types, providing more precise
separation than standard sparse autoencoders. Second, our transform block
analysis enables the automatic characterisation of feature symmetries,
revealing how different geometric features (such as curves versus lines)
exhibit distinct patterns of invariance and equivariance. These results
demonstrate that group crosscoders can provide systematic insights into how
neural networks represent symmetry, offering a promising new tool for
mechanistic interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal
  Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiayan Ji, Anton Xue, Eric Wong, Oleg Sokolsky, Insup Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is widely used for identifying critical errors and
suspicious behaviors, but current methods lack interpretability. We leverage
common properties of existing methods and recent advances in generative models
to introduce counterfactual explanations for anomaly detection. Given an input,
we generate its counterfactual as a diffusion-based repair that shows what a
non-anomalous version should have looked like. A key advantage of this approach
is that it enables a domain-independent formal specification of explainability
desiderata, offering a unified framework for generating and evaluating
explanations. We demonstrate the effectiveness of our anomaly explainability
framework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI)
anomaly datasets. The code used for the experiments is accessible at:
https://github.com/xjiae/arpro.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng-Jui Chang, Hongyu Gong, Changhan Wang, James Glass, Yu-An Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken language models (SLMs) have gained increasing attention with
advancements in text-based, decoder-only language models. SLMs process text and
speech, enabling simultaneous speech understanding and generation. This paper
presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to
improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin
extracts speaker-invariant tokens rich in phonetic information and resilient to
input variations, enhancing zero-shot SLM tasks and speech resynthesis. We
propose a chunk-wise approach to enable streamable DC-Spin without retraining
and degradation. Comparisons of tokenization methods (self-supervised and
neural audio codecs), model scalability, and downstream task proxies show that
tokens easily modeled by an n-gram LM or aligned with phonemes offer strong
performance, providing insights for designing speech tokenizers for SLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Importance of Being Scalable: Improving the Speed and Accuracy of
  Neural Network Interatomic Potentials Across Chemical Domains <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Qu, Aditi S. Krishnapriyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling has been critical in improving model performance and generalization
in machine learning. It involves how a model's performance changes with
increases in model size or input data, as well as how efficiently computational
resources are utilized to support this growth. Despite successes in other
areas, the study of scaling in Neural Network Interatomic Potentials (NNIPs)
remains limited. NNIPs act as surrogate models for ab initio quantum mechanical
calculations. The dominant paradigm here is to incorporate many physical domain
constraints into the model, such as rotational equivariance. We contend that
these complex constraints inhibit the scaling ability of NNIPs, and are likely
to lead to performance plateaus in the long run. In this work, we take an
alternative approach and start by systematically studying NNIP scaling
strategies. Our findings indicate that scaling the model through attention
mechanisms is efficient and improves model expressivity. These insights
motivate us to develop an NNIP architecture designed for scalability: the
Efficiently Scaled Attention Interatomic Potential (EScAIP). EScAIP leverages a
multi-head self-attention formulation within graph neural networks, applying
attention at the neighbor-level representations. Implemented with
highly-optimized attention GPU kernels, EScAIP achieves substantial gains in
efficiency--at least 10x faster inference, 5x less memory usage--compared to
existing NNIPs. EScAIP also achieves state-of-the-art performance on a wide
range of datasets including catalysts (OC20 and OC22), molecules (SPICE), and
materials (MPTrj). We emphasize that our approach should be thought of as a
philosophy rather than a specific model, representing a proof-of-concept for
developing general-purpose NNIPs that achieve better expressivity through
scaling, and continue to scale efficiently with increased computational
resources and training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approaches to human activity recognition via passive radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Bresciani, Federico Cerutti, Marco Cominelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The thesis explores novel methods for Human Activity Recognition (HAR) using
passive radar with a focus on non-intrusive Wi-Fi Channel State Information
(CSI) data. Traditional HAR approaches often use invasive sensors like cameras
or wearables, raising privacy issues. This study leverages the non-intrusive
nature of CSI, using Spiking Neural Networks (SNN) to interpret signal
variations caused by human movements. These networks, integrated with symbolic
reasoning frameworks such as DeepProbLog, enhance the adaptability and
interpretability of HAR systems. SNNs offer reduced power consumption, ideal
for privacy-sensitive applications. Experimental results demonstrate SNN-based
neurosymbolic models achieve high accuracy making them a promising alternative
for HAR across various domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $π_0$: A Vision-Language-Action Flow Model for General Robot Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot learning holds tremendous promise to unlock the full potential of
flexible, general, and dexterous robot systems, as well as to address some of
the deepest questions in artificial intelligence. However, bringing robot
learning to the level of generality required for effective real-world systems
faces major obstacles in terms of data, generalization, and robustness. In this
paper, we discuss how generalist robot policies (i.e., robot foundation models)
can address these challenges, and how we can design effective generalist robot
policies for complex and highly dexterous tasks. We propose a novel flow
matching architecture built on top of a pre-trained vision-language model (VLM)
to inherit Internet-scale semantic knowledge. We then discuss how this model
can be trained on a large and diverse dataset from multiple dexterous robot
platforms, including single-arm robots, dual-arm robots, and mobile
manipulators. We evaluate our model in terms of its ability to perform tasks in
zero shot after pre-training, follow language instructions from people and from
a high-level VLM policy, and its ability to acquire new skills via fine-tuning.
Our results cover a wide variety of tasks, such as laundry folding, table
cleaning, and assembling boxes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See project website for videos:
  https://physicalintelligence.company/blog/pi0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformalized Prediction of Post-Fault Voltage Trajectories Using
  <span class="highlight-title">Pre-train</span>ed and Finetuned Attention-Driven Neural Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Mollaali, Gabriel Zufferey, Gonzalo Constante-Flores, Christian Moya, Can Li, Guang Lin, Meng Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new data-driven methodology for predicting intervals of
post-fault voltage trajectories in power systems. We begin by introducing the
Quantile Attention-Fourier Deep Operator Network (QAF-DeepONet), designed to
capture the complex dynamics of voltage trajectories and reliably estimate
quantiles of the target trajectory without any distributional assumptions. The
proposed operator regression model maps the observed portion of the voltage
trajectory to its unobserved post-fault trajectory. Our methodology employs a
pre-training and fine-tuning process to address the challenge of limited data
availability. To ensure data privacy in learning the pre-trained model, we use
merging via federated learning with data from neighboring buses, enabling the
model to learn the underlying voltage dynamics from such buses without directly
sharing their data. After pre-training, we fine-tune the model with data from
the target bus, allowing it to adapt to unique dynamics and operating
conditions. Finally, we integrate conformal prediction into the fine-tuned
model to ensure coverage guarantees for the predicted intervals. We evaluated
the performance of the proposed methodology using the New England 39-bus test
system considering detailed models of voltage and frequency controllers. Two
metrics, Prediction Interval Coverage Probability (PICP) and Prediction
Interval Normalized Average Width (PINAW), are used to numerically assess the
model's performance in predicting intervals. The results show that the proposed
approach offers practical and reliable uncertainty quantification in predicting
the interval of post-fault voltage trajectories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Associative Memory Through the Lens of Random Features <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Hoover, Duen Horng Chau, Hendrik Strobelt, Parikshit Ram, Dmitry Krotov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense Associative Memories are high storage capacity variants of the Hopfield
networks that are capable of storing a large number of memory patterns in the
weights of the network of a given size. Their common formulations typically
require storing each pattern in a separate set of synaptic weights, which leads
to the increase of the number of synaptic weights when new patterns are
introduced. In this work we propose an alternative formulation of this class of
models using random features, commonly used in kernel methods. In this
formulation the number of network's parameters remains fixed. At the same time,
new memories can be added to the network by modifying existing weights. We show
that this novel network closely approximates the energy function and dynamics
of conventional Dense Associative Memories and shares their desirable
computational properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal prediction of circular data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paulo C. Marques F., Rinaldo Artes, Helton Graziadei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Split conformal prediction techniques are applied to regression problems with
circular responses by introducing a suitable conformity score, leading to
prediction sets with adaptive arc length and finite-sample coverage guarantees
for any circular predictive model under exchangeable data. Leveraging the high
performance of existing predictive models designed for linear responses, we
analyze a general projection procedure that converts any linear response
regression model into one suitable for circular responses. When random forests
serve as basis models in this projection procedure, we harness the out-of-bag
dynamics to eliminate the necessity for a separate calibration sample in the
construction of prediction sets. For synthetic and real datasets the resulting
projected random forests model produces more efficient out-of-bag conformal
prediction sets, with shorter median arc length, when compared to the split
conformal prediction sets generated by two existing alternative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-learning for Quantile MDPs: A Decomposition, Performance, and
  Convergence Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Lin Hau, Erick Delage, Esther Derman, Mohammad Ghavamzadeh, Marek Petrik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Markov decision processes (MDPs), quantile risk measures such as
Value-at-Risk are a standard metric for modeling RL agents' preferences for
certain outcomes. This paper proposes a new Q-learning algorithm for quantile
optimization in MDPs with strong convergence and performance guarantees. The
algorithm leverages a new, simple dynamic program (DP) decomposition for
quantile MDPs. Compared with prior work, our DP decomposition requires neither
known transition probabilities nor solving complex saddle point equations and
serves as a suitable foundation for other model-free RL algorithms. Our
numerical results in tabular domains show that our Q-learning algorithm
converges to its DP variant and outperforms earlier algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Repository-Level Compositional Code Translation and Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Reza Ibrahimzada, Kaiyao Ke, Mrigank Pawagi, Muhammad Salman Abid, Rangeet Pan, Saurabh Sinha, Reyhaneh Jabbarvand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code translation transforms programs from one programming language (PL) to
another. Several rule-based transpilers have been designed to automate code
translation between different pairs of PLs. However, the rules can become
obsolete as the PLs evolve and cannot generalize to other PLs. Recent studies
have explored the automation of code translation using Large Language Models
(LLMs). One key observation is that such techniques may work well for crafted
benchmarks but fail to generalize to the scale and complexity of real-world
projects with dependencies, custom types, PL-specific features, etc.
  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level
code translation. AlphaTrans translates both source and test code, and employs
multiple levels of validation to ensure the translation preserves the
functionality of the source program. To break down the problem for LLMs,
AlphaTrans leverages program analysis to decompose the program into fragments
and translates them in the reverse call order. We leveraged AlphaTrans to
translate ten real-world open-source projects consisting of <836, 8575, 2719>
classes, methods, and tests. AlphaTrans translated the entire repository of
these projects consisting of 6899 source code fragments. 99.1% of the
translated code fragments are syntactically correct, and AlphaTrans validates
the translations' runtime behavior and functional correctness for 25.8%. On
average, the integrated translation and validation take 36 hours to translate a
project, showing its scalability in practice. For the syntactically or
semantically incorrect translations, AlphaTrans generates a report including
existing translation, stack trace, test errors, or assertion failures. We
provided these artifacts to two developers to fix the translation bugs in four
projects. They were able to fix the issues in 20.1 hours on average and achieve
all passing tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIDOVECL: AI-generated <span class="highlight-title">Dataset</span> of Outpainted Vehicles for Eye-level
  Classification and Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Kazemi, Qurat ul ain Fatima, Volodymyr Kindratenko, Christopher Tessum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image labeling is a critical bottleneck in the development of computer vision
technologies, often constraining the potential of machine learning models due
to the time-intensive nature of manual annotations. This work introduces a
novel approach that leverages outpainting to address the problem of annotated
data scarcity by generating artificial contexts and annotations, significantly
reducing manual labeling efforts. We apply this technique to a particularly
acute challenge in autonomous driving, urban planning, and environmental
monitoring: the lack of diverse, eye-level vehicle images in desired classes.
Our dataset comprises AI-generated vehicle images obtained by detecting and
cropping vehicles from manually selected seed images, which are then outpainted
onto larger canvases to simulate varied real-world conditions. The outpainted
images include detailed annotations, providing high-quality ground truth data.
Advanced outpainting techniques and image quality assessments ensure visual
fidelity and contextual relevance. Augmentation with outpainted vehicles
improves overall performance metrics by up to 8\% and enhances prediction of
underrepresented classes by up to 20\%. This approach, exemplifying outpainting
as a self-annotating paradigm, presents a solution that enhances dataset
versatility across multiple domains of machine learning. The code and links to
datasets used in this study are available for further research and replication
at https://github.com/amir-kazemi/aidovecl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning Gradients as Vitamin for Online Finetuning
  Decision <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yan, Alexander G. Schwing, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision Transformers have recently emerged as a new and compelling paradigm
for offline Reinforcement Learning (RL), completing a trajectory in an
autoregressive way. While improvements have been made to overcome initial
shortcomings, online finetuning of decision transformers has been surprisingly
under-explored. The widely adopted state-of-the-art Online Decision Transformer
(ODT) still struggles when pretrained with low-reward offline data. In this
paper, we theoretically analyze the online-finetuning of the decision
transformer, showing that the commonly used Return-To-Go (RTG) that's far from
the expected return hampers the online fine-tuning process. This problem,
however, is well-addressed by the value function and advantage of standard RL
algorithms. As suggested by our analysis, in our experiments, we hence find
that simply adding TD3 gradients to the finetuning process of ODT effectively
improves the online finetuning performance of ODT, especially if ODT is
pretrained with low-reward offline data. These findings provide new directions
to further improve decision transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as NeurIPS 2024 spotlight. 33 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Sampling Strategies for Spectral Model Sharding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Korzhenkov, Christos Louizos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of heterogeneous clients in federated learning has recently drawn
a lot of attention. Spectral model sharding, i.e., partitioning the model
parameters into low-rank matrices based on the singular value decomposition,
has been one of the proposed solutions for more efficient on-device training in
such settings. In this work, we present two sampling strategies for such
sharding, obtained as solutions to specific optimization problems. The first
produces unbiased estimators of the original weights, while the second aims to
minimize the squared approximation error. We discuss how both of these
estimators can be incorporated in the federated learning loop and practical
considerations that arise during local training. Empirically, we demonstrate
that both of these methods can lead to improved performance on various commonly
used datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matchmaker: Self-Improving Large Language Model Programs for Schema
  Matching <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nabeel Seedat, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema matching -- the task of finding matches between attributes across
disparate data sources with different tables and hierarchies -- is critical for
creating interoperable machine learning (ML)-ready data. Addressing this
fundamental data-centric problem has wide implications, especially in domains
like healthcare, finance and e-commerce -- but also has the potential to
benefit ML models more generally, by increasing the data available for ML model
training. However, schema matching is a challenging ML task due to
structural/hierarchical and semantic heterogeneity between different schemas.
Previous ML approaches to automate schema matching have either required
significant labeled data for model training, which is often unrealistic or
suffer from poor zero-shot performance. To this end, we propose Matchmaker - a
compositional language model program for schema matching, comprised of
candidate generation, refinement and confidence scoring. Matchmaker also
self-improves in a zero-shot manner without the need for labeled demonstrations
via a novel optimization approach, which constructs synthetic in-context
demonstrations to guide the language model's reasoning process. Empirically, we
demonstrate on real-world medical schema matching benchmarks that Matchmaker
outperforms previous ML-based approaches, highlighting its potential to
accelerate data integration and interoperability of ML-ready data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024, GenAI for Health Workshop and Table
  Representation Learning Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering to Minimize Cluster-Aware Norm Objectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin G. Herold, Evangelos Kipouridis, Joachim Spoerhase
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate the study of the following general clustering problem. We seek to
partition a given set $P$ of data points into $k$ clusters by finding a set $X$
of $k$ centers and assigning each data point to one of the centers. The cost of
a cluster, represented by a center $x\in X$, is a monotone, symmetric norm $f$
(inner norm) of the vector of distances of points assigned to $x$. The goal is
to minimize a norm $g$ (outer norm) of the vector of cluster costs. This
problem, which we call $(f,g)$-Clustering, generalizes many fundamental
clustering problems such as $k$-Center, $k$-Median , Min-Sum of Radii, and
Min-Load $k$-Clustering . A recent line of research (Chakrabarty, Swamy
[STOC'19]) studies norm objectives that are oblivious to the cluster structure
such as $k$-Median and $k$-Center. In contrast, our problem models
cluster-aware objectives including Min-Sum of Radii and Min-Load
$k$-Clustering.
  Our main results are as follows. First, we design a constant-factor
approximation algorithm for $(\textsf{top}_\ell,\mathcal{L}_1)$-Clustering
where the inner norm ($\textsf{top}_\ell$) sums over the $\ell$ largest
distances. Second, we design a constant-factor approximation\ for
$(\mathcal{L}_\infty,\textsf{Ord})$-Clustering where the outer norm is a convex
combination of $\textsf{top}_\ell$ norms (ordered weighted norm).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at SODA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmark Data Repositories for Better Benchmarking <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel Longjohn, Markelle Kelly, Sameer Singh, Padhraic Smyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machine learning research, it is common to evaluate algorithms via their
performance on standard benchmark datasets. While a growing body of work
establishes guidelines for -- and levies criticisms at -- data and benchmarking
practices in machine learning, comparatively less attention has been paid to
the data repositories where these datasets are stored, documented, and shared.
In this paper, we analyze the landscape of these $\textit{benchmark data
repositories}$ and the role they can play in improving benchmarking. This role
includes addressing issues with both datasets themselves (e.g.,
representational harms, construct validity) and the manner in which evaluation
is carried out using such datasets (e.g., overemphasis on a few datasets and
metrics, lack of reproducibility). To this end, we identify and discuss a set
of considerations surrounding the design and use of benchmark data
repositories, with a focus on improving benchmarking practices in machine
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS Datasets and Benchmarks 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Safeguards for Safe and Model-Agnostic Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nabil Omi, Hosein Hasanbeig, Hiteshi Sharma, Sriram K. Rajamani, Siddhartha Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a formal, model-agnostic meta-learning framework for
safe reinforcement learning. Our framework is inspired by how parents safeguard
their children across a progression of increasingly riskier tasks, imparting a
sense of safety that is carried over from task to task. We model this as a
meta-learning process where each task is synchronized with a safeguard that
monitors safety and provides a reward signal to the agent. The safeguard is
implemented as a finite-state machine based on a safety specification; the
reward signal is formally shaped around this specification. The safety
specification and its corresponding safeguard can be arbitrarily complex and
non-Markovian, which adds flexibility to the training process and
explainability to the learned policy. The design of the safeguard is manual but
it is high-level and model-agnostic, which gives rise to an end-to-end safe
learning approach with wide applicability, from pixel-level game control to
language model fine-tuning. Starting from a given set of safety specifications
(tasks), we train a model such that it can adapt to new specifications using
only a small number of training samples. This is made possible by our method
for efficiently transferring safety bias between tasks, which effectively
minimizes the number of safety violations. We evaluate our framework in a
Minecraft-inspired Gridworld, a VizDoom game environment, and an LLM
fine-tuning application. Agents trained with our approach achieve near-minimal
safety violations, while baselines are shown to underperform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binghao Huang, Yixuan Wang, Xinyi Yang, Yiyue Luo, Yunzhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile and visual perception are both crucial for humans to perform
fine-grained interactions with their environment. Developing similar
multi-modal sensing capabilities for robots can significantly enhance and
expand their manipulation skills. This paper introduces \textbf{3D-ViTac}, a
multi-modal sensing and learning system designed for dexterous bimanual
manipulation. Our system features tactile sensors equipped with dense sensing
units, each covering an area of 3$mm^2$. These sensors are low-cost and
flexible, providing detailed and extensive coverage of physical contacts,
effectively complementing visual information. To integrate tactile and visual
data, we fuse them into a unified 3D representation space that preserves their
3D structures and spatial relationships. The multi-modal representation can
then be coupled with diffusion policies for imitation learning. Through
concrete hardware experiments, we demonstrate that even low-cost robots can
perform precise manipulations and significantly outperform vision-only
policies, particularly in safe interactions with fragile items and executing
long-horizon tasks involving in-hand manipulation. Our project page is
available at \url{https://binghao-huang.github.io/3D-ViTac/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Conference on Robot Learning (CoRL) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystifying Linear MDPs and Novel Dynamics Aggregation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joongkyu Lee, Min-hwan Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we prove that, in linear MDPs, the feature dimension $d$ is
lower bounded by $S/U$ in order to aptly represent transition probabilities,
where $S$ is the size of the state space and $U$ is the maximum size of
directly reachable states. Hence, $d$ can still scale with $S$ depending on the
direct reachability of the environment. To address this limitation of linear
MDPs, we propose a novel structural aggregation framework based on dynamics,
named as the "dynamics aggregation". For this newly proposed framework, we
design a provably efficient hierarchical reinforcement learning algorithm in
linear function approximation that leverages aggregated sub-structures. Our
proposed algorithm exhibits statistical efficiency, achieving a regret of $
\tilde{O} ( d_{\psi}^{3/2} H^{3/2}\sqrt{ N T} )$, where $d_{\psi}$ represents
the feature dimension of aggregated subMDPs and $N$ signifies the number of
aggregated subMDPs. We establish that the condition $d_{\psi}^3 N \ll d^{3}$ is
readily met in most real-world environments with hierarchical structures,
enabling a substantial improvement in the regret bound compared to LSVI-UCB,
which enjoys a regret of $ \tilde{O} (d^{3/2} H^{3/2} \sqrt{ T})$. To the best
of our knowledge, this work presents the first HRL algorithm with linear
function approximation that offers provable guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Fine-Tuning for Time-Series Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhimanyu Das, Matthew Faw, Rajat Sen, Yichen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the recent success of time-series foundation models for
zero-shot forecasting, we present a methodology for $\textit{in-context
fine-tuning}$ of a time-series foundation model. In particular, we design a
pretrained foundation model that can be prompted (at inference time) with
multiple time-series examples, in order to forecast a target time-series into
the future. Our foundation model is specifically trained to utilize examples
from multiple related time-series in its context window (in addition to the
history of the target time-series) to help it adapt to the specific
distribution of the target domain at inference time. We show that such a
foundation model that uses in-context examples at inference time can obtain
much better performance on popular forecasting benchmarks compared to
supervised deep learning methods, statistical models, as well as other
time-series foundation models. Interestingly, our in-context fine-tuning
approach even rivals the performance of a foundation model that is explicitly
fine-tuned on the target domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects
  Models <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlin Lai, Daniel Sheldon, Justin Domke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and
often requires advanced sampling techniques like Markov chain Monte Carlo
(MCMC). A common approach is to write the model in a probabilistic programming
language and then sample via Hamiltonian Monte Carlo (HMC). However, there are
many ways a user can transform a model that make inference more or less
efficient. In particular, marginalizing some variables can greatly improve
inference but is difficult for users to do manually. We develop an algorithm to
easily marginalize random effects in LMMs. A naive approach introduces cubic
time operations within an inference algorithm like HMC, but we reduce the
running time to linear using fast linear algebra techniques. We show that
marginalization is always beneficial when applicable and highlight improvements
in various models, especially ones from cognitive sciences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Linearity: the Key for No-regret Reinforcement Learning in
  Continuous MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Maran, Alberto Maria Metelli, Matteo Papini, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving the no-regret property for Reinforcement Learning (RL) problems in
continuous state and action-space environments is one of the major open
problems in the field. Existing solutions either work under very specific
assumptions or achieve bounds that are vacuous in some regimes. Furthermore,
many structural assumptions are known to suffer from a provably unavoidable
exponential dependence on the time horizon $H$ in the regret, which makes any
possible solution unfeasible in practice. In this paper, we identify local
linearity as the feature that makes Markov Decision Processes (MDPs) both
learnable (sublinear regret) and feasible (regret that is polynomial in $H$).
We define a novel MDP representation class, namely Locally Linearizable MDPs,
generalizing other representation classes like Linear MDPs and MDPS with low
inherent Belmman error. Then, i) we introduce Cinderella, a no-regret algorithm
for this general representation class, and ii) we show that all known learnable
and feasible MDP families are representable in this class. We first show that
all known feasible MDPs belong to a family that we call Mildly Smooth MDPs.
Then, we show how any mildly smooth MDP can be represented as a Locally
Linearizable MDP by an appropriate choice of representation. This way,
Cinderella is shown to achieve state-of-the-art regret bounds for all
previously known (and some new) continuous MDPs for which RL is learnable and
feasible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamical similarity analysis uniquely captures how computations develop
  in RNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Guilhot, Jascha Achterberg, Michał Wójcik, Rui Ponte Costa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for analyzing representations in neural systems are increasingly
popular tools in neuroscience and mechanistic interpretability. Measures
comparing neural activations across conditions, architectures, and species give
scalable ways to understand information transformation within different neural
networks. However, recent findings show that some metrics respond to spurious
signals, leading to misleading results. Establishing benchmark test cases is
thus essential for identifying the most reliable metric and potential
improvements. We propose that compositional learning in recurrent neural
networks (RNNs) can provide a test case for dynamical representation alignment
metrics. Implementing this case allows us to evaluate if metrics can identify
representations that develop throughout learning and determine if
representations identified by metrics reflect the network's actual
computations. Building both attractor and RNN based test cases, we show that
the recently proposed Dynamical Similarity Analysis (DSA) is more noise robust
and reliably identifies behaviorally relevant representations compared to prior
metrics (Procrustes, CKA). We also demonstrate how such test cases can extend
beyond metric evaluation to study new architectures. Specifically, testing DSA
in modern (Mamba) state space models suggests that these models, unlike RNNs,
may not require changes in recurrent dynamics due to their expressive hidden
states. Overall, we develop test cases that showcase how DSA's enhanced ability
to detect dynamical motifs makes it highly effective for identifying ongoing
computations in RNNs and revealing how networks learn tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Generalizability of Diffusion Models Requires Rethinking
  the Hidden Gaussian Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Yixiang Dai, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the generalizability of diffusion models by looking
into the hidden properties of the learned score functions, which are
essentially a series of deep denoisers trained on various noise levels. We
observe that as diffusion models transition from memorization to
generalization, their corresponding nonlinear diffusion denoisers exhibit
increasing linearity. This discovery leads us to investigate the linear
counterparts of the nonlinear diffusion models, which are a series of linear
models trained to match the function mappings of the nonlinear diffusion
denoisers. Surprisingly, these linear denoisers are approximately the optimal
denoisers for a multivariate Gaussian distribution characterized by the
empirical mean and covariance of the training dataset. This finding implies
that diffusion models have the inductive bias towards capturing and utilizing
the Gaussian structure (covariance information) of the training dataset for
data generation. We empirically demonstrate that this inductive bias is a
unique property of diffusion models in the generalization regime, which becomes
increasingly evident when the model's capacity is relatively small compared to
the training dataset size. In the case that the model is highly
overparameterized, this inductive bias emerges during the initial training
phases before the model fully memorizes its training data. Our study provides
crucial insights into understanding the notable strong generalization
phenomenon recently observed in real-world diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying General Mechanism Shifts in Linear Causal Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Chen, Kevin Bello, Francesco Locatello, Bryon Aragam, Pradeep Ravikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the linear causal representation learning setting where we
observe a linear mixing of $d$ unknown latent factors, which follow a linear
structural causal model. Recent work has shown that it is possible to recover
the latent factors as well as the underlying structural causal model over them,
up to permutation and scaling, provided that we have at least $d$ environments,
each of which corresponds to perfect interventions on a single latent node
(factor). After this powerful result, a key open problem faced by the community
has been to relax these conditions: allow for coarser than perfect single-node
interventions, and allow for fewer than $d$ of them, since the number of latent
factors $d$ could be very large. In this work, we consider precisely such a
setting, where we allow a smaller than $d$ number of environments, and also
allow for very coarse interventions that can very coarsely \textit{change the
entire causal graph over the latent factors}. On the flip side, we relax what
we wish to extract to simply the \textit{list of nodes that have shifted
between one or more environments}. We provide a surprising identifiability
result that it is indeed possible, under some very mild standard assumptions,
to identify the set of shifted nodes. Our identifiability proof moreover is a
constructive one: we explicitly provide necessary and sufficient conditions for
a node to be a shifted node, and show that we can check these conditions given
observed data. Our algorithm lends itself very naturally to the sample setting
where instead of just interventional distributions, we are provided datasets of
samples from each of these distributions. We corroborate our results on both
synthetic experiments as well as an interesting psychometric dataset. The code
can be found at https://github.com/TianyuCodings/iLCS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeuIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural gradient and parameter estimation for quantum Boltzmann machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhrumil Patel, Mark M. Wilde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thermal states play a fundamental role in various areas of physics, and they
are becoming increasingly important in quantum information science, with
applications related to semi-definite programming, quantum Boltzmann machine
learning, Hamiltonian learning, and the related task of estimating the
parameters of a Hamiltonian. Here we establish formulas underlying the basic
geometry of parameterized thermal states, and we delineate quantum algorithms
for estimating the values of these formulas. More specifically, we prove
formulas for the Fisher--Bures and Kubo--Mori information matrices of
parameterized thermal states, and our quantum algorithms for estimating their
matrix elements involve a combination of classical sampling, Hamiltonian
simulation, and the Hadamard test. These results have applications in
developing a natural gradient descent algorithm for quantum Boltzmann machine
learning, which takes into account the geometry of thermal states, and in
establishing fundamental limitations on the ability to estimate the parameters
of a Hamiltonian, when given access to thermal-state samples. For the latter
task, and for the special case of estimating a single parameter, we sketch an
algorithm that realizes a measurement that is asymptotically optimal for the
estimation task. We finally stress that the natural gradient descent algorithm
developed here can be used for any machine learning problem that employs the
quantum Boltzmann machine ansatz.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Predictive Quality Assessment for Ultrasonic Additive
  Manufacturing with Deep Learning Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lokendra Poudel, Sushant Jha, Ryan Meeker, Duy-Nhat Phan, Rahul Bhowmik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasonic Additive Manufacturing (UAM) employs ultrasonic welding to bond
similar or dissimilar metal foils to a substrate, resulting in solid,
consolidated metal components. However, certain processing conditions can lead
to inter-layer defects, affecting the final product's quality. This study
develops a method to monitor in-process quality using deep learning-based
convolutional neural networks (CNNs). The CNN models were evaluated on their
ability to classify samples with and without embedded thermocouples across five
power levels (300W, 600W, 900W, 1200W, 1500W) using thermal images with
supervised labeling. Four distinct CNN classification models were created for
different scenarios including without (baseline) and with thermocouples, only
without thermocouples across power levels, only with thermocouples across power
levels, and combined without and with thermocouples across power levels. The
models achieved 98.29% accuracy on combined baseline and thermocouple images,
97.10% for baseline images across power levels, 97.43% for thermocouple images,
and 97.27% for both types across power levels. The high accuracy, above 97%,
demonstrates the system's effectiveness in identifying and classifying
conditions within the UAM process, providing a reliable tool for quality
assurance and process control in manufacturing environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EigenVI: score-based variational inference with orthogonal function
  expansions <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diana Cai, Chirag Modi, Charles C. Margossian, Robert M. Gower, David M. Blei, Lawrence K. Saul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop EigenVI, an eigenvalue-based approach for black-box variational
inference (BBVI). EigenVI constructs its variational approximations from
orthogonal function expansions. For distributions over $\mathbb{R}^D$, the
lowest order term in these expansions provides a Gaussian variational
approximation, while higher-order terms provide a systematic way to model
non-Gaussianity. These approximations are flexible enough to model complex
distributions (multimodal, asymmetric), but they are simple enough that one can
calculate their low-order moments and draw samples from them. EigenVI can also
model other types of random variables (e.g., nonnegative, bounded) by
constructing variational approximations from different families of orthogonal
functions. Within these families, EigenVI computes the variational
approximation that best matches the score function of the target distribution
by minimizing a stochastic estimate of the Fisher divergence. Notably, this
optimization reduces to solving a minimum eigenvalue problem, so that EigenVI
effectively sidesteps the iterative gradient-based optimizations that are
required for many other BBVI algorithms. (Gradient-based methods can be
sensitive to learning rates, termination criteria, and other tunable
hyperparameters.) We use EigenVI to approximate a variety of target
distributions, including a benchmark suite of Bayesian models from posteriordb.
On these distributions, we find that EigenVI is more accurate than existing
methods for Gaussian BBVI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 9 figures. Advances in Neural Information Processing
  Systems (NeurIPS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention is All You Need to Optimize Wind Farm Operations and
  Maintenance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iman Kazemian, Murat Yildirim, Paritosh Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operations and maintenance (O&M) is a fundamental problem in wind energy
systems with far reaching implications for reliability and profitability.
Optimizing O&M is a multi-faceted decision optimization problem that requires a
careful balancing act across turbine level failure risks, operational revenues,
and maintenance crew logistics. The resulting O&M problems are typically solved
using large-scale mixed integer programming (MIP) models, which yield
computationally challenging problems that require either long-solution times,
or heuristics to reach a solution. To address this problem, we introduce a
novel decision-making framework for wind farm O&M that builds on a multi-head
attention (MHA) models, an emerging artificial intelligence methods that are
specifically designed to learn in rich and complex problem settings. The
development of proposed MHA framework incorporates a number of modeling
innovations that allows explicit embedding of MIP models within an MHA
structure. The proposed MHA model (i) significantly reduces the solution time
from hours to seconds, (ii) guarantees feasibility of the proposed solutions
considering complex constraints that are omnipresent in wind farm O&M, (iii)
results in significant solution quality compared to the conventional MIP
formulations, and (iv) exhibits significant transfer learning capability across
different problem settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Visual Case Study of the Training Dynamics in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ambroise Odonnat, Wassim Bouaziz, Vivien Cabannes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a visual sandbox designed to explore the training
dynamics of a small-scale transformer model, with the embedding dimension
constrained to $d=2$. This restriction allows for a comprehensive
two-dimensional visualization of each layer's dynamics. Through this approach,
we gain insights into training dynamics, circuit transferability, and the
causes of loss spikes, including those induced by the high curvature of
normalization layers. We propose strategies to mitigate these spikes,
demonstrating how good visualization facilitates the design of innovative ideas
of practical interest. Additionally, we believe our sandbox could assist
theoreticians in assessing essential training dynamics mechanisms and
integrating them into future theories. The code is available at
https://github.com/facebookresearch/pal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junliang Du, Yiru Cang, Tong Zhou, Jiacheng Hu, Weijie He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces the Hybrid Multi-modal VGG (HM-VGG) model, a
cutting-edge deep learning approach for the early diagnosis of glaucoma. The
HM-VGG model utilizes an attention mechanism to process Visual Field (VF) data,
enabling the extraction of key features that are vital for identifying early
signs of glaucoma. Despite the common reliance on large annotated datasets, the
HM-VGG model excels in scenarios with limited data, achieving remarkable
results with small sample sizes. The model's performance is underscored by its
high metrics in Precision, Accuracy, and F1-Score, indicating its potential for
real-world application in glaucoma detection. The paper also discusses the
challenges associated with ophthalmic image analysis, particularly the
difficulty of obtaining large volumes of annotated data. It highlights the
importance of moving beyond single-modality data, such as VF or Optical
Coherence Tomography (OCT) images alone, to a multimodal approach that can
provide a richer, more comprehensive dataset. This integration of different
data types is shown to significantly enhance diagnostic accuracy. The HM- VGG
model offers a promising tool for doctors, streamlining the diagnostic process
and improving patient outcomes. Furthermore, its applicability extends to
telemedicine and mobile healthcare, making diagnostic services more accessible.
The research presented in this paper is a significant step forward in the field
of medical image processing and has profound implications for clinical
ophthalmology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State- and context-dependent robotic manipulation and grasping via
  uncertainty-aware imitation learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim R. Winter, Ashok M. Sundaram, Werner Friedl, Maximo A. Roa, Freek Stulp, João Silvério
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating context-adaptive manipulation and grasping actions is a
challenging problem in robotics. Classical planning and control algorithms tend
to be inflexible with regard to parameterization by external variables such as
object shapes. In contrast, Learning from Demonstration (LfD) approaches, due
to their nature as function approximators, allow for introducing external
variables to modulate policies in response to the environment. In this paper,
we utilize this property by introducing an LfD approach to acquire
context-dependent grasping and manipulation strategies. We treat the problem as
a kernel-based function approximation, where the kernel inputs include generic
context variables describing task-dependent parameters such as the object
shape. We build on existing work on policy fusion with uncertainty
quantification to propose a state-dependent approach that automatically returns
to demonstrations, avoiding unpredictable behavior while smoothly adapting to
context changes. The approach is evaluated against the LASA handwriting dataset
and on a real 7-DoF robot in two scenarios: adaptation to slippage while
grasping and manipulating a deformable food item.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Training for Selective Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Li, Rebecca J. Passonneau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifier models are prevalent in natural language processing (NLP), often
with high accuracy. Yet in real world settings, human-in-the-loop systems can
foster trust in model outputs and even higher performance. Selective Prediction
(SP) methods determine when to adopt a classifier's output versus defer to a
human. Previous SP approaches have addressed how to improve softmax as a
measure of model confidence, or have developed separate confidence estimators.
One previous method involves learning a deferral model based on engineered
features. We introduce a novel joint-training approach that simultaneously
optimizes learned representations used by the classifier module and a learned
deferral policy. Our results on four classification tasks demonstrate that
joint training not only leads to better SP outcomes over two strong baselines,
but also improves the performance of both modules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaFlow: Opportunistic Inference on Asynchronous Mobile Data with
  Generalized Affinity Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenmin Wu, Sicong Liu, Kehao Zhu, Xiaochen Li, Bin Guo, Zhiwen Yu, Hongkai Wen, Xiangrui Xu, Lehao Wang, Xiangyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of mobile devices equipped with numerous sensors, such as LiDAR and
cameras, has spurred the adoption of multi-modal deep intelligence for
distributed sensing tasks, such as smart cabins and driving assistance.
However, the arrival times of mobile sensory data vary due to modality size and
network dynamics, which can lead to delays (if waiting for slower data) or
accuracy decline (if inference proceeds without waiting). Moreover, the
diversity and dynamic nature of mobile systems exacerbate this challenge. In
response, we present a shift to \textit{opportunistic} inference for
asynchronous distributed multi-modal data, enabling inference as soon as
partial data arrives. While existing methods focus on optimizing modality
consistency and complementarity, known as modal affinity, they lack a
\textit{computational} approach to control this affinity in open-world mobile
environments. AdaFlow pioneers the formulation of structured cross-modality
affinity in mobile contexts using a hierarchical analysis-based normalized
matrix. This approach accommodates the diversity and dynamics of modalities,
generalizing across different types and numbers of inputs. Employing an
affinity attention-based conditional GAN (ACGAN), AdaFlow facilitates flexible
data imputation, adapting to various modalities and downstream tasks without
retraining. Experiments show that AdaFlow significantly reduces inference
latency by up to 79.9\% and enhances accuracy by up to 61.9\%, outperforming
status quo approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate attention with MLP: a pruning strategy for attention-based
  model in multivariate time series forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhan Guo, Jiahong Deng, Yi Wei, Hui Dou, Furao Shen, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based architectures have become ubiquitous in time series
forecasting tasks, including spatio-temporal (STF) and long-term time series
forecasting (LTSF). Yet, our understanding of the reasons for their
effectiveness remains limited. This work proposes a new way to understand
self-attention networks: we have shown empirically that the entire attention
mechanism in the encoder can be reduced to an MLP formed by feedforward,
skip-connection, and layer normalization operations for temporal and/or spatial
modeling in multivariate time series forecasting. Specifically, the Q, K, and V
projection, the attention score calculation, the dot-product between the
attention score and the V, and the final projection can be removed from the
attention-based networks without significantly degrading the performance that
the given network remains the top-tier compared to other SOTA methods. For
spatio-temporal networks, the MLP-replace-attention network achieves a
reduction in FLOPS of $62.579\%$ with a loss in performance less than $2.5\%$;
for LTSF, a reduction in FLOPs of $42.233\%$ with a loss in performance less
than $2\%$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFM-Protein: Integrative Co-evolutionary <span class="highlight-title">Pre-train</span>ing for Advanced
  Protein Sequence Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang He, Peiran Jin, Yaosen Min, Shufang Xie, Lijun Wu, Tao Qin, Xiaozhuan Liang, Kaiyuan Gao, Yuliang Jiang, Tie-Yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proteins, essential to biological systems, perform functions intricately
linked to their three-dimensional structures. Understanding the relationship
between protein structures and their amino acid sequences remains a core
challenge in protein modeling. While traditional protein foundation models
benefit from pre-training on vast unlabeled datasets, they often struggle to
capture critical co-evolutionary information, which evolutionary-based methods
excel at. In this study, we introduce a novel pre-training strategy for protein
foundation models that emphasizes the interactions among amino acid residues to
enhance the extraction of both short-range and long-range co-evolutionary
features from sequence data. Trained on a large-scale protein sequence dataset,
our model demonstrates superior generalization ability, outperforming
established baselines of similar size, including the ESM model, across diverse
downstream tasks. Experimental results confirm the model's effectiveness in
integrating co-evolutionary information, marking a significant step forward in
protein sequence-based modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian-guided Label Mapping for Visual Reprogramming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained
vision models by adapting their input or output interfaces to solve downstream
tasks whose labels (i.e., downstream labels) might be totally different from
the labels associated with the pretrained models (i.e., pretrained labels).
When adapting the output interface, label mapping methods transform the
pretrained labels to downstream labels by establishing a gradient-free
one-to-one correspondence between the two sets of labels. However, in this
paper, we reveal that one-to-one mappings may overlook the complex relationship
between pretrained and downstream labels. Motivated by this observation, we
propose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an
iteratively-updated probabilistic label mapping matrix, with each element
quantifying a pairwise relationship between pretrained and downstream labels.
The assignment of values to the constructed matrix is guided by Bayesian
conditional probability, considering the joint distribution of the downstream
labels and the labels predicted by the pretrained model on downstream samples.
Experiments conducted on both pretrained vision models (e.g., ResNeXt) and
vision-language models (e.g., CLIP) demonstrate the superior performance of BLM
over existing label mapping methods. The success of BLM also offers a
probabilistic lens through which to understand and analyze the effectiveness of
VR. Our code is available at https://github.com/tmlr-group/BayesianLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximum Entropy Hindsight Experience Replay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Douglas C. Crowder, Matthew L. Trappett, Darrien M. McKenzie, Frances S. Chance
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hindsight experience replay (HER) is well-known to accelerate goal-based
reinforcement learning (RL). While HER is generally applied to off-policy RL
algorithms, we previously showed that HER can also accelerate on-policy
algorithms, such as proximal policy optimization (PPO), for goal-based
Predator-Prey environments. Here, we show that we can improve the previous
PPO-HER algorithm by selectively applying HER in a principled manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Twigs with Loop Guidance for Conditional Graph Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giangiacomo Mercatali, Yogesh Verma, Andre Freitas, Vikas Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel score-based diffusion framework named Twigs that
incorporates multiple co-evolving flows for enriching conditional generation
tasks. Specifically, a central or trunk diffusion process is associated with a
primary variable (e.g., graph structure), and additional offshoot or stem
processes are dedicated to dependent variables (e.g., graph properties or
labels). A new strategy, which we call loop guidance, effectively orchestrates
the flow of information between the trunk and the stem processes during
sampling. This approach allows us to uncover intricate interactions and
dependencies, and unlock new generative capabilities. We provide extensive
experiments to demonstrate strong performance gains of the proposed method over
contemporary baselines in the context of conditional graph generation,
underscoring the potential of Twigs in challenging generative tasks such as
inverse molecular design and molecular optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code is available at
  https://github.com/Aalto-QuML/Diffusion_twigs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffPAD: Denoising Diffusion-based Adversarial Patch Decontamination <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Fu, Xiao Zhang, Sepideh Pashami, Fatemeh Rahimian, Anders Holst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving adversarial machine learning landscape, developing
effective defenses against patch attacks has become a critical challenge,
necessitating reliable solutions to safeguard real-world AI systems. Although
diffusion models have shown remarkable capacity in image synthesis and have
been recently utilized to counter $\ell_p$-norm bounded attacks, their
potential in mitigating localized patch attacks remains largely underexplored.
In this work, we propose DiffPAD, a novel framework that harnesses the power of
diffusion models for adversarial patch decontamination. DiffPAD first performs
super-resolution restoration on downsampled input images, then adopts
binarization, dynamic thresholding scheme and sliding window for effective
localization of adversarial patches. Such a design is inspired by the
theoretically derived correlation between patch size and diffusion restoration
error that is generalized across diverse patch attack scenarios. Finally,
DiffPAD applies inpainting techniques to the original input images with the
estimated patch region being masked. By integrating closed-form solutions for
super-resolution restoration and image inpainting into the conditional reverse
sampling process of a pre-trained diffusion model, DiffPAD obviates the need
for text guidance or fine-tuning. Through comprehensive experiments, we
demonstrate that DiffPAD not only achieves state-of-the-art adversarial
robustness against patch attacks but also excels in recovering naturalistic
images without patch remnants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Testing: A New Paradigm for Model Testing with Large
  Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paulius Rauba, Nabeel Seedat, Max Ruiz Luyten, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The predominant de facto paradigm of testing ML models relies on either using
only held-out data to compute aggregate evaluation metrics or by assessing the
performance on different subgroups. However, such data-only testing methods
operate under the restrictive assumption that the available empirical data is
the sole input for testing ML models, disregarding valuable contextual
information that could guide model testing. In this paper, we challenge the
go-to approach of data-only testing and introduce context-aware testing (CAT)
which uses context as an inductive bias to guide the search for meaningful
model failures. We instantiate the first CAT system, SMART Testing, which
employs large language models to hypothesize relevant and likely failures,
which are evaluated on data using a self-falsification mechanism. Through
empirical evaluations in diverse settings, we show that SMART automatically
identifies more relevant and impactful failures than alternatives,
demonstrating the potential of CAT as a testing paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024). *Rauba & Seedat contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Information Criterion for Controlled Disentanglement of Multimodal
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Wang, Sharut Gupta, Xinyi Zhang, Sana Tonekaboni, Stefanie Jegelka, Tommi Jaakkola, Caroline Uhler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal representation learning seeks to relate and decompose information
inherent in multiple modalities. By disentangling modality-specific information
from information that is shared across modalities, we can improve
interpretability and robustness and enable downstream tasks such as the
generation of counterfactual outcomes. Separating the two types of information
is challenging since they are often deeply entangled in many real-world
applications. We propose Disentangled Self-Supervised Learning
(DisentangledSSL), a novel self-supervised approach for learning disentangled
representations. We present a comprehensive analysis of the optimality of each
disentangled representation, particularly focusing on the scenario not covered
in prior work where the so-called Minimum Necessary Information (MNI) point is
not attainable. We demonstrate that DisentangledSSL successfully learns shared
and modality-specific features on multiple synthetic and real-world datasets
and consistently outperforms baselines on various downstream tasks, including
prediction tasks for vision-language data, as well as molecule-phenotype
retrieval tasks for biological data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using
  Discrete State Space Diffusion Model <span class="chip">NeurIPS'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjia Xie, Hao Wang, Luankang Zhang, Rui Zhou, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation (SR) aims to predict items that users may be
interested in based on their historical behavior sequences. We revisit SR from
a novel information-theoretic perspective and find that conventional sequential
modeling methods fail to adequately capture the randomness and unpredictability
of user behavior. Inspired by fuzzy information processing theory, this paper
introduces the DDSR model, which uses fuzzy sets of interaction sequences to
overcome the limitations and better capture the evolution of users' real
interests. Formally based on diffusion transition processes in discrete state
spaces, which is unlike common diffusion models such as DDPM that operate in
continuous domains. It is better suited for discrete data, using structured
transitions instead of arbitrary noise introduction to avoid information loss.
Additionally, to address the inefficiency of matrix transformations due to the
vast discrete space, we use semantic labels derived from quantization or RQ-VAE
to replace item IDs, enhancing efficiency and improving cold start issues.
Testing on three public benchmark datasets shows that DDSR outperforms existing
state-of-the-art methods in various settings, demonstrating its potential and
effectiveness in handling SR tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS'2024, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ada-MSHyper: Adaptive Multi-Scale Hypergraph <span class="highlight-title">Transformer</span> for Time Series
  Forecasting <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongjiang Shang, Ling Chen, Binqing wu, Dongliang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although transformer-based methods have achieved great success in multi-scale
temporal pattern interaction modeling, two key challenges limit their further
development: (1) Individual time points contain less semantic information, and
leveraging attention to model pair-wise interactions may cause the information
utilization bottleneck. (2) Multiple inherent temporal variations (e.g.,
rising, falling, and fluctuating) entangled in temporal patterns. To this end,
we propose Adaptive Multi-Scale Hypergraph Transformer (Ada-MSHyper) for time
series forecasting. Specifically, an adaptive hypergraph learning module is
designed to provide foundations for modeling group-wise interactions, then a
multi-scale interaction module is introduced to promote more comprehensive
pattern interactions at different scales. In addition, a node and hyperedge
constraint mechanism is introduced to cluster nodes with similar semantic
information and differentiate the temporal variations within each scales.
Extensive experiments on 11 real-world datasets demonstrate that Ada-MSHyper
achieves state-of-the-art performance, reducing prediction errors by an average
of 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and
ultra-long-range time series forecasting, respectively. Code is available at
https://github.com/shangzongjiang/Ada-MSHyper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS, 21 pages, and 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrAct: Making First-layer Pre-Activations Trainable <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Petersen, Christian Borgelt, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the training of the first layer of vision models and notice the
clear relationship between pixel values and gradient update magnitudes: the
gradients arriving at the weights of a first layer are by definition directly
proportional to (normalized) input pixel values. Thus, an image with low
contrast has a smaller impact on learning than an image with higher contrast,
and a very bright or very dark image has a stronger impact on the weights than
an image with moderate brightness. In this work, we propose performing gradient
descent on the embeddings produced by the first layer of the model. However,
switching to discrete inputs with an embedding layer is not a reasonable option
for vision models. Thus, we propose the conceptual procedure of (i) a gradient
descent step on first layer activations to construct an activation proposal,
and (ii) finding the optimal weights of the first layer, i.e., those weights
which minimize the squared distance to the activation proposal. We provide a
closed form solution of the procedure and adjust it for robust stochastic
training while computing everything efficiently. Empirically, we find that
TrAct (Training Activations) speeds up training by factors between 1.25x and 4x
while requiring only a small computational overhead. We demonstrate the utility
of TrAct with different optimizers for a range of different vision models
including convolutional and transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive proofs for verifying (quantum) learning and testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias C. Caro, Jens Eisert, Marcel Hinsche, Marios Ioannou, Alexander Nietner, Ryan Sweke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of testing and learning from data in the presence of
resource constraints, such as limited memory or weak data access, which place
limitations on the efficiency and feasibility of testing or learning. In
particular, we ask the following question: Could a resource-constrained
learner/tester use interaction with a resource-unconstrained but untrusted
party to solve a learning or testing problem more efficiently than they could
without such an interaction? In this work, we answer this question both
abstractly and for concrete problems, in two complementary ways: For a wide
variety of scenarios, we prove that a resource-constrained learner cannot gain
any advantage through classical interaction with an untrusted prover. As a
special case, we show that for the vast majority of testing and learning
problems in which quantum memory is a meaningful resource, a memory-constrained
quantum algorithm cannot overcome its limitations via classical communication
with a memory-unconstrained quantum prover. In contrast, when quantum
communication is allowed, we construct a variety of interactive proof
protocols, for specific learning and testing problems, which allow
memory-constrained quantum verifiers to gain significant advantages through
delegation to untrusted provers. These results highlight both the limitations
and potential of delegating learning and testing problems to resource-rich but
untrusted third parties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 33 + 13 pages; 1 table; 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Analysis of Speech <span class="highlight-title">Self-Supervised</span> Learning at Multiple
  Resolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theo Clark, Benedetta Cevoli, Eloy de Jong, Timofey Abramski, Jamie Dougherty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) models have become crucial in speech
processing, with recent advancements concentrating on developing architectures
that capture representations across multiple timescales. The primary goal of
these multi-scale architectures is to exploit the hierarchical nature of
speech, where lower-resolution components aim to capture representations that
align with increasingly abstract concepts (e.g., from phones to words to
sentences). Although multi-scale approaches have demonstrated some improvements
over single-scale models, the precise reasons for these enhancements have poor
empirical support. In this study, we present an initial analysis of layer-wise
representations in multi-scale architectures, with a focus on Canonical
Correlation Analysis (CCA) and Mutual Information (MI). We apply this analysis
to Multi-Resolution HuBERT (MR-HuBERT) and find that (1) the improved
performance on SUPERB tasks is primarily due to the auxiliary low-resolution
loss rather than the downsampling itself, and (2) downsampling to lower
resolutions neither improves downstream performance nor correlates with
higher-level information (e.g., words), though it does improve computational
efficiency. These findings challenge assumptions about the multi-scale nature
of MR-HuBERT and motivate the importance of disentangling computational
efficiency from learning better representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representative Social Choice: From Learning Theory to AI Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social choice theory is the study of preference aggregation across a
population, used both in mechanism design for human agents and in the
democratic alignment of language models. In this study, we propose the
representative social choice framework for the modeling of democratic
representation in collective decisions, where the number of issues and
individuals are too large for mechanisms to consider all preferences directly.
These scenarios are widespread in real-world decision-making processes, such as
jury trials, indirect elections, legislation processes, corporate governance,
and, more recently, language model alignment. In representative social choice,
the population is represented by a finite sample of individual-issue pairs
based on which social choice decisions are made. We show that many of the
deepest questions in representative social choice can be naturally formulated
as statistical learning problems, and prove the generalization properties of
social choice mechanisms using the theory of machine learning. We further
formulate axioms for representative social choice, and prove Arrow-like
impossibility theorems with new combinatorial tools of analysis. Our framework
introduces the representative approach to social choice, opening up research
directions at the intersection of social choice, learning theory, and AI
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full version (20 pages). Under review. An excerpt was previously
  accepted to NeurIPS 2024 Pluralistic Alignment Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Kernel Inverse Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youyuan Long, Tolga Ok, Pedro Zattoni Scroccaro, Peyman Mohajerin Esfahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse Optimization (IO) is a framework for learning the unknown objective
function of an expert decision-maker from a past dataset. In this paper, we
extend the hypothesis class of IO objective functions to a reproducing kernel
Hilbert space (RKHS), thereby enhancing feature representation to an
infinite-dimensional space. We demonstrate that a variant of the representer
theorem holds for a specific training loss, allowing the reformulation of the
problem as a finite-dimensional convex optimization program. To address
scalability issues commonly associated with kernel methods, we propose the
Sequential Selection Optimization (SSO) algorithm to efficiently train the
proposed Kernel Inverse Optimization (KIO) model. Finally, we validate the
generalization capabilities of the proposed KIO model and the effectiveness of
the SSO algorithm through learning-from-demonstration tasks on the MuJoCo
benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Frameworks for Cognitive Radio Networks: <span class="highlight-title">Review</span> and Open
  Research Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senthil Kumar Jagatheesaperumal, Ijaz Ahmad, Marko Höyhtyä, Suleman Khan, Andrei Gurtov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has been proven to be a powerful tool for addressing the most
significant issues in cognitive radio networks, such as spectrum sensing,
spectrum sharing, resource allocation, and security attacks. The utilization of
deep learning techniques in cognitive radio networks can significantly enhance
the network's capability to adapt to changing environments and improve the
overall system's efficiency and reliability. As the demand for higher data
rates and connectivity increases, B5G/6G wireless networks are expected to
enable new services and applications significantly. Therefore, the significance
of deep learning in addressing cognitive radio network challenges cannot be
overstated. This review article provides valuable insights into potential
solutions that can serve as a foundation for the development of future B5G/6G
services. By leveraging the power of deep learning, cognitive radio networks
can pave the way for the next generation of wireless networks capable of
meeting the ever-increasing demands for higher data rates, improved
reliability, and security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted for publication in "Journal of Network
  and Computer Applications" during October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s to Predict the Applicability of Symbolic Integration
  Routines <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rashid Barket, Uzma Shafiq, Matthew England, Juergen Gerhard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic integration is a fundamental problem in mathematics: we consider how
machine learning may be used to optimise this task in a Computer Algebra System
(CAS). We train transformers that predict whether a particular integration
method will be successful, and compare against the existing human-made
heuristics (called guards) that perform this task in a leading CAS. We find the
transformer can outperform these guards, gaining up to 30% accuracy and 70%
precision. We further show that the inference time of the transformer is
inconsequential which shows that it is well-suited to include as a guard in a
CAS. Furthermore, we use Layer Integrated Gradients to interpret the decisions
that the transformer is making. If guided by a subject-matter expert, the
technique can explain some of the predictions based on the input tokens, which
can lead to further optimisations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, to be published in NeurIPS 2024 MATH-AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Deep Equilibrium Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Schleich, Marta Skreta, Lasse B. Kristensen, Rodrigo A. Vargas-Hernández, Alán Aspuru-Guzik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The feasibility of variational quantum algorithms, the most popular
correspondent of neural networks on noisy, near-term quantum hardware, is
highly impacted by the circuit depth of the involved parametrized quantum
circuits (PQCs). Higher depth increases expressivity, but also results in a
detrimental accumulation of errors. Furthermore, the number of parameters
involved in the PQC significantly influences the performance through the
necessary number of measurements to evaluate gradients, which scales linearly
with the number of parameters.
  Motivated by this, we look at deep equilibrium models (DEQs), which mimic an
infinite-depth, weight-tied network using a fraction of the memory by employing
a root solver to find the fixed points of the network. In this work, we present
Quantum Deep Equilibrium Models (QDEQs): a training paradigm that learns
parameters of a quantum machine learning model given by a PQC using DEQs. To
our knowledge, no work has yet explored the application of DEQs to QML models.
We apply QDEQs to find the parameters of a quantum circuit in two settings: the
first involves classifying MNIST-4 digits with 4 qubits; the second extends it
to 10 classes of MNIST, FashionMNIST and CIFAR. We find that QDEQ is not only
competitive with comparable existing baseline models, but also achieves higher
performance than a network with 5 times more layers. This demonstrates that the
QDEQ paradigm can be used to develop significantly more shallow quantum
circuits for a given task, something which is essential for the utility of
near-term quantum computers.
  Our code is available at https://github.com/martaskrt/qdeq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Macroscopic Dynamics from Partial Microscopic Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyi Chen, Qianxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Macroscopic observables of a system are of keen interest in real applications
such as the design of novel materials. Current methods rely on microscopic
trajectory simulations, where the forces on all microscopic coordinates need to
be computed or measured. However, this can be computationally prohibitive for
realistic systems. In this paper, we propose a method to learn macroscopic
dynamics requiring only force computations on a subset of the microscopic
coordinates. Our method relies on a sparsity assumption: the force on each
microscopic coordinate relies only on a small number of other coordinates. The
main idea of our approach is to map the training procedure on the macroscopic
coordinates back to the microscopic coordinates, on which partial force
computations can be used as stochastic estimation to update model parameters.
We provide a theoretical justification of this under suitable conditions. We
demonstrate the accuracy, force computation efficiency, and robustness of our
method on learning macroscopic closure models from a variety of microscopic
systems, including those modeled by partial differential equations or molecular
dynamics simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Sparse Regression with Non-Isotropic Designs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Hung Liu, Gleb Novikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a technique to design efficiently computable estimators for sparse
linear regression in the simultaneous presence of two adversaries: oblivious
and adaptive. We design several robust algorithms that outperform the state of
the art even in the special case when oblivious adversary simply adds Gaussian
noise. In particular, we provide a polynomial-time algorithm that with high
probability recovers the signal up to error $O(\sqrt{\varepsilon})$ as long as
the number of samples $n \ge \tilde{O}(k^2/\varepsilon)$, only assuming some
bounds on the third and the fourth moments of the distribution ${D}$ of the
design.
  In addition, prior to this work, even in the special case of Gaussian design
and noise, no polynomial time algorithm was known to achieve error
$o(\sqrt{\varepsilon})$ in the sparse setting $n < d^2$. We show that under
some assumptions on the fourth and the eighth moments of ${D}$, there is a
polynomial-time algorithm that achieves error $o(\sqrt{\varepsilon})$ as long
as $n \ge \tilde{O}(k^4 / \varepsilon^3)$. For Gaussian distribution, this
algorithm achieves error $O(\varepsilon^{3/4})$. Moreover, our algorithm
achieves error $o(\sqrt{\varepsilon})$ for all log-concave distributions if
$\varepsilon \le 1/\text{polylog(d)}$.
  Our algorithms are based on the filtering of the covariates that uses
sum-of-squares relaxations, and weighted Huber loss minimization with $\ell_1$
regularizer. We provide a novel analysis of weighted penalized Huber loss that
is suitable for heavy-tailed designs in the presence of two adversaries.
Furthermore, we complement our algorithmic results with Statistical Query lower
bounds, providing evidence that our estimators are likely to have nearly
optimal sample complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; Authors have equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing & Reducing the Need for Learning Rate Warmup in <span class="highlight-title">GPT</span> Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atli Kosson, Bettina Messmer, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning Rate Warmup is a popular heuristic for training neural networks,
especially at larger batch sizes, despite limited understanding of its
benefits. Warmup decreases the update size $\Delta \mathbf{w}_t = \eta_t
\mathbf{u}_t$ early in training by using lower values for the learning rate
$\eta_t$. In this work we argue that warmup benefits training by keeping the
overall size of $\Delta \mathbf{w}_t$ limited, counteracting large initial
values of $\mathbf{u}_t$. Focusing on small-scale GPT training with AdamW/Lion,
we explore the following question: Why and by which criteria are early updates
$\mathbf{u}_t$ too large? We analyze different metrics for the update size
including the $\ell_2$-norm, resulting directional change, and impact on the
representations of the network, providing a new perspective on warmup. In
particular, we find that warmup helps counteract large angular updates as well
as a limited critical batch size early in training. Finally, we show that the
need for warmup can be significantly reduced or eliminated by modifying the
optimizer to explicitly normalize $\mathbf{u}_t$ based on the aforementioned
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BitStack: Fine-Grained Size Control for Compressed Large Language Models
  in Variable Memory Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized numerous applications, yet
their deployment remains challenged by memory constraints on local devices.
While scaling laws have enhanced LLM capabilities, the primary bottleneck has
shifted from \textit{capability} to \textit{availability}, emphasizing the need
for efficient memory management. Traditional compression methods, such as
quantization, often require predefined compression ratios and separate
compression processes for each setting, complicating deployment in variable
memory environments. In this paper, we introduce \textbf{BitStack}, a novel,
training-free weight compression approach that enables megabyte-level
trade-offs between memory usage and model performance. By leveraging weight
decomposition, BitStack can dynamically adjust the model size with minimal
transmission between running memory and storage devices. Our approach
iteratively decomposes weight matrices while considering the significance of
each parameter, resulting in an approximately 1-bit per parameter residual
block in each decomposition iteration. These blocks are sorted and stacked in
storage as basic transmission units, with different quantities loaded based on
current memory availability. Extensive experiments across a wide range of tasks
demonstrate that, despite offering fine-grained size control, BitStack
consistently matches or surpasses strong quantization baselines, particularly
at extreme compression ratios. To the best of our knowledge, this is the first
decomposition-based method that effectively bridges the gap to practical
compression techniques like quantization. Code is available at
https://github.com/xinghaow99/BitStack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for
  Self-Taught Reasoner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu-Chieh Chang, Yu-Ting Lee, Hui-Ying Shih, Pei-Yuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reasoning abilities of large language models (LLMs) have improved with
chain-of-thought (CoT) prompting, allowing models to solve complex tasks in a
stepwise manner. However, training CoT capabilities requires detailed reasoning
data, which is often scarce. The self-taught reasoner (STaR) framework
addresses this by using reinforcement learning to automatically generate
reasoning steps, reducing reliance on human-labeled data. Although STaR and its
variants have demonstrated empirical success, a theoretical foundation
explaining these improvements is lacking. This work provides a theoretical
framework for understanding the effectiveness of reinforcement learning on CoT
reasoning and STaR. Our contributions are: (1) an analysis of policy
improvement, showing why LLM reasoning improves iteratively with STaR; (2)
conditions for convergence to an optimal reasoning policy; (3) an examination
of STaR's robustness, explaining how it can improve reasoning even when
incorporating occasional incorrect steps; and (4) criteria for the quality of
pre-trained models necessary to initiate effective reasoning improvement. This
framework aims to bridge empirical findings with theoretical insights,
advancing reinforcement learning approaches for reasoning in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network Verification with PyRAT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Augustin Lemesle, Julien Lehmann, Tristan Le Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI systems are becoming more and more popular and used in various critical
domains (health, transport, energy, ...), the need to provide guarantees and
trust of their safety is undeniable. To this end, we present PyRAT, a tool
based on abstract interpretation to verify the safety and the robustness of
neural networks. In this paper, we describe the different abstractions used by
PyRAT to find the reachable states of a neural network starting from its input
as well as the main features of the tool to provide fast and accurate analysis
of neural networks. PyRAT has already been used in several collaborations to
ensure safety guarantees, with its second place at the VNN-Comp 2024 showcasing
its performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metamorphic Malware Evolution: The Potential and Peril of Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooria Madani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code metamorphism refers to a computer programming exercise wherein the
program modifies its own code (partial or entire) consistently and
automatically while retaining its core functionality. This technique is often
used for online performance optimization and automated crash recovery in
certain mission-critical applications. However, the technique has been
misappropriated by malware creators to bypass signature-based detection
measures instituted by anti-malware engines. However, current code mutation
engines used by threat actors offer only a limited degree of mutation, which is
frequently detectable via static code analysis. The advent of large language
models (LLMs), such as ChatGPT 4.0 and Google Bard may lead to a significant
evolution in this landscape. These models have demonstrated a level of
algorithm comprehension and code synthesis capability that closely resembles
human abilities. This advancement has sparked concerns among experts that such
models could be exploited by threat actors to generate sophisticated
metamorphic malware. This paper explores the potential of several prominent
LLMs for software code mutation that may be used to reconstruct (with mutation)
existing malware code bases or create new forms of embedded mutation engines
for next-gen metamorphic malwares. In this work, we introduce a framework for
creating self-testing program mutation engines based on LLM/Transformer-based
models. The proposed framework serves as an essential tool in testing next-gen
metamorphic malware detection engines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffBatt: A Diffusion Model for Battery Degradation Prediction and
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Eivazi, André Hebenbrock, Raphael Ginster, Steffen Blömeke, Stefan Wittek, Christoph Hermann, Thomas S. Spengler, Thomas Turek, Andreas Rausch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Battery degradation remains a critical challenge in the pursuit of green
technologies and sustainable energy solutions. Despite significant research
efforts, predicting battery capacity loss accurately remains a formidable task
due to its complex nature, influenced by both aging and cycling behaviors. To
address this challenge, we introduce a novel general-purpose model for battery
degradation prediction and synthesis, DiffBatt. Leveraging an innovative
combination of conditional and unconditional diffusion models with
classifier-free guidance and transformer architecture, DiffBatt achieves high
expressivity and scalability. DiffBatt operates as a probabilistic model to
capture uncertainty in aging behaviors and a generative model to simulate
battery degradation. The performance of the model excels in prediction tasks
while also enabling the generation of synthetic degradation curves,
facilitating enhanced model training by data augmentation. In the remaining
useful life prediction task, DiffBatt provides accurate results with a mean
RMSE of 196 cycles across all datasets, outperforming all other models and
demonstrating superior generalizability. This work represents an important step
towards developing foundational models for battery degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GEPS: Boosting Generalization in Parametric PDE Neural Solvers through
  Adaptive Conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Kassaï Koupaï, Jorge Misfut Benet, Yuan Yin, Jean-Noël Vittaut, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving parametric partial differential equations (PDEs) presents significant
challenges for data-driven methods due to the sensitivity of spatio-temporal
dynamics to variations in PDE parameters. Machine learning approaches often
struggle to capture this variability. To address this, data-driven approaches
learn parametric PDEs by sampling a very large variety of trajectories with
varying PDE parameters. We first show that incorporating conditioning
mechanisms for learning parametric PDEs is essential and that among them,
$\textit{adaptive conditioning}$, allows stronger generalization. As existing
adaptive conditioning methods do not scale well with respect to the number of
parameters to adapt in the neural solver, we propose GEPS, a simple adaptation
mechanism to boost GEneralization in Pde Solvers via a first-order optimization
and low-rank rapid adaptation of a small set of context parameters. We
demonstrate the versatility of our approach for both fully data-driven and for
physics-aware neural solvers. Validation performed on a whole range of
spatio-temporal forecasting problems demonstrates excellent performance for
generalizing to unseen conditions including initial conditions, PDE
coefficients, forcing terms and solution domain. $\textit{Project page}$:
https://geps-project.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Failure Modes of LLMs for Causal Reasoning on Narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khurram Yamin, Shantanu Gupta, Gaurav R. Ghosal, Zachary C. Lipton, Bryan Wilder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the causal reasoning abilities of large language
models (LLMs) through the representative problem of inferring causal
relationships from narratives. We find that even state-of-the-art language
models rely on unreliable shortcuts, both in terms of the narrative
presentation and their parametric knowledge. For example, LLMs tend to
determine causal relationships based on the topological ordering of events
(i.e., earlier events cause later ones), resulting in lower performance
whenever events are not narrated in their exact causal order. Similarly, we
demonstrate that LLMs struggle with long-term causal reasoning and often fail
when the narratives are long and contain many events. Additionally, we show
LLMs appear to rely heavily on their parametric knowledge at the expense of
reasoning over the provided narrative. This degrades their abilities whenever
the narrative opposes parametric knowledge. We extensively validate these
failure modes through carefully controlled synthetic experiments, as well as
evaluations on real-world narratives. Finally, we observe that explicitly
generating a causal graph generally improves performance while naive
chain-of-thought is ineffective. Collectively, our results distill precise
failure modes of current state-of-the-art models and can pave the way for
future techniques to enhance causal reasoning in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 'No' Matters: Out-of-Distribution Detection in Multimodality Long
  Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rena Gao, Xuetong Wu, Siwen Luo, Caren Han, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection in multimodal contexts is essential for
identifying deviations in combined inputs from different modalities,
particularly in applications like open-domain dialogue systems or real-life
dialogue interactions. This paper aims to improve the user experience that
involves multi-round long dialogues by efficiently detecting OOD dialogues and
images. We introduce a novel scoring framework named Dialogue Image Aligning
and Enhancing Framework (DIAEF) that integrates the visual language models with
the novel proposed scores that detect OOD in two key scenarios (1) mismatches
between the dialogue and image input pair and (2) input pairs with previously
unseen labels. Our experimental results, derived from various benchmarks,
demonstrate that integrating image and multi-round dialogue OOD detection is
more effective with previously unseen labels than using either modality
independently. In the presence of mismatched pairs, our proposed score
effectively identifies these mismatches and demonstrates strong robustness in
long dialogues. This approach enhances domain-aware, adaptive conversational
agents and establishes baselines for future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaSplit: A Hardware-Software Co-Design Framework for Energy-Aware
  Inference on Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel May, Alessandro Tundo, Shashikant Ilager, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of ML models on edge devices is challenged by limited
computational resources and energy availability. While split computing enables
the decomposition of large neural networks (NNs) and allows partial computation
on both edge and cloud devices, identifying the most suitable split layer and
hardware configurations is a non-trivial task. This process is in fact hindered
by the large configuration space, the non-linear dependencies between software
and hardware parameters, the heterogeneous hardware and energy characteristics,
and the dynamic workload conditions. To overcome this challenge, we propose
DynaSplit, a two-phase framework that dynamically configures parameters across
both software (i.e., split layer) and hardware (e.g., accelerator usage, CPU
frequency). During the Offline Phase, we solve a multi-objective optimization
problem with a meta-heuristic approach to discover optimal settings. During the
Online Phase, a scheduling algorithm identifies the most suitable settings for
an incoming inference request and configures the system accordingly. We
evaluate DynaSplit using popular pre-trained NNs on a real-world testbed.
Experimental results show a reduction in energy consumption up to 72% compared
to cloud-only computation, while meeting ~90% of user request's latency
threshold compared to baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Directly Optimizing Explanations for Desired Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiwot Belay Tadesse, Alihan Hüyük, Weiwei Pan, Finale Doshi-Velez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When explaining black-box machine learning models, it's often important for
explanations to have certain desirable properties. Most existing methods
`encourage' desirable properties in their construction of explanations. In this
work, we demonstrate that these forms of encouragement do not consistently
create explanations with the properties that are supposedly being targeted.
Moreover, they do not allow for any control over which properties are
prioritized when different properties are at odds with each other. We propose
to directly optimize explanations for desired properties. Our direct approach
not only produces explanations with optimal properties more consistently but
also empowers users to control trade-offs between different properties,
allowing them to create explanations with exactly what is needed for a
particular task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise as a Double-Edged Sword: Reinforcement Learning Exploits
  Randomized Defenses in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steve Bakos, Pooria Madani, Heidar Davoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates a counterintuitive phenomenon in adversarial machine
learning: the potential for noise-based defenses to inadvertently aid evasion
attacks in certain scenarios. While randomness is often employed as a defensive
strategy against adversarial examples, our research reveals that this approach
can sometimes backfire, particularly when facing adaptive attackers using
reinforcement learning (RL). Our findings show that in specific cases,
especially with visually noisy classes, the introduction of noise in the
classifier's confidence values can be exploited by the RL attacker, leading to
a significant increase in evasion success rates. In some instances, the
noise-based defense scenario outperformed other strategies by up to 20\% on a
subset of classes. However, this effect was not consistent across all
classifiers tested, highlighting the complexity of the interaction between
noise-based defenses and different models. These results suggest that in some
cases, noise-based defenses can inadvertently create an adversarial training
loop beneficial to the RL attacker. Our study emphasizes the need for a more
nuanced approach to defensive strategies in adversarial machine learning,
particularly in safety-critical applications. It challenges the assumption that
randomness universally enhances defense against evasion attacks and highlights
the importance of considering adaptive, RL-based attackers when designing
robust defense mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuACK: A Multipurpose Queuing Algorithm for Cooperative $k$-Armed
  Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Howson, Sarah Filippi, Ciara Pike-Burke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the cooperative stochastic $k$-armed bandit problem, where a network
of $m$ agents collaborate to find the optimal action. In contrast to most prior
work on this problem, which focuses on extending a specific algorithm to the
multi-agent setting, we provide a black-box reduction that allows us to extend
any single-agent bandit algorithm to the multi-agent setting. Under mild
assumptions on the bandit environment, we prove that our reduction transfers
the regret guarantees of the single-agent algorithm to the multi-agent setting.
These guarantees are tight in subgaussian environments, in that using a near
minimax optimal single-player algorithm is near minimax optimal in the
multi-player setting up to an additive graph-dependent quantity. Our reduction
and theoretical results are also general, and apply to many different bandit
settings. By plugging in appropriate single-player algorithms, we can easily
develop provably efficient algorithms for many multi-player settings such as
heavy-tailed bandits, duelling bandits and bandits with local differential
privacy, among others. Experimentally, our approach is competitive with or
outperforms specialised multi-agent algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $ψ$DAG: Projected Stochastic Approximation Iteration for DAG
  Structure Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Klea Ziu, Slavomír Hanzely, Loka Li, Kun Zhang, Martin Takáč, Dmitry Kamzolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning the structure of Directed Acyclic Graphs (DAGs) presents a
significant challenge due to the vast combinatorial search space of possible
graphs, which scales exponentially with the number of nodes. Recent
advancements have redefined this problem as a continuous optimization task by
incorporating differentiable acyclicity constraints. These methods commonly
rely on algebraic characterizations of DAGs, such as matrix exponentials, to
enable the use of gradient-based optimization techniques. Despite these
innovations, existing methods often face optimization difficulties due to the
highly non-convex nature of DAG constraints and the per-iteration computational
complexity. In this work, we present a novel framework for learning DAGs,
employing a Stochastic Approximation approach integrated with Stochastic
Gradient Descent (SGD)-based optimization techniques. Our framework introduces
new projection methods tailored to efficiently enforce DAG constraints,
ensuring that the algorithm converges to a feasible local minimum. With its low
iteration complexity, the proposed method is well-suited for handling
large-scale problems with improved computational efficiency. We demonstrate the
effectiveness and scalability of our framework through comprehensive
experimental evaluations, which confirm its superior performance across various
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network Matrix Product Operator: A Multi-Dimensionally Integrable
  Machine Learning Potential 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kentaro Hino, Yuki Kurashige
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A neural network-based machine learning potential energy surface (PES)
expressed in a matrix product operator (NN-MPO) is proposed. The MPO form
enables efficient evaluation of high-dimensional integrals that arise in
solving the time-dependent and time-independent Schr\"odinger equation and
effectively overcomes the so-called curse of dimensionality. This starkly
contrasts with other neural network-based machine learning PES methods, such as
multi-layer perceptrons (MLPs), where evaluating high-dimensional integrals is
not straightforward due to the fully connected topology in their backbone
architecture. Nevertheless, the NN-MPO retains the high representational
capacity of neural networks. NN-MPO can achieve spectroscopic accuracy with a
test mean absolute error (MAE) of 3.03 cm$^{-1}$ for a fully coupled
six-dimensional ab initio PES, using only 625 training points distributed
across a 0 to 17,000 cm$^{-1}$ energy range. Our Python implementation is
available at https://github.com/KenHino/Pompon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Models Perform Robust Reasoning in Chain-of-thought
  <span class="highlight-title">Prompt</span>ing with Noisy Rationales? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanke Zhou, Rong Tao, Jianing Zhu, Yiwen Luo, Zengmao Wang, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates an under-explored challenge in large language models
(LLMs): chain-of-thought prompting with noisy rationales, which include
irrelevant or inaccurate reasoning thoughts within examples used for in-context
learning. We construct NoRa dataset that is tailored to evaluate the robustness
of reasoning in the presence of noisy rationales. Our findings on NoRa dataset
reveal a prevalent vulnerability to such noise among current LLMs, with
existing robust methods like self-correction and self-consistency showing
limited efficacy. Notably, compared to prompting with clean rationales, base
LLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more
drastically by 2.2%-40.4% with inaccurate thoughts.
  Addressing this challenge necessitates external supervision that should be
accessible in practice. Here, we propose the method of contrastive denoising
with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning
capabilities by contrasting noisy rationales with only one clean rationale,
which can be the minimal requirement for denoising-purpose prompting. This
method follows a principle of exploration and exploitation: (1) rephrasing and
selecting rationales in the input space to achieve explicit denoising and (2)
exploring diverse reasoning paths and voting on answers in the output space.
Empirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy
over the base model and shows significantly stronger denoising capabilities
than baseline methods. The source code is publicly available at:
https://github.com/tmlr-group/NoisyRationales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGraph: A General Retrieval-Augmented Graph Learning Framework <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinke Jiang, Rihong Qiu, Yongxin Xu, Wentao Zhang, Yichen Zhu, Ruizhe Zhang, Yuchen Fang, Xu Chu, Junfeng Zhao, Yasha Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have become essential in interpreting relational
data across various domains, yet, they often struggle to generalize to unseen
graph data that differs markedly from training instances. In this paper, we
introduce a novel framework called General Retrieval-Augmented Graph Learning
(RAGraph), which brings external graph data into the general graph foundation
model to improve model generalization on unseen scenarios. On the top of our
framework is a toy graph vector library that we established, which captures key
attributes, such as features and task-specific label information. During
inference, the RAGraph adeptly retrieves similar toy graphs based on key
similarities in downstream tasks, integrating the retrieved data to enrich the
learning context via the message-passing prompting mechanism. Our extensive
experimental evaluations demonstrate that RAGraph significantly outperforms
state-of-the-art graph learning methods in multiple tasks such as node
classification, link prediction, and graph classification across both dynamic
and static datasets. Furthermore, extensive testing confirms that RAGraph
consistently maintains high performance without the need for task-specific
fine-tuning, highlighting its adaptability, robustness, and broad
applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Airway Labeling Meets Clinical Applications: Reflecting Topology
  Consistency and Outliers via Learnable Attentions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Li, Minghui Zhang, Chuyan Zhang, Yun Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate airway anatomical labeling is crucial for clinicians to identify and
navigate complex bronchial structures during bronchoscopy. Automatic airway
anatomical labeling is challenging due to significant individual variability
and anatomical variations. Previous methods are prone to generate inconsistent
predictions, which is harmful for preoperative planning and intraoperative
navigation. This paper aims to address these challenges by proposing a novel
method that enhances topological consistency and improves the detection of
abnormal airway branches.
  We propose a novel approach incorporating two modules: the Soft Subtree
Consistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module
constructs a soft subtree to capture clinically relevant topological
relationships, allowing for flexible feature aggregation within and across
subtrees. The ABS module facilitates the interaction between node features and
prototypes to distinguish abnormal branches, preventing the erroneous
aggregation of features between normal and abnormal nodes.
  Evaluated on a challenging dataset characterized by severe airway distortion
and atrophy, our method achieves superior performance compared to
state-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the
segmental level and an 83.7% accuracy at the subsegmental level, representing a
1.4% increase in subsegmental accuracy and a 3.1% increase in topological
consistency. Notably, the method demonstrates reliable performance in cases
with disease-induced airway deformities, ensuring consistent and accurate
labeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Case ID detection based on time series data -- the mining use case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edyta Brzychczy, Tomasz Pełech-Pilichowski, Ziemowit Dworakowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process mining gains increasing popularity in business process analysis, also
in heavy industry. It requires a specific data format called an event log, with
the basic structure including a case identifier (case ID), activity (event)
name, and timestamp. In the case of industrial processes, data is very often
provided by a monitoring system as time series of low level sensor readings.
This data cannot be directly used for process mining since there is no explicit
marking of activities in the event log, and sometimes, case ID is not provided.
We propose a novel rule-based algorithm for identification patterns, based on
the identification of significant changes in short-term mean values of selected
variable to detect case ID. We present our solution on the mining use case. We
compare computed results (identified patterns) with expert labels of the same
dataset. Experiments show that the developed algorithm in the most of the cases
correctly detects IDs in datasets with and without outliers reaching F1 score
values: 96.8% and 97% respectively. We also evaluate our algorithm on dataset
from manufacturing domain reaching value 92.6% for F1 score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at EdbA'24 - Fifth International Workshop on Event Data and
  Behavioral Analytics, ICPM 2024, Kopenhagen, Denmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deterministic Exploration via Stationary Bellman Error Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Griesbach, Carlo D'Eramo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploration is a crucial and distinctive aspect of reinforcement learning
(RL) that remains a fundamental open problem. Several methods have been
proposed to tackle this challenge. Commonly used methods inject random noise
directly into the actions, indirectly via entropy maximization, or add
intrinsic rewards that encourage the agent to steer to novel regions of the
state space. Another previously seen idea is to use the Bellman error as a
separate optimization objective for exploration. In this paper, we introduce
three modifications to stabilize the latter and arrive at a deterministic
exploration policy. Our separate exploration agent is informed about the state
of the exploitation, thus enabling it to account for previous experiences.
Further components are introduced to make the exploration objective agnostic
toward the episode length and to mitigate instability introduced by
far-off-policy learning. Our experimental results show that our approach can
outperform $\varepsilon$-greedy in dense and sparse reward settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 17th European Workshop for Reinforcement Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and
  Benchmarking <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarking vision-based driving policies is challenging. On one hand,
open-loop evaluation with real data is easy, but these results do not reflect
closed-loop performance. On the other, closed-loop evaluation is possible in
simulation, but is hard to scale due to its significant computational demands.
Further, the simulators available today exhibit a large domain gap to real
data. This has resulted in an inability to draw clear conclusions from the
rapidly growing body of research on end-to-end autonomous driving. In this
paper, we present NAVSIM, a middle ground between these evaluation paradigms,
where we use large datasets in combination with a non-reactive simulator to
enable large-scale real-world benchmarking. Specifically, we gather
simulation-based metrics, such as progress and time to collision, by unrolling
bird's eye view abstractions of the test scenes for a short simulation horizon.
Our simulation is non-reactive, i.e., the evaluated policy and environment do
not influence each other. As we demonstrate empirically, this decoupling allows
open-loop metric computation while being better aligned with closed-loop
evaluations than traditional displacement errors. NAVSIM enabled a new
competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting
in several new insights. On a large set of challenging scenarios, we observe
that simple methods with moderate compute requirements such as TransFuser can
match recent large-scale end-to-end driving architectures such as UniAD. Our
modular framework can potentially be extended with new datasets, data curation
strategies, and metrics, and will be continually maintained to host future
challenges. Our code is available at
https://github.com/autonomousvision/navsim.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self
  Attention at the Threadblock Level <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hassani, Wen-Mei Hwu, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neighborhood attention reduces the cost of self attention by restricting each
token's attention span to its nearest neighbors. This restriction,
parameterized by a window size and dilation factor, draws a spectrum of
possible attention patterns between linear projection and self attention.
Neighborhood attention, and more generally sliding window attention patterns,
have long been bounded by infrastructure, particularly in higher-rank spaces
(2-D and 3-D), calling for the development of custom kernels, which have been
limited in either functionality, or performance, if not both. In this work, we
aim to massively improve upon existing infrastructure by providing two new
methods for implementing neighborhood attention. We first show that
neighborhood attention can be represented as a batched GEMM problem, similar to
standard attention, and implement it for 1-D and 2-D neighborhood attention.
These kernels on average provide 895% and 272% improvement in full precision
runtime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood
attention respectively. We find that aside from being heavily bound by memory
bandwidth, certain inherent inefficiencies exist in all unfused implementations
of neighborhood attention, which in most cases undo their theoretical
efficiency gain. Motivated by the progress made into fused dot-product
attention kernels, we developed fused neighborhood attention; an adaptation of
fused dot-product attention kernels that allow fine-grained control over
attention across different spatial axes. Known for reducing the quadratic time
complexity of self attention to a linear complexity, neighborhood attention can
now enjoy a reduced and constant memory footprint, and record-breaking half
precision runtime. We observe that our fused implementation successfully
circumvents some of the unavoidable inefficiencies in unfused
implementations...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in 38th Conference on Neural Information Processing Systems
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ab Initio Structure Solutions from Nanocrystalline Powder Diffraction
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10796v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10796v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabe Guo, Tristan Saidi, Maxwell Terban, Michele Valsecchi, Simon JL Billinge, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in materials science is the determination of the structure
of nanometer sized objects. Here we present a novel approach that uses a
generative machine learning model based on diffusion processes that is trained
on 45,229 known structures. The model factors both the measured diffraction
pattern as well as relevant statistical priors on the unit cell of atomic
cluster structures. Conditioned only on the chemical formula and the
information-scarce finite-size broadened powder diffraction pattern, we find
that our model, PXRDnet, can successfully solve simulated nanocrystals as small
as 10 angstroms across 200 materials of varying symmetry and complexity,
including structures from all seven crystal systems. We show that our model can
successfully and verifiably determine structural candidates four out of five
times, with average error among these candidates being only 7% (as measured by
post-Rietveld refinement R-factor). Furthermore, PXRDnet is capable of solving
structures from noisy diffraction patterns gathered in real-world experiments.
We suggest that data driven approaches, bootstrapped from theoretical
simulation, will ultimately provide a path towards determining the structure of
previously unsolved nano-materials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSI-Bench: Benchmarking Time Series Imputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Du, Jun Wang, Linglong Qian, Yiyuan Yang, Zina Ibrahim, Fanxing Liu, Zepu Wang, Haoxin Liu, Zhiyuan Zhao, Yingjie Zhou, Wenjia Wang, Kaize Ding, Yuxuan Liang, B. Aditya Prakash, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective imputation is a crucial preprocessing step for time series
analysis. Despite the development of numerous deep learning algorithms for time
series imputation, the community lacks standardized and comprehensive benchmark
platforms to effectively evaluate imputation performance across different
settings. Moreover, although many deep learning forecasting algorithms have
demonstrated excellent performance, whether their modelling achievements can be
transferred to time series imputation tasks remains unexplored. To bridge these
gaps, we develop TSI-Bench, the first (to our knowledge) comprehensive
benchmark suite for time series imputation utilizing deep learning techniques.
The TSI-Bench pipeline standardizes experimental settings to enable fair
evaluation of imputation algorithms and identification of meaningful insights
into the influence of domain-appropriate missing rates and patterns on model
performance. Furthermore, TSI-Bench innovatively provides a systematic paradigm
to tailor time series forecasting algorithms for imputation purposes. Our
extensive study across 34,804 experiments, 28 algorithms, and 8 datasets with
diverse missingness scenarios demonstrates TSI-Bench's effectiveness in diverse
downstream tasks and potential to unlock future directions in time series
imputation research and analysis. All source code and experiment logs are
released at https://github.com/WenjieDu/AwesomeImputation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Alternative Data for Credit Underwriting Using Causal
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alternative data provides valuable insights for lenders to evaluate a
borrower's creditworthiness, which could help expand credit access to
underserved groups and lower costs for borrowers. But some forms of alternative
data have historically been excluded from credit underwriting because it could
act as an illegal proxy for a protected class like race or gender, causing
redlining. We propose a method for applying causal inference to a supervised
machine learning model to debias alternative data so that it might be used for
credit underwriting. We demonstrate how our algorithm can be used against a
public credit dataset to improve model accuracy across different racial groups,
while providing theoretically robust nondiscrimination guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPO: Sequential Monte Carlo Policy Optimisation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07963v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07963v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew V Macfarlane, Edan Toledo, Donal Byrne, Paul Duckworth, Alexandre Laterre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging planning during learning and decision-making is central to the
long-term development of intelligent agents. Recent works have successfully
combined tree-based search methods and self-play learning mechanisms to this
end. However, these methods typically face scaling challenges due to the
sequential nature of their search. While practical engineering solutions can
partly overcome this, they often result in a negative impact on performance. In
this paper, we introduce SPO: Sequential Monte Carlo Policy Optimisation, a
model-based reinforcement learning algorithm grounded within the Expectation
Maximisation (EM) framework. We show that SPO provides robust policy
improvement and efficient scaling properties. The sample-based search makes it
directly applicable to both discrete and continuous action spaces without
modifications. We demonstrate statistically significant improvements in
performance relative to model-free and model-based baselines across both
continuous and discrete environments. Furthermore, the parallel nature of SPO's
search enables effective utilisation of hardware accelerators, yielding
favourable scaling laws.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. 34 pages, 3 main figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Optimization Bias of Next-Token Prediction in Linear Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Thrampoulidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate an investigation into the optimization properties of next-token
prediction (NTP), the dominant training paradigm for modern language models.
Specifically, we study the structural properties of the solutions selected by
gradient-based optimizers among the many possible minimizers of the NTP
objective. By framing NTP as cross-entropy minimization across distinct
contexts, each tied with a sparse conditional probability distribution across a
finite vocabulary of tokens, we introduce "NTP-separability conditions" that
enable reaching the data-entropy lower bound. With this setup, and focusing on
linear models with fixed context embeddings, we characterize the optimization
bias of gradient descent (GD): Within the data subspace defined by the sparsity
patterns of distinct contexts, GD selects parameters that equate the logits'
differences of in-support tokens to their log-odds. In the orthogonal subspace,
the GD parameters diverge in norm and select the direction that maximizes a
margin specific to NTP. These findings extend previous research on implicit
bias in one-hot classification to the NTP setting, highlighting key differences
and prompting further research into the optimization and generalization
properties of NTP, irrespective of the specific architecture used to generate
the context embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: fixed typos and writing in various parts; updated figures and
  future-work section</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Statistical Rates and Provably Efficient Criteria of Latent Diffusion
  <span class="highlight-title">Transformer</span>s (DiTs) <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01079v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01079v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Yao-Chieh Hu, Weimin Wu, Zhao Song, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the statistical and computational limits of latent Diffusion
Transformers (DiTs) under the low-dimensional linear latent space assumption.
Statistically, we study the universal approximation and sample complexity of
the DiTs score function, as well as the distribution recovery property of the
initial data. Specifically, under mild data assumptions, we derive an
approximation error bound for the score network of latent DiTs, which is
sub-linear in the latent space dimension. Additionally, we derive the
corresponding sample complexity bound and show that the data distribution
generated from the estimated score function converges toward a proximate area
of the original one. Computationally, we characterize the hardness of both
forward inference and backward computation of latent DiTs, assuming the Strong
Exponential Time Hypothesis (SETH). For forward inference, we identify
efficient criteria for all possible latent DiTs inference algorithms and
showcase our theory by pushing the efficiency toward almost-linear time
inference. For backward computation, we leverage the low-rank structure within
the gradient computation of DiTs training for possible algorithmic speedup.
Specifically, we show that such speedup achieves almost-linear time latent DiTs
training by casting the DiTs gradient as a series of chained low-rank
approximations with bounded error. Under the low-dimensional assumption, we
show that the statistical rates and the computational efficiency are all
dominated by the dimension of the subspace, suggesting that latent DiTs have
the potential to bypass the challenges associated with the high dimensionality
of initial data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024. v3 updated to camera-ready version with
  many typos fixed; v2 fixed typos, added Fig. 1 and added clarifications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource
  Programming and Formal Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03636v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03636v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Mora, Justin Wong, Haley Lepe, Sahil Bhatia, Karim Elmaaroufi, George Varghese, Joseph E. Gonzalez, Elizabeth Polgreen, Sanjit A. Seshia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) for code applications have
demonstrated remarkable zero-shot fluency and instruction following on
challenging code related tasks ranging from test case generation to
self-repair. Unsurprisingly, however, models struggle to compose syntactically
valid programs in programming languages unrepresented in pre-training, referred
to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial
settings, including domain-specific languages for internal tools, tool-chains
for legacy languages, and formal verification frameworks. Inspired by a
technique called natural programming elicitation, we propose designing an
intermediate language that LLMs "naturally" know how to use and which can be
automatically compiled to a target VLPL. When LLMs generate code that lies
outside of this intermediate language, we use compiler techniques to repair the
code into programs in the intermediate language. Overall, we introduce
\emph{synthetic programming elicitation and compilation} (SPEAC), an approach
that enables LLMs to generate syntactically valid code even for VLPLs. We
empirically evaluate the performance of SPEAC in a case study for the UCLID5
formal verification language and find that, compared to existing retrieval and
fine-tuning baselines, SPEAC produces syntactically correct programs more
frequently and without sacrificing semantic correctness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Aware Diffusion for Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calvin Luo, Mandy He, Zilai Zeng, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training an agent to achieve particular goals or perform desired behaviors is
often accomplished through reinforcement learning, especially in the absence of
expert demonstrations. However, supporting novel goals or behaviors through
reinforcement learning requires the ad-hoc design of appropriate reward
functions, which quickly becomes intractable. To address this challenge, we
propose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a
pretrained, frozen text-conditioned diffusion model to compute dense zero-shot
reward signals for text-aligned policy learning. We hypothesize that
large-scale pretrained generative models encode rich priors that can supervise
a policy to behave not only in a text-aligned manner, but also in alignment
with a notion of naturalness summarized from internet-scale training data. In
our experiments, we demonstrate that TADPoLe is able to learn policies for
novel goal-achievement and continuous locomotion behaviors specified by natural
language, in both Humanoid and Dog environments. The behaviors are learned
zero-shot without ground-truth rewards or expert demonstrations, and are
qualitatively more natural according to human evaluation. We further show that
TADPoLe performs competitively when applied to robotic manipulation tasks in
the Meta-World environment, without having access to any in-domain
demonstrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Optimism-based Approach to Online Evaluation of Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Hu, Ho-fung Leung, Farzan Farnia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing frameworks for evaluating and comparing generative models typically
target an offline setting, where the evaluator has access to full batches of
data produced by the models. However, in many practical scenarios, the goal is
to identify the best model using the fewest generated samples to minimize the
costs of querying data from the models. Such an online comparison is
challenging with current offline assessment methods. In this work, we propose
an online evaluation framework to find the generative model that maximizes a
standard assessment score among a group of available models. Our method uses an
optimism-based multi-armed bandit framework to identify the model producing
data with the highest evaluation score, quantifying the quality and diversity
of generated data. Specifically, we study the online assessment of generative
models based on the Fr\'echet Inception Distance (FID) and Inception Score (IS)
metrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper
confidence bound approach in online learning. We prove sub-linear regret bounds
for these algorithms and present numerical results on standard image datasets,
demonstrating their effectiveness in identifying the score-maximizing
generative model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Calibrating Conformal Prediction <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07307v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07307v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars van der Laan, Ahmed M. Alaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machine learning, model calibration and predictive inference are essential
for producing reliable predictions and quantifying uncertainty to support
decision-making. Recognizing the complementary roles of point and interval
predictions, we introduce Self-Calibrating Conformal Prediction, a method that
combines Venn-Abers calibration and conformal prediction to deliver calibrated
point predictions alongside prediction intervals with finite-sample validity
conditional on these predictions. To achieve this, we extend the original
Venn-Abers procedure from binary classification to regression. Our theoretical
framework supports analyzing conformal prediction methods that involve
calibrating model predictions and subsequently constructing conditionally valid
prediction intervals on the same data, where the conditioning set or conformity
scores may depend on the calibrated predictions. Real-data experiments show
that our method improves interval efficiency through model calibration and
offers a practical alternative to feature-conditional validity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2024. Preprint previously titled Self-Consistent
  Conformal Prediction.
  https://openreview.net/forum?id=BJ6HkT7qIk&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2024%2FConference%2FAuthors%23your-submissions)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mini-Sequence <span class="highlight-title">Transformer</span>: Optimizing Intermediate Memory for Long
  Sequences Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15892v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15892v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Luo, Jiawei Zhao, Zhuoming Chen, Beidi Chen, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Mini-Sequence Transformer (MsT), a simple and effective
methodology for highly efficient and accurate LLM training with extremely long
sequences. MsT partitions input sequences and iteratively processes
mini-sequences to reduce intermediate memory usage. Integrated with activation
recomputation, it enables significant memory savings in both forward and
backward passes. In experiments with the Llama3-8B model, with MsT, we measure
no degradation in throughput or convergence even with 12x longer sequences than
standard implementations. MsT is fully general, implementation-agnostic, and
requires minimal code changes to integrate with existing LLM training
frameworks. Integrated with the huggingface library, MsT successfully extends
the maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Score identity Distillation: Rapidly Surpassing the Teacher
  in One Step 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, Hai Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score identity Distillation (SiD) is a data-free method that has achieved
state-of-the-art performance in image generation by leveraging only a
pretrained diffusion model, without requiring any training data. However, the
ultimate performance of SiD is constrained by the accuracy with which the
pretrained model captures the true data scores at different stages of the
diffusion process. In this paper, we introduce SiDA (SiD with Adversarial
Loss), which not only enhances generation quality but also improves
distillation efficiency by incorporating real images and adversarial loss. SiDA
utilizes the encoder from the generator's score network as a discriminator,
boosting its ability to distinguish between real images and those generated by
SiD. The adversarial loss is batch-normalized within each GPU and then combined
with the original SiD loss. This integration effectively incorporates the
average "fakeness" per GPU batch into the pixel-based SiD loss, enabling SiDA
to distill a single-step generator either from scratch or by fine-tuning an
existing one. SiDA converges significantly faster than its predecessor when
trained from scratch, and swiftly improves upon the original model's
performance after an initial warmup period during fine-tuning from a
pre-distilled SiD generator. This one-step adversarial distillation method
establishes new benchmarks in generation performance when distilling EDM
diffusion models pretrained on CIFAR-10 (32x32) and ImageNet (64x64), achieving
FID score of 1.110 on ImageNet 64x64. It sets record-low FID scores when
distilling EDM2 models trained on ImageNet (512x512), surpassing even the
largest teacher model, EDM2-XXL. Our SiDA's results record FID scores of 2.156
for EDM2-XS, 1.669 for EDM2-S, 1.488 for EDM2-M, and 1.465 for EDM2-L,
demonstrating significant improvements across all model sizes. Our open-source
code will be integrated into the SiD codebase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline Multitask Representation Learning for Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haque Ishfaq, Thanh Nguyen-Tang, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study offline multitask representation learning in reinforcement learning
(RL), where a learner is provided with an offline dataset from different tasks
that share a common representation and is asked to learn the shared
representation. We theoretically investigate offline multitask low-rank RL, and
propose a new algorithm called MORL for offline multitask representation
learning. Furthermore, we examine downstream RL in reward-free, offline and
online scenarios, where a new task is introduced to the agent that shares the
same representation as the upstream offline tasks. Our theoretical results
demonstrate the benefits of using the learned representation from the upstream
offline task instead of directly learning the representation of the low-rank
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 38th Conference on Neural Information Processing Systems
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Federated Learning against Heterogeneous and Non-stationary
  Client Unavailability <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Xiang, Stratis Ioannidis, Edmund Yeh, Carlee Joe-Wong, Lili Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing intermittent client availability is critical for the real-world
deployment of federated learning algorithms. Most prior work either overlooks
the potential non-stationarity in the dynamics of client unavailability or
requires substantial memory/computation overhead. We study federated learning
in the presence of heterogeneous and non-stationary client availability, which
may occur when the deployment environments are uncertain, or the clients are
mobile. The impacts of heterogeneity and non-stationarity on client
unavailability can be significant, as we illustrate using FedAvg, the most
widely adopted federated learning algorithm. We propose FedAPM, which includes
novel algorithmic structures that (i) compensate for missed computations due to
unavailability with only $O(1)$ additional memory and computation with respect
to standard FedAvg, and (ii) evenly diffuse local updates within the federated
learning system through implicit gossiping, despite being agnostic to
non-stationary dynamics. We show that FedAPM converges to a stationary point of
even non-convex objectives while achieving the desired linear speedup property.
We corroborate our analysis with numerical experiments over diversified client
unavailability dynamics on real-world data sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Univariate Conditional Variational Autoencoder for Morphogenic Patterns
  Design in Frontal Polymerization-Based Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibang Liu, Pengfei Cai, Diab Abueidda, Sagar Vyas, Seid Koric, Rafael Gomez-Bombarelli, Philippe Geubelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under some initial and boundary conditions, the rapid reaction-thermal
diffusion process taking place during frontal polymerization (FP) destabilizes
the planar mode of front propagation, leading to spatially varying, complex
hierarchical patterns in thermoset polymeric materials. Although modern
reaction-diffusion models can predict the patterns resulting from unstable FP,
the inverse design of patterns, which aims to retrieve process conditions that
produce a desired pattern, remains an open challenge due to the non-unique and
non-intuitive mapping between process conditions and manufactured patterns. In
this work, we propose a probabilistic generative model named univariate
conditional variational autoencoder (UcVAE) for the inverse design of
hierarchical patterns in FP-based manufacturing. Unlike the cVAE, which encodes
both the design space and the design target, the UcVAE encodes only the design
space. In the encoder of the UcVAE, the number of training parameters is
significantly reduced compared to the cVAE, resulting in a shorter training
time while maintaining comparable performance. Given desired pattern images,
the trained UcVAE can generate multiple process condition solutions that
produce high-fidelity hierarchical patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revealing Fine-Grained Values and Opinions in Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering latent values and opinions embedded in large language models
(LLMs) can help identify biases and mitigate potential harm. Recently, this has
been approached by prompting LLMs with survey questions and quantifying the
stances in the outputs towards morally and politically charged statements.
However, the stances generated by LLMs can vary greatly depending on how they
are prompted, and there are many ways to argue for or against a given position.
In this work, we propose to address this by analysing a large and robust
dataset of 156k LLM responses to the 62 propositions of the Political Compass
Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform
coarse-grained analysis of their generated stances and fine-grained analysis of
the plain text justifications for those stances. For fine-grained analysis, we
propose to identify tropes in the responses: semantically similar phrases that
are recurrent and consistent across different prompts, revealing natural
patterns in the text that a given LLM is prone to produce. We find that
demographic features added to prompts significantly affect outcomes on the PCT,
reflecting bias, as well as disparities between the results of tests when
eliciting closed-form vs. open domain responses. Additionally, patterns in the
plain text rationales via tropes show that similar justifications are
repeatedly generated across models and prompts even with disparate stances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree of Attacks: Jailbreaking Black-Box LLMs Automatically <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02119v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02119v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, Amin Karbasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) display versatile functionality, they
continue to generate harmful, biased, and toxic content, as demonstrated by the
prevalence of human-designed jailbreaks. In this work, we present Tree of
Attacks with Pruning (TAP), an automated method for generating jailbreaks that
only requires black-box access to the target LLM. TAP utilizes an attacker LLM
to iteratively refine candidate (attack) prompts until one of the refined
prompts jailbreaks the target. In addition, before sending prompts to the
target, TAP assesses them and prunes the ones unlikely to result in jailbreaks,
reducing the number of queries sent to the target LLM. In empirical
evaluations, we observe that TAP generates prompts that jailbreak
state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the
prompts. This significantly improves upon the previous state-of-the-art
black-box methods for generating jailbreaks while using a smaller number of
queries than them. Furthermore, TAP is also capable of jailbreaking LLMs
protected by state-of-the-art guardrails, e.g., LlamaGuard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at NeurIPS 2024. Code:
  https://github.com/RICommunity/TAP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification Diffusion Models: Revitalizing Density Ratio Estimation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10095v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10095v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahar Yadin, Noam Elata, Tomer Michaeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A prominent family of methods for learning data distributions relies on
density ratio estimation (DRE), where a model is trained to $\textit{classify}$
between data samples and samples from some reference distribution. DRE-based
models can directly output the likelihood for any given input, a highly desired
property that is lacking in most generative techniques. Nevertheless, to date,
DRE methods have failed in accurately capturing the distributions of complex
high-dimensional data, like images, and have thus been drawing reduced research
attention in recent years. In this work we present $\textit{classification
diffusion models}$ (CDMs), a DRE-based generative method that adopts the
formalism of denoising diffusion models (DDMs) while making use of a classifier
that predicts the level of noise added to a clean signal. Our method is based
on an analytical connection that we derive between the MSE-optimal denoiser for
removing white Gaussian noise and the cross-entropy-optimal classifier for
predicting the noise level. Our method is the first DRE-based technique that
can successfully generate images beyond the MNIST dataset. Furthermore, it can
output the likelihood of any input in a single forward pass, achieving
state-of-the-art negative log likelihood (NLL) among methods with this
property. Code is available on the project's webpage in
https://shaharYadin.github.io/CDM/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Need a Small Specialized Language Model? Plan Early! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are versatile tools but are not suitable for small
inference budgets. Small models have more efficient inference, but their lower
capacity means that their performance can be good only if one limits their
scope to a specialized domain. This paper explores how to get good specialized
small language models using a large, generic, pretraining set and a limited
amount of specialized data. We consider two scenarios, depending on whether (i)
one can afford pretraining a model for each specialization task, or (ii) one
wants to cheaply adapt a single pretrained model for each task. In the first
scenario, we propose an effective solution based on importance sampling: we
resample the pretraining set to imitate the specialization data and train a
small model on it. In the second scenario, we propose a novel architecture,
projected networks (PN). PN is a large network whose parameters can be linearly
projected into a small network for specialization. For both scenarios, we
demonstrate the empirical effectiveness of our solutions across various
domains, training set sizes, and training budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unconditional stability of a recurrent neural circuit implementing
  divisive normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivang Rawat, David J. Heeger, Stefano Martiniani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stability in recurrent neural models poses a significant challenge,
particularly in developing biologically plausible neurodynamical models that
can be seamlessly trained. Traditional cortical circuit models are notoriously
difficult to train due to expansive nonlinearities in the dynamical system,
leading to an optimization problem with nonlinear stability constraints that
are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in
tasks involving sequential data but lack biological plausibility and
interpretability. In this work, we address these challenges by linking dynamic
divisive normalization (DN) to the stability of ORGaNICs, a biologically
plausible recurrent cortical circuit model that dynamically achieves DN and
that has been shown to simulate a wide range of neurophysiological phenomena.
By using the indirect method of Lyapunov, we prove the remarkable property of
unconditional local stability for an arbitrary-dimensional ORGaNICs circuit
when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a
system of coupled damped harmonic oscillators, which enables us to derive the
circuit's energy function, providing a normative principle of what the circuit,
and individual neurons, aim to accomplish. Further, for a generic recurrent
weight matrix, we prove the stability of the 2D model and demonstrate
empirically that stability holds in higher dimensions. Finally, we show that
ORGaNICs can be trained by backpropagation through time without gradient
clipping/scaling, thanks to its intrinsic stability property and adaptive time
constants, which address the problems of exploding, vanishing, and oscillating
gradients. By evaluating the model's performance on RNN benchmarks, we find
that ORGaNICs outperform alternative neurodynamical models on static image
classification tasks and perform comparably to LSTMs on sequential tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-light Pedestrian Detection in Visible and Infrared Image Feeds:
  Issues and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thangarajah Akilan, Hrishikesh Vachhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrian detection has become a cornerstone for several high-level tasks,
including autonomous driving, intelligent transportation, and traffic
surveillance. There are several works focussed on pedestrian detection using
visible images, mainly in the daytime. However, this task is very intriguing
when the environmental conditions change to poor lighting or nighttime.
Recently, new ideas have been spurred to use alternative sources, such as Far
InfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light
conditions. This study reviews recent developments in low-light pedestrian
detection approaches. It systematically categorizes and analyses various
algorithms from region-based to non-region-based and graph-based learning
methodologies by highlighting their methodologies, implementation issues, and
challenges. It also outlines the key benchmark datasets that can be used for
research and development of advanced pedestrian detection algorithms,
particularly in low-light situations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSLO: One-Shot Label-Only Membership Inference Attacks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuefeng Peng, Jaechul Roh, Subhransu Maji, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs),
which accurately infer a given sample's membership in a target model's training
set with high precision using just \emph{a single query}, where the target
model only returns the predicted hard label. This is in contrast to
state-of-the-art label-only attacks which require $\sim6000$ queries, yet get
attack precisions lower than OSLO's. OSLO leverages transfer-based black-box
adversarial attacks. The core idea is that a member sample exhibits more
resistance to adversarial perturbations than a non-member. We compare OSLO
against state-of-the-art label-only attacks and demonstrate that, despite
requiring only one query, our method significantly outperforms previous attacks
in terms of precision and true positive rate (TPR) under the same false
positive rates (FPR). For example, compared to previous label-only MIAs, OSLO
achieves a TPR that is at least 7$\times$ higher under a 1\% FPR and at least
22$\times$ higher under a 0.1\% FPR on CIFAR100 for a ResNet18 model. We
evaluated multiple defense mechanisms against OSLO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SR-CACO-2: A <span class="highlight-title">Dataset</span> for Confocal Fluorescence Microscopy Image
  Super-Resolution <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soufiane Belharbi, Mara KM Whitford, Phuong Hoang, Shakeeb Murtaza, Luke McCaffrey, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confocal fluorescence microscopy is one of the most accessible and widely
used imaging techniques for the study of biological processes at the cellular
and subcellular levels. Scanning confocal microscopy allows the capture of
high-quality images from thick three-dimensional (3D) samples, yet suffers from
well-known limitations such as photobleaching and phototoxicity of specimens
caused by intense light exposure, limiting its applications. Cellular damage
can be alleviated by changing imaging parameters to reduce light exposure,
often at the expense of image quality. Machine/deep learning methods for
single-image super-resolution (SISR) can be applied to restore image quality by
upscaling lower-resolution (LR) images to yield high-resolution images (HR).
These SISR methods have been successfully applied to photo-realistic images due
partly to the abundance of publicly available data. In contrast, the lack of
publicly available data partly limits their application and success in scanning
confocal microscopy. In this paper, we introduce a large scanning confocal
microscopy dataset named SR-CACO-2 that is comprised of low- and
high-resolution image pairs marked for three different fluorescent markers. It
allows the evaluation of performance of SISR methods on three different
upscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell
line Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured
with four resolutions and three markers, forming 9,937 image patches for SISR
methods. We provide benchmarking results for 16 state-of-the-art methods of the
main SISR families. Results show that these methods have limited success in
producing high-resolution textures. The dataset is freely accessible under a
Creative Commons license (CC BY-NC-SA 4.0). Our dataset, code and pretrained
weights for SISR methods are available: https://github.com/sbelharbi/sr-caco-2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 15 figures, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Linear System Solvers for Hyperparameter Optimisation in
  Iterative Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18457v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18457v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Andreas Lin, Shreyas Padhy, Bruno Mlodozeniec, Javier Antorán, José Miguel Hernández-Lobato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling hyperparameter optimisation to very large datasets remains an open
problem in the Gaussian process community. This paper focuses on iterative
methods, which use linear system solvers, like conjugate gradients, alternating
projections or stochastic gradient descent, to construct an estimate of the
marginal likelihood gradient. We discuss three key improvements which are
applicable across solvers: (i) a pathwise gradient estimator, which reduces the
required number of solver iterations and amortises the computational cost of
making predictions, (ii) warm starting linear system solvers with the solution
from the previous step, which leads to faster solver convergence at the cost of
negligible bias, (iii) early stopping linear system solvers after a limited
computational budget, which synergises with warm starting, allowing solver
progress to accumulate over multiple marginal likelihood steps. These
techniques provide speed-ups of up to $72\times$ when solving to tolerance, and
decrease the average residual norm by up to $7\times$ when stopping early.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Advances in Neural Information Processing Systems 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble sampling for linear bandits: small ensembles suffice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Janz, Alexander E. Litvak, Csaba Szepesvári
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide the first useful and rigorous analysis of ensemble sampling for
the stochastic linear bandit setting. In particular, we show that, under
standard assumptions, for a $d$-dimensional stochastic linear bandit with an
interaction horizon $T$, ensemble sampling with an ensemble of size of order
$\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2}
\sqrt{T}}$. Ours is the first result in any structured setting not to require
the size of the ensemble to scale linearly with $T$ -- which defeats the
purpose of ensemble sampling -- while obtaining near $\smash{\sqrt{T}}$ order
regret. Ours is also the first result that allows infinite action sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Gradient for Robust Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhao Wang, Shaohang Xu, Chin Pang Ho, Marek Petrik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a generic policy gradient method with the global optimality
guarantee for robust Markov Decision Processes (MDPs). While policy gradient
methods are widely used for solving dynamic decision problems due to their
scalable and efficient nature, adapting these methods to account for model
ambiguity has been challenging, often making it impractical to learn robust
policies. This paper introduces a novel policy gradient method, Double-Loop
Robust Policy Mirror Descent (DRPMD), for solving robust MDPs. DRPMD employs a
general mirror descent update rule for the policy optimization with adaptive
tolerance per iteration, guaranteeing convergence to a globally optimal policy.
We provide a comprehensive analysis of DRPMD, including new convergence results
under both direct and softmax parameterizations, and provide novel insights
into the inner problem solution through Transition Mirror Ascent (TMA).
Additionally, we propose innovative parametric transition kernels for both
discrete and continuous state-action spaces, broadening the applicability of
our approach. Empirical results validate the robustness and global convergence
of DRPMD across various challenging robust MDP settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active, anytime-valid risk controlling prediction sets <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Xu, Nikos Karampatziakis, Paul Mineiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rigorously establishing the safety of black-box machine learning models
concerning critical risk measures is important for providing guarantees about
model behavior. Recently, Bates et. al. (JACM '24) introduced the notion of a
risk controlling prediction set (RCPS) for producing prediction sets that are
statistically guaranteed low risk from machine learning models. Our method
extends this notion to the sequential setting, where we provide guarantees even
when the data is collected adaptively, and ensures that the risk guarantee is
anytime-valid, i.e., simultaneously holds at all time steps. Further, we
propose a framework for constructing RCPSes for active labeling, i.e., allowing
one to use a labeling policy that chooses whether to query the true label for
each received data point and ensures that the expected proportion of data
points whose labels are queried are below a predetermined label budget. We also
describe how to use predictors (i.e., the machine learning model for which we
provide risk control guarantees) to further improve the utility of our RCPSes
by estimating the expected risk conditioned on the covariates. We characterize
the optimal choices of label policy and predictor under a fixed label budget
and show a regret result that relates the estimation error of the optimal
labeling policy and predictor to the wealth process that underlies our RCPSes.
Lastly, we present practical ways of formulating label policies and empirically
show that our label policies use fewer labels to reach higher utility than
naive baseline labeling strategies on both simulations and real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 figures. Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Optimization with Sparse Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badih Ghazi, Cristóbal Guzmán, Pritish Kamath, Ravi Kumar, Pasin Manurangsi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications of large embedding models, we study differentially
private (DP) optimization problems under sparsity of individual gradients. We
start with new near-optimal bounds for the classic mean estimation problem but
with sparse data, improving upon existing algorithms particularly for the
high-dimensional regime. Building on this, we obtain pure- and approximate-DP
algorithms with almost optimal rates for stochastic convex optimization with
sparse gradients; the former represents the first nearly dimension-independent
rates for this problem. Finally, we study the approximation of stationary
points for the empirical loss in approximate-DP optimization and obtain rates
that depend on sparsity instead of dimension, modulo polylogarithmic factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor corrections and re-structuring of the presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms
  for Optimization under Orthogonality Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Ablin, Simon Vary, Bin Gao, P. -A. Absil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Orthogonality constraints naturally appear in many machine learning problems,
from principal component analysis to robust neural network training. They are
usually solved using Riemannian optimization algorithms, which minimize the
objective function while enforcing the constraint. However, enforcing the
orthogonality constraint can be the most time-consuming operation in such
algorithms. Recently, Ablin & Peyr\'e (2022) proposed the landing algorithm, a
method with cheap iterations that does not enforce the orthogonality
constraints but is attracted towards the manifold in a smooth manner. This
article provides new practical and theoretical developments for the landing
algorithm. First, the method is extended to the Stiefel manifold, the set of
rectangular orthogonal matrices. We also consider stochastic and variance
reduction algorithms when the cost function is an average of many functions. We
demonstrate that all these methods have the same rate of convergence as their
Riemannian counterparts that exactly enforce the constraint, and converge to
the manifold. Finally, our experiments demonstrate the promise of our approach
to an array of machine-learning problems that involve orthogonality
constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter-free Clipped Gradient Descent Meets Polyak <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Takezawa, Han Bao, Ryoma Sato, Kenta Niwa, Makoto Yamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient descent and its variants are de facto standard algorithms for
training machine learning models. As gradient descent is sensitive to its
hyperparameters, we need to tune the hyperparameters carefully using a grid
search. However, the method is time-consuming, particularly when multiple
hyperparameters exist. Therefore, recent studies have analyzed parameter-free
methods that adjust the hyperparameters on the fly. However, the existing work
is limited to investigations of parameter-free methods for the stepsize, and
parameter-free methods for other hyperparameters have not been explored. For
instance, although the gradient clipping threshold is a crucial hyperparameter
in addition to the stepsize for preventing gradient explosion issues, none of
the existing studies have investigated parameter-free methods for clipped
gradient descent. Therefore, in this study, we investigate the parameter-free
methods for clipped gradient descent. Specifically, we propose Inexact Polyak
Stepsize, which converges to the optimal solution without any hyperparameters
tuning, and its convergence rate is asymptotically independent of $L$ under
$L$-smooth and $(L_0, L_1)$-smooth assumptions of the loss function, similar to
that of clipped gradient descent with well-tuned hyperparameters. We
numerically validated our convergence results using a synthetic function and
demonstrated the effectiveness of our proposed methods using LSTM, Nano-GPT,
and T5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Knowledge for Augmented Modular Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01158v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01158v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenz Wolf, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing modular Reinforcement Learning (RL) architectures are generally
based on reusable components, also allowing for "plug-and-play" integration.
However, these modules are homogeneous in nature - in fact, they essentially
provide policies obtained via RL through the maximization of individual reward
functions. Consequently, such solutions still lack the ability to integrate and
process multiple types of information (i.e., heterogeneous knowledge
representations), such as rules, sub-goals, and skills from various sources. In
this paper, we discuss several practical examples of heterogeneous knowledge
and propose Augmented Modular Reinforcement Learning (AMRL) to address these
limitations. Our framework uses a selector to combine heterogeneous modules and
seamlessly incorporate different types of knowledge representations and
processing mechanisms. Our results demonstrate the performance and efficiency
improvements, also in terms of generalization, that can be achieved by
augmenting traditional modular RL with heterogeneous knowledge sources and
processing mechanisms. Finally, we examine the safety, robustness, and
interpretability issues stemming from the introduction of knowledge
heterogeneity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kermut: Composite kernel regression for protein variant effects <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00002v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00002v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Mørch Groth, Mads Herbert Kerrn, Lars Olsen, Jesper Salomon, Wouter Boomsma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable prediction of protein variant effects is crucial for both protein
optimization and for advancing biological understanding. For practical use in
protein engineering, it is important that we can also provide reliable
uncertainty estimates for our predictions, and while prediction accuracy has
seen much progress in recent years, uncertainty metrics are rarely reported. We
here provide a Gaussian process regression model, Kermut, with a novel
composite kernel for modeling mutation similarity, which obtains
state-of-the-art performance for supervised protein variant effect prediction
while also offering estimates of uncertainty through its posterior. An analysis
of the quality of the uncertainty estimates demonstrates that our model
provides meaningful levels of overall calibration, but that instance-specific
uncertainty calibration remains more challenging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 as Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIO: A Foundation Model on Multimodal Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. Codes and models are available in
  https://github.com/MIO-Team/MIO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning over Connected Modes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03333v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03333v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Grinwald, Philipp Wiesner, Shinichi Nakajima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical heterogeneity in federated learning poses two major challenges:
slow global training due to conflicting gradient signals, and the need of
personalization for local distributions. In this work, we tackle both
challenges by leveraging recent advances in \emph{linear mode connectivity} --
identifying a linearly connected low-loss region in the parameter space of
neural networks, which we call solution simplex. We propose federated learning
over connected modes (\textsc{Floco}), where clients are assigned local
subregions in this simplex based on their gradient signals, and together learn
the shared global solution simplex. This allows personalization of the client
models to fit their local distributions within the degrees of freedom in the
solution simplex and homogenizes the update signals for the global simplex
training. Our experiments show that \textsc{Floco} accelerates the global
training process, and significantly improves the local accuracy with minimal
computational overhead in cross-silo federated learning settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency Diffusion Bridge Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guande He, Kaiwen Zheng, Jianfei Chen, Fan Bao, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have become the dominant paradigm of generative
modeling in a variety of domains by learning stochastic processes from noise to
data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of
generative modeling that builds stochastic processes between fixed data
endpoints based on a reference diffusion process, have achieved empirical
success across tasks with coupled data distribution, such as image-to-image
translation. However, DDBM's sampling process typically requires hundreds of
network evaluations to achieve decent performance, which may impede their
practical deployment due to high computational demands. In this work, inspired
by the recent advance of consistency models in DMs, we tackle this problem by
learning the consistency function of the probability-flow ordinary differential
equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting
step given any point on the ODE trajectory. Based on a dedicated general-form
ODE solver, we propose two paradigms: consistency bridge distillation and
consistency bridge training, which is flexible to apply on DDBMs with broad
design choices. Experimental results show that our proposed method could sample
$4\times$ to $50\times$ faster than the base DDBM and produce better visual
quality given the same step in various tasks with pixel resolution ranging from
$64 \times 64$ to $256 \times 256$, as well as supporting downstream tasks such
as semantic interpolation in the data space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Fractional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17638v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17638v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Nobis, Maximilian Springenberg, Marco Aversa, Michael Detzel, Rembert Daems, Roderick Murray-Smith, Shinichi Nakajima, Sebastian Lapuschkin, Stefano Ermon, Tolga Birdal, Manfred Opper, Christoph Knochenhauer, Luis Oala, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the first continuous-time score-based generative model that
leverages fractional diffusion processes for its underlying dynamics. Although
diffusion models have excelled at capturing data distributions, they still
suffer from various limitations such as slow convergence, mode-collapse on
imbalanced data, and lack of diversity. These issues are partially linked to
the use of light-tailed Brownian motion (BM) with independent increments. In
this paper, we replace BM with an approximation of its non-Markovian
counterpart, fractional Brownian motion (fBM), characterized by correlated
increments and Hurst index $H \in (0,1)$, where $H=0.5$ recovers the classical
BM. To ensure tractable inference and learning, we employ a recently
popularized Markov approximation of fBM (MA-fBM) and derive its reverse-time
model, resulting in generative fractional diffusion models (GFDM). We
characterize the forward dynamics using a continuous reparameterization trick
and propose augmented score matching to efficiently learn the score function,
which is partly known in closed form, at minimal added cost. The ability to
drive our diffusion model via MA-fBM offers flexibility and control. $H \leq
0.5$ enters the regime of rough paths whereas $H>0.5$ regularizes diffusion
paths and invokes long-term memory. The Markov approximation allows added
control by varying the number of Markov processes linearly combined to
approximate fBM. Our evaluations on real image datasets demonstrate that GFDM
achieves greater pixel-wise diversity and enhanced image quality, as indicated
by a lower FID, offering a promising alternative to traditional diffusion
models
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Learning Algorithms Do Not Learn Preference Rankings <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19534v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19534v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelica Chen, Sadhika Malladi, Lily H. Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, Kyunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference learning algorithms (e.g., RLHF and DPO) are frequently used to
steer LLMs to produce generations that are more preferred by humans, but our
understanding of their inner workings is still limited. In this work, we study
the conventional wisdom that preference learning trains models to assign higher
likelihoods to more preferred outputs than less preferred outputs, measured via
ranking accuracy. Surprisingly, we find that most state-of-the-art
preference-tuned models achieve a ranking accuracy of less than 60% on common
preference datasets. We furthermore derive the idealized ranking accuracy that
a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective
perfectly. We demonstrate that existing models exhibit a significant alignment
gap -- i.e., a gap between the observed and idealized ranking accuracies. We
attribute this discrepancy to the DPO objective, which is empirically and
theoretically ill-suited to fix even mild ranking errors in the reference
model, and derive a simple and efficient formula for quantifying the difficulty
of learning a given preference datapoint. Finally, we demonstrate that ranking
accuracy strongly correlates with the empirically popular win rate metric when
the model is close to the reference model used in the objective, shedding
further light on the differences between on-policy (e.g., RLHF) and off-policy
(e.g., DPO) preference learning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniJARVIS: Unified Vision-Language-Action Tokenization Enables
  Open-World Instruction Following Agents <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, Yitao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model
for open-world instruction-following agents in Minecraft. Compared to prior
works that either emit textual goals to separate controllers or produce the
control command directly, OmniJARVIS seeks a different path to ensure both
strong reasoning and efficient decision-making capabilities via unified
tokenization of multimodal interaction data. First, we introduce a
self-supervised approach to learn a behavior encoder that produces discretized
tokens for behavior trajectories $\tau = \{o_0, a_0, \dots\}$ and an imitation
learning policy decoder conditioned on these tokens. These additional behavior
tokens will be augmented to the vocabulary of pretrained Multimodal Language
Models. With this encoder, we then pack long-term multimodal interactions
involving task instructions, memories, thoughts, observations, textual
responses, behavior trajectories, etc into unified token sequences and model
them with autoregressive transformers. Thanks to the semantically meaningful
behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing
chain-of-thoughts), plan, answer questions, and act (by producing behavior
tokens for the imitation learning policy decoder). OmniJARVIS demonstrates
excellent performances on a comprehensive collection of atomic, programmatic,
and open-ended tasks in open-world Minecraft. Our analysis further unveils the
crucial design principles in interaction data formation, unified tokenization,
and its scaling potentials. The dataset, models, and code will be released at
https://craftjarvis.org/OmniJARVIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted on NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Efficient Learning with Neural Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alaia Solko-Breslin, Seewon Choi, Ziyang Li, Neelay Velingker, Rajeev Alur, Mayur Naik, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many computational tasks can be naturally expressed as a composition of a DNN
followed by a program written in a traditional programming language or an API
call to an LLM. We call such composites "neural programs" and focus on the
problem of learning the DNN parameters when the training data consist of
end-to-end input-output labels for the composite. When the program is written
in a differentiable logic programming language, techniques from neurosymbolic
learning are applicable, but in general, the learning for neural programs
requires estimating the gradients of black-box components. We present an
algorithm for learning neural programs, called ISED, that only relies on
input-output samples of black-box components. For evaluation, we introduce new
benchmarks that involve calls to modern LLMs such as GPT-4 and also consider
benchmarks from the neurosymbolic learning literature. Our evaluation shows
that for the latter benchmarks, ISED has comparable performance to
state-of-the-art neurosymbolic frameworks. For the former, we use adaptations
of prior work on gradient approximations of black-box components as a baseline,
and show that ISED achieves comparable accuracy but in a more data- and
sample-efficient manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Personalization in Language Models: A Systematic Study <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Nils Heil, Jiarui Liu, Shehzaad Dhuliawala, Yahang Qi, Bernhard Schölkopf, Rada Mihalcea, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Personalization (IP) is a phenomenon of language models inferring a
user's background from the implicit cues in the input prompts and tailoring the
response based on this inference. While previous work has touched upon various
instances of this problem, there lacks a unified framework to study this
behavior. This work systematically studies IP through a rigorous mathematical
formulation, a multi-perspective moral reasoning framework, and a set of case
studies. Our theoretical foundation for IP relies on a structural causal model
and introduces a novel method, indirect intervention, to estimate the causal
effect of a mediator variable that cannot be directly intervened upon. Beyond
the technical approach, we also introduce a set of moral reasoning principles
based on three schools of moral philosophy to study when IP may or may not be
ethically appropriate. Equipped with both mathematical and ethical insights, we
present three diverse case studies illustrating the varied nature of the IP
problem and offer recommendations for future research. Our code is at
https://github.com/jiarui-liu/IP, and our data is at
https://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerated Bayesian parameter estimation and model selection for
  gravitational waves with normalizing flows <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicja Polanska, Thibeau Wouters, Peter T. H. Pang, Kaze K. W. Wong, Jason D. McEwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an accelerated pipeline, based on high-performance computing
techniques and normalizing flows, for joint Bayesian parameter estimation and
model selection and demonstrate its efficiency in gravitational wave
astrophysics. We integrate the Jim inference toolkit, a normalizing
flow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned
harmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are
consistent with traditional nested sampling techniques run on $16$ CPU cores,
while reducing the computation time by factors of $5\times$ and $15\times$ for
$4$-dimensional and $11$-dimensional gravitational wave inference problems,
respectively. Our code is available in well-tested and thoroughly documented
open-source packages, ensuring accessibility and reproducibility for the wider
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to NeurIPS 2024 workshop on Machine Learning and the
  Physical Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearning as multi-task optimization: A normalized gradient difference
  approach with an adaptive learning rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Bu, Xiaomeng Jin, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Mingyi Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning has been used to remove unwanted knowledge acquired by
large language models (LLMs). In this paper, we examine machine unlearning from
an optimization perspective, framing it as a regularized multi-task
optimization problem, where one task optimizes a forgetting objective and
another optimizes the model performance. In particular, we introduce a
normalized gradient difference (NGDiff) algorithm, enabling us to have better
control over the trade-off between the objectives, while integrating a new,
automatic learning rate scheduler. We provide a theoretical analysis and
empirically demonstrate the superior performance of NGDiff among
state-of-the-art unlearning methods on the TOFU and MUSE datasets while
exhibiting stable training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic
  Signal Control with Missing Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22938v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22938v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyang Chen, Yang Jiang, Shengnan Guo, Xiaowei Mao, Youfang Lin, Huaiyu Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of reinforcement learning in traffic signal control (TSC) has
been extensively researched and yielded notable achievements. However, most
existing works for TSC assume that traffic data from all surrounding
intersections is fully and continuously available through sensors. In
real-world applications, this assumption often fails due to sensor malfunctions
or data loss, making TSC with missing data a critical challenge. To meet the
needs of practical applications, we introduce DiffLight, a novel conditional
diffusion model for TSC under data-missing scenarios in the offline setting.
Specifically, we integrate two essential sub-tasks, i.e., traffic data
imputation and decision-making, by leveraging a Partial Rewards Conditioned
Diffusion (PRCD) model to prevent missing rewards from interfering with the
learning process. Meanwhile, to effectively capture the spatial-temporal
dependencies among intersections, we design a Spatial-Temporal transFormer
(STFormer) architecture. In addition, we propose a Diffusion Communication
Mechanism (DCM) to promote better communication and control performance under
data-missing scenarios. Extensive experiments on five datasets with various
data-missing scenarios demonstrate that DiffLight is an effective controller to
address TSC with missing data. The code of DiffLight is released at
https://github.com/lokol5579/DiffLight-release.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GoRINNs: Godunov-Riemann Informed Neural Networks for Learning
  Hyperbolic Conservation Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios G. Patsatzis, Mario di Bernardo, Lucia Russo, Constantinos Siettos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GoRINNs: numerical analysis-informed neural networks for the
solution of inverse problems of non-linear systems of conservation laws.
GoRINNs are based on high-resolution Godunov schemes for the solution of the
Riemann problem in hyperbolic Partial Differential Equations (PDEs). In
contrast to other existing machine learning methods that learn the numerical
fluxes of conservative Finite Volume methods, GoRINNs learn the physical flux
function per se. Due to their structure, GoRINNs provide interpretable,
conservative schemes, that learn the solution operator on the basis of
approximate Riemann solvers that satisfy the Rankine-Hugoniot condition. The
performance of GoRINNs is assessed via four benchmark problems, namely the
Burgers', the Shallow Water, the Lighthill-Whitham-Richards and the
Payne-Whitham traffic flow models. The solution profiles of these PDEs exhibit
shock waves, rarefactions and/or contact discontinuities at finite times. We
demonstrate that GoRINNs provide a very high accuracy both in the smooth and
discontinuous regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Equivalent Training Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09160v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09160v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William T. Redman, Juan M. Bello-Rivas, Maria Fonoberova, Ryan Mohr, Ioannis G. Kevrekidis, Igor Mezić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Study of the nonlinear evolution deep neural network (DNN) parameters undergo
during training has uncovered regimes of distinct dynamical behavior. While a
detailed understanding of these phenomena has the potential to advance
improvements in training efficiency and robustness, the lack of methods for
identifying when DNN models have equivalent dynamics limits the insight that
can be gained from prior work. Topological conjugacy, a notion from dynamical
systems theory, provides a precise definition of dynamical equivalence,
offering a possible route to address this need. However, topological
conjugacies have historically been challenging to compute. By leveraging
advances in Koopman operator theory, we develop a framework for identifying
conjugate and non-conjugate training dynamics. To validate our approach, we
demonstrate that comparing Koopman eigenvalues can correctly identify a known
equivalence between online mirror descent and online gradient descent. We then
utilize our approach to: (a) identify non-conjugate training dynamics between
shallow and wide fully connected neural networks; (b) characterize the early
phase of training dynamics in convolutional neural networks; (c) uncover
non-conjugate training dynamics in Transformers that do and do not undergo
grokking. Our results, across a range of DNN architectures, illustrate the
flexibility of our framework and highlight its potential for shedding new light
on training dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures, 6 supplemental figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProgressGym: Alignment with a Millennium of Moral Progress <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Qiu, Yang Zhang, Xuchuan Huang, Jasmine Xinze Li, Jiaming Ji, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frontier AI systems, including large language models (LLMs), hold increasing
influence over the epistemology of human users. Such influence can reinforce
prevailing societal values, potentially contributing to the lock-in of
misguided moral beliefs and, consequently, the perpetuation of problematic
moral practices on a broad scale. We introduce progress alignment as a
technical solution to mitigate this imminent risk. Progress alignment
algorithms learn to emulate the mechanics of human moral progress, thereby
addressing the susceptibility of existing alignment methods to contemporary
moral blindspots. To empower research in progress alignment, we introduce
ProgressGym, an experimental framework allowing the learning of moral progress
mechanics from history, in order to facilitate future progress in real-world
moral decisions. Leveraging 9 centuries of historical text and 18 historical
LLMs, ProgressGym enables codification of real-world progress alignment
challenges into concrete benchmarks. Specifically, we introduce three core
challenges: tracking evolving values (PG-Follow), preemptively anticipating
moral progress (PG-Predict), and regulating the feedback loop between human and
AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension
are inapplicable to these tasks. In response, we present lifelong and
extrapolative algorithms as baseline methods of progress alignment, and build
an open leaderboard soliciting novel algorithms and challenges. The framework
and the leaderboard are available at
https://github.com/PKU-Alignment/ProgressGym and
https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blind Inpainting with Object-aware Discrimination for Artificial Marker
  Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuechen Guo, Wenhao Hu, Chiming Ni, Wenhao Chai, Shiyan Li, Gaoang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images often incorporate doctor-added markers that can hinder
AI-based diagnosis. This issue highlights the need of inpainting techniques to
restore the corrupted visual contents. However, existing methods require manual
mask annotation as input, limiting the application scenarios. In this paper, we
propose a novel blind inpainting method that automatically reconstructs visual
contents within the corrupted regions without mask input as guidance. Our model
includes a blind reconstruction network and an object-aware discriminator for
adversarial training. The reconstruction network contains two branches that
predict corrupted regions in images and simultaneously restore the missing
visual contents. Leveraging the potent recognition capability of a dense object
detector, the object-aware discriminator ensures markers undetectable after
inpainting. Thus, the restored images closely resemble the clean ones. We
evaluate our method on three datasets of various medical imaging modalities,
confirming better performance over other state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Activity Recognition After Stroke: Generative Adversarial
  Networks for Kinematic Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron J. Hadley, Christopher L. Pulliam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalizability of machine learning (ML) models for wearable monitoring
in stroke rehabilitation is often constrained by the limited scale and
heterogeneity of available data. Data augmentation addresses this challenge by
adding computationally derived data to real data to enrich the variability
represented in the training set. Traditional augmentation methods, such as
rotation, permutation, and time-warping, have shown some benefits in improving
classifier performance, but often fail to produce realistic training examples.
This study employs Conditional Generative Adversarial Networks (cGANs) to
create synthetic kinematic data from a publicly available dataset, closely
mimicking the experimentally measured reaching movements of stroke survivors.
This approach not only captures the complex temporal dynamics and common
movement patterns after stroke, but also significantly enhances the training
dataset. By training deep learning models on both synthetic and experimental
data, we enhanced task classification accuracy: models incorporating synthetic
data attained an overall accuracy of 80.0%, significantly higher than the 66.1%
seen in models trained solely with real data. These improvements allow for more
precise task classification, offering clinicians the potential to monitor
patient progress more accurately and tailor rehabilitation interventions more
effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 3 tables; resubmitted to Sensors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular
  Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Holzmüller, Léo Grinsztajn, Ingo Steinwart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For classification and regression on tabular data, the dominance of
gradient-boosted decision trees (GBDTs) has recently been challenged by often
much slower deep learning methods with extensive hyperparameter tuning. We
address this discrepancy by introducing (a) RealMLP, an improved multilayer
perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and
RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark
with 118 datasets and compare them to hyperparameter-optimized versions on a
disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly
benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large
tabular datasets (1K--500K samples) show that RealMLP offers a favorable
time-accuracy tradeoff compared to other neural baselines and is competitive
with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and
GBDTs with improved default parameters can achieve excellent results without
hyperparameter tuning. Finally, we demonstrate that some of RealMLP's
improvements can also considerably improve the performance of TabR with default
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Changes in v2: more baselines, more ablations,
  introduced RealTabR-D, updated Grinstzajn benchmark protocol, updated
  text/figures. Code is available at github.com/dholzmueller/pytabkit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annealed Multiple Choice Learning: Overcoming limitations of
  Winner-takes-all with annealing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Perera, Victor Letzelter, Théo Mariotte, Adrien Cortés, Mickael Chen, Slim Essid, Gaël Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Annealed Multiple Choice Learning (aMCL) which combines
simulated annealing with MCL. MCL is a learning framework handling ambiguous
tasks by predicting a small set of plausible hypotheses. These hypotheses are
trained using the Winner-takes-all (WTA) scheme, which promotes the diversity
of the predictions. However, this scheme may converge toward an arbitrarily
suboptimal local minimum, due to the greedy nature of WTA. We overcome this
limitation using annealing, which enhances the exploration of the hypothesis
space during training. We leverage insights from statistical physics and
information theory to provide a detailed description of the model training
trajectory. Additionally, we validate our algorithm by extensive experiments on
synthetic datasets, on the standard UCI benchmark, and on speech separation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large
  Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Jin, Zheng Li, Chenwei Zhang, Tianyu Cao, Yifan Gao, Pratik Jayarao, Mao Li, Xin Liu, Ritesh Sarkhel, Xianfeng Tang, Haodong Wang, Zhengyang Wang, Wenju Xu, Jingfeng Yang, Qingyu Yin, Xian Li, Priyanka Nigam, Yi Xu, Kai Chen, Qiang Yang, Meng Jiang, Bing Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online shopping is a complex multi-task, few-shot learning problem with a
wide and evolving range of entities, relations, and tasks. However, existing
models and benchmarks are commonly tailored to specific tasks, falling short of
capturing the full complexity of online shopping. Large Language Models (LLMs),
with their multi-task and few-shot learning abilities, have the potential to
profoundly transform online shopping by alleviating task-specific engineering
efforts and by providing users with interactive conversations. Despite the
potential, LLMs face unique challenges in online shopping, such as
domain-specific concepts, implicit knowledge, and heterogeneous user behaviors.
Motivated by the potential and challenges, we propose Shopping MMLU, a diverse
multi-task online shopping benchmark derived from real-world Amazon data.
Shopping MMLU consists of 57 tasks covering 4 major shopping skills: concept
understanding, knowledge reasoning, user behavior alignment, and
multi-linguality, and can thus comprehensively evaluate the abilities of LLMs
as general shop assistants. With Shopping MMLU, we benchmark over 20 existing
LLMs and uncover valuable insights about practices and prospects of building
versatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at
https://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we
host a competition in KDD Cup 2024 with over 500 participating teams. The
winning solutions and the associated workshop can be accessed at our website
https://amazon-kddcup24.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track Accepted. Modified typos
  in Figure 9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Analysis of Fairness Notions under Differential Privacy <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anderson Santana de Oliveira, Caelin Kaplan, Khawla Mallat, Tanmay Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that selecting an optimal model architecture suited
to the differential privacy setting is necessary to achieve the best possible
utility for a given privacy budget using differentially private stochastic
gradient descent (DP-SGD)(Tramer and Boneh 2020; Cheng et al. 2022). In light
of these findings, we empirically analyse how different fairness notions,
belonging to distinct classes of statistical fairness criteria (independence,
separation and sufficiency), are impacted when one selects a model architecture
suitable for DP-SGD, optimized for utility. Using standard datasets from ML
fairness literature, we show using a rigorous experimental protocol, that by
selecting the optimal model architecture for DP-SGD, the differences across
groups concerning the relevant fairness metrics (demographic parity, equalized
odds and predictive parity) more often decrease or are negligibly impacted,
compared to the non-private baseline, for which optimal model architecture has
also been selected to maximize utility. These findings challenge the
understanding that differential privacy will necessarily exacerbate unfairness
in deep learning models trained on biased datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at the The Fourth AAAI Workshop on
  Privacy-Preserving Artificial Intelligence (PPAI-23)
  https://aaai-ppai23.github.io/#accepted_papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Pfaffians: Solving Many Many-Electron Schrödinger Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Gao, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural wave functions accomplished unprecedented accuracies in approximating
the ground state of many-electron systems, though at a high computational cost.
Recent works proposed amortizing the cost by learning generalized wave
functions across different structures and compounds instead of solving each
problem independently. Enforcing the permutation antisymmetry of electrons in
such generalized neural wave functions remained challenging as existing methods
require discrete orbital selection via non-learnable hand-crafted algorithms.
This work tackles the problem by defining overparametrized, fully learnable
neural wave functions suitable for generalization across molecules. We achieve
this by relying on Pfaffians rather than Slater determinants. The Pfaffian
allows us to enforce the antisymmetry on arbitrary electronic systems without
any constraint on electronic spin configurations or molecular structure. Our
empirical evaluation finds that a single neural Pfaffian calculates the ground
state and ionization energies with chemical accuracy across various systems. On
the TinyMol dataset, we outperform the `gold-standard' CCSD(T) CBS reference
energies by 1.9m$E_h$ and reduce energy errors compared to previous generalized
neural wave functions by up to an order of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Adaptive Regularization for Safe Control of
  Critical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15199v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15199v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Tian, Homayoun Hamedmoghadam, Robert Shorten, Pietro Ferraro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) is a powerful method for controlling dynamic
systems, but its learning mechanism can lead to unpredictable actions that
undermine the safety of critical systems. Here, we propose RL with Adaptive
Regularization (RL-AR), an algorithm that enables safe RL exploration by
combining the RL policy with a policy regularizer that hard-codes the safety
constraints. RL-AR performs policy combination via a "focus module," which
determines the appropriate combination depending on the state--relying more on
the safe policy regularizer for less-exploited states while allowing unbiased
convergence for well-exploited states. In a series of critical control
applications, we demonstrate that RL-AR not only ensures safety during training
but also achieves a return competitive with the standards of model-free RL that
disregards safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovery of the Hidden World with Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Liu, Yongqiang Chen, Tongliang Liu, Mingming Gong, James Cheng, Bo Han, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Revealing the underlying causal mechanisms in the real world is the key to
the development of science. Despite the progress in the past decades,
traditional causal discovery approaches (CDs) mainly rely on high-quality
measured variables, usually given by human experts, to find causal relations.
The lack of well-defined high-level variables in many real-world applications
has already been a longstanding roadblock to a broader application of CDs. To
this end, this paper presents Causal representatiOn AssistanT (COAT) that
introduces large language models (LLMs) to bridge the gap. LLMs are trained on
massive observations of the world and have demonstrated great capability in
extracting key information from unstructured data. Therefore, it is natural to
employ LLMs to assist with proposing useful high-level factors and crafting
their measurements. Meanwhile, COAT also adopts CDs to find causal relations
among the identified variables as well as to provide feedback to LLMs to
iteratively refine the proposed factors. We show that LLMs and CDs are mutually
beneficial and the constructed feedback provably also helps with the factor
proposal. We construct and curate several synthetic and real-world benchmarks
including analysis of human reviews and diagnosis of neuropathic and brain
tumors, to comprehensively evaluate COAT. Extensive empirical results confirm
the effectiveness and reliability of COAT with significant improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; Chenxi and Yongqiang contributed equally; 59 pages, 72
  figures; Project page: https://causalcoat.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EPIC: Effective <span class="highlight-title">Prompt</span>ing for Imbalanced-Class Data Synthesis in Tabular
  Data Classification via Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12404v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12404v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhee Kim, Taesung Kim, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable in-context learning
capabilities across diverse applications. In this work, we explore the
effectiveness of LLMs for generating realistic synthetic tabular data,
identifying key prompt design elements to optimize performance. We introduce
EPIC, a novel approach that leverages balanced, grouped data samples and
consistent formatting with unique variable mapping to guide LLMs in generating
accurate synthetic data across all classes, even for imbalanced datasets.
Evaluations on real-world datasets show that EPIC achieves state-of-the-art
machine learning classification performance, significantly improving generation
efficiency. These findings highlight the effectiveness of EPIC for synthetic
tabular data generation, particularly in addressing class imbalance. Our source
code for our work is available at:
https://seharanul17.github.io/project-synthetic-tabular-llm/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fight Back Against Jailbreaking via <span class="highlight-title">Prompt</span> Adversarial Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06255v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06255v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Mo, Yuji Wang, Zeming Wei, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have achieved tremendous success in
various applications, they are also susceptible to jailbreaking attacks.
Several primary defense strategies have been proposed to protect LLMs from
producing harmful information, mostly focusing on model fine-tuning or
heuristical defense designs. However, how to achieve intrinsic robustness
through prompt optimization remains an open problem. In this paper, motivated
by adversarial training paradigms for achieving reliable robustness, we propose
an approach named Prompt Adversarial Tuning (PAT) that trains a prompt control
attached to the user prompt as a guard prefix. To achieve our defense goal
whilst maintaining natural performance, we optimize the control prompt with
both adversarial and benign prompts. Comprehensive experiments show that our
method is effective against both grey-box and black-box attacks, reducing the
success rate of advanced attacks to nearly 0%, while maintaining the model's
utility on the benign task and incurring only negligible computational
overhead, charting a new perspective for future explorations in LLM security.
Our code is available at https://github.com/PKU-ML/PAT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can In-context Learning Really Generalize to Out-of-distribution Tasks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qixun Wang, Yifei Wang, Yisen Wang, Xianghua Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore the mechanism of in-context learning (ICL) on
out-of-distribution (OOD) tasks that were not encountered during training. To
achieve this, we conduct synthetic experiments where the objective is to learn
OOD mathematical functions through ICL using a GPT-2 model. We reveal that
Transformers may struggle to learn OOD task functions through ICL.
Specifically, ICL performance resembles implementing a function within the
pretraining hypothesis space and optimizing it with gradient descent based on
the in-context examples. Additionally, we investigate ICL's well-documented
ability to learn unseen abstract labels in context. We demonstrate that such
ability only manifests in the scenarios without distributional shifts and,
therefore, may not serve as evidence of new-task-learning ability. Furthermore,
we assess ICL's performance on OOD tasks when the model is pretrained on
multiple tasks. Both empirical and theoretical analyses demonstrate the
existence of the \textbf{low-test-error preference} of ICL, where it tends to
implement the pretraining function that yields low test error in the testing
context. We validate this through numerical experiments. This new theoretical
result, combined with our empirical findings, elucidates the mechanism of ICL
in addressing OOD tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controlling Continuous Relaxation for Combinatorial Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16965v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16965v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuma Ichikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised learning (UL)-based solvers for combinatorial optimization (CO)
train a neural network that generates a soft solution by directly optimizing
the CO objective using a continuous relaxation strategy. These solvers offer
several advantages over traditional methods and other learning-based methods,
particularly for large-scale CO problems. However, UL-based solvers face two
practical issues: (I) an optimization issue, where UL-based solvers are easily
trapped at local optima, and (II) a rounding issue, where UL-based solvers
require artificial post-learning rounding from the continuous space back to the
original discrete space, undermining the robustness of the results. This study
proposes a Continuous Relaxation Annealing (CRA) strategy, an effective
rounding-free learning method for UL-based solvers. CRA introduces a penalty
term that dynamically shifts from prioritizing continuous solutions,
effectively smoothing the non-convexity of the objective function, to enforcing
discreteness, eliminating artificial rounding. Experimental results demonstrate
that CRA significantly enhances the performance of UL-based solvers,
outperforming existing UL-based solvers and greedy algorithms in complex CO
problems. Additionally, CRA effectively eliminates artificial rounding and
accelerates the learning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-based Reinforcement Learning via Q-weighted Variational Policy
  Optimization <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutong Ding, Ke Hu, Zhenhao Zhang, Kan Ren, Weinan Zhang, Jingyi Yu, Jingya Wang, Ye Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have garnered widespread attention in Reinforcement Learning
(RL) for their powerful expressiveness and multimodality. It has been verified
that utilizing diffusion policies can significantly improve the performance of
RL algorithms in continuous control tasks by overcoming the limitations of
unimodal policies, such as Gaussian policies, and providing the agent with
enhanced exploration capabilities. However, existing works mainly focus on the
application of diffusion policies in offline RL, while their incorporation into
online RL is less investigated. The training objective of the diffusion model,
known as the variational lower bound, cannot be optimized directly in online RL
due to the unavailability of 'good' actions. This leads to difficulties in
conducting diffusion policy improvement. To overcome this, we propose a novel
model-free diffusion-based online RL algorithm, Q-weighted Variational Policy
Optimization (QVPO). Specifically, we introduce the Q-weighted variational
loss, which can be proved to be a tight lower bound of the policy objective in
online RL under certain conditions. To fulfill these conditions, the Q-weight
transformation functions are introduced for general scenarios. Additionally, to
further enhance the exploration capability of the diffusion policy, we design a
special entropy regularization term. We also develop an efficient behavior
policy to enhance sample efficiency by reducing the variance of the diffusion
policy during online interactions. Consequently, the QVPO algorithm leverages
the exploration capabilities and multimodality of diffusion policies,
preventing the RL agent from converging to a sub-optimal policy. To verify the
effectiveness of QVPO, we conduct comprehensive experiments on MuJoCo
benchmarks. The final results demonstrate that QVPO achieves state-of-the-art
performance on both cumulative reward and sample efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NM-FlowGAN: Modeling sRGB Noise without Paired Images using a Hybrid
  Approach of Normalizing Flows and GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10112v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10112v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young Joo Han, Ha-Jin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling and synthesizing real sRGB noise is crucial for various low-level
vision tasks, such as building datasets for training image denoising systems.
The distribution of real sRGB noise is highly complex and affected by a
multitude of factors, making its accurate modeling extremely challenging.
Therefore, recent studies have proposed methods that employ data-driven
generative models, such as Generative Adversarial Networks (GAN) and
Normalizing Flows. These studies achieve more accurate modeling of sRGB noise
compared to traditional noise modeling methods. However, there are performance
limitations due to the inherent characteristics of each generative model. To
address this issue, we propose NM-FlowGAN, a hybrid approach that exploits the
strengths of both GAN and Normalizing Flows. We combine pixel-wise noise
modeling networks based on Normalizing Flows and spatial correlation modeling
networks based on GAN. Specifically, the pixel-wise noise modeling network
leverages the high training stability of Normalizing Flows to capture noise
characteristics that are affected by a multitude of factors, and the spatial
correlation networks efficiently model pixel-to-pixel relationships. In
particular, unlike recent methods that rely on paired noisy images, our method
synthesizes noise using clean images and factors that affect noise
characteristics, such as easily obtainable parameters like camera type and ISO
settings, making it applicable to various fields where obtaining noisy-clean
image pairs is not feasible. In our experiments, our NM-FlowGAN outperforms
other baselines in the sRGB noise synthesis task. Moreover, the denoising
neural network trained with synthesized image pairs from our model shows
superior performance compared to other baselines. Our code is available at:
\url{https://github.com/YoungJooHan/NM-FlowGAN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enriching Disentanglement: From Logical Definitions to Quantitative
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yivan Zhang, Masashi Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disentangling the explanatory factors in complex data is a promising approach
for generalizable and data-efficient representation learning. While a variety
of quantitative metrics for learning and evaluating disentangled
representations have been proposed, it remains unclear what properties these
metrics truly quantify. In this work, we establish algebraic relationships
between logical definitions and quantitative metrics to derive theoretically
grounded disentanglement metrics. Concretely, we introduce a compositional
approach for converting a higher-order predicate into a real-valued quantity by
replacing (i) equality with a strict premetric, (ii) the Heyting algebra of
binary truth values with a quantale of continuous values, and (iii) quantifiers
with aggregators. The metrics induced by logical definitions have strong
theoretical guarantees, and some of them are easily differentiable and can be
used as learning objectives directly. Finally, we empirically demonstrate the
effectiveness of the proposed metrics by isolating different aspects of
disentangled representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neural Information Processing Systems 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptive
  Residual Module <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05368v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05368v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingbo Zhou, Yixuan Du, Ruqiong Zhang, Jun Xia, Zhizhi Yu, Zelin Zang, Di Jin, Carl Yang, Rui Zhang, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs), a type of neural network that can learn from
graph-structured data through neighborhood information aggregation, have shown
superior performance in various downstream tasks. However, as the number of
layers increases, node representations become indistinguishable, which is known
as over-smoothing. To address this issue, many residual methods have emerged.
In this paper, we focus on the over-smoothing issue and related residual
methods. Firstly, we revisit over-smoothing from the perspective of overlapping
neighborhood subgraphs, and based on this, we explain how residual methods can
alleviate over-smoothing by integrating multiple orders neighborhood subgraphs
to avoid the indistinguishability of the single high-order neighborhood
subgraphs. Additionally, we reveal the drawbacks of previous residual methods,
such as the lack of node adaptability and severe loss of high-order
neighborhood subgraph information, and propose a
\textbf{Posterior-Sampling-based, Node-Adaptive Residual module (PSNR)}. We
theoretically demonstrate that PSNR can alleviate the drawbacks of previous
residual methods. Furthermore, extensive experiments verify the superiority of
the PSNR module in fully observed node classification and missing feature
scenarios. Our code is available at https://github.com/jingbo02/PSNR-GNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transition Constrained Bayesian Optimization via Markov Decision
  Processes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Pablo Folch, Calvin Tsay, Robert M Lee, Behrang Shafei, Weronika Ormaniec, Andreas Krause, Mark van der Wilk, Ruth Misener, Mojmír Mutný
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization is a methodology to optimize black-box functions.
Traditionally, it focuses on the setting where you can arbitrarily query the
search space. However, many real-life problems do not offer this flexibility;
in particular, the search space of the next query may depend on previous ones.
Example challenges arise in the physical sciences in the form of local movement
constraints, required monotonicity in certain variables, and transitions
influencing the accuracy of measurements. Altogether, such transition
constraints necessitate a form of planning. This work extends classical
Bayesian optimization via the framework of Markov Decision Processes. We
iteratively solve a tractable linearization of our utility function using
reinforcement learning to obtain a policy that plans ahead for the entire
horizon. This is a parallel to the optimization of an acquisition function in
policy space. The resulting policy is potentially history-dependent and
non-Markovian. We showcase applications in chemical reactor optimization,
informative path planning, machine calibration, and other synthetic examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages main, 34 pages total, 17 figures, 2 tables. Accepted to
  NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 'No' Matters: Out-of-Distribution Detection in Multimodality Long
  Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rena Gao, Xuetong Wu, Siwen Luo, Caren Han, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection in multimodal contexts is essential for
identifying deviations in combined inputs from different modalities,
particularly in applications like open-domain dialogue systems or real-life
dialogue interactions. This paper aims to improve the user experience that
involves multi-round long dialogues by efficiently detecting OOD dialogues and
images. We introduce a novel scoring framework named Dialogue Image Aligning
and Enhancing Framework (DIAEF) that integrates the visual language models with
the novel proposed scores that detect OOD in two key scenarios (1) mismatches
between the dialogue and image input pair and (2) input pairs with previously
unseen labels. Our experimental results, derived from various benchmarks,
demonstrate that integrating image and multi-round dialogue OOD detection is
more effective with previously unseen labels than using either modality
independently. In the presence of mismatched pairs, our proposed score
effectively identifies these mismatches and demonstrates strong robustness in
long dialogues. This approach enhances domain-aware, adaptive conversational
agents and establishes baselines for future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have demonstrated the ability to interact with
humans under real-world conditions by combining Large Language Models (LLMs)
and modality encoders to align multimodal information (visual and auditory)
with text. However, such models raise new safety challenges of whether models
that are safety-aligned on text also exhibit consistent safeguards for
multimodal inputs. Despite recent safety-alignment research on vision LMMs, the
safety of audio LMMs remains under-explored. In this work, we comprehensively
red team the safety of five advanced audio LMMs under three settings: (i)
harmful questions in both audio and text formats, (ii) harmful questions in
text format accompanied by distracting non-speech audio, and (iii)
speech-specific jailbreaks. Our results under these settings demonstrate that
open-source audio LMMs suffer an average attack success rate of 69.14% on
harmful audio questions, and exhibit safety vulnerabilities when distracted
with non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro
achieve an attack success rate of 70.67% on the harmful query benchmark. We
provide insights on what could cause these reported safety-misalignments.
Warning: this paper contains offensive examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Nie, Jiangqun Ni, Jian Zhang, Bin Zhang, Weizhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of deepfake generation techniques, the importance of
deepfake detection in protecting multimedia content integrity has become
increasingly obvious. Recently, temporal inconsistency clues have been explored
to improve the generalizability of deepfake video detection. According to our
observation, the temporal artifacts of forged videos in terms of motion
information usually exhibits quite distinct inconsistency patterns along
horizontal and vertical directions, which could be leveraged to improve the
generalizability of detectors. In this paper, a transformer-based framework for
Diffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits
directional inconsistencies for deepfake video detection. Specifically, DIP
begins with a spatiotemporal encoder to represent spatiotemporal information. A
directional inconsistency decoder is adopted accordingly, where direction-aware
attention and inconsistency diffusion are incorporated to explore potential
inconsistency patterns and jointly learn the inherent relationships. In
addition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to
contrast spatiotemporally augmented sample pairs and prevent the model from
overfitting nonessential forgery artifacts. Extensive experiments on several
public datasets demonstrate that our method could effectively identify
directional forgery clues and achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, accepted with IEEE Trans. on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Audio-Visual Joint Representations with an Agentic Workflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shentong Mo, Yibing Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual content and accompanied audio signals naturally formulate a joint
representation to improve audio-visual (AV) related applications. While studies
develop various AV representation learning frameworks, the importance of AV
data alignment is usually undermined for achieving high-quality representation.
We observe that an audio signal may contain background noise interference.
Also, non-synchronization may appear between audio and video streams. These
non-strict data alignment limits representation quality and downgrade
application performance. In this paper, we propose to improve AV joint
representations from a data-centric perspective by aligning audio signals to
visual data. Our alignment is conducted in an agentic workflow controlled by an
LLM-based assistant named AVAgent. For each input AV data pair, our AVAgent
uses a multi-modal LLM to convert audio and visual data into language
descriptions separately (i.e., tool use). Then, AVAgent reasons whether this
paired data is aligned well and plans to edit the audio signal if needed (i.e.,
planning). The audio editing is executed by predefined actions that filter
noise or augment data. Moreover, we use a VLM to evaluate how modified audio
signals match the visual content and provide feedback to AVAgent (i.e.,
reflection). The tool use, planning, and reflection steps operate cyclically to
become an agentic workflow where audio signals are gradually aligned to visual
content. To this end, existing methods can directly leverage the aligned AV
data via our agentic workflow to improve AV joint representations. The
experimental results comprehensively demonstrate the state-of-the-art
performance of the proposed approach against previous baselines in diverse
downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-30T00:00:00Z">2024-10-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural spell-checker: Beyond words with synthetic data generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matej Klemen, Martin Božič, Špela Arhar Holdt, Marko Robnik-Šikonja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spell-checkers are valuable tools that enhance communication by identifying
misspelled words in written texts. Recent improvements in deep learning, and in
particular in large language models, have opened new opportunities to improve
traditional spell-checkers with new functionalities that not only assess
spelling correctness but also the suitability of a word for a given context. In
our work, we present and compare two new spell-checkers and evaluate them on
synthetic, learner, and more general-domain Slovene datasets. The first
spell-checker is a traditional, fast, word-based approach, based on a
morphological lexicon with a significantly larger word list compared to
existing spell-checkers. The second approach uses a language model trained on a
large corpus with synthetically inserted errors. We present the training data
construction strategies, which turn out to be a crucial component of neural
spell-checkers. Further, the proposed neural model significantly outperforms
all existing spell-checkers for Slovene in both precision and recall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version. Accepted to TSD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Strategy Planning for Efficient Question Answering with Large
  Language Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmay Parekh, Pradyot Prakash, Alexander Radovic, Akshay Shekher, Denis Savenkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought),
planning (e.g., SelfAsk), and retrieval augmented generation strategies to
improve the performance of Large Language Models (LLMs) on various tasks, such
as question answering. However, using a single fixed strategy to answer
different kinds of questions is suboptimal in performance and inefficient in
terms of generated output tokens and performed retrievals. In our work, we
propose a novel technique DyPlan, to induce a dynamic strategy selection
process in LLMs, to improve performance and reduce costs in question-answering.
DyPlan incorporates an initial decision step to select the most suitable
strategy conditioned on the input question and guides the LLM's response
generation accordingly. We extend DyPlan to DyPlan-verify, adding an internal
verification and correction process to further enrich the generated answer.
Experiments on three prominent multi-hop question answering (MHQA) datasets
reveal how DyPlan can improve model performance by 7-13% while reducing the
cost by 11-32% relative to the best baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ACL Rolling Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tiny <span class="highlight-title">Transformer</span>s Excel at Sentence Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Belcak, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is staggering that words of the English language, which are on average
represented by 5--6 bytes of ASCII, require as much as 24 kilobytes when served
to large language models. We show that there is room for more information in
every token embedding. We demonstrate that 1--3-layer transformers are capable
of encoding and subsequently decoding standard English sentences into as little
as a single 3-kilobyte token. Our work implies that even small networks can
learn to construct valid English sentences and suggests the possibility of
optimising large language models by moving from sub-word token embeddings
towards larger fragments of text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Interpretable Grammatical Error Correction with Mixture of
  Experts <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Reza Qorib, Alham Fikri Aji, Hwee Tou Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error type information has been widely used to improve the performance of
grammatical error correction (GEC) models, whether for generating corrections,
re-ranking them, or combining GEC models. Combining GEC models that have
complementary strengths in correcting different error types is very effective
in producing better corrections. However, system combination incurs a high
computational cost due to the need to run inference on the base systems before
running the combination method itself. Therefore, it would be more efficient to
have a single model with multiple sub-networks that specialize in correcting
different error types. In this paper, we propose a mixture-of-experts model,
MoECE, for grammatical error correction. Our model successfully achieves the
performance of T5-XL with three times fewer effective parameters. Additionally,
our model produces interpretable corrections by also identifying the error type
during inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Achieve Goals with Belief State <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward S. Hu, Kwangjun Ahn, Qinghua Liu, Haoran Xu, Manan Tomar, Ada Langford, Dinesh Jayaraman, Alex Lamb, John Langford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the "Belief State Transformer", a next-token predictor that
takes both a prefix and suffix as inputs, with a novel objective of predicting
both the next token for the prefix and the previous token for the suffix. The
Belief State Transformer effectively learns to solve challenging problems that
conventional forward-only transformers struggle with, in a domain-independent
fashion. Key to this success is learning a compact belief state that captures
all relevant information necessary for accurate predictions. Empirical
ablations show that each component of the model is essential in difficult
scenarios where standard Transformers fall short. For the task of story writing
with known prefixes and suffixes, our approach outperforms the
Fill-in-the-Middle method for reaching known goals and demonstrates improved
performance even when the goals are unknown. Altogether, the Belief State
Transformer enables more efficient goal-conditioned decoding, better test-time
inference, and high-quality text representations on small scale problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All or None: Identifiable Linear Properties of Next-token Predictors in
  Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Marconato, Sébastien Lachapelle, Sebastian Weichwald, Luigi Gresele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze identifiability as a possible explanation for the ubiquity of
linear properties across language models, such as the vector difference between
the representations of "easy" and "easiest" being parallel to that between
"lucky" and "luckiest". For this, we ask whether finding a linear property in
one model implies that any model that induces the same distribution has that
property, too. To answer that, we first prove an identifiability result to
characterize distribution-equivalent next-token predictors, lifting a diversity
requirement of previous results. Second, based on a refinement of relational
linearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how
many notions of linearity are amenable to our analysis. Finally, we show that
under suitable conditions, these linear properties either hold in all or none
distribution-equivalent next-token predictors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smaller Large Language Models Can Do Moral Self-Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangliang Liu, Zhiyu Xue, Rongrong Wang, Kristen Marie Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-correction is one of the most amazing emerging capabilities of Large
Language Models (LLMs), enabling LLMs to self-modify an inappropriate output
given a natural language feedback which describes the problems of that output.
Moral self-correction is a post-hoc approach correcting unethical generations
without requiring a gradient update, making it both computationally lightweight
and capable of preserving the language modeling ability. Previous works have
shown that LLMs can self-debias, and it has been reported that small models,
i.e., those with less than 22B parameters, are not capable of moral
self-correction. However, there is no direct proof as to why such smaller
models fall short of moral self-correction, though previous research
hypothesizes that larger models are skilled in following instructions and
understanding abstract social norms. In this paper, we empirically validate
this hypothesis in the context of social stereotyping, through meticulous
prompting. Our experimental results indicate that (i) surprisingly, 3.8B LLMs
with proper safety alignment fine-tuning can achieve very good moral
self-correction performance, highlighting the significant effects of safety
alignment; and (ii) small LLMs are indeed weaker than larger-scale models in
terms of comprehending social norms and self-explanation through CoT, but all
scales of LLMs show bad self-correction performance given unethical
instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collage: Decomposable Rapid Prototyping for Information Extraction on
  Scientific PDFs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sireesh Gururaja, Yueheng Zhang, Guannan Tang, Tianhao Zhang, Kevin Murphy, Yu-Tsen Yi, Junwon Seo, Anthony Rollett, Emma Strubell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years in NLP have seen the continued development of domain-specific
information extraction tools for scientific documents, alongside the release of
increasingly multimodal pretrained transformer models. While the opportunity
for scientists outside of NLP to evaluate and apply such systems to their own
domains has never been clearer, these models are difficult to compare: they
accept different input formats, are often black-box and give little insight
into processing failures, and rarely handle PDF documents, the most common
format of scientific publication. In this work, we present Collage, a tool
designed for rapid prototyping, visualization, and evaluation of different
information extraction models on scientific PDFs. Collage allows the use and
evaluation of any HuggingFace token classifier, several LLMs, and multiple
other task-specific models out of the box, and provides extensible software
interfaces to accelerate experimentation with new models. Further, we enable
both developers and users of NLP-based tools to inspect, debug, and better
understand modeling pipelines by providing granular views of intermediate
states of processing. We demonstrate our system in the context of information
extraction to assist with literature review in materials science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MDCure: A Scalable Pipeline for Multi-Document Instruction-Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrielle Kaili-May Liu, Bowen Shi, Avi Caciularu, Idan Szpektor, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-document (MD) processing is crucial for LLMs to handle real-world tasks
such as summarization and question-answering across large sets of documents.
While LLMs have improved at processing long inputs, MD contexts still present
challenges, such as managing inter-document dependencies, redundancy, and
incoherent structures. We introduce MDCure, a scalable and effective
fine-tuning pipeline to enhance the MD capabilities of LLMs without the
computational cost of pre-training or reliance on human annotated data. MDCure
is based on generation of high-quality synthetic MD instruction data from sets
of related articles via targeted prompts. We further introduce MDCureRM, a
multi-objective reward model which filters generated data based on their
training utility for MD settings. With MDCure, we fine-tune a variety of LLMs,
from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in
size. Extensive evaluations on a wide range of MD and long-context benchmarks
spanning various tasks show MDCure consistently improves performance over
pre-trained baselines and over corresponding base models by up to 75.5%. Our
code, datasets, and models are available at https://github.com/yale-nlp/MDCure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Augmented Relation Extraction Model with LLMs-Generated Support
  Document 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicky Dong, Hao Yu, Yao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a novel approach to sentence-level relation extraction
(RE) that integrates Graph Neural Networks (GNNs) with Large Language Models
(LLMs) to generate contextually enriched support documents. By harnessing the
power of LLMs to generate auxiliary information, our approach crafts an
intricate graph representation of textual data. This graph is subsequently
processed through a Graph Neural Network (GNN) to refine and enrich the
embeddings associated with each entity ensuring a more nuanced and
interconnected understanding of the data. This methodology addresses the
limitations of traditional sentence-level RE models by incorporating broader
contexts and leveraging inter-entity interactions, thereby improving the
model's ability to capture complex relationships across sentences. Our
experiments, conducted on the CrossRE dataset, demonstrate the effectiveness of
our approach, with notable improvements in performance across various domains.
The results underscore the potential of combining GNNs with LLM-generated
context to advance the field of relation extraction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning and Transferring Sparse Contextual Bigrams with Linear
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunwei Ren, Zixuan Wang, Jason D. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have excelled in natural language modeling and one reason behind
this success is their exceptional ability to combine contextual informal and
global knowledge. However, the theoretical basis remains unclear. In this
paper, first we introduce the Sparse Contextual Bigram (SCB), a natural
extension of the classical bigram model, where the next token's generation
depends on a sparse set of earlier positions determined by the last token. We
then analyze the training dynamics and sample complexity of learning SCB using
a one-layer linear transformer with a gradient-based algorithm. We show that
when trained from scratch, the training process can be split into an initial
sample-intensive stage where the correlation is boosted from zero to a
nontrivial value, followed by a more sample-efficient stage of further
improvement. Additionally, we prove that, provided a nontrivial correlation
between the downstream and pretraining tasks, finetuning from a pretrained
model allows us to bypass the initial sample-intensive stage. We also
empirically demonstrate that our algorithm can outperform SGD in this setting
and discuss its relationship with the usual softmax-based transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind the Gap: A Generalized Approach for Cross-Modal Embedding Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arihan Yadav, Alan McMillan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems enhance text generation by
incorporating external knowledge but often struggle when retrieving context
across different text modalities due to semantic gaps. We introduce a
generalized projection-based method, inspired by adapter modules in transfer
learning, that efficiently bridges these gaps between various text types, such
as programming code and pseudocode, or English and French sentences. Our
approach emphasizes speed, accuracy, and data efficiency, requiring minimal
resources for training and inference. By aligning embeddings from heterogeneous
text modalities into a unified space through a lightweight projection network,
our model significantly outperforms traditional retrieval methods like the
Okapi BM25 algorithm and models like Dense Passage Retrieval (DPR), while
approaching the accuracy of Sentence Transformers. Extensive evaluations
demonstrate the effectiveness and generalizability of our method across
different tasks, highlighting its potential for real-time, resource-constrained
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social Science Meets LLMs: How Reliable Are Large Language Models in
  Social Simulations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Huang, Zhengqing Yuan, Yujun Zhou, Kehan Guo, Xiangqi Wang, Haomin Zhuang, Weixiang Sun, Lichao Sun, Jindong Wang, Yanfang Ye, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly employed for simulations,
enabling applications in role-playing agents and Computational Social Science
(CSS). However, the reliability of these simulations is under-explored, which
raises concerns about the trustworthiness of LLMs in these applications. In
this paper, we aim to answer ``How reliable is LLM-based simulation?'' To
address this, we introduce TrustSim, an evaluation dataset covering 10
CSS-related topics, to systematically investigate the reliability of the LLM
simulation. We conducted experiments on 14 LLMs and found that inconsistencies
persist in the LLM-based simulated roles. In addition, the consistency level of
LLMs does not strongly correlate with their general performance. To enhance the
reliability of LLMs in simulation, we proposed Adaptive Learning Rate Based
ORPO (AdaORPO), a reinforcement learning-based algorithm to improve the
reliability in simulation across 7 LLMs. Our research provides a foundation for
future studies to explore more robust and trustworthy LLM-based simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Abhyankar, Vivek Gupta, Dan Roth, Chandan K. Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular reasoning involves interpreting natural language queries about
tabular data, which presents a unique challenge of combining language
understanding with structured data analysis. Existing methods employ either
textual reasoning, which excels in semantic interpretation but struggles with
mathematical operations, or symbolic reasoning, which handles computations well
but lacks semantic understanding. This paper introduces a novel algorithm
H-STAR that integrates both symbolic and semantic (textual) approaches in a
two-stage process to address these limitations. H-STAR employs: (1) step-wise
table extraction using `multi-view' column retrieval followed by row
extraction, and (2) adaptive reasoning that adapts reasoning strategies based
on question types, utilizing semantic reasoning for direct lookup and complex
lexical queries while augmenting textual reasoning with symbolic reasoning
support for quantitative and logical tasks. Our extensive experiments
demonstrate that H-STAR significantly outperforms state-of-the-art methods
across three tabular question-answering (QA) and fact-verification datasets,
underscoring its effectiveness and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 16 tables, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Noising: A Defence Mechanism Against Harmful Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14577v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14577v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, David Atanasov, Robie Gonzales, Subhabrata Majumdar, Carsten Maple, Hassan Sajjad, Frank Rudzicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Releasing open-source large language models (LLMs) presents a dual-use risk
since bad actors can easily fine-tune these models for harmful purposes. Even
without the open release of weights, weight stealing and fine-tuning APIs make
closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety
measures like preventing jailbreaks and improving safety guardrails are
important, such measures can easily be reversed through fine-tuning. In this
work, we propose Representation Noising (RepNoise), a defence mechanism that
operates even when attackers have access to the weights. RepNoise works by
removing information about harmful representations such that it is difficult to
recover them during fine-tuning. Importantly, our defence is also able to
generalize across different subsets of harm that have not been seen during the
defence process as long as they are drawn from the same distribution of the
attack set. Our method does not degrade the general capability of LLMs and
retains the ability to train the model on harmless tasks. We provide empirical
evidence that the efficacy of our defence lies in its ``depth'': the degree to
which information about harmful representations is removed across all layers of
the LLM. We also find areas where RepNoise still remains ineffective and
highlight how those limitations can inform future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPs 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compute-Constrained Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Oscar Yin, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data selection can reduce the amount of training data needed to finetune
LLMs; however, the efficacy of data selection scales directly with its compute.
Motivated by the practical challenge of compute-constrained finetuning, we
consider the setting in which both the cost of selecting data and training are
budgeted for. We first formalize the problem of data selection with a
cost-aware utility function, and model the data selection problem as trading
off initial-selection cost for training gain. We run a comprehensive sweep of
experiments across multiple tasks, varying compute budget by scaling finetuning
tokens, model sizes, and data selection compute. These experiments show the
validity of this model in real-world experiments. Interestingly we find that
many powerful data selection methods are almost never compute-optimal, and that
cheaper data selection alternatives dominate both from a theoretical and
empirical perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superposed Decoding: Multiple Generations from a Single Autoregressive
  Inference Pass <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18400v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18400v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Shen, Alan Fan, Sarah M. Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many applications today provide users with multiple auto-complete drafts as
they type, including GitHub's code completion, Gmail's smart compose, and
Apple's messaging auto-suggestions. Under the hood, language models support
this by running an autoregressive inference pass to provide a draft.
Consequently, providing $k$ drafts to the user requires running an expensive
language model $k$ times. To alleviate the computation cost of running $k$
inference passes, we propose Superposed Decoding, a new decoding algorithm that
generates $k$ drafts at the computation cost of one autoregressive inference
pass. We achieve this by feeding a superposition of the most recent token
embeddings from the $k$ drafts as input to the next decoding step of the
language model. At every inference step we combine the $k$ drafts with the
top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,
using an n-gram interpolation with minimal compute overhead to filter out
incoherent generations. Our experiments show that $k$ drafts from Superposed
Decoding are at least as coherent and factual as Nucleus Sampling and Greedy
Decoding respectively, while being at least $2.44\times$ faster for $k\ge3$. In
a compute-normalized setting, user evaluations demonstrably favor text
generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can
also be combined with other decoding strategies, resulting in universal
coverage gains when scaling inference time compute. Code and more examples
open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 16 figures, accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Hidden Structure of Self-Attention via Kernel Principal
  Component Analysis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel S. Y. Teo, Tan M. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable success of transformers in sequence modeling tasks, spanning
various applications in natural language processing and computer vision, is
attributed to the critical role of self-attention. Similar to the development
of most deep learning models, the construction of these attention mechanisms
relies on heuristics and experience. In our work, we derive self-attention from
kernel principal component analysis (kernel PCA) and show that self-attention
projects its query vectors onto the principal component axes of its key matrix
in a feature space. We then formulate the exact formula for the value matrix in
self-attention, theoretically and empirically demonstrating that this value
matrix captures the eigenvectors of the Gram matrix of the key vectors in
self-attention. Leveraging our kernel PCA framework, we propose Attention with
Robust Principal Components (RPC-Attention), a novel class of robust attention
that is resilient to data contamination. We empirically demonstrate the
advantages of RPC-Attention over softmax attention on the ImageNet-1K object
classification, WikiText-103 language modeling, and ADE20K image segmentation
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages in the main text. Published at NeurIPS 2024. The code is
  available at https://github.com/rachtsy/KPCA_code</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INDUS: Effective and Efficient Language Models for Scientific
  Applications <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10725v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10725v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, Iksha Gurung, Nishan Pantha, Rong Zhang, Bharath Dandala, Rahul Ramachandran, Manil Maskey, Kaylin Bugbee, Mike Little, Elizabeth Fancher, Irina Gerasimov, Armin Mehrabian, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grezes, Megan Ansdell, Alberto Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panagiotis Vagenas, S. Karthik Mukkavilli, Peter Staar, Sanaz Vahidinia, Ryan McGranaghan, Tsendgar Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) trained on general domain corpora showed
remarkable results on natural language processing (NLP) tasks. However,
previous research demonstrated LLMs trained using domain-focused corpora
perform better on specialized tasks. Inspired by this insight, we developed
INDUS, a comprehensive suite of LLMs tailored for the closely-related domains
of Earth science, biology, physics, heliophysics, planetary sciences and
astrophysics, and trained using curated scientific corpora drawn from diverse
data sources. The suite of models include: (1) an encoder model trained using
domain-specific vocabulary and corpora to address NLP tasks, (2) a
contrastive-learning based text embedding model trained using a diverse set of
datasets to address information retrieval tasks and (3) smaller versions of
these models created using knowledge distillation for applications which have
latency or resource constraints. We also created three new scientific benchmark
datasets, CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA) and
NASA-IR (IR) to accelerate research in these multi-disciplinary fields. We show
that our models outperform both general-purpose (RoBERTa) and domain-specific
(SCIBERT) encoders on these new tasks as well as existing tasks in the domains
of interest. Furthermore, we demonstrate the use of these models in two
industrial settings -- as a retrieval model for large-scale vector search
applications and in automatic content tagging systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Industry Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refusal in Language Models Is Mediated by a Single Direction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11717v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11717v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, Neel Nanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational large language models are fine-tuned for both
instruction-following and safety, resulting in models that obey benign requests
but refuse harmful ones. While this refusal behavior is widespread across chat
models, its underlying mechanisms remain poorly understood. In this work, we
show that refusal is mediated by a one-dimensional subspace, across 13 popular
open-source chat models up to 72B parameters in size. Specifically, for each
model, we find a single direction such that erasing this direction from the
model's residual stream activations prevents it from refusing harmful
instructions, while adding this direction elicits refusal on even harmless
instructions. Leveraging this insight, we propose a novel white-box jailbreak
method that surgically disables refusal with minimal effect on other
capabilities. Finally, we mechanistically analyze how adversarial suffixes
suppress propagation of the refusal-mediating direction. Our findings
underscore the brittleness of current safety fine-tuning methods. More broadly,
our work showcases how an understanding of model internals can be leveraged to
develop practical methods for controlling model behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grokked <span class="highlight-title">Transformer</span>s are Implicit Reasoners: A Mechanistic Journey to
  the Edge of Generalization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boshi Wang, Xiang Yue, Yu Su, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study whether transformers can learn to implicitly reason over parametric
knowledge, a skill that even the most capable language models struggle with.
Focusing on two representative reasoning types, composition and comparison, we
consistently find that transformers can learn implicit reasoning, but only
through grokking, i.e., extended training far beyond overfitting. The levels of
generalization also vary across reasoning types: when faced with
out-of-distribution examples, transformers fail to systematically generalize
for composition but succeed for comparison. We delve into the model's internals
throughout training, conducting analytical experiments that reveal: 1) the
mechanism behind grokking, such as the formation of the generalizing circuit
and its relation to the relative efficiency of generalizing and memorizing
circuits, and 2) the connection between systematicity and the configuration of
the generalizing circuit. Our findings guide data and training setup to better
induce implicit reasoning and suggest potential improvements to the transformer
architecture, such as encouraging cross-layer knowledge sharing. Furthermore,
we demonstrate that for a challenging reasoning task with a large search space,
GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly
regardless of prompting styles or retrieval augmentation, while a fully grokked
transformer can achieve near-perfect accuracy, showcasing the power of
parametric memory for complex reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code and data:
  https://github.com/OSU-NLP-Group/GrokkedTransformer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Model Strategic Thinking, Small Model Efficiency: Transferring
  Theory of Mind in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05241v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05241v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nunzio Lore, Sepehr Ilami, Babak Heydari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the performance of larger, newer Large Language Models continues to
improve for strategic Theory of Mind (ToM) tasks, the demand for these
state-of-the-art models increases commensurately. However, their deployment is
costly both in terms of processing power and time. In this paper, we
investigate the feasibility of creating smaller, highly-performing specialized
algorithms by way of fine-tuning. To do this, we first present a large
pre-trained model with 20 unique scenarios that combine different social
contexts with games of varying social dilemmas, record its answers, and use
them for Q&A fine-tuning on a smaller model of the same family. Our focus is on
in-context game-theoretic decision-making, the same domain within which human
interaction occurs and that requires both a theory of mind (or a semblance
thereof) and an understanding of social dynamics. The smaller model is
therefore trained not just on the answers provided, but also on the motivations
provided by the larger model, which should contain advice and guidelines to
navigate both strategic dilemmas and social cues. We find that the fine-tuned
smaller language model consistently bridged the gap in performance between the
smaller pre-trained version of the model and its larger relative and that its
improvements extended in areas and contexts beyond the ones provided in the
training examples, including on out-of-sample scenarios that include completely
different game structures. On average for all games, through fine-tuning, the
smaller model showed a 46% improvement measured as alignment towards the
behavior of the larger model, with 100% representing indistinguishable
behavior. When presented with out-of-sample social contexts and games, the
fine-tuned model still displays remarkable levels of alignment, reaching an
improvement of 18% and 28% respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Chat<span class="highlight-title">GPT</span> Have a Poetic Style? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15299v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15299v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie Walsh, Anna Preus, Elizabeth Gronski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating poetry has become a popular application of LLMs, perhaps
especially of OpenAI's widely-used chatbot ChatGPT. What kind of poet is
ChatGPT? Does ChatGPT have its own poetic style? Can it successfully produce
poems in different styles? To answer these questions, we prompt the GPT-3.5 and
GPT-4 models to generate English-language poems in 24 different poetic forms
and styles, about 40 different subjects, and in response to 3 different writing
prompt templates. We then analyze the resulting 5.7k poems, comparing them to a
sample of 3.7k poems from the Poetry Foundation and the Academy of American
Poets. We find that the GPT models, especially GPT-4, can successfully produce
poems in a range of both common and uncommon English-language forms in
superficial yet noteworthy ways, such as by producing poems of appropriate
lengths for sonnets (14 lines), villanelles (19 lines), and sestinas (39
lines). But the GPT models also exhibit their own distinct stylistic
tendencies, both within and outside of these specific forms. Our results show
that GPT poetry is much more constrained and uniform than human poetry, showing
a strong penchant for rhyme, quatrains (4-line stanzas), iambic meter,
first-person plural perspectives (we, us, our), and specific vocabulary like
"heart," "embrace," "echo," and "whisper."
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHR 2024: Computational Humanities Research Conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind the Gap: A Generalized Approach for Cross-Modal Embedding Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arihan Yadav, Alan McMillan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems enhance text generation by
incorporating external knowledge but often struggle when retrieving context
across different text modalities due to semantic gaps. We introduce a
generalized projection-based method, inspired by adapter modules in transfer
learning, that efficiently bridges these gaps between various text types, such
as programming code and pseudocode, or English and French sentences. Our
approach emphasizes speed, accuracy, and data efficiency, requiring minimal
resources for training and inference. By aligning embeddings from heterogeneous
text modalities into a unified space through a lightweight projection network,
our model significantly outperforms traditional retrieval methods like the
Okapi BM25 algorithm and models like Dense Passage Retrieval (DPR), while
approaching the accuracy of Sentence Transformers. Extensive evaluations
demonstrate the effectiveness and generalizability of our method across
different tasks, highlighting its potential for real-time, resource-constrained
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReasoningRec: Bridging Personalized Recommendations and
  Human-Interpretable Explanations through LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Millennium Bismay, Xiangjue Dong, James Caverlee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ReasoningRec, a reasoning-based recommendation framework
that leverages Large Language Models (LLMs) to bridge the gap between
recommendations and human-interpretable explanations. In contrast to
conventional recommendation systems that rely on implicit user-item
interactions, ReasoningRec employs LLMs to model users and items, focusing on
preferences, aversions, and explanatory reasoning. The framework utilizes a
larger LLM to generate synthetic explanations for user preferences,
subsequently used to fine-tune a smaller LLM for enhanced recommendation
accuracy and human-interpretable explanation. Our experimental study
investigates the impact of reasoning and contextual information on personalized
recommendations, revealing that the quality of contextual and personalized data
significantly influences the LLM's capacity to generate plausible explanations.
Empirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art
methods by up to 12.5\% in recommendation prediction while concurrently
providing human-intelligible explanations. The code is available here:
https://github.com/millenniumbismay/reasoningrec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Large Language Model, Recommendation, Human-Interpretable Reasoning,
  Personalization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciPIP: An LLM-based Scientific Paper Idea Proposer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of knowledge and the increasing complexity of
interdisciplinary research pose significant challenges for researchers,
including information overload and difficulties in exploring novel ideas. The
advancements in large language models (LLMs), such as GPT-4, have shown great
potential in enhancing idea proposals, but how to effectively utilize large
models for reasonable idea proposal has not been thoroughly explored. This
paper proposes a scientific paper idea proposer (SciPIP). Based on a
user-provided research background, SciPIP retrieves helpful papers from a
literature database while leveraging the capabilities of LLMs to generate more
novel and feasible ideas. To this end, 1) we construct a literature retrieval
database, extracting lots of papers' multi-dimension information for fast
access. Then, a literature retrieval method based on semantics, entity, and
citation co-occurrences is proposed to search relevant literature from multiple
aspects based on the user-provided background. 2) After literature retrieval,
we introduce dual-path idea proposal strategies, where one path infers
solutions from the retrieved literature and the other path generates original
ideas through model brainstorming. We then combine the two to achieve a good
balance between feasibility and originality. Through extensive experiments on
the natural language processing (NLP) field, we demonstrate that SciPIP can
retrieve citations similar to those of existing top conference papers and
generate many ideas consistent with them. Additionally, we evaluate the
originality of other ideas generated by SciPIP using large language models,
further validating the effectiveness of our proposed method. The code and the
database are released at https://github.com/cheerss/SciPIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures, 19 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Personalization for LLM-based Recommendation with Customized
  In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqin Bao, Ming Yan, Yang Zhang, Jizhi Zhang, Wenjie Wang, Fuli Feng, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frequently updating Large Language Model (LLM)-based recommender systems to
adapt to new user interests -- as done for traditional ones -- is impractical
due to high training costs, even with acceleration methods. This work explores
adapting to dynamic user interests without any model updates by leveraging
In-Context Learning (ICL), which allows LLMs to learn new tasks from few-shot
examples provided in the input. Using new-interest examples as the ICL few-shot
examples, LLMs may learn real-time interest directly, avoiding the need for
model updates. However, existing LLM-based recommenders often lose the
in-context learning ability during recommendation tuning, while the original
LLM's in-context learning lacks recommendation-specific focus. To address this,
we propose RecICL, which customizes recommendation-specific in-context learning
for real-time recommendations. RecICL organizes training examples in an
in-context learning format, ensuring that in-context learning ability is
preserved and aligned with the recommendation task during tuning.
  Extensive experiments demonstrate RecICL's effectiveness in delivering
real-time recommendations without requiring model updates. Our code is
available at https://github.com/ym689/rec_icl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiruo Cheng, Kelong Mao, Ziliang Zhao, Guanting Dong, Hongjin Qian, Yongkang Wu, Tetsuya Sakai, Ji-Rong Wen, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has become a powerful paradigm for
enhancing large language models (LLMs) through external knowledge retrieval.
Despite its widespread attention, existing academic research predominantly
focuses on single-turn RAG, leaving a significant gap in addressing the
complexities of multi-turn conversations found in real-world applications. To
bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess
RAG systems in realistic multi-turn conversational settings. CORAL includes
diverse information-seeking conversations automatically derived from Wikipedia
and tackles key challenges such as open-domain coverage, knowledge intensity,
free-form responses, and topic shifts. It supports three core tasks of
conversational RAG: passage retrieval, response generation, and citation
labeling. We propose a unified framework to standardize various conversational
RAG methods and conduct a comprehensive evaluation of these methods on CORAL,
demonstrating substantial opportunities for improving existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Universal Sets-level Optimization Framework for Next Set
  Recommendation <span class="chip">CIKM2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuli Liu, Min Liu, Christian Walder, Lexing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next Set Recommendation (NSRec), encompassing related tasks such as next
basket recommendation and temporal sets prediction, stands as a trending
research topic. Although numerous attempts have been made on this topic, there
are certain drawbacks: (i) Existing studies are still confined to utilizing
objective functions commonly found in Next Item Recommendation (NIRec), such as
binary cross entropy and BPR, which are calculated based on individual item
comparisons; (ii) They place emphasis on building sophisticated learning models
to capture intricate dependency relationships across sequential sets, but
frequently overlook pivotal dependency in their objective functions; (iii)
Diversity factor within sequential sets is frequently overlooked. In this
research, we endeavor to unveil a universal and S ets-level optimization
framework for N ext Set Recommendation (SNSRec), offering a holistic fusion of
diversity distribution and intricate dependency relationships within temporal
sets. To realize this, the following contributions are made: (i) We directly
model the temporal set in a sequence as a cohesive entity, leveraging the
Structured Determinantal Point Process (SDPP), wherein the probabilistic DPP
distribution prioritizes collections of structures (sequential sets) instead of
individual items; (ii) We introduce a co-occurrence representation to discern
and acknowledge the importance of different sets; (iii) We propose a sets-level
optimization criterion, which integrates the diversity distribution and
dependency relations across the entire sequence of sets, guiding the model to
recommend relevant and diversified set. Extensive experiments on real-world
datasets show that our approach consistently outperforms previous methods on
both relevance and diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepter at CIKM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DataRec: A Framework for Standardizing Recommendation Data Processing
  and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Carlo Maria Mancino, Salvatore Bufi, Angela Di Fazio, Daniele Malitesta, Claudio Pomo, Antonio Ferrara, Tommaso Di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the great interest posed by researchers and companies,
recommendation systems became a cornerstone of machine learning applications.
However, concerns have arisen recently about the need for reproducibility,
making it challenging to identify suitable pipelines. Several frameworks have
been proposed to improve reproducibility, covering the entire process from data
reading to performance evaluation. Despite this effort, these solutions often
overlook the role of data management, do not promote interoperability, and
neglect data analysis despite its well-known impact on recommender performance.
To address these gaps, we propose DataRec, which facilitates using and
manipulating recommendation datasets. DataRec supports reading and writing in
various formats, offers filtering and splitting techniques, and enables data
distribution analysis using well-known metrics. It encourages a unified
approach to data manipulation by allowing data export in formats compatible
with several recommendation frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Improving Adversarial Collaborative Filtering for
  Robust Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Collaborative Filtering (ACF), which typically applies
adversarial perturbations at user and item embeddings through adversarial
training, is widely recognized as an effective strategy for enhancing the
robustness of Collaborative Filtering (CF) recommender systems against
poisoning attacks. Besides, numerous studies have empirically shown that ACF
can also improve recommendation performance compared to traditional CF. Despite
these empirical successes, the theoretical understanding of ACF's effectiveness
in terms of both performance and robustness remains unclear. To bridge this
gap, in this paper, we first theoretically show that ACF can achieve a lower
recommendation error compared to traditional CF with the same training epochs
in both clean and poisoned data contexts. Furthermore, by establishing bounds
for reductions in recommendation error during ACF's optimization process, we
find that applying personalized magnitudes of perturbation for different users
based on their embedding scales can further improve ACF's effectiveness.
Building on these theoretical understandings, we propose Personalized Magnitude
Adversarial Collaborative Filtering (PamaCF). Extensive experiments demonstrate
that PamaCF effectively defends against various types of poisoning attacks
while significantly enhancing recommendation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Zhang, Qinfeng Li, Tianyu Du, Xuhong Zhang, Xinkui Zhao, Zhengwen Feng, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge, making them adaptable and
cost-effective for various applications. However, the growing reliance on these
systems also introduces potential security risks. In this work, we reveal a
novel vulnerability, the retrieval prompt hijack attack (HijackRAG), which
enables attackers to manipulate the retrieval mechanisms of RAG systems by
injecting malicious texts into the knowledge database. When the RAG system
encounters target questions, it generates the attacker's pre-determined answers
instead of the correct ones, undermining the integrity and trustworthiness of
the system. We formalize HijackRAG as an optimization problem and propose both
black-box and white-box attack strategies tailored to different levels of the
attacker's knowledge. Extensive experiments on multiple benchmark datasets show
that HijackRAG consistently achieves high attack success rates, outperforming
existing baseline attacks. Furthermore, we demonstrate that the attack is
transferable across different retriever models, underscoring the widespread
risk it poses to RAG systems. Lastly, our exploration of various defense
mechanisms reveals that they are insufficient to counter HijackRAG, emphasizing
the urgent need for more robust security measures to protect RAG systems in
real-world deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causality-Enhanced Behavior Sequence Modeling in LLMs for Personalized
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhang, Juntao You, Yimeng Bai, Jizhi Zhang, Keqin Bao, Wenjie Wang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in recommender systems have focused on leveraging Large
Language Models (LLMs) to improve user preference modeling, yielding promising
outcomes. However, current LLM-based approaches struggle to fully leverage user
behavior sequences, resulting in suboptimal preference modeling for
personalized recommendations. In this study, we propose a novel Counterfactual
Fine-Tuning (CFT) method to address this issue by explicitly emphasizing the
role of behavior sequences when generating recommendations. Specifically, we
employ counterfactual reasoning to identify the causal effects of behavior
sequences on model output and introduce a task that directly fits the
ground-truth labels based on these effects, achieving the goal of explicit
emphasis. Additionally, we develop a token-level weighting mechanism to adjust
the emphasis strength for different item tokens, reflecting the diminishing
influence of behavior sequences from earlier to later tokens during predicting
an item. Extensive experiments on real-world datasets demonstrate that CFT
effectively improves behavior sequence modeling. Our codes are available at
https://github.com/itsmeyjt/CFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Contrastive <span class="highlight-title">Transformer</span> for Hierarchical Preference Modeling in
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkai Huang, Shoujin Wang, Xianzhi Wang, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems (SRSs) aim to predict the subsequent items
which may interest users via comprehensively modeling users' complex preference
embedded in the sequence of user-item interactions. However, most of existing
SRSs often model users' single low-level preference based on item ID
information while ignoring the high-level preference revealed by item attribute
information, such as item category. Furthermore, they often utilize limited
sequence context information to predict the next item while overlooking richer
inter-item semantic relations. To this end, in this paper, we proposed a novel
hierarchical preference modeling framework to substantially model the complex
low- and high-level preference dynamics for accurate sequential recommendation.
Specifically, in the framework, a novel dual-transformer module and a novel
dual contrastive learning scheme have been designed to discriminatively learn
users' low- and high-level preference and to effectively enhance both low- and
high-level preference learning respectively. In addition, a novel
semantics-enhanced context embedding module has been devised to generate more
informative context embedding for further improving the recommendation
performance. Extensive experiments on six real-world datasets have demonstrated
both the superiority of our proposed method over the state-of-the-art ones and
the rationality of our design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modern Hopfield Networks meet Encoded Neural Representations --
  Addressing Practical Considerations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satyananda Kashyap, Niharika S. D'Souza, Luyao Shi, Ken C. L. Wong, Hongzhi Wang, Tanveer Syeda-Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content-addressable memories such as Modern Hopfield Networks (MHN) have been
studied as mathematical models of auto-association and storage/retrieval in the
human declarative memory, yet their practical use for large-scale content
storage faces challenges. Chief among them is the occurrence of meta-stable
states, particularly when handling large amounts of high dimensional content.
This paper introduces Hopfield Encoding Networks (HEN), a framework that
integrates encoded neural representations into MHNs to improve pattern
separability and reduce meta-stable states. We show that HEN can also be used
for retrieval in the context of hetero association of images with natural
language queries, thus removing the limitation of requiring access to partial
content in the same domain. Experimental results demonstrate substantial
reduction in meta-stable states and increased storage capacity while still
enabling perfect recall of a significantly larger number of inputs advancing
the practical utility of associative memory networks for real-world tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, accepted as a workshop paper at UniReps @
  Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INDUS: Effective and Efficient Language Models for Scientific
  Applications <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10725v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10725v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, Iksha Gurung, Nishan Pantha, Rong Zhang, Bharath Dandala, Rahul Ramachandran, Manil Maskey, Kaylin Bugbee, Mike Little, Elizabeth Fancher, Irina Gerasimov, Armin Mehrabian, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grezes, Megan Ansdell, Alberto Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panagiotis Vagenas, S. Karthik Mukkavilli, Peter Staar, Sanaz Vahidinia, Ryan McGranaghan, Tsendgar Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) trained on general domain corpora showed
remarkable results on natural language processing (NLP) tasks. However,
previous research demonstrated LLMs trained using domain-focused corpora
perform better on specialized tasks. Inspired by this insight, we developed
INDUS, a comprehensive suite of LLMs tailored for the closely-related domains
of Earth science, biology, physics, heliophysics, planetary sciences and
astrophysics, and trained using curated scientific corpora drawn from diverse
data sources. The suite of models include: (1) an encoder model trained using
domain-specific vocabulary and corpora to address NLP tasks, (2) a
contrastive-learning based text embedding model trained using a diverse set of
datasets to address information retrieval tasks and (3) smaller versions of
these models created using knowledge distillation for applications which have
latency or resource constraints. We also created three new scientific benchmark
datasets, CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA) and
NASA-IR (IR) to accelerate research in these multi-disciplinary fields. We show
that our models outperform both general-purpose (RoBERTa) and domain-specific
(SCIBERT) encoders on these new tasks as well as existing tasks in the domains
of interest. Furthermore, we demonstrate the use of these models in two
industrial settings -- as a retrieval model for large-scale vector search
applications and in automatic content tagging systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Industry Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retention Induced Biases in a Recommendation System with Heterogeneous
  Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13959v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13959v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shichao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I examine a conceptual model of a recommendation system (RS) with user inflow
and churn dynamics. When inflow and churn balance out, the user distribution
reaches a steady state. Changing the recommendation algorithm alters the steady
state and creates a transition period. During this period, the RS behaves
differently from its new steady state. In particular, A/B experiment metrics
obtained in transition periods are biased indicators of the RS's long-term
performance. Scholars and practitioners, however, often conduct A/B tests
shortly after introducing new algorithms to validate their effectiveness. This
A/B experiment paradigm, widely regarded as the gold standard for assessing RS
improvements, may consequently yield false conclusions. I also briefly touch on
the data bias caused by the user retention dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review (when applicable) or any
  post-submission improvements or corrections. The Version of Record of this
  contribution is published in advances in Bias and Fairness in Information
  Retrieval. BIAS 2024. Communications in Computer and Information Science, vol
  2227. Springer, and is available online at
  https://doi.org/10.1007/978-3-031-71975-2_2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scientific and Technological Information Oriented Semantics-adversarial
  and Media-adversarial Cross-media Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08615v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08615v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Li, Junping Du, Feifei Kou, Zhe Xue, Xin Xu, Mingying Xu, Yang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-media retrieval of scientific and technological information is one of
the important tasks in the cross-media study. Cross-media scientific and
technological information retrieval obtain target information from massive
multi-source and heterogeneous scientific and technological resources, which
helps to design applications that meet users' needs, including scientific and
technological information recommendation, personalized scientific and
technological information retrieval, etc. The core of cross-media retrieval is
to learn a common subspace, so that data from different media can be directly
compared with each other after being mapped into this subspace. In subspace
learning, existing methods often focus on modeling the discrimination of
intra-media data and the invariance of inter-media data after mapping; however,
they ignore the semantic consistency of inter-media data before and after
mapping and media discrimination of intra-semantics data, which limit the
result of cross-media retrieval. In light of this, we propose a scientific and
technological information oriented Semantics-adversarial and Media-adversarial
Cross-media Retrieval method (SMCR) to find an effective common subspace.
Specifically, SMCR minimizes the loss of inter-media semantic consistency in
addition to modeling intra-media semantic discrimination, to preserve semantic
similarity before and after mapping. Furthermore, SMCR constructs a basic
feature mapping network and a refined feature mapping network to jointly
minimize the media discriminative loss within semantics, so as to enhance the
feature mapping network's ability to confuse the media discriminant network.
Experimental results on two datasets demonstrate that the proposed SMCR
outperforms state-of-the-art methods in cross-media retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLIP: Fine-grained Alignment between ID-based Models and <span class="highlight-title">Pretrain</span>ed
  Language Models for CTR Prediction <span class="chip">RecSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19453v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19453v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangyu Wang, Jianghao Lin, Xiangyang Li, Bo Chen, Chenxu Zhu, Ruiming Tang, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction plays as a core function module in
various personalized online services. The traditional ID-based models for CTR
prediction take as inputs the one-hot encoded ID features of tabular modality,
which capture the collaborative signals via feature interaction modeling. But
the one-hot encoding discards the semantic information included in the textual
features. Recently, the emergence of Pretrained Language Models(PLMs) has given
rise to another paradigm, which takes as inputs the sentences of textual
modality obtained by hard prompt templates and adopts PLMs to extract the
semantic knowledge. However, PLMs often face challenges in capturing field-wise
collaborative signals and distinguishing features with subtle textual
differences. In this paper, to leverage the benefits of both paradigms and
meanwhile overcome their limitations, we propose to conduct Fine-grained
feature-level ALignment between ID-based Models and Pretrained Language
Models(FLIP) for CTR prediction. Unlike most methods that solely rely on global
views through instance-level contrastive learning, we design a novel jointly
masked tabular/language modeling task to learn fine-grained alignment between
tabular IDs and word tokens. Specifically, the masked data of one modality (IDs
and tokens) has to be recovered with the help of the other modality, which
establishes the feature-level interaction and alignment via sufficient mutual
information extraction between dual modalities. Moreover, we propose to jointly
finetune the ID-based model and PLM by adaptively combining the output of both
models, thus achieving superior performance in downstream CTR prediction tasks.
Extensive experiments on three real-world datasets demonstrate that FLIP
outperforms SOTA baselines, and is highly compatible with various ID-based
models and PLMs. The code is at \url{https://github.com/justarter/FLIP}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R^2AG: Incorporating Retrieval Information into Retrieval Augmented
  Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation (RAG) has been applied in many scenarios to
augment large language models (LLMs) with external documents provided by
retrievers. However, a semantic gap exists between LLMs and retrievers due to
differences in their training objectives and architectures. This misalignment
forces LLMs to passively accept the documents provided by the retrievers,
leading to incomprehension in the generation process, where the LLMs are
burdened with the task of distinguishing these documents using their inherent
knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill
this gap by incorporating Retrieval information into Retrieval Augmented
Generation. Specifically, R$^2$AG utilizes the nuanced features from the
retrievers and employs a R$^2$-Former to capture retrieval information. Then, a
retrieval-aware prompting strategy is designed to integrate retrieval
information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios
where LLMs and retrievers are frozen. Extensive experiments across five
datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our
analysis reveals that retrieval information serves as an anchor to aid LLMs in
the generation process, thereby filling the semantic gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CIDGMed: Causal Inference-Driven Medication Recommendation with Enhanced
  Dual-Granularity Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunpan Liang, Xiang Li, Shi Mu, Chen Li, Yu Lei, Yulei Hou, Tengfei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medication recommendation aims to integrate patients' long-term health
records to provide accurate and safe medication combinations for specific
health states. Existing methods often fail to deeply explore the true causal
relationships between diseases/procedures and medications, resulting in biased
recommendations. Additionally, in medication representation learning, the
relationships between information at different granularities of medications,
coarse-grained (medication itself) and fine-grained (molecular level), are not
effectively integrated, leading to biases in representation learning. To
address these limitations, we propose the Causal Inference-driven
Dual-Granularity Medication Recommendation method (CIDGMed). Our approach
leverages causal inference to uncover the relationships between
diseases/procedures and medications, thereby enhancing the rationality and
interpretability of recommendations. By integrating coarse-grained medication
effects with fine-grained molecular structure information, CIDGMed provides a
comprehensive representation of medications. Additionally, we employ a bias
correction model during the prediction phase to further refine recommendations,
ensuring both accuracy and safety. Through extensive experiments, CIDGMed
significantly outperforms current state-of-the-art models across multiple
metrics, achieving a 2.54% increase in accuracy, a 3.65% reduction in side
effects, and a 39.42% improvement in time efficiency. Additionally, we
demonstrate the rationale of CIDGMed through a case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatQA: Surpassing <span class="highlight-title">GPT</span>-4 on Conversational QA and RAG <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10225v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10225v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on
retrieval-augmented generation (RAG) and conversational question answering
(QA). To enhance generation, we propose a two-stage instruction tuning method
that significantly boosts the performance of RAG. For effective retrieval, we
introduce a dense retriever optimized for conversational QA, which yields
results comparable to the alternative state-of-the-art query rewriting models,
while substantially reducing deployment costs. We also present the ChatRAG
Bench, which encompasses ten datasets covering comprehensive evaluations on
RAG, table-related QA, arithmetic calculations, and scenarios involving
unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a
weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score:
53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without
relying on any synthetic data from OpenAI GPT models. Notably, the
Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09,
achieving a 4.4% improvement. To advance research in this field, we
open-sourced the model weights, instruction tuning data, ChatRAG Bench, and
retriever for the community: https://chatqa-project.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AugTriever: Unsupervised Dense Retrieval and Domain Adaptation by
  Scalable Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08841v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08841v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu, Ning Yu, Jianguo Zhang, Meghana Bhat, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have made significant strides in text retrieval and
open-domain question answering. However, most of these achievements have relied
heavily on extensive human-annotated supervision. In this study, we aim to
develop unsupervised methods for improving dense retrieval models. We propose
two approaches that enable annotation-free and scalable training by creating
pseudo querydocument pairs: query extraction and transferred query generation.
The query extraction method involves selecting salient spans from the original
document to generate pseudo queries. On the other hand, the transferred query
generation method utilizes generation models trained for other NLP tasks, such
as summarization, to produce pseudo queries. Through extensive experimentation,
we demonstrate that models trained using these augmentation methods can achieve
comparable, if not better, performance than multiple strong dense baselines.
Moreover, combining these strategies leads to further improvements, resulting
in superior performance of unsupervised dense retrieval, unsupervised domain
adaptation and supervised finetuning, benchmarked on both BEIR and ODQA
datasets. Code and datasets are publicly available at
https://github.com/salesforce/AugTriever.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DCAI24, October 25, 2024, Boise, ID</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Summarization-Based Document IDs for Generative Retrieval with Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxin Li, Daniel Cheng, Phillip Keung, Jungo Kasai, Noah A. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a popular
approach for end-to-end document retrieval that directly generates document
identifiers given an input query. We introduce summarization-based document
IDs, in which each document's ID is composed of an extractive summary or
abstractive keyphrases generated by a language model, rather than an integer ID
sequence or bags of n-grams as proposed in past work. We find that abstractive,
content-based IDs (ACID) and an ID based on the first 30 tokens are very
effective in direct comparisons with previous approaches to ID creation. We
show that using ACID improves top-10 and top-20 recall by 15.6% and 14.4%
(relative) respectively versus the cluster-based integer ID baseline on the
MSMARCO 100k retrieval task, and 9.8% and 9.9% respectively on the
Wikipedia-based NQ 100k retrieval task. Our results demonstrate the
effectiveness of human-readable, natural-language IDs created through
summarization for generative retrieval. We also observed that extractive IDs
outperformed abstractive IDs on Wikipedia articles in NQ but not the snippets
in MSMARCO, which suggests that document characteristics affect generative
retrieval performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the NLP for Wikipedia Workshop in EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Trail Making Test in Virtual Reality (TMT-VR): The Effects of
  Interaction Modes and Gaming Skills on Cognitive Performance of Young Adults 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenia Giatzoglou, Panagiotis Vorias, Ryan Kemm, Irene Karayianni, Chrysanthi Nega, Panagiotis Kourtesis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual Reality (VR) is increasingly used in neuropsychological assessments
due to its ability to simulate real-world environments. This study aimed to
develop and evaluate the Trail Making Test in VR (TMT-VR) and investigate the
effects of different interaction modes and gaming skills on cognitive
performance. A total of 71 young female and male adults (aged 18-35) with high
and low gaming skills participated in this study. Participants completed the
TMT-VR using three interaction modes as follows: eye tracking, head movement,
and controller. Performance metrics included task completion time and accuracy.
User experience, usability, and acceptability of TMT-VR were also examined.
Results showed that both eye tracking and head movement modes significantly
outperformed the controller in terms of task completion time and accuracy. No
significant differences were found between eye tracking and head movement
modes. Gaming skills did not significantly influence task performance using any
interaction mode. The TMT-VR demonstrates high usability, acceptability, and
user experience among participants. The findings suggest that VR-based
assessments can effectively measure cognitive performance without being
influenced by prior gaming skills, indicating potential applicability for
diverse populations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 Pages, 7 Figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning in Vocal Education: Technical Evaluation of Limited
  Samples Describing Mezzo-soprano 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyi Hou, Xu Zhao, Kejie Ye, Xinyu Sheng, Shanggerile Jiang, Jiajing Xia, Yitao Zhang, Chenxi Ban, Daijun Luo, Jiaxing Chen, Yan Zou, Yuchao Feng, Guangyu Fan, Xin Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vocal education in the music field is difficult to quantify due to the
individual differences in singers' voices and the different quantitative
criteria of singing techniques. Deep learning has great potential to be applied
in music education due to its efficiency to handle complex data and perform
quantitative analysis. However, accurate evaluations with limited samples over
rare vocal types, such as Mezzo-soprano, requires extensive well-annotated data
support using deep learning models. In order to attain the objective, we
perform transfer learning by employing deep learning models pre-trained on the
ImageNet and Urbansound8k datasets for the improvement on the precision of
vocal technique evaluation. Furthermore, we tackle the problem of the lack of
samples by constructing a dedicated dataset, the Mezzo-soprano Vocal Set (MVS),
for vocal technique assessment. Our experimental results indicate that transfer
learning increases the overall accuracy (OAcc) of all models by an average of
8.3%, with the highest accuracy at 94.2%. We not only provide a novel approach
to evaluating Mezzo-soprano vocal techniques but also introduce a new
quantitative assessment method for music education.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOA-Aware Audio-Visual <span class="highlight-title">Self-Supervised</span> Learning for Sound Event
  Localization and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoto Fujita, Yoshiaki Bando, Keisuke Imoto, Masaki Onishi, Kazuyoshi Yoshii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes sound event localization and detection (SELD) for
spatial audio recordings captured by firstorder ambisonics (FOA) microphones.
In this task, one may train a deep neural network (DNN) using FOA data
annotated with the classes and directions of arrival (DOAs) of sound events.
However, the performance of this approach is severely bounded by the amount of
annotated data. To overcome this limitation, we propose a novel method of
pretraining the feature extraction part of the DNN in a self-supervised manner.
We use spatial audio-visual recordings abundantly available as virtual reality
contents. Assuming that sound objects are concurrently observed by the FOA
microphones and the omni-directional camera, we jointly train audio and visual
encoders with contrastive learning such that the audio and visual embeddings of
the same recording and DOA are made close. A key feature of our method is that
the DOA-wise audio embeddings are jointly extracted from the raw audio data,
while the DOA-wise visual embeddings are separately extracted from the local
visual crops centered on the corresponding DOA. This encourages the latent
features of the audio encoder to represent both the classes and DOAs of sound
events. The experiment using the DCASE2022 Task 3 dataset of 20 hours shows
non-annotated audio-visual recordings of 100 hours reduced the error score of
SELD from 36.4 pts to 34.9 pts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to APSIPA2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video
  Understanding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14515v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14515v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large vision-language models (LVLMs) has spurred research into
their applications in multi-modal contexts, particularly in video
understanding. Traditional VideoQA benchmarks, despite providing quantitative
metrics, often fail to encompass the full spectrum of video content and
inadequately assess models' temporal comprehension. To address these
limitations, we introduce MMBench-Video, a quantitative benchmark designed to
rigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video
incorporates lengthy videos from YouTube and employs free-form questions,
mirroring practical use cases. The benchmark is meticulously crafted to probe
the models' temporal reasoning skills, with all questions human-annotated
according to a carefully constructed ability taxonomy. We employ GPT-4 for
automated assessment, demonstrating superior accuracy and robustness over
earlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted
comprehensive evaluations that include both proprietary and open-source LVLMs
for images and videos. MMBench-Video stands as a valuable resource for the
research community, facilitating improved evaluation of LVLMs and catalyzing
progress in the field of video understanding. The evalutation code of
MMBench-Video will be integrated into VLMEvalKit:
https://github.com/open-compass/VLMEvalKit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature distribution Adaptation Network for Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Li, Yixuan Ji, Peng Song, Haoqin Sun, Wenming Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel deep inductive transfer learning framework,
named feature distribution adaptation network, to tackle the challenging
multi-modal speech emotion recognition problem. Our method aims to use deep
transfer learning strategies to align visual and audio feature distributions to
obtain consistent representation of emotion, thereby improving the performance
of speech emotion recognition. In our model, the pre-trained ResNet-34 is
utilized for feature extraction for facial expression images and acoustic Mel
spectrograms, respectively. Then, the cross-attention mechanism is introduced
to model the intrinsic similarity relationships of multi-modal features.
Finally, the multi-modal feature distribution adaptation is performed
efficiently with feed-forward network, which is extended using the local
maximum mean discrepancy loss. Experiments are carried out on two benchmark
datasets, and the results demonstrate that our model can achieve excellent
performance compared with existing ones.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-29T00:00:00Z">2024-10-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Pointer Network-based Approach for Joint Extraction and Detection of
  Multi-Label Multi-Class Intents <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankan Mullick, Sombit Bose, Abhilash Nandy, Gajula Sai Chaitanya, Pawan Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In task-oriented dialogue systems, intent detection is crucial for
interpreting user queries and providing appropriate responses. Existing
research primarily addresses simple queries with a single intent, lacking
effective systems for handling complex queries with multiple intents and
extracting different intent spans. Additionally, there is a notable absence of
multilingual, multi-intent datasets. This study addresses three critical tasks:
extracting multiple intent spans from queries, detecting multiple intents, and
developing a multi-lingual multi-label intent dataset. We introduce a novel
multi-label multi-class intent detection dataset (MLMCID-dataset) curated from
existing benchmark datasets. We also propose a pointer network-based
architecture (MLMCID) to extract intent spans and detect multiple intents with
coarse and fine-grained labels in the form of sextuplets. Comprehensive
analysis demonstrates the superiority of our pointer network-based system over
baseline approaches in terms of accuracy and F1-score across various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Findings (Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pushing the Performance Envelope of DNN-based Recommendation Systems
  Inference on GPUs <span class="chip">MICRO</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Jain, Vivek M. Bhasi, Adwait Jog, Anand Sivasubramaniam, Mahmut T. Kandemir, Chita R. Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized recommendation is a ubiquitous application on the internet, with
many industries and hyperscalers extensively leveraging Deep Learning
Recommendation Models (DLRMs) for their personalization needs (like ad serving
or movie suggestions). With growing model and dataset sizes pushing computation
and memory requirements, GPUs are being increasingly preferred for executing
DLRM inference. However, serving newer DLRMs, while meeting acceptable
latencies, continues to remain challenging, making traditional deployments
increasingly more GPU-hungry, resulting in higher inference serving costs. In
this paper, we show that the embedding stage continues to be the primary
bottleneck in the GPU inference pipeline, leading up to a 3.2x embedding-only
performance slowdown.
  To thoroughly grasp the problem, we conduct a detailed microarchitecture
characterization and highlight the presence of low occupancy in the standard
embedding kernels. By leveraging direct compiler optimizations, we achieve
optimal occupancy, pushing the performance by up to 53%. Yet, long memory
latency stalls continue to exist. To tackle this challenge, we propose
specialized plug-and-play-based software prefetching and L2 pinning techniques,
which help in hiding and decreasing the latencies. Further, we propose
combining them, as they complement each other. Experimental evaluations using
A100 GPUs with large models and datasets show that our proposed techniques
improve performance by up to 103% for the embedding stage, and up to 77% for
the overall DLRM inference pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted in the 57th MICRO
  (https://microarch.org/micro57/program/). Please check appendix for details
  on reproducing our work including codebase and steps</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContextIQ: A Multimodal Expert-Based Video Retrieval System for
  Contextual Advertising <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Chaubey, Anoubhav Agarwaal, Sartaki Sinha Roy, Aayush Agarwal, Susmita Ghose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual advertising serves ads that are aligned to the content that the
user is viewing. The rapid growth of video content on social platforms and
streaming services, along with privacy concerns, has increased the need for
contextual advertising. Placing the right ad in the right context creates a
seamless and pleasant ad viewing experience, resulting in higher audience
engagement and, ultimately, better ad monetization. From a technology
standpoint, effective contextual advertising requires a video retrieval system
capable of understanding complex video content at a very granular level.
Current text-to-video retrieval models based on joint multimodal training
demand large datasets and computational resources, limiting their practicality
and lacking the key functionalities required for ad ecosystem integration. We
introduce ContextIQ, a multimodal expert-based video retrieval system designed
specifically for contextual advertising. ContextIQ utilizes modality-specific
experts-video, audio, transcript (captions), and metadata such as objects,
actions, emotion, etc.-to create semantically rich video representations. We
show that our system, without joint training, achieves better or comparable
results to state-of-the-art models and commercial solutions on multiple
text-to-video retrieval benchmarks. Our ablation studies highlight the benefits
of leveraging multiple modalities for enhanced video retrieval accuracy instead
of using a vision-language model alone. Furthermore, we show how video
retrieval systems such as ContextIQ can be used for contextual advertising in
an ad ecosystem while also addressing concerns related to brand safety and
filtering inappropriate content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Data Generation with Large Language Models for Personalized
  Community Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Braga, Pranav Kasela, Alessandro Raganato, Gabriella Pasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalization in Information Retrieval (IR) is a topic studied by the
research community since a long time. However, there is still a lack of
datasets to conduct large-scale evaluations of personalized IR; this is mainly
due to the fact that collecting and curating high-quality user-related
information requires significant costs and time investment. Furthermore, the
creation of datasets for Personalized IR (PIR) tasks is affected by both
privacy concerns and the need for accurate user-related data, which are often
not publicly available. Recently, researchers have started to explore the use
of Large Language Models (LLMs) to generate synthetic datasets, which is a
possible solution to generate data for low-resource tasks. In this paper, we
investigate the potential of Large Language Models (LLMs) for generating
synthetic documents to train an IR system for a Personalized Community Question
Answering task. To study the effectiveness of IR models fine-tuned on
LLM-generated data, we introduce a new dataset, named Sy-SE-PQA. We build
Sy-SE-PQA based on an existing dataset, SE-PQA, which consists of questions and
answers posted on the popular StackExchange communities. Starting from
questions in SE-PQA, we generate synthetic answers using different prompt
techniques and LLMs. Our findings suggest that LLMs have high potential in
generating data tailored to users' needs. The synthetic data can replace
human-written training data, even if the generated data may contain incorrect
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WI-IAT '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimRec: Mitigating the Cold-Start Problem in Sequential Recommendation
  by Integrating Item Similarity <span class="chip">RecSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaked Brody, Shoval Lagziel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems often struggle to make predictions or take
action when dealing with cold-start items that have limited amount of
interactions. In this work, we propose SimRec - a new approach to mitigate the
cold-start problem in sequential recommendation systems. SimRec addresses this
challenge by leveraging the inherent similarity among items, incorporating item
similarities into the training process through a customized loss function.
Importantly, this enhancement is attained with identical model architecture and
the same amount of trainable parameters, resulting in the same inference time
and requiring minimal additional effort. This novel approach results in a
robust contextual sequential recommendation model capable of effectively
handling rare items, including those that were not explicitly seen during
training, thereby enhancing overall recommendation performance. Rigorous
evaluations against multiple baselines on diverse datasets showcase SimRec's
superiority, particularly in scenarios involving items occurring less than 10
times in the training data. The experiments reveal an impressive improvement,
with SimRec achieving up to 78% higher HR@10 compared to SASRec. Notably,
SimRec outperforms strong baselines on sparse datasets while delivering on-par
performance on dense datasets. Our code is available at
https://github.com/amazon-science/sequential-recommendation-using-similarity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM RecSys 2024 Workshop on Context-Aware Recommender Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing Identity of Distributions under Kolmogorov Distance in
  Polylogarithmic Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Janos Lebeda, Jakub Tětek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Suppose we have a sample from a distribution $D$ and we want to test whether
$D = D^*$ for a fixed distribution $D^*$. Specifically, we want to reject with
constant probability, if the distance of $D$ from $D^*$ is $\geq \varepsilon$
in a given metric. In the case of continuous distributions, this has been
studied thoroughly in the statistics literature. Namely, for the well-studied
Kolmogorov metric a test is known that uses the optimal $O(1/\varepsilon^2)$
samples.
  However, this test naively uses also space $O(1/\varepsilon^2)$, and previous
work improved this to $O(1/\varepsilon)$. In this paper, we show that much less
space suffices -- we give an algorithm that uses space $O(\log^4
\varepsilon^{-1})$ in the streaming setting while also using an asymptotically
optimal number of samples. This is in contrast with the standard total
variation distance on discrete distributions for which such space reduction is
known to be impossible. Finally, we state 9 related open problems that we hope
will spark interest in this and related problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Temporal Positive and Negative Excitation for Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkai Huang, Shoujin Wang, Xianzhi Wang, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation aims to predict the next item which interests users
via modeling their interest in items over time. Most of the existing works on
sequential recommendation model users' dynamic interest in specific items while
overlooking users' static interest revealed by some static attribute
information of items, e.g., category, or brand. Moreover, existing works often
only consider the positive excitation of a user's historical interactions on
his/her next choice on candidate items while ignoring the commonly existing
negative excitation, resulting in insufficient modeling dynamic interest. The
overlook of static interest and negative excitation will lead to incomplete
interest modeling and thus impede the recommendation performance. To this end,
in this paper, we propose modeling both static interest and negative excitation
for dynamic interest to further improve the recommendation performance.
Accordingly, we design a novel Static-Dynamic Interest Learning (SDIL)
framework featured with a novel Temporal Positive and Negative Excitation
Modeling (TPNE) module for accurate sequential recommendation. TPNE is
specially designed for comprehensively modeling dynamic interest based on
temporal positive and negative excitation learning. Extensive experiments on
three real-world datasets show that SDIL can effectively capture both static
and dynamic interest and outperforms state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Conditional Diffusion Models for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongtao Huang, Chengkai Huang, Xiaojun Chang, Wen Hu, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models have shown promising results in
sequential recommendation (SR). However, current diffusion-based methods still
exhibit two key limitations. First, they implicitly model the diffusion process
for target item embeddings rather than the discrete target item itself, leading
to inconsistency in the recommendation process. Second, existing methods rely
on either implicit or explicit conditional diffusion models, limiting their
ability to fully capture the context of user behavior and leading to less
robust target item embeddings. In this paper, we propose the Dual Conditional
Diffusion Models for Sequential Recommendation (DCRec), introducing a
discrete-to-continuous sequential recommendation diffusion framework. Our
framework introduces a complete Markov chain to model the transition from the
reversed target item representation to the discrete item index, bridging the
discrete and continuous item spaces for diffusion models and ensuring
consistency with the diffusion framework. Building on this framework, we
present the Dual Conditional Diffusion Transformer (DCDT) that incorporates the
implicit conditional and the explicit conditional for diffusion-based SR.
Extensive experiments on public benchmark datasets demonstrate that DCRec
outperforms state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Diffusion-based Counterfactual Augmentation for Robust
  Session-based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muskan Gupta, Priyanka Gupta, Lovekesh Vig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based recommendation (SR) models aim to recommend top-K items to a
user, based on the user's behaviour during the current session. Several SR
models are proposed in the literature, however,concerns have been raised about
their susceptibility to inherent biases in the training data (observed data)
such as popularity bias. SR models when trained on the biased training data may
encounter performance challenges on out-of-distribution data in real-world
scenarios. One way to mitigate popularity bias is counterfactual data
augmentation. Compared to prior works that rely on generating data using SR
models, we focus on utilizing the capabilities of state-of-the art diffusion
models for generating counterfactual data. We propose a guided diffusion-based
counterfactual augmentation framework for SR. Through a combination of offline
and online experiments on a real-world and simulated dataset, respectively, we
show that our approach performs significantly better than the baseline SR
models and other state-of-the art augmentation frameworks. More importantly,
our framework shows significant improvement on less popular target items, by
achieving up to 20% gain in Recall and 13% gain in CTR on real-world and
simulated datasets,respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of Audio Fingerprinting Techniques for Real-Time Scalable
  Speech Retrieval and Speech Clusterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kemal Altwlkany, Sead Delalić, Adis Alihodžić, Elmedin Selmanović, Damir Hasić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio fingerprinting techniques have seen great advances in recent years,
enabling accurate and fast audio retrieval even in conditions when the queried
audio sample has been highly deteriorated or recorded in noisy conditions.
Expectedly, most of the existing work is centered around music, with popular
music identification services such as Apple's Shazam or Google's Now Playing
designed for individual audio recognition on mobile devices. However, the
spectral content of speech differs from that of music, necessitating
modifications to current audio fingerprinting approaches. This paper offers
fresh insights into adapting existing techniques to address the specialized
challenge of speech retrieval in telecommunications and cloud communications
platforms. The focus is on achieving rapid and accurate audio retrieval in
batch processing instead of facilitating single requests, typically on a
centralized server. Moreover, the paper demonstrates how this approach can be
utilized to support audio clustering based on speech transcripts without
undergoing actual speech-to-text conversion. This optimization enables
significantly faster processing without the need for GPU computing, a
requirement for real-time operation that is typically associated with
state-of-the-art speech-to-text tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PerSRV: Personalized Sticker Retrieval with Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Er Metilda Chee, Jiayin Wang, Zhiqiang Guo, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instant Messaging is a popular means for daily communication, allowing users
to send text and stickers. As the saying goes, "a picture is worth a thousand
words", so developing an effective sticker retrieval technique is crucial for
enhancing user experience. However, existing sticker retrieval methods rely on
labeled data to interpret stickers, and general-purpose Vision-Language Models
(VLMs) often struggle to capture the unique semantics of stickers.
Additionally, relevant-based sticker retrieval methods lack personalization,
creating a gap between diverse user expectations and retrieval results. To
address these, we propose the Personalized Sticker Retrieval with
Vision-Language Model framework, namely PerSRV, structured into offline
calculations and online processing modules. The online retrieval part follows
the paradigm of relevant recall and personalized ranking, supported by the
offline pre-calculation parts, which are sticker semantic understanding,
utility evaluation and personalization modules. Firstly, for sticker-level
semantic understanding, we supervised fine-tuned LLaVA-1.5-7B to generate
human-like sticker semantics, complemented by textual content extracted from
figures and historical interaction queries. Secondly, we investigate three
crowd-sourcing metrics for sticker utility evaluation. Thirdly, we cluster
style centroids based on users' historical interactions to achieve personal
preference modeling. Finally, we evaluate our proposed PerSRV method on a
public sticker retrieval dataset from WeChat, containing 543,098 candidates and
12,568 interactions. Experimental results show that PerSRV significantly
outperforms existing methods in multi-modal sticker retrieval. Additionally,
our fine-tuned VLM delivers notable improvements in sticker semantic
understandings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dual Adaptive Assignment Approach for Robust Graph-Based Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xiang, Li Fan, Tulika Saha, Yushan Pan, Haiyang Zhang, Chengtao Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph clustering is an essential aspect of network analysis that involves
grouping nodes into separate clusters. Recent developments in deep learning
have resulted in advanced deep graph clustering techniques, which have proven
effective in many applications. Nonetheless, these methods often encounter
difficulties when dealing with the complexities of real-world graphs,
particularly in the presence of noisy edges. Additionally, many denoising graph
clustering strategies tend to suffer from lower performance compared to their
non-denoised counterparts, training instability, and challenges in scaling to
large datasets. To tackle these issues, we introduce a new framework called the
Dual Adaptive Assignment Approach for Robust Graph-Based Clustering (RDSA).
RDSA consists of three key components: (i) a node embedding module that
effectively integrates the graph's topological features and node attributes;
(ii) a structure-based soft assignment module that improves graph modularity by
utilizing an affinity matrix for node assignments; and (iii) a node-based soft
assignment module that identifies community landmarks and refines node
assignments to enhance the model's robustness. We assess RDSA on various
real-world datasets, demonstrating its superior performance relative to
existing state-of-the-art methods. Our findings indicate that RDSA provides
robust clustering across different graph types, excelling in clustering
effectiveness and robustness, including adaptability to noise, stability, and
scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroSym-BioCAT: Leveraging Neuro-Symbolic Methods for Biomedical
  Scholarly Document Categorization and Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parvez Zamil, Gollam Rabby, Md. Sadekur Rahman, Sören Auer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing volume of biomedical scholarly document abstracts presents an
increasing challenge in efficiently retrieving accurate and relevant
information. To address this, we introduce a novel approach that integrates an
optimized topic modelling framework, OVB-LDA, with the BI-POP CMA-ES
optimization technique for enhanced scholarly document abstract categorization.
Complementing this, we employ the distilled MiniLM model, fine-tuned on
domain-specific data, for high-precision answer extraction. Our approach is
evaluated across three configurations: scholarly document abstract retrieval,
gold-standard scholarly documents abstract, and gold-standard snippets,
consistently outperforming established methods such as RYGH and bio-answer
finder. Notably, we demonstrate that extracting answers from scholarly
documents abstracts alone can yield high accuracy, underscoring the sufficiency
of abstracts for many biomedical queries. Despite its compact size, MiniLM
exhibits competitive performance, challenging the prevailing notion that only
large, resource-intensive models can handle such complex tasks. Our results,
validated across various question types and evaluation batches, highlight the
robustness and adaptability of our method in real-world biomedical
applications. While our approach shows promise, we identify challenges in
handling complex list-type questions and inconsistencies in evaluation metrics.
Future work will focus on refining the topic model with more extensive
domain-specific datasets, further optimizing MiniLM and utilizing large
language models (LLM) to improve both precision and efficiency in biomedical
question answering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness in Ranking under Disparate Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01610v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01610v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richa Rastogi, Thorsten Joachims
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking is a ubiquitous method for focusing the attention of human evaluators
on a manageable subset of options. Its use as part of human decision-making
processes ranges from surfacing potentially relevant products on an e-commerce
site to prioritizing college applications for human review. While ranking can
make human evaluation more effective by focusing attention on the most
promising options, we argue that it can introduce unfairness if the uncertainty
of the underlying relevance model differs between groups of options.
Unfortunately, such disparity in uncertainty appears widespread, often to the
detriment of minority groups for which relevance estimates can have higher
uncertainty due to a lack of data or appropriate features. To address this
fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness
criterion for ranking and show that it corresponds to a group-wise fair lottery
among the relevant options even in the presence of disparate uncertainty. EOR
optimizes for an even cost burden on all groups, unlike the conventional
Probability Ranking Principle, and is fundamentally different from existing
notions of fairness in rankings, such as demographic parity and proportional
Rooney rule constraints that are motivated by proportional representation
relative to group size. To make EOR ranking practical, we present an efficient
algorithm for computing it in time $O(n \log(n))$ and prove its close
approximation guarantee to the globally optimal solution. In a comprehensive
empirical evaluation on synthetic data, a US Census dataset, and a real-world
audit of Amazon search queries, we find that the algorithm reliably guarantees
EOR fairness while providing effective rankings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version at EAAMO'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Sholehrasa, Sanaz Saki Norouzi, Pascal Hitzler, Majid Jaberi-Douraki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating structured knowledge from tabular formats poses significant
challenges within natural language processing (NLP), mainly when dealing with
complex, semi-structured tables like those found in the FeTaQA dataset. These
tables require advanced methods to interpret and generate meaningful responses
accurately. Traditional approaches, such as SQL and SPARQL, often fail to fully
capture the semantics of such data, especially in the presence of irregular
table structures like web tables. This paper addresses these challenges by
proposing a novel approach that extracts triples straightforward from tabular
data and integrates it with a retrieval-augmented generation (RAG) model to
enhance the accuracy, coherence, and contextual richness of responses generated
by a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly
outperforms existing baselines on the FeTaQA dataset, particularly excelling in
Sacre-BLEU and ROUGE metrics. It effectively generates contextually accurate
and detailed long-form answers from tables, showcasing its strength in complex
data interpretation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We are withdrawing this paper to address foundational aspects that
  are critical for ensuring its accuracy and integrity before any potential
  resubmission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context Embeddings for Efficient Answer Generation in RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09252v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09252v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Rau, Shuai Wang, Hervé Déjean, Stéphane Clinchant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge
of LLMs by extending the input with external information. As a consequence, the
contextual inputs to the model become much longer which slows down decoding
time directly translating to the time a user has to wait for an answer. We
address this challenge by presenting COCOM, an effective context compression
method, reducing long contexts to only a handful of Context Embeddings speeding
up the generation time by a large margin. Our method allows for different
compression rates trading off decoding time for answer quality. Compared to
earlier methods, COCOM allows for handling multiple contexts more effectively,
significantly reducing decoding time for long inputs. Our method demonstrates a
speed-up of up to 5.69 $\times$ while achieving higher performance compared to
existing efficient context compression methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agentic Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What will information entry look like in the next generation of digital
products? Since the 1970s, user access to relevant information has relied on
domain-specific architectures of information retrieval (IR). Over the past two
decades, the advent of modern IR systems, including web search engines and
personalized recommender systems, has greatly improved the efficiency of
retrieving relevant information from vast data corpora. However, the core
paradigm of these IR systems remains largely unchanged, relying on filtering a
predefined set of candidate items. Since 2022, breakthroughs in large language
models (LLMs) have begun transforming how information is accessed, establishing
a new technical paradigm. In this position paper, we introduce Agentic
Information Retrieval (Agentic IR), a novel IR paradigm shaped by the
capabilities of LLM agents. Agentic IR expands the scope of accessible tasks
and leverages a suite of new techniques to redefine information retrieval. We
discuss three types of cutting-edge applications of agentic IR and the
challenges faced. We propose that agentic IR holds promise for generating
innovative applications, potentially becoming a central information entry point
in future digital ecosystems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, position paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building a Scalable, Effective, and Steerable Search and Ranking
  Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marjan Celikik, Jacek Wasilewski, Ana Peleteiro Ramallo, Alexey Kurennoy, Evgeny Labzin, Danilo Ascione, Tural Gurbanov, Géraud Le Falher, Andrii Dzhoha, Ian Harris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern e-commerce platforms offer vast product selections, making it
difficult for customers to find items that they like and that are relevant to
their current session intent. This is why it is key for e-commerce platforms to
have near real-time scalable and adaptable personalized ranking and search
systems. While numerous methods exist in the scientific literature for building
such systems, many are unsuitable for large-scale industrial use due to
complexity and performance limitations. Consequently, industrial ranking
systems often resort to computationally efficient yet simplistic retrieval or
candidate generation approaches, which overlook near real-time and
heterogeneous customer signals, which results in a less personalized and
relevant experience. Moreover, related customer experiences are served by
completely different systems, which increases complexity, maintenance, and
inconsistent experiences.
  In this paper, we present a personalized, adaptable near real-time ranking
platform that is reusable across various use cases, such as browsing and
search, and that is able to cater to millions of items and customers under
heavy load (thousands of requests per second). We employ transformer-based
models through different ranking layers which can learn complex behavior
patterns directly from customer action sequences while being able to
incorporate temporal (e.g. in-session) and contextual information. We validate
our system through a series of comprehensive offline and online real-world
experiments at a large online e-commerce platform, and we demonstrate its
superiority when compared to existing systems, both in terms of customer
experience as well as in net revenue. Finally, we share the lessons learned
from building a comprehensive, modern ranking platform for use in a large-scale
e-commerce environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ USimAgent: Large Language Models for Simulating Search Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erhan Zhang, Xingzhu Wang, Peiyuan Gong, Yankai Lin, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the advantages in the cost-efficiency and reproducibility, user
simulation has become a promising solution to the user-centric evaluation of
information retrieval systems. Nonetheless, accurately simulating user search
behaviors has long been a challenge, because users' actions in search are
highly complex and driven by intricate cognitive processes such as learning,
reasoning, and planning. Recently, Large Language Models (LLMs) have
demonstrated remarked potential in simulating human-level intelligence and have
been used in building autonomous agents for various tasks. However, the
potential of using LLMs in simulating search behaviors has not yet been fully
explored. In this paper, we introduce a LLM-based user search behavior
simulator, USimAgent. The proposed simulator can simulate users' querying,
clicking, and stopping behaviors during search, and thus, is capable of
generating complete search sessions for specific search tasks. Empirical
investigation on a real user behavior dataset shows that the proposed simulator
outperforms existing methods in query generation and is comparable to
traditional methods in predicting user clicks and stopping behaviors. These
results not only validate the effectiveness of using LLMs for user simulation
but also shed light on the development of a more robust and generic user
simulators. The code and data are accessible at
https://github.com/Meow-E/USimAgent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Performance and Bias of Negative Sampling in Large-Scale
  Sequential Recommendation Models <span class="chip">RecSys</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arushi Prakash, Dimitrios Bermperidis, Srivas Chennu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale industrial recommendation models predict the most relevant items
from catalogs containing millions or billions of options. To train these models
efficiently, a small set of irrelevant items (negative samples) is selected
from the vast catalog for each relevant item (positive example), helping the
model distinguish between relevant and irrelevant items. Choosing the right
negative sampling method is a common challenge. We address this by implementing
and comparing various negative sampling methods - random, popularity-based,
in-batch, mixed, adaptive, and adaptive with mixed variants - on modern
sequential recommendation models. Our experiments, including hyperparameter
optimization and 20x repeats on three benchmark datasets with varying
popularity biases, show how the choice of method and dataset characteristics
impact key model performance metrics. We also reveal that average performance
metrics often hide imbalances across popularity bands (head, mid, tail). We
find that commonly used random negative sampling reinforces popularity bias and
performs best for head items. Popularity-based methods (in-batch and global
popularity negative sampling) can offer balanced performance at the cost of
lower overall model performance results. Our study serves as a practical guide
to the trade-offs in selecting a negative sampling method for large-scale
sequential recommendation models. Code, datasets, experimental results and
hyperparameters are available at:
https://github.com/apple/ml-negative-sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop for Large Recommender Systems (LargeRecSys), 18th ACM
  Conference on Recommender Systems, 2024, Bari, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Group Interest Modeling of Full Lifelong User Behaviors for CTR
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Liu, Xuyang Hou, Haoran Jin, Xiaolong Chen, Jin Chen, Defu Lian, Zhe Wang, Jia Cheng, Jun Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting users' interests from their lifelong behavior sequence is crucial
for predicting Click-Through Rate (CTR). Most current methods employ a
two-stage process for efficiency: they first select historical behaviors
related to the candidate item and then deduce the user's interest from this
narrowed-down behavior sub-sequence. This two-stage paradigm, though effective,
leads to information loss. Solely using users' lifelong click behaviors doesn't
provide a complete picture of their interests, leading to suboptimal
performance. In our research, we introduce the Deep Group Interest Network
(DGIN), an end-to-end method to model the user's entire behavior history. This
includes all post-registration actions, such as clicks, cart additions,
purchases, and more, providing a nuanced user understanding. We start by
grouping the full range of behaviors using a relevant key (like item_id) to
enhance efficiency. This process reduces the behavior length significantly,
from O(10^4) to O(10^2). To mitigate the potential loss of information due to
grouping, we incorporate two categories of group attributes. Within each group,
we calculate statistical information on various heterogeneous behaviors (like
behavior counts) and employ self-attention mechanisms to highlight unique
behavior characteristics (like behavior type). Based on this reorganized
behavior data, the user's interests are derived using the Transformer
technique. Additionally, we identify a subset of behaviors that share the same
item_id with the candidate item from the lifelong behavior sequence. The
insights from this subset reveal the user's decision-making process related to
the candidate item, improving prediction accuracy. Our comprehensive
evaluation, both on industrial and public datasets, validates DGIN's efficacy
and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Semantic Communication for Generative Audio-Driven Video
  Conferencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Tong, Haopeng Li, Hongyang Du, Zhaohui Yang, Changchuan Yin, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies an efficient multimodal data communication scheme for
video conferencing. In our considered system, a speaker gives a talk to the
audiences, with talking head video and audio being transmitted. Since the
speaker does not frequently change posture and high-fidelity transmission of
audio (speech and music) is required, redundant visual video data exists and
can be removed by generating the video from the audio. To this end, we propose
a wave-to-video (Wav2Vid) system, an efficient video transmission framework
that reduces transmitted data by generating talking head video from audio. In
particular, full-duration audio and short-duration video data are synchronously
transmitted through a wireless channel, with neural networks (NNs) extracting
and encoding audio and video semantics. The receiver then combines the decoded
audio and video data, as well as uses a generative adversarial network (GAN)
based model to generate the lip movement videos of the speaker. Simulation
results show that the proposed Wav2Vid system can reduce the amount of
transmitted data by up to 83% while maintaining the perceptual quality of the
generated conferencing video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE Wireless Communications Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHORDONOMICON: A <span class="highlight-title">Dataset</span> of 666,000 Songs and their Chord Progressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spyridon Kantarelis, Konstantinos Thomas, Vassilis Lyberatos, Edmund Dervakos, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chord progressions encapsulate important information about music, pertaining
to its structure and conveyed emotions. They serve as the backbone of musical
composition, and in many cases, they are the sole information required for a
musician to play along and follow the music. Despite their importance, chord
progressions as a data domain remain underexplored. There is a lack of
large-scale datasets suitable for deep learning applications, and limited
research exploring chord progressions as an input modality. In this work, we
present Chordonomicon, a dataset of over 666,000 songs and their chord
progressions, annotated with structural parts, genre, and release date -
created by scraping various sources of user-generated progressions and
associated metadata. We demonstrate the practical utility of the Chordonomicon
dataset for classification and generation tasks, and discuss its potential to
provide valuable insights to the research community. Chord progressions are
unique in their ability to be represented in multiple formats (e.g. text,
graph) and the wealth of information chords convey in given contexts, such as
their harmonic function . These characteristics make the Chordonomicon an ideal
testbed for exploring advanced machine learning techniques, including
transformers, graph machine learning, and hybrid systems that combine knowledge
representation and machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Saliency-Based diversity and fairness Metric and
  FaceKeepOriginalAugment: A Novel Approach for Enhancing Fairness and
  Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teerath Kumar, Alessandra Mileo, Malika Bendechache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation has become a pivotal tool in enhancing the performance of
computer vision tasks, with the KeepOriginalAugment method emerging as a
standout technique for its intelligent incorporation of salient regions within
less prominent areas, enabling augmentation in both regions. Despite its
success in image classification, its potential in addressing biases remains
unexplored. In this study, we introduce an extension of the KeepOriginalAugment
method, termed FaceKeepOriginalAugment, which explores various debiasing
aspects-geographical, gender, and stereotypical biases-in computer vision
models. By maintaining a delicate balance between data diversity and
information preservation, our approach empowers models to exploit both diverse
salient and non-salient regions, thereby fostering increased diversity and
debiasing effects. We investigate multiple strategies for determining the
placement of the salient region and swapping perspectives to decide which part
undergoes augmentation. Leveraging the Image Similarity Score (ISS), we
quantify dataset diversity across a range of datasets, including Flickr Faces
HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse
Dataset. We evaluate the effectiveness of FaceKeepOriginalAugment in mitigating
gender bias across CEO, Engineer, Nurse, and School Teacher datasets, utilizing
the Image-Image Association Score (IIAS) in convolutional neural networks
(CNNs) and vision transformers (ViTs). Our findings shows the efficacy of
FaceKeepOriginalAugment in promoting fairness and inclusivity within computer
vision models, demonstrated by reduced gender bias and enhanced overall
fairness. Additionally, we introduce a novel metric, Saliency-Based Diversity
and Fairness Metric, which quantifies both diversity and fairness while
handling data imbalance across various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper is underReview in Image and Vision Computing Journal special
  issue: Advancing Transparency and Privacy: Explainable AI and Synthetic Data
  in Biometrics and Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Learned Image Compression via Cross Window-based Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Mudgal, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, learned image compression methods have demonstrated superior
rate-distortion performance compared to traditional image compression methods.
Recent methods utilize convolutional neural networks (CNN), variational
autoencoders (VAE), invertible neural networks (INN), and transformers. Despite
their significant contributions, a main drawback of these models is their poor
performance in capturing local redundancy. Therefore, to leverage global
features along with local redundancy, we propose a CNN-based solution
integrated with a feature encoding module. The feature encoding module encodes
important features before feeding them to the CNN and then utilizes cross-scale
window-based attention, which further captures local redundancy. Cross-scale
window-based attention is inspired by the attention mechanism in transformers
and effectively enlarges the receptive field. Both the feature encoding module
and the cross-scale window-based attention module in our architecture are
flexible and can be incorporated into any other network architecture. We
evaluate our method on the Kodak and CLIC datasets and demonstrate that our
approach is effective and on par with state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted and presented in ISVC'24. Copyrights stay with ISVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Multi-Track Accompaniment Arrangement via Style Prior
  Modelling <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Zhao, Gus Xia, Ziyu Wang, Ye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of music AI, arranging rich and structured multi-track
accompaniments from a simple lead sheet presents significant challenges. Such
challenges include maintaining track cohesion, ensuring long-term coherence,
and optimizing computational efficiency. In this paper, we introduce a novel
system that leverages prior modelling over disentangled style factors to
address these challenges. Our method presents a two-stage process: initially, a
piano arrangement is derived from the lead sheet by retrieving piano texture
styles; subsequently, a multi-track orchestration is generated by infusing
orchestral function styles into the piano arrangement. Our key design is the
use of vector quantization and a unique multi-stream Transformer to model the
long-term flow of the orchestration style, which enables flexible,
controllable, and structured music generation. Experiments show that by
factorizing the arrangement task into interpretable sub-stages, our approach
enhances generative capacity while improving efficiency. Additionally, our
system supports a variety of music genres and provides style control at
different composition hierarchies. We further show that our system achieves
superior coherence, structure, and overall arrangement quality compared to
existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling Encoder-Free Vision-Language Models <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing vision-language models (VLMs) mostly rely on vision encoders to
extract visual features followed by large language models (LLMs) for
visual-language tasks. However, the vision encoders set a strong inductive bias
in abstracting visual representation, e.g., resolution, aspect ratio, and
semantic priors, which could impede the flexibility and efficiency of the VLMs.
Training pure VLMs that accept the seamless vision and language inputs, i.e.,
without vision encoders, remains challenging and rarely explored. Empirical
observations reveal that direct training without encoders results in slow
convergence and large performance gaps. In this work, we bridge the gap between
encoder-based and encoder-free models, and present a simple yet effective
training recipe towards pure VLMs. Specifically, we unveil the key aspects of
training encoder-free VLMs efficiently via thorough experiments: (1) Bridging
vision-language representation inside one unified decoder; (2) Enhancing visual
recognition capability via extra supervision. With these strategies, we launch
EVE, an encoder-free vision-language model that can be trained and forwarded
efficiently. Notably, solely utilizing 35M publicly accessible data, EVE can
impressively rival the encoder-based VLMs of similar capacities across multiple
vision-language benchmarks. It significantly outperforms the counterpart
Fuyu-8B with mysterious training procedures and undisclosed training data. We
believe that EVE provides a transparent and efficient route for developing a
pure decoder-only architecture across modalities. Our code and models are
publicly available at: https://github.com/baaivision/EVE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, Accepted by NeurIPS2024 (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document Parsing Unveiled: Techniques, Challenges, and Prospects for
  Structured Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang, Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Conghui He, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document parsing is essential for converting unstructured and semi-structured
documents-such as contracts, academic papers, and invoices-into structured,
machine-readable data. Document parsing extract reliable structured data from
unstructured inputs, providing huge convenience for numerous applications.
Especially with recent achievements in Large Language Models, document parsing
plays an indispensable role in both knowledge base construction and training
data generation. This survey presents a comprehensive review of the current
state of document parsing, covering key methodologies, from modular pipeline
systems to end-to-end models driven by large vision-language models. Core
components such as layout detection, content extraction (including text,
tables, and mathematical expressions), and multi-modal data integration are
examined in detail. Additionally, this paper discusses the challenges faced by
modular document parsing systems and vision-language models in handling complex
layouts, integrating multiple modules, and recognizing high-density text. It
emphasizes the importance of developing larger and more diverse datasets and
outlines future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-28T00:00:00Z">2024-10-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Search Evaluation <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chujie Zheng, Jeffrey Wang, Shuqian Albee Zhang, Anand Kishore, Siddharth Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method for evaluating the performance of a content search
system that measures the semantic match between a query and the results
returned by the search system. We introduce a metric called "on-topic rate" to
measure the percentage of results that are relevant to the query. To achieve
this, we design a pipeline that defines a golden query set, retrieves the top K
results for each query, and sends calls to GPT 3.5 with formulated prompts. Our
semantic evaluation pipeline helps identify common failure patterns and goals
against the metric for relevance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 3rd International Workshop on Industrial Recommendation
  Systems (at CIKM 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Users Detect Biases or Factual Errors in Generated Responses in
  Conversational Information-Seeking? <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weronika Łajewska, Krisztian Balog, Damiano Spina, Johanne Trippas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information-seeking dialogues span a wide range of questions, from simple
factoid to complex queries that require exploring multiple facets and
viewpoints. When performing exploratory searches in unfamiliar domains, users
may lack background knowledge and struggle to verify the system-provided
information, making them vulnerable to misinformation. We investigate the
limitations of response generation in conversational information-seeking
systems, highlighting potential inaccuracies, pitfalls, and biases in the
responses. The study addresses the problem of query answerability and the
challenge of response incompleteness. Our user studies explore how these issues
impact user experience, focusing on users' ability to identify biased,
incorrect, or incomplete responses. We design two crowdsourcing tasks to assess
user experience with different system response variants, highlighting critical
issues to be addressed in future conversational information-seeking research.
Our analysis reveals that it is easier for users to detect response
incompleteness than query answerability and user satisfaction is mostly
associated with response diversity, not factual correctness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper that appeared in the Proceedings of the
  2024 Annual International ACM SIGIR Conference on Research and Development in
  Information Retrieval in the Asia Pacific Region (SIGIR-AP '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing CTR Prediction in Recommendation Domain with Search Query
  Representation <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuening Wang, Man Chen, Yaochen Hu, Wei Guo, Yingxue Zhang, Huifeng Guo, Yong Liu, Mark Coates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many platforms, such as e-commerce websites, offer both search and
recommendation services simultaneously to better meet users' diverse needs.
Recommendation services suggest items based on user preferences, while search
services allow users to search for items before providing recommendations.
Since users and items are often shared between the search and recommendation
domains, there is a valuable opportunity to enhance the recommendation domain
by leveraging user preferences extracted from the search domain. Existing
approaches either overlook the shift in user intention between these domains or
fail to capture the significant impact of learning from users' search queries
on understanding their interests.
  In this paper, we propose a framework that learns from user search query
embeddings within the context of user preferences in the recommendation domain.
Specifically, user search query sequences from the search domain are used to
predict the items users will click at the next time point in the recommendation
domain. Additionally, the relationship between queries and items is explored
through contrastive learning. To address issues of data sparsity, the diffusion
model is incorporated to infer positive items the user will select after
searching with certain queries in a denoising manner, which is particularly
effective in preventing false positives. Effectively extracting this
information, the queries are integrated into click-through rate prediction in
the recommendation domain. Experimental analysis demonstrates that our model
outperforms state-of-the-art models in the recommendation domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CIKM 2024 Full Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic <span class="highlight-title">Review</span> of Machine Learning in Sports Betting: Techniques,
  Challenges, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René Manassé Galekwa, Jean Marie Tshimula, Etienne Gael Tajeuna, Kyamakya Kyandoghere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sports betting industry has experienced rapid growth, driven largely by
technological advancements and the proliferation of online platforms. Machine
learning (ML) has played a pivotal role in the transformation of this sector by
enabling more accurate predictions, dynamic odds-setting, and enhanced risk
management for both bookmakers and bettors. This systematic review explores
various ML techniques, including support vector machines, random forests, and
neural networks, as applied in different sports such as soccer, basketball,
tennis, and cricket. These models utilize historical data, in-game statistics,
and real-time information to optimize betting strategies and identify value
bets, ultimately improving profitability. For bookmakers, ML facilitates
dynamic odds adjustment and effective risk management, while bettors leverage
data-driven insights to exploit market inefficiencies. This review also
underscores the role of ML in fraud detection, where anomaly detection models
are used to identify suspicious betting patterns. Despite these advancements,
challenges such as data quality, real-time decision-making, and the inherent
unpredictability of sports outcomes remain. Ethical concerns related to
transparency and fairness are also of significant importance. Future research
should focus on developing adaptive models that integrate multimodal data and
manage risk in a manner akin to financial portfolios. This review provides a
comprehensive examination of the current applications of ML in sports betting,
and highlights both the potential and the limitations of these technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nour Jedidi, Yung-Sung Chuang, Leslie Shing, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building effective dense retrieval systems remains difficult when relevance
supervision is not available. Recent work has looked to overcome this challenge
by using a Large Language Model (LLM) to generate hypothetical documents that
can be used to find the closest real document. However, this approach relies
solely on the LLM to have domain-specific knowledge relevant to the query,
which may not be practical. Furthermore, generating hypothetical documents can
be inefficient as it requires the LLM to generate a large number of tokens for
each query. To address these challenges, we introduce Real Document Embeddings
from Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF
proposes to re-frame hypothetical document generation as a relevance estimation
task, using an LLM to select which documents should be used for nearest
neighbor search. Through this re-framing, the LLM no longer needs
domain-specific knowledge but only needs to judge what is relevant.
Additionally, relevance estimation only requires the LLM to output a single
token, thereby improving search latency. Our experiments show that ReDE-RF
consistently surpasses state-of-the-art zero-shot dense retrieval methods
across a wide range of low-resource retrieval datasets while also making
significant improvements in latency per-query.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Search Assistant: Empower Vision-Language Models as Multimodal
  Search Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Zhang, Yiyuan Zhang, Xiaohan Ding, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search engines enable the retrieval of unknown information with texts.
However, traditional methods fall short when it comes to understanding
unfamiliar visual content, such as identifying an object that the model has
never seen before. This challenge is particularly pronounced for large
vision-language models (VLMs): if the model has not been exposed to the object
depicted in an image, it struggles to generate reliable answers to the user's
question regarding that image. Moreover, as new objects and events continuously
emerge, frequently updating VLMs is impractical due to heavy computational
burdens. To address this limitation, we propose Vision Search Assistant, a
novel framework that facilitates collaboration between VLMs and web agents.
This approach leverages VLMs' visual understanding capabilities and web agents'
real-time information access to perform open-world Retrieval-Augmented
Generation via the web. By integrating visual and textual representations
through this collaboration, the model can provide informed responses even when
the image is novel to the system. Extensive experiments conducted on both
open-set and closed-set QA benchmarks demonstrate that the Vision Search
Assistant significantly outperforms the other models and can be widely applied
to existing VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/cnzzx/VSA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pay Attention to Attention for Sequential Recommendation <span class="chip">RecSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuli Liu, Min Liu, Xiaojing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based approaches have demonstrated remarkable success in various
sequence-based tasks. However, traditional self-attention models may not
sufficiently capture the intricate dependencies within items in sequential
recommendation scenarios. This is due to the lack of explicit emphasis on
attention weights, which play a critical role in allocating attention and
understanding item-to-item correlations. To better exploit the potential of
attention weights and improve the capability of sequential recommendation in
learning high-order dependencies, we propose a novel sequential recommendation
(SR) approach called attention weight refinement (AWRSR). AWRSR enhances the
effectiveness of self-attention by additionally paying attention to attention
weights, allowing for more refined attention distributions of correlations
among items. We conduct comprehensive experiments on multiple real-world
datasets, demonstrating that our approach consistently outperforms
state-of-the-art SR models. Moreover, we provide a thorough analysis of AWRSR's
effectiveness in capturing higher-level dependencies. These findings suggest
that AWRSR offers a promising new direction for enhancing the performance of
self-attention architecture in SR tasks, with potential applications in other
sequence-based problems as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simultaneous Unlearning of Multiple Protected User Attributes From
  Variational Autoencoder Recommenders Using Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Escobedo, Christian Ganhör, Stefan Brandl, Mirjam Augstein, Markus Schedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In widely used neural network-based collaborative filtering models, users'
history logs are encoded into latent embeddings that represent the users'
preferences. In this setting, the models are capable of mapping users'
protected attributes (e.g., gender or ethnicity) from these user embeddings
even without explicit access to them, resulting in models that may treat
specific demographic user groups unfairly and raise privacy issues. While prior
work has approached the removal of a single protected attribute of a user at a
time, multiple attributes might come into play in real-world scenarios. In the
work at hand, we present AdvXMultVAE which aims to unlearn multiple protected
attributes (exemplified by gender and age) simultaneously to improve fairness
across demographic user groups. For this purpose, we couple a variational
autoencoder (VAE) architecture with adversarial training (AdvMultVAE) to
support simultaneous removal of the users' protected attributes with continuous
and/or categorical values. Our experiments on two datasets, LFM-2b-100k and
Ml-1m, from the music and movie domains, respectively, show that our approach
can yield better results than its singular removal counterparts (based on
AdvMultVAE) in effectively mitigating demographic biases whilst improving the
anonymity of latent embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges in Implementing a Recommender System for Historical Research
  in the Humanities <span class="chip">RecSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Atzenhofer-Baumgartner, Bernhard C. Geiger, Christoph Trattner, Georg Vogeler, Dominik Kowald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This extended abstract describes the challenges in implementing recommender
systems for digital archives in the humanities, focusing on Monasterium.net, a
platform for historical legal documents. We discuss three key aspects: (i) the
unique characteristics of so-called charters as items for recommendation, (ii)
the complex multi-stakeholder environment, and (iii) the distinct
information-seeking behavior of scholars in the humanities. By examining these
factors, we aim to contribute to the development of more effective and tailored
recommender systems for (digital) humanities research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at AltRecSys 2024: The First Workshop on Alternative,
  Unexpected, and Critical Ideas in Recommendation, October 18, 2024,
  co-located with the ACM Conference on Recommender Systems 2024 (RecSys 2024),
  Bari, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RecFlow: An Industrial Full Flow Recommendation <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Liu, Kai Zheng, Rui Huang, Wuchao Li, Kuo Cai, Yuan Chai, Yanan Niu, Yiqun Hui, Bing Han, Na Mou, Hongning Wang, Wentian Bao, Yunen Yu, Guorui Zhou, Han Li, Yang Song, Defu Lian, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial recommendation systems (RS) rely on the multi-stage pipeline to
balance effectiveness and efficiency when delivering items from a vast corpus
to users. Existing RS benchmark datasets primarily focus on the exposure space,
where novel RS algorithms are trained and evaluated. However, when these
algorithms transition to real world industrial RS, they face a critical
challenge of handling unexposed items which are a significantly larger space
than the exposed one. This discrepancy profoundly impacts their practical
performance. Additionally, these algorithms often overlook the intricate
interplay between multiple RS stages, resulting in suboptimal overall system
performance. To address this issue, we introduce RecFlow, an industrial full
flow recommendation dataset designed to bridge the gap between offline RS
benchmarks and the real online environment. Unlike existing datasets, RecFlow
includes samples not only from the exposure space but also unexposed items
filtered at each stage of the RS funnel. Our dataset comprises 38M interactions
from 42K users across nearly 9M items with additional 1.9B stage samples
collected from 9.3M online requests over 37 days and spanning 6 stages.
Leveraging the RecFlow dataset, we conduct courageous exploration experiments,
showcasing its potential in designing new algorithms to enhance effectiveness
by incorporating stage-specific samples. Some of these algorithms have already
been deployed online, consistently yielding significant gains. We propose
RecFlow as the first comprehensive benchmark dataset for the RS community,
supporting research on designing algorithms at any stage, study of selection
bias, debiased algorithms, multi-stage consistency and optimality, multi-task
recommendation, and user behavior modeling. The RecFlow dataset, along with the
corresponding source code, is available at
https://github.com/RecFlow-ICLR/RecFlow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging AI and Sentiment Analysis for Forecasting Election Outcomes
  in Mauritius 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Missie Chercheur, Malkenzie Bovafiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the use of AI-driven sentiment analysis as a novel tool
for forecasting election outcomes, focusing on Mauritius' 2024 elections. In
the absence of reliable polling data, we analyze media sentiment toward two
main political parties L'Alliance Lepep and L'Alliance Du Changement by
classifying news articles from prominent Mauritian media outlets as positive,
negative, or neutral. We employ a multilingual BERT-based model and a custom
Sentiment Scoring Algorithm to quantify sentiment dynamics and apply the
Sentiment Impact Score (SIS) for measuring sentiment influence over time. Our
forecast model suggests L'Alliance Du Changement is likely to secure a minimum
of 37 seats, while L'Alliance Lepep is predicted to obtain the remaining 23
seats out of the 60 available. Findings indicate that positive media sentiment
strongly correlates with projected electoral gains, underscoring the role of
media in shaping public perception. This approach not only mitigates media bias
through adjusted scoring but also serves as a reliable alternative to
traditional polling. The study offers a scalable methodology for political
forecasting in regions with limited polling infrastructure and contributes to
advancements in the field of political data science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Streaming Batch Principal Component Analysis for Time Series
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enshuo Yan, Huachuan Wang, Weihao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multivariate time series classification, although current sequence
analysis models have excellent classification capabilities, they show
significant shortcomings when dealing with long sequence multivariate data,
such as prolonged training times and decreased accuracy. This paper focuses on
optimizing model performance for long-sequence multivariate data by mitigating
the impact of extended time series and multiple variables on the model. We
propose a principal component analysis (PCA)-based temporal streaming
compression and dimensionality reduction algorithm for time series data
(temporal streaming batch PCA, TSBPCA), which continuously updates the compact
representation of the entire sequence through streaming PCA time estimation
with time block updates, enhancing the data representation capability of a
range of sequence analysis models. We evaluated this method using various
models on five real datasets, and the experimental results show that our method
performs well in terms of classification accuracy and time efficiency. Notably,
our method demonstrates a trend of increasing effectiveness as sequence length
grows; on the two longest sequence datasets, accuracy improved by about 7.2%,
and execution time decreased by 49.5%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Positive History: Re-ranking with List-level Hybrid Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyan Weng, Yunjia Xi, Weiwen Liu, Bo Chen, Jianghao Lin, Ruiming Tang, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the last stage of recommender systems, re-ranking generates a re-ordered
list that aligns with the user's preference. However, previous works generally
focus on item-level positive feedback as history (e.g., only clicked items) and
ignore that users provide positive or negative feedback on items in the entire
list. This list-level hybrid feedback can reveal users' holistic preferences
and reflect users' comparison behavior patterns manifesting within a list. Such
patterns could predict user behaviors on candidate lists, thus aiding better
re-ranking. Despite appealing benefits, extracting and integrating preferences
and behavior patterns from list-level hybrid feedback into re-ranking multiple
items remains challenging. To this end, we propose Re-ranking with List-level
Hybrid Feedback (dubbed RELIFE). It captures user's preferences and behavior
patterns with three modules: a Disentangled Interest Miner to disentangle the
user's preferences into interests and disinterests, a Sequential Preference
Mixer to learn users' entangled preferences considering the context of
feedback, and a Comparison-aware Pattern Extractor to capture user's behavior
patterns within each list. Moreover, for better integration of patterns,
contrastive learning is adopted to align the behavior patterns of candidate and
historical lists. Extensive experiments show that RELIFE significantly
outperforms SOTA re-ranking baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPRec: Bi-level User Modeling for Deep Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejing Wang, Dong Xu, Xiangyu Zhao, Zhiren Mao, Peng Xiang, Ling Yan, Yao Hu, Zijian Zhang, Xuetao Wei, Qidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPRec explicitly categorizes users into groups in a learnable manner and
aligns them with corresponding group embeddings. We design the dual group
embedding space to offer a diverse perspective on group preferences by
contrasting positive and negative patterns. On the individual level, GPRec
identifies personal preferences from ID-like features and refines the obtained
individual representations to be independent of group ones, thereby providing a
robust complement to the group-level modeling. We also present various
strategies for the flexible integration of GPRec into various DRS models.
Rigorous testing of GPRec on three public datasets has demonstrated significant
improvements in recommendation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenUP: Generative User Profilers as In-Context Learners for Next POI
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wilson Wongso, Hao Xue, Flora D. Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional POI recommendation systems often lack transparency,
interpretability, and scrutability due to their reliance on dense vector-based
user embeddings. Furthermore, the cold-start problem -- where systems have
insufficient data for new users -- limits their ability to generate accurate
recommendations. Existing methods often address this by leveraging similar
trajectories from other users, but this approach can be computationally
expensive and increases the context length for LLM-based methods, making them
difficult to scale. To address these limitations, we propose a method that
generates natural language (NL) user profiles from large-scale, location-based
social network (LBSN) check-ins, utilizing robust personality assessments and
behavioral theories. These NL profiles capture user preferences, routines, and
behaviors, improving POI prediction accuracy while offering enhanced
transparency. By incorporating NL profiles as system prompts to LLMs, our
approach reduces reliance on extensive historical data, while remaining
flexible, easily updated, and computationally efficient. Our method is not only
competitive with other LLM-based and complex agentic frameworks but is also
more scalable for real-world scenarios and on-device POI recommendations.
Results demonstrate that our approach consistently outperforms baseline
methods, offering a more interpretable and resource-efficient solution for POI
recommendation systems. Our source code is available at:
\url{https://github.com/w11wo/GenUP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Knowledge Fusion: A Novel Approach for Multi-task
  Recommender Systems via LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Zhao, Xing Su, Ming He, Hongke Zhao, Jianping Fan, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the impressive general intelligence of large language models (LLMs),
there has been a growing trend to integrate them into recommender systems to
gain a more profound insight into human interests and intentions. Existing
LLMs-based recommender systems primarily leverage item attributes and user
interaction histories in textual format, improving the single task like rating
prediction or explainable recommendation. Nevertheless, these approaches
overlook the crucial contribution of traditional collaborative signals in
discerning users' profound intentions and disregard the interrelatedness among
tasks. To address these limitations, we introduce a novel framework known as
CKF, specifically developed to boost multi-task recommendations via
personalized collaborative knowledge fusion into LLMs. Specifically, our method
synergizes traditional collaborative filtering models to produce collaborative
embeddings, subsequently employing the meta-network to construct personalized
mapping bridges tailored for each user. Upon mapped, the embeddings are
incorporated into meticulously designed prompt templates and then fed into an
advanced LLM to represent user interests. To investigate the intrinsic
relationship among diverse recommendation tasks, we develop Multi-Lora, a new
parameter-efficient approach for multi-task optimization, adept at distinctly
segregating task-shared and task-specific information. This method forges a
connection between LLMs and recommendation scenarios, while simultaneously
enriching the supervisory signal through mutual knowledge transfer among
various tasks. Extensive experiments and in-depth robustness analyses across
four common recommendation tasks on four large public data sets substantiate
the effectiveness and superiority of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does Message Passing Improve Collaborative Filtering? <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Ju, William Shiao, Zhichun Guo, Yanfang Ye, Yozen Liu, Neil Shah, Tong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering (CF) has exhibited prominent results for recommender
systems and been broadly utilized for real-world applications. A branch of
research enhances CF methods by message passing used in graph neural networks,
due to its strong capabilities of extracting knowledge from graph-structured
data, like user-item bipartite graphs that naturally exist in CF. They assume
that message passing helps CF methods in a manner akin to its benefits for
graph-based learning tasks in general. However, even though message passing
empirically improves CF, whether or not this assumption is correct still needs
verification. To address this gap, we formally investigate why message passing
helps CF from multiple perspectives and show that many assumptions made by
previous works are not entirely accurate. With our curated ablation studies and
theoretical analyses, we discover that (1) message passing improves the CF
performance primarily by additional representations passed from neighbors
during the forward pass instead of additional gradient updates to neighbor
representations during the model back-propagation and (ii) message passing
usually helps low-degree nodes more than high-degree nodes. Utilizing these
novel findings, we present Test-time Aggregation for CF, namely TAG-CF, a
test-time augmentation framework that only conducts message passing once at
inference time. The key novelty of TAG-CF is that it effectively utilizes graph
knowledge while circumventing most of notorious computational overheads of
message passing. Besides, TAG-CF is extremely versatile can be used as a
plug-and-play module to enhance representations trained by different CF
supervision signals. Evaluated on six datasets, TAG-CF consistently improves
the recommendation performance of CF methods without graph by up to 39.2% on
cold users and 31.7% on all users, with little to no extra computational
overheads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS'24. Code available at:
  https://github.com/snap-research/Test-time-Aggregation-for-CF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing Brittleness of Image-Text Retrieval Benchmarks from
  Vision-Language Models Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15239v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15239v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariya Hendriksen, Shuo Zhang, Ridho Reinanda, Mohamed Yahya, Edgar Meij, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the brittleness of the image-text retrieval (ITR) evaluation
pipeline with a focus on concept granularity. We start by analyzing two common
benchmarks, MS-COCO and Flickr30k, and compare them with augmented,
fine-grained versions, MS-COCO-FG and Flickr30k-FG, given a specified set of
linguistic features capturing concept granularity. Flickr30k-FG and MS COCO-FG
consistently give rise to higher scores across all the selected features. To
further our understanding of the impact of granularity we consider a novel
taxonomy of query perturbations. We apply these perturbations to the selected
datasets. We evaluate four diverse state-of-the-art Vision-Language models on
both the standard and fine-grained datasets under zero-shot conditions, with
and without the applied perturbations. The results demonstrate that although
perturbations generally degrade model performance, the fine-grained datasets
exhibit a smaller performance drop than their standard counterparts. The
relative performance drop across all setups is consistent across all models and
datasets, indicating that the issue lies within the benchmarks themselves. We
conclude by providing an agenda for improving ITR evaluation pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sentiment-Driven Community Detection in a Network of Perfume Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19177v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19177v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamand Kalashi, Sajjad Saed, Babak Teimourpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network analysis is increasingly important across various fields, including
the fragrance industry, where perfumes are represented as nodes and shared user
preferences as edges in perfume networks. Community detection can uncover
clusters of similar perfumes, providing insights into consumer preferences,
enhancing recommendation systems, and informing targeted marketing strategies.
  This study aims to apply community detection techniques to group perfumes
favored by users into relevant clusters for better recommendations. We
constructed a bipartite network from user reviews on the Persian retail
platform "Atrafshan," with nodes representing users and perfumes, and edges
formed by positive comments. This network was transformed into a Perfume
Co-Preference Network, connecting perfumes liked by the same users. By applying
community detection algorithms, we identified clusters based on shared
preferences, enhancing our understanding of user sentiment in the fragrance
market.
  To improve sentiment analysis, we integrated emojis and a user voting system
for greater accuracy. Emojis, aligned with their Persian counterparts, captured
the emotional tone of reviews, while user ratings for scent, longevity, and
sillage refined sentiment classification. Edge weights were adjusted by
combining adjacency values with user ratings in a 60:40 ratio, reflecting both
connection strength and user preferences. These enhancements led to improved
modularity of detected communities, resulting in more accurate perfume
groupings.
  This research pioneers the use of community detection in perfume networks,
offering new insights into consumer preferences. Our advancements in sentiment
analysis and edge weight refinement provide actionable insights for optimizing
product recommendations and marketing strategies in the fragrance industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transforming Location Retrieval at Airbnb: A Journey from Heuristics to
  Reinforcement Learning <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dillon Davis, Huiji Gao, Thomas Legrand, Weiwei Guo, Malay Haldar, Alex Deng, Han Zhao, Liwei He, Sanjeev Katariya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Airbnb search system grapples with many unique challenges as it continues
to evolve. We oversee a marketplace that is nuanced by geography, diversity of
homes, and guests with a variety of preferences. Crafting an efficient search
system that can accommodate diverse guest needs, while showcasing relevant
homes lies at the heart of Airbnb's success. Airbnb search has many challenges
that parallel other recommendation and search systems but it has a unique
information retrieval problem, upstream of ranking, called location retrieval.
It requires defining a topological map area that is relevant to the searched
query for homes listing retrieval. The purpose of this paper is to demonstrate
the methodology, challenges, and impact of building a machine learning based
location retrieval product from the ground up. Despite the lack of suitable,
prevalent machine learning based approaches, we tackle cold start,
generalization, differentiation and algorithmic bias. We detail the efficacy of
heuristics, statistics, machine learning, and reinforcement learning approaches
to solve these challenges, particularly for systems that are often unexplored
by current literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19014v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19014v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegyu Kim, Taeyang Jeon, Seunghwan Choi, Seungtaek Choi, Hyunsouk Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL systems have become crucial for translating natural language into
SQL queries in various industries, enabling non-technical users to perform
complex data operations. The need for accurate evaluation methods has increased
as these systems have grown more sophisticated. However, the Execution Accuracy
(EX), the most prevalent evaluation metric, still shows many false positives
and negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel
approach to evaluating text-to-SQL systems using large language models (LLMs)
to emulate human expert-level evaluation of SQL queries. Our metric improves
agreement with human experts (from 62 to 87.04 in Cohen's kappa) with
comprehensive context and sophisticated criteria. Our extensive experiments
yield several key insights: (1) Models' performance increases by over 2.6
points on average, substantially affecting rankings on Spider and BIRD
benchmarks; (2) The underestimation of models in EX primarily stems from
annotation quality issues; and (3) Model performance on particularly
challenging questions tends to be overestimated. This work contributes to a
more accurate and nuanced evaluation of text-to-SQL systems, potentially
reshaping our understanding of state-of-the-art performance in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenResearcher: Unleashing AI for Accelerated Scientific Research <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, Yang Xu, Qingkai Min, Zizhao Zhang, Yiwen Wang, Wenjie Li, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of scientific literature imposes significant challenges for
researchers endeavoring to stay updated with the latest advancements in their
fields and delve into new areas. We introduce OpenResearcher, an innovative
platform that leverages Artificial Intelligence (AI) techniques to accelerate
the research process by answering diverse questions from researchers.
OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to
integrate Large Language Models (LLMs) with up-to-date, domain-specific
knowledge. Moreover, we develop various tools for OpenResearcher to understand
researchers' queries, search from the scientific literature, filter retrieved
information, provide accurate and comprehensive answers, and self-refine these
answers. OpenResearcher can flexibly use these tools to balance efficiency and
effectiveness. As a result, OpenResearcher enables researchers to save time and
increase their potential to discover new insights and drive scientific
breakthroughs. Demo, video, and code are available at:
https://github.com/GAIR-NLP/OpenResearcher.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Demo track of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Late Than Never: Formulating and Benchmarking Recommendation
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyu Lai, Sheng Zhou, Zhimeng Jiang, Qiaoyu Tan, Yuanchen Bei, Jiawei Chen, Ningyu Zhang, Jiajun Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems play a pivotal role in suggesting items to users based
on their preferences. However, in online platforms, these systems inevitably
offer unsuitable recommendations due to limited model capacity, poor data
quality, or evolving user interests. Enhancing user experience necessitates
efficiently rectify such unsuitable recommendation behaviors. This paper
introduces a novel and significant task termed recommendation editing, which
focuses on modifying known and unsuitable recommendation behaviors.
Specifically, this task aims to adjust the recommendation model to eliminate
known unsuitable items without accessing training data or retraining the model.
We formally define the problem of recommendation editing with three primary
objectives: strict rectification, collaborative rectification, and concentrated
rectification. Three evaluation metrics are developed to quantitatively assess
the achievement of each objective. We present a straightforward yet effective
benchmark for recommendation editing using novel Editing Bayesian Personalized
Ranking Loss. To demonstrate the effectiveness of the proposed method, we
establish a comprehensive benchmark that incorporates various methods from
related fields. Codebase is available at
https://github.com/cycl2018/Recommendation-Editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Information Extraction in Low-Resource Scenarios: <span class="highlight-title">Survey</span> and Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08063v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08063v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shumin Deng, Yubo Ma, Ningyu Zhang, Yixin Cao, Bryan Hooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Extraction (IE) seeks to derive structured information from
unstructured texts, often facing challenges in low-resource scenarios due to
data scarcity and unseen classes. This paper presents a review of neural
approaches to low-resource IE from \emph{traditional} and \emph{LLM-based}
perspectives, systematically categorizing them into a fine-grained taxonomy.
Then we conduct empirical study on LLM-based methods compared with previous
state-of-the-art models, and discover that (1) well-tuned LMs are still
predominant; (2) tuning open-resource LLMs and ICL with GPT family is promising
in general; (3) the optimal LLM-based technical solution for low-resource IE
can be task-dependent. In addition, we discuss low-resource IE with LLMs,
highlight promising applications, and outline potential research directions.
This survey aims to foster understanding of this field, inspire new ideas, and
encourage widespread applications in both academia and industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 15th IEEE International Conference on Knowledge Graphs
  (ICKG2024). Paper List:
  \url{https://github.com/zjunlp/Low-resource-KEPapers}; Data and Code: \url{
  https://github.com/mayubo2333/LLM_project}</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Distillation for Real-Time Classification of Early Media in
  Voice Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kemal Altwlkany, Hadžem Hadžić, Amar Kurić, Emanuel Lacic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the industrial setting of real-time classification of
early media exchanged during the initialization phase of voice calls. We
explore the application of state-of-the-art audio tagging models and highlight
some limitations when applied to the classification of early media. While most
existing approaches leverage convolutional neural networks, we propose a novel
approach for low-resource requirements based on gradient-boosted trees. Our
approach not only demonstrates a substantial improvement in runtime
performance, but also exhibits a comparable accuracy. We show that leveraging
knowledge distillation and class aggregation techniques to train a simpler and
smaller model accelerates the classification of early media in voice calls. We
provide a detailed analysis of the results on a proprietary and publicly
available dataset, regarding accuracy and runtime performance. We additionally
report a case study of the achieved performance improvements at a regional data
center in India.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xize Cheng, Siqi Zheng, Zehan Wang, Minghui Fang, Ziang Zhang, Rongjie Huang, Ziyang Ma, Shengpeng Ji, Jialong Zuo, Tao Jin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling up has brought tremendous success in the fields of vision and
language in recent years. When it comes to audio, however, researchers
encounter a major challenge in scaling up the training data, as most natural
audio contains diverse interfering signals. To address this limitation, we
introduce Omni-modal Sound Separation (OmniSep), a novel framework capable of
isolating clean soundtracks based on omni-modal queries, encompassing both
single-modal and multi-modal composed queries. Specifically, we introduce the
Query-Mixup strategy, which blends query features from different modalities
during training. This enables OmniSep to optimize multiple modalities
concurrently, effectively bringing all modalities under a unified framework for
sound separation. We further enhance this flexibility by allowing queries to
influence sound separation positively or negatively, facilitating the retention
or removal of specific sounds as desired. Finally, OmniSep employs a
retrieval-augmented approach known as Query-Aug, which enables open-vocabulary
sound separation. Experimental evaluations on MUSIC, VGGSOUND-CLEAN+, and
MUSIC-CLEAN+ datasets demonstrate effectiveness of OmniSep, achieving
state-of-the-art performance in text-, image-, and audio-queried sound
separation tasks. For samples and further information, please visit the demo
page at \url{https://omnisep.github.io/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative
  Framework <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Arkhipkin, Viacheslav Vasilev, Andrei Filatov, Igor Pavlov, Julia Agafonova, Nikolai Gerasimenko, Anna Averchenkova, Evelina Mironova, Anton Bukashkin, Konstantin Kulikov, Andrey Kuznetsov, Denis Dimitrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) diffusion models are popular for introducing image
manipulation methods, such as editing, image fusion, inpainting, etc. At the
same time, image-to-video (I2V) and text-to-video (T2V) models are also built
on top of T2I models. We present Kandinsky 3, a novel T2I model based on latent
diffusion, achieving a high level of quality and photorealism. The key feature
of the new architecture is the simplicity and efficiency of its adaptation for
many types of generation tasks. We extend the base T2I model for various
applications and create a multifunctional generation system that includes
text-guided inpainting/outpainting, image fusion, text-image fusion, image
variations generation, I2V and T2V generation. We also present a distilled
version of the T2I model, evaluating inference in 4 steps of the reverse
process without reducing image quality and 3 times faster than the base model.
We deployed a user-friendly demo system in which all the features can be tested
in the public domain. Additionally, we released the source code and checkpoints
for the Kandinsky 3 and extended models. Human evaluations show that Kandinsky
3 demonstrates one of the highest quality scores among open source generation
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for EMNLP 2024 (Demo track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairStream: Fair Multimedia Streaming Benchmark for Reinforcement
  Learning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannis Weil, Jonas Ringsdorf, Julian Barthel, Yi-Ping Phoebe Chen, Tobias Meuser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia streaming accounts for the majority of traffic in today's
internet. Mechanisms like adaptive bitrate streaming control the bitrate of a
stream based on the estimated bandwidth, ideally resulting in smooth playback
and a good Quality of Experience (QoE). However, selecting the optimal bitrate
is challenging under volatile network conditions. This motivated researchers to
train Reinforcement Learning (RL) agents for multimedia streaming. The
considered training environments are often simplified, leading to promising
results with limited applicability. Additionally, the QoE fairness across
multiple streams is seldom considered by recent RL approaches. With this work,
we propose a novel multi-agent environment that comprises multiple challenges
of fair multimedia streaming: partial observability, multiple objectives, agent
heterogeneity and asynchronicity. We provide and analyze baseline approaches
across five different traffic classes to gain detailed insights into the
behavior of the considered agents, and show that the commonly used Proximal
Policy Optimization (PPO) algorithm is outperformed by a simple greedy
heuristic. Future work includes the adaptation of multi-agent RL algorithms and
further expansions of the environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-Instruct*: Towards Human-Preferred One-step Text-to-image
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Luo, Colin Zhang, Debing Zhang, Zhengyang Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the Diff-Instruct*(DI*), a data-free approach for
building one-step text-to-image generative models that align with human
preference while maintaining the ability to generate highly realistic images.
We frame human preference alignment as online reinforcement learning using
human feedback (RLHF), where the goal is to maximize the reward function while
regularizing the generator distribution to remain close to a reference
diffusion process. Unlike traditional RLHF approaches, which rely on the KL
divergence for regularization, we introduce a novel score-based divergence
regularization, which leads to significantly better performances. Although the
direct calculation of this divergence remains intractable, we demonstrate that
we can efficiently compute its \emph{gradient} by deriving an equivalent yet
tractable loss function. Remarkably, with Stable Diffusion V1.5 as the
reference diffusion model, DI* outperforms \emph{all} previously leading models
by a large margin. When using the 0.6B PixelArt-$\alpha$ model as the reference
diffusion, DI* achieves a new record Aesthetic Score of 6.30 and an Image
Reward of 1.31 with only a single generation step, almost doubling the scores
of the rest of the models with similar sizes. It also achieves an HPSv2 score
of 28.70, establishing a new state-of-the-art benchmark. We also observe that
DI* can improve the layout and enrich the colors of generated images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ByteNet: Rethinking Multimedia File Fragment Classification through
  Visual Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyang Liu, Kejun Wu, Tianyi Liu, Yi Wang, Kim-Hui Yap, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia file fragment classification (MFFC) aims to identify file fragment
types, e.g., image/video, audio, and text without system metadata. It is of
vital importance in multimedia storage and communication. Existing MFFC methods
typically treat fragments as 1D byte sequences and emphasize the relations
between separate bytes (interbytes) for classification. However, the more
informative relations inside bytes (intrabytes) are overlooked and seldom
investigated. By looking inside bytes, the bit-level details of file fragments
can be accessed, enabling a more accurate classification. Motivated by this, we
first propose Byte2Image, a novel visual representation model that incorporates
previously overlooked intrabyte information into file fragments and
reinterprets these fragments as 2D grayscale images. This model involves a
sliding byte window to reveal the intrabyte information and a rowwise stacking
of intrabyte ngrams for embedding fragments into a 2D space. Thus, complex
interbyte and intrabyte correlations can be mined simultaneously using powerful
vision networks. Additionally, we propose an end-to-end dual-branch network
ByteNet to enhance robust correlation mining and feature representation.
ByteNet makes full use of the raw 1D byte sequence and the converted 2D image
through a shallow byte branch feature extraction (BBFE) and a deep image branch
feature extraction (IBFE) network. In particular, the BBFE, composed of a
single fully-connected layer, adaptively recognizes the co-occurrence of
several some specific bytes within the raw byte sequence, while the IBFE, built
on a vision Transformer, effectively mines the complex interbyte and intrabyte
correlations from the converted image. Experiments on the two representative
benchmarks, including 14 cases, validate that our proposed method outperforms
state-of-the-art approaches on different cases by up to 12.2%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segmenting Watermarked Texts From Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchi Li, Guanxun Li, Xianyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking is a technique that involves embedding nearly unnoticeable
statistical signals within generated content to help trace its source. This
work focuses on a scenario where an untrusted third-party user sends prompts to
a trusted language model (LLM) provider, who then generates a text from their
LLM with a watermark. This setup makes it possible for a detector to later
identify the source of the text if the user publishes it. The user can modify
the generated text by substitutions, insertions, or deletions. Our objective is
to develop a statistical method to detect if a published text is LLM-generated
from the perspective of a detector. We further propose a methodology to segment
the published text into watermarked and non-watermarked sub-strings. The
proposed approach is built upon randomization tests and change point detection
techniques. We demonstrate that our method ensures Type I and Type II error
control and can accurately identify watermarked sub-strings by finding the
corresponding change point locations. To validate our technique, we apply it to
texts generated by several language models with prompts extracted from Google's
C4 dataset and obtain encouraging numerical results. We release all code
publicly at https://github.com/doccstat/llm-watermark-cpd.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures, 2 tables, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Gloss-free Sign Language Translation by Reducing
  Representation Density <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhui Ye, Xing Wang, Wenxiang Jiao, Junwei Liang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gloss-free sign language translation (SLT) aims to develop well-performing
SLT systems with no requirement for the costly gloss annotations, but currently
still lags behind gloss-based approaches significantly. In this paper, we
identify a representation density problem that could be a bottleneck in
restricting the performance of gloss-free SLT. Specifically, the representation
density problem describes that the visual representations of semantically
distinct sign gestures tend to be closely packed together in feature space,
which makes gloss-free methods struggle with distinguishing different sign
gestures and suffer from a sharp performance drop. To address the
representation density problem, we introduce a simple but effective contrastive
learning strategy, namely SignCL, which encourages gloss-free models to learn
more discriminative feature representation in a self-supervised manner. Our
experiments demonstrate that the proposed SignCL can significantly reduce the
representation density and improve performance across various translation
frameworks. Specifically, SignCL achieves a significant improvement in BLEU
score for the Sign Language Transformer and GFSLT-VLP on the CSL-Daily dataset
by 39% and 46%, respectively, without any increase of model parameters.
Compared to Sign2GPT, a state-of-the-art method based on large-scale
pre-trained vision and language models, SignCL achieves better performance with
only 35% of its parameters. Implementation and Checkpoints are available at
https://github.com/JinhuiYE/SignCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS'24; Representation Density Problem and
  Performance Drop in Gloss-free SLT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohao Guo, Xianghua Ying, Yaru Chen, Dantong Niu, Guangyao Li, Liao Qu, Yanyu Qi, Bowei Xing, Wenzhen Yue, Ji Shi, Qixun Wang, Peiliang Zhang, Buwen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new multi-modal task, termed audio-visual
instance segmentation (AVIS), which aims to simultaneously identify, segment
and track individual sounding object instances in audible videos. To facilitate
this research, we introduce a high-quality benchmark named AVISeg, containing
over 90K instance masks from 26 semantic categories in 926 long videos.
Additionally, we propose a strong baseline model for this task. Our model first
localizes sound source within each frame, and condenses object-specific
contexts into concise tokens. Then it builds long-range audio-visual
dependencies between these tokens using window-based attention, and tracks
sounding objects among the entire video sequences. Extensive experiments reveal
that our method performs best on AVISeg, surpassing the existing methods from
related tasks. We further conduct the evaluation on several multi-modal large
models; however, they exhibits subpar performance on instance-level sound
source localization and temporal perception. We expect that AVIS will inspire
the community towards a more comprehensive multi-modal understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Explicit and Implicit Cross-Modal Interaction Network for Anterior
  Chamber Inflammation Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06171v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06171v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Ye Dai, Haochao Ying, Kan Xu, Jinhong Wang, Wei Chi, Jian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uveitis demands the precise diagnosis of anterior chamber inflammation (ACI)
for optimal treatment. However, current diagnostic methods only rely on a
limited single-modal disease perspective, which leads to poor performance. In
this paper, we investigate a promising yet challenging way to fuse multimodal
data for ACI diagnosis. Notably, existing fusion paradigms focus on empowering
implicit modality interactions (i.e., self-attention and its variants), but
neglect to inject explicit modality interactions, especially from clinical
knowledge and imaging property. To this end, we propose a jointly Explicit and
implicit Cross-Modal Interaction Network (EiCI-Net) for Anterior Chamber
Inflammation Diagnosis that uses anterior segment optical coherence tomography
(AS-OCT) images, slit-lamp images, and clinical data jointly. Specifically, we
first develop CNN-Based Encoders and Tabular Processing Module (TPM) to extract
efficient feature representations in different modalities. Then, we devise an
Explicit Cross-Modal Interaction Module (ECIM) to generate attention maps as a
kind of explicit clinical knowledge based on the tabular feature maps, then
integrated them into the slit-lamp feature maps, allowing the CNN-Based Encoder
to focus on more effective informativeness of the slit-lamp images. After that,
the Implicit Cross-Modal Interaction Module (ICIM), a transformer-based
network, further implicitly enhances modality interactions. Finally, we
construct a considerable real-world dataset from our collaborative hospital and
conduct sufficient experiments to demonstrate the superior performance of our
proposed EiCI-Net compared with the state-of-the-art classification methods in
various metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE MedAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double Mixture: Towards Continual Event Detection from Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingqi Kang, Tongtong Wu, Jinming Zhao, Guitao Wang, Yinwei Wei, Hao Yang, Guilin Qi, Yuan-Fang Li, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech event detection is crucial for multimedia retrieval, involving the
tagging of both semantic and acoustic events. Traditional ASR systems often
overlook the interplay between these events, focusing solely on content, even
though the interpretation of dialogue can vary with environmental context. This
paper tackles two primary challenges in speech event detection: the continual
integration of new events without forgetting previous ones, and the
disentanglement of semantic from acoustic events. We introduce a new task,
continual event detection from speech, for which we also provide two benchmark
datasets. To address the challenges of catastrophic forgetting and effective
disentanglement, we propose a novel method, 'Double Mixture.' This method
merges speech expertise with robust memory mechanisms to enhance adaptability
and prevent forgetting. Our comprehensive experiments show that this task
presents significant challenges that are not effectively addressed by current
state-of-the-art methods in either computer vision or natural language
processing. Our approach achieves the lowest rates of forgetting and the
highest levels of generalization, proving robust across various continual
learning sequences. Our code and data are available at
https://anonymous.4open.science/status/Continual-SpeechED-6461.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MemeCLIP: Leveraging CLIP Representations for Multimodal Meme
  Classification <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Bikram Shah, Shuvam Shiwakoti, Maheep Chaudhary, Haohan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity of text-embedded images presents a formidable challenge in
machine learning given the need for multimodal understanding of multiple
aspects of expression conveyed by them. While previous research in multimodal
analysis has primarily focused on singular aspects such as hate speech and its
subclasses, this study expands this focus to encompass multiple aspects of
linguistics: hate, targets of hate, stance, and humor. We introduce a novel
dataset PrideMM comprising 5,063 text-embedded images associated with the
LGBTQ+ Pride movement, thereby addressing a serious gap in existing resources.
We conduct extensive experimentation on PrideMM by using unimodal and
multimodal baseline methods to establish benchmarks for each task.
Additionally, we propose a novel framework MemeCLIP for efficient downstream
learning while preserving the knowledge of the pre-trained CLIP model. The
results of our experiments show that MemeCLIP achieves superior performance
compared to previously proposed frameworks on two real-world datasets. We
further compare the performance of MemeCLIP and zero-shot GPT-4 on the hate
classification task. Finally, we discuss the shortcomings of our model by
qualitatively analyzing misclassified samples. Our code and dataset are
publicly available at: https://github.com/SiddhantBikram/MemeCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-11-05T09:08:44.938100450Z">
            2024-11-05 09:08:44 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
