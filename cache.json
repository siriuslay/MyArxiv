{"2024-10-31T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.24218v1","updated":"2024-10-31T17:59:52Z","published":"2024-10-31T17:59:52Z","title":"Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use","summary":"  In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL\n","authors":["Jiajun Xi","Yinong He","Jianing Yang","Yinpei Dai","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2410.24218v1.pdf","comment":"EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL"},{"id":"http://arxiv.org/abs/2410.24201v1","updated":"2024-10-31T17:55:45Z","published":"2024-10-31T17:55:45Z","title":"P-Masking: Power Law Masking Improves Multi-attribute Controlled\n  Generation","summary":"  We introduce LingGen, a novel approach for controlled text generation that\noffers precise control over a wide array of linguistic attributes, even as the\nnumber of attributes varies. LingGen employs a dynamic P-MASKING strategy,\nwhich samples masking rates from a power law distribution during training. This\ninnovative approach enables the model to develop robust representations and\nadapt its attribute control capabilities across a variable number of\nattributes, from a single attribute to multiple complex configurations. The\nP-MASKING technique enhances LingGen's ability to manage different levels of\nattribute visibility, resulting in superior performance in multi-attribute\ngeneration tasks. Our experiments demonstrate that LingGen surpasses current\nstate-of-the-art models in both attribute control accuracy and text fluency,\nparticularly excelling in scenarios with varying attribute demands.\nAdditionally, our ablation studies highlight the effectiveness of P-MASKING and\nthe influence of different base language models on performance. These findings\ndemonstrate LingGen's potential for applications requiring precise and\nadaptable control over multiple linguistic attributes in text generation.\n","authors":["Mohamed Elgaar","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2410.24201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24200v1","updated":"2024-10-31T17:55:36Z","published":"2024-10-31T17:55:36Z","title":"Length-Induced Embedding Collapse in Transformer-based Models","summary":"  Text embeddings enable various applications, but their performance\ndeteriorates on longer texts. In this paper, we find that the performance\ndegradation is due to a phenomenon called Length Collapse, where longer text\nembeddings collapse into a narrow space. This collapse results in a\ndistributional inconsistency between embeddings of different text lengths,\nultimately hurting the performance of downstream tasks. Theoretically, by\nconsidering the self-attention mechanism inherently functions as a low-pass\nfilter, we prove that long sequences increase the attenuation rate of the\nlow-pass filter effect of the self-attention mechanism. With layers going\ndeeper, excessive low-pass filtering causes the token signals to retain only\ntheir Direct-Current (DC) component, which means the input token feature maps\nwill collapse into a narrow space, especially in long texts. Based on the above\nanalysis, we propose to mitigate the undesirable length collapse limitation by\nintroducing a temperature in softmax(), which achieves a higher low-filter\nattenuation rate. The tuning-free method, called TempScale, can be plugged into\nmultiple transformer-based embedding models. Empirically, we demonstrate that\nTempScale can improve existing embedding models, especially on long text\ninputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text\nEmbedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from\nLongEmbed, which specifically focuses on long context retrieval.\n","authors":["Yuqi Zhou","Sunhao Dai","Zhanshuo Cao","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.24200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24199v1","updated":"2024-10-31T17:55:27Z","published":"2024-10-31T17:55:27Z","title":"Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation","summary":"  We present a novel approach to paraphrase generation that enables precise\ncontrol and fine-tuning of 40 linguistic attributes for English. Our model is\nan encoder-decoder architecture that takes as input a source sentence and\ndesired linguistic attributes, and produces paraphrases of the source that\nsatisfy the desired attributes. To guarantee high-quality outputs at inference\ntime, our method is equipped with a quality control mechanism that gradually\nadjusts the embedding of linguistic attributes to find the nearest and most\nattainable configuration of desired attributes for paraphrase generation. We\nevaluate the effectiveness of our method by comparing it to recent controllable\ngeneration models. Experimental results demonstrate that the proposed model\noutperforms baselines in generating paraphrases that satisfy desired linguistic\nattributes.\n","authors":["Mohamed Elgaar","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2410.24199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24198v1","updated":"2024-10-31T17:55:13Z","published":"2024-10-31T17:55:13Z","title":"SelfCodeAlign: Self-Alignment for Code Generation","summary":"  Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.\n","authors":["Yuxiang Wei","Federico Cassano","Jiawei Liu","Yifeng Ding","Naman Jain","Zachary Mueller","Harm de Vries","Leandro von Werra","Arjun Guha","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24198v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24190v1","updated":"2024-10-31T17:51:00Z","published":"2024-10-31T17:51:00Z","title":"Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters","summary":"  How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions.\n","authors":["Yujin Potter","Shiyang Lai","Junsol Kim","James Evans","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2410.24190v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.24177v1","updated":"2024-10-31T17:43:13Z","published":"2024-10-31T17:43:13Z","title":"DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models","summary":"  Spoken language models (SLMs) have gained increasing attention with\nadvancements in text-based, decoder-only language models. SLMs process text and\nspeech, enabling simultaneous speech understanding and generation. This paper\npresents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to\nimprove speech tokenization by bridging audio signals and SLM tokens. DC-Spin\nextracts speaker-invariant tokens rich in phonetic information and resilient to\ninput variations, enhancing zero-shot SLM tasks and speech resynthesis. We\npropose a chunk-wise approach to enable streamable DC-Spin without retraining\nand degradation. Comparisons of tokenization methods (self-supervised and\nneural audio codecs), model scalability, and downstream task proxies show that\ntokens easily modeled by an n-gram LM or aligned with phonemes offer strong\nperformance, providing insights for designing speech tokenizers for SLMs.\n","authors":["Heng-Jui Chang","Hongyu Gong","Changhan Wang","James Glass","Yu-An Chung"],"pdf_url":"https://arxiv.org/pdf/2410.24177v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.24175v1","updated":"2024-10-31T17:42:26Z","published":"2024-10-31T17:42:26Z","title":"Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models","summary":"  Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.\n","authors":["Yunjia Qi","Hao Peng","Xiaozhi Wang","Bin Xu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2410.24175v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.24174v1","updated":"2024-10-31T17:41:14Z","published":"2024-10-31T17:41:14Z","title":"Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices","summary":"  This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.\n","authors":["Biman Barua","M. Shamim Kaiser"],"pdf_url":"https://arxiv.org/pdf/2410.24174v1.pdf","comment":"20 pages, 12 figures"},{"id":"http://arxiv.org/abs/2409.17005v2","updated":"2024-10-31T17:21:13Z","published":"2024-09-25T15:08:08Z","title":"Models Can and Should Embrace the Communicative Nature of\n  Human-Generated Math","summary":"  Math is constructed by people for people: just as natural language corpora\nreflect not just propositions but the communicative goals of language users,\nthe math data that models are trained on reflects not just idealized\nmathematical entities but rich communicative intentions. While there are\nimportant advantages to treating math in a purely symbolic manner, we here\nhypothesize that there are benefits to treating math as situated linguistic\ncommunication and that language models are well suited for this goal, in ways\nthat are not fully appreciated. We illustrate these points with two case\nstudies. First, we ran an experiment in which we found that language models\ninterpret the equals sign in a humanlike way -- generating systematically\ndifferent word problems for the same underlying equation arranged in different\nways. Second, we found that language models prefer proofs to be ordered in\nnaturalistic ways, even though other orders would be logically equivalent. We\nadvocate for AI systems that learn from and represent the communicative\nintentions latent in human-generated math.\n","authors":["Sasha Boguraev","Ben Lipkin","Leonie Weissweiler","Kyle Mahowald"],"pdf_url":"https://arxiv.org/pdf/2409.17005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24160v1","updated":"2024-10-31T17:19:03Z","published":"2024-10-31T17:19:03Z","title":"Redefining <Creative> in Dictionary: Towards a Enhanced Semantic\n  Understanding of Creative Generation","summary":"  Creativity, both in human and diffusion models, remains an inherently\nabstract concept; thus, simply adding \"creative\" to a prompt does not yield\nreliable semantic recognition by the model. In this work, we concretize the\nabstract notion of \"creative\" through the TP2O task, which aims to merge two\nunrelated concepts, and introduce CreTok, redefining \"creative\" as the token\n$\\texttt{<CreTok>}$. This redefinition offers a more concrete and universally\nadaptable representation for concept blending. This redefinition occurs\ncontinuously, involving the repeated random sampling of text pairs with\ndifferent concepts and optimizing cosine similarity between target and constant\nprompts. This approach enables $\\texttt{<CreTok>}$ to learn a method for\ncreative concept fusion. Extensive experiments demonstrate that the creative\ncapability enabled by $\\texttt{<CreTok>}$ substantially surpasses recent SOTA\ndiffusion models and achieves superior creative generation. CreTok exhibits\ngreater flexibility and reduced time overhead, as $\\texttt{<CreTok>}$ can\nfunction as a universal token for any concept, facilitating creative generation\nwithout retraining.\n","authors":["Fu Feng","Yucheng Xie","Jing Wang","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2410.24160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24159v1","updated":"2024-10-31T17:18:11Z","published":"2024-10-31T17:18:11Z","title":"GPT or BERT: why not both?","summary":"  We present a simple way to merge masked language modeling with causal\nlanguage modeling. This hybrid training objective results in a model that\ncombines the strengths of both modeling paradigms within a single transformer\nstack: GPT-BERT can be transparently used like any standard causal or masked\nlanguage model. We test the pretraining process that enables this flexible\nbehavior on the BabyLM Challenge 2024. The results show that the hybrid\npretraining outperforms masked-only or causal-only models. We openly release\nthe models, training corpora and code.\n","authors":["Lucas Georges Gabriel Charpentier","David Samuel"],"pdf_url":"https://arxiv.org/pdf/2410.24159v1.pdf","comment":"22 pages; submission to the BabyLM Challenge 2024"},{"id":"http://arxiv.org/abs/2410.24155v1","updated":"2024-10-31T17:12:14Z","published":"2024-10-31T17:12:14Z","title":"Thought Space Explorer: Navigating and Expanding Thought Space for Large\n  Language Model Reasoning","summary":"  Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities.\n","authors":["Jinghan Zhang","Fengran Mo","Xiting Wang","Kunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24151v1","updated":"2024-10-31T17:09:55Z","published":"2024-10-31T17:09:55Z","title":"Scaling Concept With Text-Guided Diffusion Models","summary":"  Text-guided diffusion models have revolutionized generative tasks by\nproducing high-fidelity content from text descriptions. They have also enabled\nan editing paradigm where concepts can be replaced through text conditioning\n(e.g., a dog to a tiger). In this work, we explore a novel approach: instead of\nreplacing a concept, can we enhance or suppress the concept itself? Through an\nempirical study, we identify a trend where concepts can be decomposed in\ntext-guided diffusion models. Leveraging this insight, we introduce\nScalingConcept, a simple yet effective method to scale decomposed concepts up\nor down in real input without introducing new elements. To systematically\nevaluate our approach, we present the WeakConcept-10 dataset, where concepts\nare imperfect and need to be enhanced. More importantly, ScalingConcept enables\na variety of novel zero-shot applications across image and audio domains,\nincluding tasks such as canonical pose generation and generative sound\nhighlighting or removal.\n","authors":["Chao Huang","Susan Liang","Yunlong Tang","Yapeng Tian","Anurag Kumar","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.24151v1.pdf","comment":"Project page: https://wikichao.github.io/ScalingConcept/"},{"id":"http://arxiv.org/abs/2406.17947v2","updated":"2024-10-31T17:08:00Z","published":"2024-06-25T21:47:53Z","title":"Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias","summary":"  The variations between in-group and out-group speech (intergroup bias) are\nsubtle and could underlie many social phenomena like stereotype perpetuation\nand implicit bias. In this paper, we model the intergroup bias as a tagging\ntask on English sports comments from forums dedicated to fandom for NFL teams.\nWe curate a unique dataset of over 6 million game-time comments from opposing\nperspectives (the teams in the game), each comment grounded in a non-linguistic\ndescription of the events that precipitated these comments (live win\nprobabilities for each team). Expert and crowd annotations justify modeling the\nbias through tagging of implicit and explicit referring expressions and reveal\nthe rich, contextual understanding of language and the world required for this\ntask. For large-scale analysis of intergroup variation, we use LLMs for\nautomated tagging, and discover that some LLMs perform best when prompted with\nlinguistic descriptions of the win probability at the time of the comment,\nrather than numerical probability. Further, large-scale tagging of comments\nusing LLMs uncovers linear variations in the form of referent across win\nprobabilities that distinguish in-group and out-group utterances. Code and data\nare available at https://github.com/venkatasg/intergroup-nfl .\n","authors":["Venkata S Govindarajan","Matianyu Zang","Kyle Mahowald","David Beaver","Junyi Jessy Li"],"pdf_url":"https://arxiv.org/pdf/2406.17947v2.pdf","comment":"Accepted to Findings@EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.24140v1","updated":"2024-10-31T17:03:44Z","published":"2024-10-31T17:03:44Z","title":"Don't Touch My Diacritics","summary":"  The common practice of preprocessing text before feeding it into NLP models\nintroduces many decision points which have unintended consequences on model\nperformance. In this opinion piece, we focus on the handling of diacritics in\ntexts originating in many languages and scripts. We demonstrate, through\nseveral case studies, the adverse effects of inconsistent encoding of\ndiacritized characters and of removing diacritics altogether. We call on the\ncommunity to adopt simple but necessary steps across all models and toolkits in\norder to improve handling of diacritized text and, by extension, increase\nequity in multilingual NLP.\n","authors":["Kyle Gorman","Yuval Pinter"],"pdf_url":"https://arxiv.org/pdf/2410.24140v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2402.18551v2","updated":"2024-10-31T17:01:45Z","published":"2024-02-28T18:34:53Z","title":"Implicit Optimization Bias of Next-Token Prediction in Linear Models","summary":"  We initiate an investigation into the optimization properties of next-token\nprediction (NTP), the dominant training paradigm for modern language models.\nSpecifically, we study the structural properties of the solutions selected by\ngradient-based optimizers among the many possible minimizers of the NTP\nobjective. By framing NTP as cross-entropy minimization across distinct\ncontexts, each tied with a sparse conditional probability distribution across a\nfinite vocabulary of tokens, we introduce \"NTP-separability conditions\" that\nenable reaching the data-entropy lower bound. With this setup, and focusing on\nlinear models with fixed context embeddings, we characterize the optimization\nbias of gradient descent (GD): Within the data subspace defined by the sparsity\npatterns of distinct contexts, GD selects parameters that equate the logits'\ndifferences of in-support tokens to their log-odds. In the orthogonal subspace,\nthe GD parameters diverge in norm and select the direction that maximizes a\nmargin specific to NTP. These findings extend previous research on implicit\nbias in one-hot classification to the NTP setting, highlighting key differences\nand prompting further research into the optimization and generalization\nproperties of NTP, irrespective of the specific architecture used to generate\nthe context embeddings.\n","authors":["Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2402.18551v2.pdf","comment":"v2: fixed typos and writing in various parts; updated figures and\n  future-work section"},{"id":"http://arxiv.org/abs/2401.12794v3","updated":"2024-10-31T16:58:51Z","published":"2024-01-23T14:29:17Z","title":"Benchmarking LLMs via Uncertainty Quantification","summary":"  The proliferation of open-source Large Language Models (LLMs) from various\ninstitutions has highlighted the urgent need for comprehensive evaluation\nmethods. However, current evaluation platforms, such as the widely recognized\nHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,\nwhich is vital for thoroughly assessing LLMs. To bridge this gap, we introduce\na new benchmarking approach for LLMs that integrates uncertainty\nquantification. Our examination involves nine LLMs (LLM series) spanning five\nrepresentative natural language processing tasks. Our findings reveal that: I)\nLLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs\nmay display greater uncertainty compared to their smaller counterparts; and\nIII) Instruction-finetuning tends to increase the uncertainty of LLMs. These\nresults underscore the significance of incorporating uncertainty in the\nevaluation of LLMs.\n","authors":["Fanghua Ye","Mingming Yang","Jianhui Pang","Longyue Wang","Derek F. Wong","Emine Yilmaz","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2401.12794v3.pdf","comment":"30 pages, accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24126v1","updated":"2024-10-31T16:50:39Z","published":"2024-10-31T16:50:39Z","title":"Multi-environment Topic Models","summary":"  Probabilistic topic models are a powerful tool for extracting latent themes\nfrom large text datasets. In many text datasets, we also observe per-document\ncovariates (e.g., source, style, political affiliation) that act as\nenvironments that modulate a \"global\" (environment-agnostic) topic\nrepresentation. Accurately learning these representations is important for\nprediction on new documents in unseen environments and for estimating the\ncausal effect of topics on real-world outcomes. To this end, we introduce the\nMulti-environment Topic Model (MTM), an unsupervised probabilistic model that\nseparates global and environment-specific terms. Through experimentation on\nvarious political content, from ads to tweets and speeches, we show that the\nMTM produces interpretable global topics with distinct environment-specific\nwords. On multi-environment data, the MTM outperforms strong baselines in and\nout-of-distribution. It also enables the discovery of accurate causal effects.\n","authors":["Dominic Sobhani","Amir Feder","David Blei"],"pdf_url":"https://arxiv.org/pdf/2410.24126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04823v2","updated":"2024-10-31T16:48:51Z","published":"2024-06-07T10:48:45Z","title":"BERTs are Generative In-Context Learners","summary":"  While in-context learning is commonly associated with causal language models,\nsuch as GPT, we demonstrate that this capability also 'emerges' in masked\nlanguage models. Through an embarrassingly simple inference technique, we\nenable an existing masked model, DeBERTa, to perform generative tasks without\nadditional training or architectural changes. Our evaluation reveals that the\nmasked and causal language models behave very differently, as they clearly\noutperform each other on different categories of tasks. These complementary\nstrengths suggest that the field's focus on causal models for in-context\nlearning may be limiting - both architectures can develop these capabilities,\nbut with distinct advantages; pointing toward promising hybrid approaches that\ncombine the strengths of both objectives.\n","authors":["David Samuel"],"pdf_url":"https://arxiv.org/pdf/2406.04823v2.pdf","comment":"26 pages, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24114v1","updated":"2024-10-31T16:44:10Z","published":"2024-10-31T16:44:10Z","title":"Nearest Neighbor Normalization Improves Multimodal Retrieval","summary":"  Multimodal models leverage large-scale pre-training to achieve strong but\nstill imperfect performance on tasks such as image captioning, visual question\nanswering, and cross-modal retrieval. In this paper, we present a simple and\nefficient method for correcting errors in trained contrastive image-text\nretrieval models with no additional training, called Nearest Neighbor\nNormalization (NNN). We show an improvement on retrieval metrics in both text\nretrieval and image retrieval for all of the contrastive models that we tested\n(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used\n(MS-COCO and Flickr30k). NNN requires a reference database, but does not\nrequire any training on this database, and can even increase the retrieval\naccuracy of a model after finetuning.\n","authors":["Neil Chowdhury","Franklin Wang","Sumedh Shenoy","Douwe Kiela","Sarah Schwettmann","Tristan Thrush"],"pdf_url":"https://arxiv.org/pdf/2410.24114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24087v1","updated":"2024-10-31T16:20:04Z","published":"2024-10-31T16:20:04Z","title":"In-Context Fine-Tuning for Time-Series Foundation Models","summary":"  Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain.\n","authors":["Abhimanyu Das","Matthew Faw","Rajat Sen","Yichen Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.24087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18760v3","updated":"2024-10-31T16:12:16Z","published":"2023-11-30T18:02:44Z","title":"TaskBench: Benchmarking Large Language Models for Task Automation","summary":"  In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents.\n","authors":["Yongliang Shen","Kaitao Song","Xu Tan","Wenqi Zhang","Kan Ren","Siyu Yuan","Weiming Lu","Dongsheng Li","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.18760v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2312.05061v3","updated":"2024-10-31T16:08:43Z","published":"2023-12-08T14:30:08Z","title":"LaCour!: Enabling Research on Argumentation in Hearings of the European\n  Court of Human Rights","summary":"  Why does an argument end up in the final court decision? Was it deliberated\nor questioned during the oral hearings? Was there something in the hearings\nthat triggered a particular judge to write a dissenting opinion? Despite the\navailability of the final judgments of the European Court of Human Rights\n(ECHR), none of these legal research questions can currently be answered as the\nECHR's multilingual oral hearings are not transcribed, structured, or\nspeaker-attributed. We address this fundamental gap by presenting LaCour!, the\nfirst corpus of textual oral arguments of the ECHR, consisting of 154 full\nhearings (2.1 million tokens from over 267 hours of video footage) in English,\nFrench, and other court languages, each linked to the corresponding final\njudgment documents. In addition to the transcribed and partially manually\ncorrected text from the video, we provide sentence-level timestamps and\nmanually annotated role and language labels. We also showcase LaCour! in a set\nof preliminary experiments that explore the interplay between questions and\ndissenting opinions. Apart from the use cases in legal NLP, we hope that law\nstudents or other interested parties will also use LaCour! as a learning\nresource, as it is freely available in various formats at\nhttps://huggingface.co/datasets/TrustHLT/LaCour.\n","authors":["Lena Held","Ivan Habernal"],"pdf_url":"https://arxiv.org/pdf/2312.05061v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19238v2","updated":"2024-10-31T16:06:22Z","published":"2024-06-27T15:01:53Z","title":"Revealing Fine-Grained Values and Opinions in Large Language Models","summary":"  Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.\n","authors":["Dustin Wright","Arnav Arora","Nadav Borenstein","Srishti Yadav","Serge Belongie","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2406.19238v2.pdf","comment":"Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables"},{"id":"http://arxiv.org/abs/2409.05977v2","updated":"2024-10-31T16:01:59Z","published":"2024-09-09T18:21:28Z","title":"Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4","summary":"  Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra.\n","authors":["Xichen Tang"],"pdf_url":"https://arxiv.org/pdf/2409.05977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02119v3","updated":"2024-10-31T15:57:42Z","published":"2023-12-04T18:49:23Z","title":"Tree of Attacks: Jailbreaking Black-Box LLMs Automatically","summary":"  While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard.\n","authors":["Anay Mehrotra","Manolis Zampetakis","Paul Kassianik","Blaine Nelson","Hyrum Anderson","Yaron Singer","Amin Karbasi"],"pdf_url":"https://arxiv.org/pdf/2312.02119v3.pdf","comment":"Accepted for presentation at NeurIPS 2024. Code:\n  https://github.com/RICommunity/TAP"},{"id":"http://arxiv.org/abs/2402.01093v2","updated":"2024-10-31T15:56:08Z","published":"2024-02-02T01:45:18Z","title":"Need a Small Specialized Language Model? Plan Early!","summary":"  Large language models are versatile tools but are not suitable for small\ninference budgets. Small models have more efficient inference, but their lower\ncapacity means that their performance can be good only if one limits their\nscope to a specialized domain. This paper explores how to get good specialized\nsmall language models using a large, generic, pretraining set and a limited\namount of specialized data. We consider two scenarios, depending on whether (i)\none can afford pretraining a model for each specialization task, or (ii) one\nwants to cheaply adapt a single pretrained model for each task. In the first\nscenario, we propose an effective solution based on importance sampling: we\nresample the pretraining set to imitate the specialization data and train a\nsmall model on it. In the second scenario, we propose a novel architecture,\nprojected networks (PN). PN is a large network whose parameters can be linearly\nprojected into a small network for specialization. For both scenarios, we\ndemonstrate the empirical effectiveness of our solutions across various\ndomains, training set sizes, and training budgets.\n","authors":["David Grangier","Angelos Katharopoulos","Pierre Ablin","Awni Hannun"],"pdf_url":"https://arxiv.org/pdf/2402.01093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24049v1","updated":"2024-10-31T15:45:23Z","published":"2024-10-31T15:45:23Z","title":"Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs","summary":"  Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.\n","authors":["Muhammed Saeed","Elgizouli Mohamed","Mukhtar Mohamed","Shaina Raza","Shady Shehata","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2410.24049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24032v1","updated":"2024-10-31T15:30:55Z","published":"2024-10-31T15:30:55Z","title":"Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks","summary":"  The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration.\n","authors":["Yingzhe Peng","Xiaoting Qin","Zhiyang Zhang","Jue Zhang","Qingwei Lin","Xu Yang","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24029v1","updated":"2024-10-31T15:28:26Z","published":"2024-10-31T15:28:26Z","title":"Joint Training for Selective Prediction","summary":"  Classifier models are prevalent in natural language processing (NLP), often\nwith high accuracy. Yet in real world settings, human-in-the-loop systems can\nfoster trust in model outputs and even higher performance. Selective Prediction\n(SP) methods determine when to adopt a classifier's output versus defer to a\nhuman. Previous SP approaches have addressed how to improve softmax as a\nmeasure of model confidence, or have developed separate confidence estimators.\nOne previous method involves learning a deferral model based on engineered\nfeatures. We introduce a novel joint-training approach that simultaneously\noptimizes learned representations used by the classifier module and a learned\ndeferral policy. Our results on four classification tasks demonstrate that\njoint training not only leads to better SP outcomes over two strong baselines,\nbut also improves the performance of both modules.\n","authors":["Zhaohui Li","Rebecca J. Passonneau"],"pdf_url":"https://arxiv.org/pdf/2410.24029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24021v1","updated":"2024-10-31T15:21:27Z","published":"2024-10-31T15:21:27Z","title":"Detecting text level intellectual influence with knowledge graph\n  embeddings","summary":"  Introduction: Tracing the spread of ideas and the presence of influence is a\nquestion of special importance across a wide range of disciplines, ranging from\nintellectual history to cultural analytics, computational social science, and\nthe science of science.\n  Method: We collect a corpus of open source journal articles, generate\nKnowledge Graph representations using the Gemini LLM, and attempt to predict\nthe existence of citations between sampled pairs of articles using previously\npublished methods and a novel Graph Neural Network based embedding model.\n  Results: We demonstrate that our knowledge graph embedding method is superior\nat distinguishing pairs of articles with and without citation. Once trained, it\nruns efficiently and can be fine-tuned on specific corpora to suit individual\nresearcher needs.\n  Conclusion(s): This experiment demonstrates that the relationships encoded in\na knowledge graph, especially the types of concepts brought together by\nspecific relations can encode information capable of revealing intellectual\ninfluence. This suggests that further work in analyzing document level\nknowledge graphs to understand latent structures could provide valuable\ninsights.\n","authors":["Lucian Li","Eryclis Silva"],"pdf_url":"https://arxiv.org/pdf/2410.24021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24019v1","updated":"2024-10-31T15:20:50Z","published":"2024-10-31T15:20:50Z","title":"Speech is More Than Words: Do Speech-to-Text Translation Systems\n  Leverage Prosody?","summary":"  The prosody of a spoken utterance, including features like stress, intonation\nand rhythm, can significantly affect the underlying semantics, and as a\nconsequence can also affect its textual translation. Nevertheless, prosody is\nrarely studied within the context of speech-to-text translation (S2TT) systems.\nIn particular, end-to-end (E2E) systems have been proposed as well-suited for\nprosody-aware translation because they have direct access to the speech signal\nwhen making translation decisions, but the understanding of whether this is\nsuccessful in practice is still limited. A main challenge is the difficulty of\nevaluating prosody awareness in translation. To address this challenge, we\nintroduce an evaluation methodology and a focused benchmark (named ContraProST)\naimed at capturing a wide range of prosodic phenomena. Our methodology uses\nlarge language models and controllable text-to-speech (TTS) to generate\ncontrastive examples. Through experiments in translating English speech into\nGerman, Spanish, and Japanese, we find that (a) S2TT models possess some\ninternal representation of prosody, but the prosody signal is often not strong\nenough to affect the translations, (b) E2E systems outperform cascades of\nspeech recognition and text translation systems, confirming their theoretical\nadvantage in this regard, and (c) certain cascaded systems also capture\nprosodic information in the translation, but only to a lesser extent that\ndepends on the particulars of the transcript's surface form.\n","authors":["Ioannis Tsiamas","Matthias Sperber","Andrew Finch","Sarthak Garg"],"pdf_url":"https://arxiv.org/pdf/2410.24019v1.pdf","comment":"WMT 2024"},{"id":"http://arxiv.org/abs/2409.17692v2","updated":"2024-10-31T14:38:27Z","published":"2024-09-26T09:57:16Z","title":"MIO: A Foundation Model on Multimodal Tokens","summary":"  In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.\n","authors":["Zekun Wang","King Zhu","Chunpu Xu","Wangchunshu Zhou","Jiaheng Liu","Yibo Zhang","Jiashuo Wang","Ning Shi","Siyu Li","Yizhi Li","Haoran Que","Zhaoxiang Zhang","Yuanxing Zhang","Ge Zhang","Ke Xu","Jie Fu","Wenhao Huang"],"pdf_url":"https://arxiv.org/pdf/2409.17692v2.pdf","comment":"Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO"},{"id":"http://arxiv.org/abs/2405.19534v4","updated":"2024-10-31T14:32:28Z","published":"2024-05-29T21:29:44Z","title":"Preference Learning Algorithms Do Not Learn Preference Rankings","summary":"  Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms.\n","authors":["Angelica Chen","Sadhika Malladi","Lily H. Zhang","Xinyi Chen","Qiuyi Zhang","Rajesh Ranganath","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2405.19534v4.pdf","comment":"NeurIPS 2024 camera-ready"},{"id":"http://arxiv.org/abs/2407.00114v2","updated":"2024-10-31T14:27:50Z","published":"2024-06-27T13:46:11Z","title":"OmniJARVIS: Unified Vision-Language-Action Tokenization Enables\n  Open-World Instruction Following Agents","summary":"  This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps://craftjarvis.org/OmniJARVIS.\n","authors":["Zihao Wang","Shaofei Cai","Zhancun Mu","Haowei Lin","Ceyao Zhang","Xuejie Liu","Qing Li","Anji Liu","Xiaojian Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2407.00114v2.pdf","comment":"accepted on NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.14808v2","updated":"2024-10-31T14:19:49Z","published":"2024-05-23T17:18:46Z","title":"Implicit Personalization in Language Models: A Systematic Study","summary":"  Implicit Personalization (IP) is a phenomenon of language models inferring a\nuser's background from the implicit cues in the input prompts and tailoring the\nresponse based on this inference. While previous work has touched upon various\ninstances of this problem, there lacks a unified framework to study this\nbehavior. This work systematically studies IP through a rigorous mathematical\nformulation, a multi-perspective moral reasoning framework, and a set of case\nstudies. Our theoretical foundation for IP relies on a structural causal model\nand introduces a novel method, indirect intervention, to estimate the causal\neffect of a mediator variable that cannot be directly intervened upon. Beyond\nthe technical approach, we also introduce a set of moral reasoning principles\nbased on three schools of moral philosophy to study when IP may or may not be\nethically appropriate. Equipped with both mathematical and ethical insights, we\npresent three diverse case studies illustrating the varied nature of the IP\nproblem and offer recommendations for future research. Our code is at\nhttps://github.com/jiarui-liu/IP, and our data is at\nhttps://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData.\n","authors":["Zhijing Jin","Nils Heil","Jiarui Liu","Shehzaad Dhuliawala","Yahang Qi","Bernhard Schölkopf","Rada Mihalcea","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.14808v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.10476v2","updated":"2024-10-31T14:15:49Z","published":"2024-10-14T13:10:45Z","title":"Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?","summary":"  The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub.\n","authors":["Gabriel Roccabruna","Massimo Rizzoli","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2410.10476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23956v1","updated":"2024-10-31T14:09:50Z","published":"2024-10-31T14:09:50Z","title":"Multilingual Pretraining Using a Large Corpus Machine-Translated from a\n  Single Source Language","summary":"  English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM.\n","authors":["Jiayi Wang","Yao Lu","Maurice Weber","Max Ryabinin","Yihong Chen","Raphael Tang","Pontus Stenetorp"],"pdf_url":"https://arxiv.org/pdf/2410.23956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23953v1","updated":"2024-10-31T14:07:26Z","published":"2024-10-31T14:07:26Z","title":"Representative Social Choice: From Learning Theory to AI Alignment","summary":"  Social choice theory is the study of preference aggregation across a\npopulation, used both in mechanism design for human agents and in the\ndemocratic alignment of language models. In this study, we propose the\nrepresentative social choice framework for the modeling of democratic\nrepresentation in collective decisions, where the number of issues and\nindividuals are too large for mechanisms to consider all preferences directly.\nThese scenarios are widespread in real-world decision-making processes, such as\njury trials, indirect elections, legislation processes, corporate governance,\nand, more recently, language model alignment. In representative social choice,\nthe population is represented by a finite sample of individual-issue pairs\nbased on which social choice decisions are made. We show that many of the\ndeepest questions in representative social choice can be naturally formulated\nas statistical learning problems, and prove the generalization properties of\nsocial choice mechanisms using the theory of machine learning. We further\nformulate axioms for representative social choice, and prove Arrow-like\nimpossibility theorems with new combinatorial tools of analysis. Our framework\nintroduces the representative approach to social choice, opening up research\ndirections at the intersection of social choice, learning theory, and AI\nalignment.\n","authors":["Tianyi Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23953v1.pdf","comment":"Full version (20 pages). Under review. An excerpt was previously\n  accepted to NeurIPS 2024 Pluralistic Alignment Workshop"},{"id":"http://arxiv.org/abs/2407.16724v2","updated":"2024-10-31T14:03:06Z","published":"2024-07-23T12:38:48Z","title":"Structure-aware Domain Knowledge Injection for Large Language Models","summary":"  This paper introduces a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly reduces the training corpus requirement to a mere\n0.3%, while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes of human\nstudents, particularly how structured domain knowledge from textbooks is\nassimilated and subsequently applied to tackle real-world challenges through\nspecific exercises. Based on this, we propose a novel two-stage strategy for\nknowledge injection and alignment: Structure-aware Continual Pre-Training\n(SCPT) and Structure-aware Supervised Fine-Tuning (SSFT). In the SCPT phase, we\nautomatically extract the domain knowledge taxonomy and reorganize the training\ncorpora, enabling LLMs to effectively link textual segments to targeted\nknowledge points within the taxonomy. In the SSFT phase, we explicitly prompt\nmodels to elucidate the underlying knowledge structure in their outputs,\nleveraging the structured domain insight to address practical problems. Our\nultimate method has undergone extensive evaluations across model architectures\nand scales, using closed-book question-answering tasks on LongBench and\nMMedBench datasets. Remarkably, our method demonstrates the potential of\ncomparable improvement against the state-of-the-art MMedLM2 on MMedBench, while\nsignificantly reducing the training costs to 5%. This breakthrough paves the\nway for scaling up our StructTuning for stronger domain-specific LLMs with\ncomprehensive data utilization. Code is available at\nhttps://github.com/alibaba/struxgpt.\n","authors":["Kai Liu","Ze Chen","Zhihang Fu","Rongxin Jiang","Fan Zhou","Yaowu Chen","Yue Wu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16724v2.pdf","comment":"Preprint. Code is available at https://github.com/alibaba/struxgpt"},{"id":"http://arxiv.org/abs/2406.19073v2","updated":"2024-10-31T13:59:05Z","published":"2024-06-27T10:43:04Z","title":"AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database\n  Queries","summary":"  Practical semantic parsers are expected to understand user utterances and map\nthem to executable programs, even when these are ambiguous. We introduce a new\nbenchmark, AMBROSIA, which we hope will inform and inspire the development of\ntext-to-SQL parsers capable of recognizing and interpreting ambiguous requests.\nOur dataset contains questions showcasing three different types of ambiguity\n(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,\nand corresponding SQL queries. In each case, the ambiguity persists even when\nthe database context is provided. This is achieved through a novel approach\nthat involves controlled generation of databases from scratch. We benchmark\nvarious LLMs on AMBROSIA, revealing that even the most advanced models struggle\nto identify and interpret ambiguity in questions.\n","authors":["Irina Saparina","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2406.19073v2.pdf","comment":"NeurIPS 2024 D&B Track Spotlight"},{"id":"http://arxiv.org/abs/2409.07146v2","updated":"2024-10-31T13:54:35Z","published":"2024-09-11T09:49:50Z","title":"Gated Slot Attention for Efficient Linear-Time Sequence Modeling","summary":"  Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via\n$\\operatorname{softmax}$, utilizing context-aware memory reading and adaptive\nforgetting to improve memory capacity while maintaining compact recurrent state\nsize. This design greatly enhances both training and inference efficiency\nthrough GLA's hardware-efficient training algorithm and reduced state size.\nAdditionally, retaining the $\\operatorname{softmax}$ operation is particularly\nbeneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings,\nreducing the need for extensive training from scratch. Extensive experiments\nconfirm GSA's superior performance in scenarios requiring in-context recall and\nin T2R settings.\n","authors":["Yu Zhang","Songlin Yang","Ruijie Zhu","Yue Zhang","Leyang Cui","Yiqiao Wang","Bolun Wang","Freda Shi","Bailin Wang","Wei Bi","Peng Zhou","Guohong Fu"],"pdf_url":"https://arxiv.org/pdf/2409.07146v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.22086v2","updated":"2024-10-31T13:50:02Z","published":"2024-10-29T14:41:44Z","title":"Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate","summary":"  Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.\n","authors":["Zhiqi Bu","Xiaomeng Jin","Bhanukiran Vinzamuri","Anil Ramakrishna","Kai-Wei Chang","Volkan Cevher","Mingyi Hong"],"pdf_url":"https://arxiv.org/pdf/2410.22086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23933v1","updated":"2024-10-31T13:47:10Z","published":"2024-10-31T13:47:10Z","title":"Language Models can Self-Lengthen to Generate Long Texts","summary":"  Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to process long contexts, yet a notable gap remains in\ngenerating long, aligned outputs. This limitation stems from a training gap\nwhere pre-training lacks effective instructions for long-text generation, and\npost-training data primarily consists of short query-response pairs. Current\napproaches, such as instruction backtranslation and behavior imitation, face\nchallenges including data quality, copyright issues, and constraints on\nproprietary model usage. In this paper, we introduce an innovative iterative\ntraining framework called Self-Lengthen that leverages only the intrinsic\nknowledge and skills of LLMs without the need for auxiliary data or proprietary\nmodels. The framework consists of two roles: the Generator and the Extender.\nThe Generator produces the initial response, which is then split and expanded\nby the Extender. This process results in a new, longer response, which is used\nto train both the Generator and the Extender iteratively. Through this process,\nthe models are progressively trained to handle increasingly longer responses.\nExperiments on benchmarks and human evaluations show that Self-Lengthen\noutperforms existing methods in long-text generation, when applied to top\nopen-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at\nhttps://github.com/QwenLM/Self-Lengthen.\n","authors":["Shanghaoran Quan","Tianyi Tang","Bowen Yu","An Yang","Dayiheng Liu","Bofei Gao","Jianhong Tu","Yichang Zhang","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2410.23933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23918v1","updated":"2024-10-31T13:26:11Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20087v2","updated":"2024-10-31T13:10:12Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v2.pdf","comment":"NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)"},{"id":"http://arxiv.org/abs/2407.16434v2","updated":"2024-10-31T13:06:41Z","published":"2024-07-23T12:33:58Z","title":"Enhancing LLM's Cognition via Structurization","summary":"  When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including a series of\nauto-regressive LLMs as well as BERT-like masking models) on a diverse set of\nNLP tasks (e.g., context-based question-answering, exhaustive hallucination\nevaluation, and passage-level dense retrieval). Empirical results show\nconsistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost the open-sourced LLaMA2-70B model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code is available\nat https://github.com/alibaba/struxgpt.\n","authors":["Kai Liu","Zhihang Fu","Chao Chen","Wei Zhang","Rongxin Jiang","Fan Zhou","Yaowu Chen","Yue Wu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16434v2.pdf","comment":"This paper has been accepted by NeurIPS 2024. Code is available at\n  https://github.com/alibaba/struxgpt"},{"id":"http://arxiv.org/abs/2410.23902v1","updated":"2024-10-31T13:05:39Z","published":"2024-10-31T13:05:39Z","title":"Responsible Retrieval Augmented Generation for Climate Decision Making\n  from Documents","summary":"  Climate decision making is constrained by the complexity and inaccessibility\nof key information within lengthy, technical, and multi-lingual documents.\nGenerative AI technologies offer a promising route for improving the\naccessibility of information contained within these documents, but suffer from\nlimitations. These include (1) a tendency to hallucinate or mis-represent\ninformation, (2) difficulty in steering or guaranteeing properties of generated\noutput, and (3) reduced performance in specific technical domains. To address\nthese challenges, we introduce a novel evaluation framework with\ndomain-specific dimensions tailored for climate-related documents. We then\napply this framework to evaluate Retrieval-Augmented Generation (RAG)\napproaches and assess retrieval- and generation-quality within a prototype tool\nthat answers questions about individual climate law and policy documents. In\naddition, we publish a human-annotated dataset and scalable automated\nevaluation tools, with the aim of facilitating broader adoption and robust\nassessment of these systems in the climate domain. Our findings highlight the\nkey components of responsible deployment of RAG to enhance decision-making,\nwhile also providing insights into user experience (UX) considerations for\nsafely deploying such systems to build trust with users in high-risk domains.\n","authors":["Matyas Juhasz","Kalyan Dutia","Henry Franks","Conor Delahunty","Patrick Fawbert Mills","Harrison Pim"],"pdf_url":"https://arxiv.org/pdf/2410.23902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04559v3","updated":"2024-10-31T12:59:46Z","published":"2024-02-07T03:37:19Z","title":"Can Large Language Model Agents Simulate Human Trust Behavior?","summary":"  Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one elemental behavior in\nhuman interactions, trust, and investigate whether LLM agents can simulate\nhuman trust behavior. We first find that LLM agents generally exhibit trust\nbehavior, referred to as agent trust, under the framework of Trust Games, which\nare widely recognized in behavioral economics. Then, we discover that GPT-4\nagents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.\n","authors":["Chengxing Xie","Canyu Chen","Feiran Jia","Ziyu Ye","Shiyang Lai","Kai Shu","Jindong Gu","Adel Bibi","Ziniu Hu","David Jurgens","James Evans","Philip Torr","Bernard Ghanem","Guohao Li"],"pdf_url":"https://arxiv.org/pdf/2402.04559v3.pdf","comment":"Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org"},{"id":"http://arxiv.org/abs/2410.23890v1","updated":"2024-10-31T12:52:26Z","published":"2024-10-31T12:52:26Z","title":"Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource\n  languages","summary":"  In an evolving landscape of crisis communication, the need for robust and\nadaptable Machine Translation (MT) systems is more pressing than ever,\nparticularly for low-resource languages. This study presents a comprehensive\nexploration of leveraging Large Language Models (LLMs) and Multilingual LLMs\n(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique\nchallenges posed by crisis situations where speed, accuracy, and the ability to\nhandle a wide range of languages are paramount, this research outlines a novel\napproach that combines the cutting-edge capabilities of LLMs with fine-tuning\ntechniques and community-driven corpus development strategies. At the core of\nthis study is the development and empirical evaluation of MT systems tailored\nfor two low-resource language pairs, illustrating the process from initial\nmodel selection and fine-tuning through to deployment. Bespoke systems are\ndeveloped and modelled on the recent Covid-19 pandemic. The research highlights\nthe importance of community involvement in creating highly specialised,\ncrisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM\nmodels. It identifies fine-tuned MLLM models as offering superior performance\ncompared with their LLM counterparts. A scalable and replicable model for rapid\nMT system development in crisis scenarios is outlined. Our approach enhances\nthe field of humanitarian technology by offering a blueprint for developing\nmultilingual communication systems during emergencies.\n","authors":["Séamus Lankford","Andy Way"],"pdf_url":"https://arxiv.org/pdf/2410.23890v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.02370,\n  arXiv:2403.01580"},{"id":"http://arxiv.org/abs/2410.23884v1","updated":"2024-10-31T12:48:58Z","published":"2024-10-31T12:48:58Z","title":"Failure Modes of LLMs for Causal Reasoning on Narratives","summary":"  In this work, we investigate the causal reasoning abilities of large language\nmodels (LLMs) through the representative problem of inferring causal\nrelationships from narratives. We find that even state-of-the-art language\nmodels rely on unreliable shortcuts, both in terms of the narrative\npresentation and their parametric knowledge. For example, LLMs tend to\ndetermine causal relationships based on the topological ordering of events\n(i.e., earlier events cause later ones), resulting in lower performance\nwhenever events are not narrated in their exact causal order. Similarly, we\ndemonstrate that LLMs struggle with long-term causal reasoning and often fail\nwhen the narratives are long and contain many events. Additionally, we show\nLLMs appear to rely heavily on their parametric knowledge at the expense of\nreasoning over the provided narrative. This degrades their abilities whenever\nthe narrative opposes parametric knowledge. We extensively validate these\nfailure modes through carefully controlled synthetic experiments, as well as\nevaluations on real-world narratives. Finally, we observe that explicitly\ngenerating a causal graph generally improves performance while naive\nchain-of-thought is ineffective. Collectively, our results distill precise\nfailure modes of current state-of-the-art models and can pave the way for\nfuture techniques to enhance causal reasoning in LLMs.\n","authors":["Khurram Yamin","Shantanu Gupta","Gaurav R. Ghosal","Zachary C. Lipton","Bryan Wilder"],"pdf_url":"https://arxiv.org/pdf/2410.23884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23883v1","updated":"2024-10-31T12:45:54Z","published":"2024-10-31T12:45:54Z","title":"'No' Matters: Out-of-Distribution Detection in Multimodality Long\n  Dialogue","summary":"  Out-of-distribution (OOD) detection in multimodal contexts is essential for\nidentifying deviations in combined inputs from different modalities,\nparticularly in applications like open-domain dialogue systems or real-life\ndialogue interactions. This paper aims to improve the user experience that\ninvolves multi-round long dialogues by efficiently detecting OOD dialogues and\nimages. We introduce a novel scoring framework named Dialogue Image Aligning\nand Enhancing Framework (DIAEF) that integrates the visual language models with\nthe novel proposed scores that detect OOD in two key scenarios (1) mismatches\nbetween the dialogue and image input pair and (2) input pairs with previously\nunseen labels. Our experimental results, derived from various benchmarks,\ndemonstrate that integrating image and multi-round dialogue OOD detection is\nmore effective with previously unseen labels than using either modality\nindependently. In the presence of mismatched pairs, our proposed score\neffectively identifies these mismatches and demonstrates strong robustness in\nlong dialogues. This approach enhances domain-aware, adaptive conversational\nagents and establishes baselines for future studies.\n","authors":["Rena Gao","Xuetong Wu","Siwen Luo","Caren Han","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23883v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.02691v2","updated":"2024-10-31T12:40:33Z","published":"2024-10-03T17:18:03Z","title":"On the Proper Treatment of Tokenization in Psycholinguistics","summary":"  Language models are widely used in computational psycholinguistics to test\ntheories that relate the negative log probability (the surprisal) of a region\nof interest (a substring of characters) under a language model to its cognitive\ncost experienced by readers, as operationalized, for example, by gaze duration\non the region. However, the application of modern language models to\npsycholinguistic studies is complicated by the practice of using tokenization\nas an intermediate step in training a model. Doing so results in a language\nmodel over token strings rather than one over character strings. Vexingly,\nregions of interest are generally misaligned with these token strings. The\npaper argues that token-level language models should be (approximately)\nmarginalized into character-level language models before they are used in\npsycholinguistic studies to compute the surprisal of a region of interest;\nthen, the marginalized character-level language model can be used to compute\nthe surprisal of an arbitrary character substring, which we term a focal area,\nthat the experimenter may wish to use as a predictor. Our proposal of\nmarginalizing a token-level model into a character-level one solves this\nmisalignment issue independently of the tokenization scheme. Empirically, we\ndiscover various focal areas whose surprisal is a better psychometric predictor\nthan the surprisal of the region of interest itself.\n","authors":["Mario Giulianelli","Luca Malagutti","Juan Luis Gastaldi","Brian DuSell","Tim Vieira","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2410.02691v2.pdf","comment":"Main conference long paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.10245v2","updated":"2024-10-31T12:34:17Z","published":"2024-09-16T12:55:14Z","title":"From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs","summary":"  As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5\\% of extraversion-related test instances, while\nMistral-7B-Instruct did so in 92.5\\% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analysing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods.\n","authors":["Navya Jain","Zekun Wu","Cristian Munoz","Airlie Hilliard","Adriano Koshiyama","Emre Kazim","Philip Treleaven"],"pdf_url":"https://arxiv.org/pdf/2409.10245v2.pdf","comment":"NeurIPS 2024 Workshop on Behavioral Machine Learning"},{"id":"http://arxiv.org/abs/2402.02130v5","updated":"2024-10-31T12:27:33Z","published":"2024-02-03T12:19:47Z","title":"GITA: Graph to Visual and Textual Integration for Vision-Language Graph\n  Reasoning","summary":"  Large Language Models (LLMs) are increasingly used for various tasks with\ngraph structures. Though LLMs can process graph information in a textual\nformat, they overlook the rich vision modality, which is an intuitive way for\nhumans to comprehend structural information and conduct general graph\nreasoning. The potential benefits and capabilities of representing graph\nstructures as visual images (i.e., $\\textit{visual graph}$) are still\nunexplored. To fill the gap, we innovatively propose an end-to-end framework,\ncalled $\\textbf{G}$raph to v$\\textbf{I}$sual and $\\textbf{T}$extual\nIntegr$\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into\ngeneral graph reasoning. Besides, we establish $\\textbf{G}$raph-based\n$\\textbf{V}$ision-$\\textbf{L}$anguage $\\textbf{Q}$uestion $\\textbf{A}$nswering\n(GVLQA) dataset from existing graph data, which is the first vision-language\ndataset for general graph reasoning purposes. Extensive experiments on the\nGVLQA dataset and five real-world datasets show that GITA outperforms\nmainstream LLMs in terms of general graph reasoning capabilities. Moreover, We\nhighlight the effectiveness of the layout augmentation on visual graphs and\npretraining on the GVLQA dataset.\n","authors":["Yanbin Wei","Shuai Fu","Weisen Jiang","Zejian Zhang","Zhixiong Zeng","Qi Wu","James T. Kwok","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02130v5.pdf","comment":"NeurIPS 2024; Project Page: v-graph.github.io; Code:\n  https://github.com/WEIYanbin1999/GITA/"},{"id":"http://arxiv.org/abs/2402.06255v4","updated":"2024-10-31T12:24:14Z","published":"2024-02-09T09:09:39Z","title":"Fight Back Against Jailbreaking via Prompt Adversarial Tuning","summary":"  While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT.\n","authors":["Yichuan Mo","Yuji Wang","Zeming Wei","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.06255v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23861v1","updated":"2024-10-31T12:11:17Z","published":"2024-10-31T12:11:17Z","title":"Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models","summary":"  Large Multimodal Models (LMMs) have demonstrated the ability to interact with\nhumans under real-world conditions by combining Large Language Models (LLMs)\nand modality encoders to align multimodal information (visual and auditory)\nwith text. However, such models raise new safety challenges of whether models\nthat are safety-aligned on text also exhibit consistent safeguards for\nmultimodal inputs. Despite recent safety-alignment research on vision LMMs, the\nsafety of audio LMMs remains under-explored. In this work, we comprehensively\nred team the safety of five advanced audio LMMs under three settings: (i)\nharmful questions in both audio and text formats, (ii) harmful questions in\ntext format accompanied by distracting non-speech audio, and (iii)\nspeech-specific jailbreaks. Our results under these settings demonstrate that\nopen-source audio LMMs suffer an average attack success rate of 69.14% on\nharmful audio questions, and exhibit safety vulnerabilities when distracted\nwith non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro\nachieve an attack success rate of 70.67% on the harmful query benchmark. We\nprovide insights on what could cause these reported safety-misalignments.\nWarning: this paper contains offensive examples.\n","authors":["Hao Yang","Lizhen Qu","Ehsan Shareghi","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2410.23861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23856v1","updated":"2024-10-31T12:07:44Z","published":"2024-10-31T12:07:44Z","title":"Can Language Models Perform Robust Reasoning in Chain-of-thought\n  Prompting with Noisy Rationales?","summary":"  This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales.\n","authors":["Zhanke Zhou","Rong Tao","Jianing Zhu","Yiwen Luo","Zengmao Wang","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2410.23856v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23850v1","updated":"2024-10-31T12:01:12Z","published":"2024-10-31T12:01:12Z","title":"The Automated Verification of Textual Claims (AVeriTeC) Shared Task","summary":"  The Automated Verification of Textual Claims (AVeriTeC) shared task asks\nparticipants to retrieve evidence and predict veracity for real-world claims\nchecked by fact-checkers. Evidence can be found either via a search engine, or\nvia a knowledge store provided by the organisers. Submissions are evaluated\nusing AVeriTeC score, which considers a claim to be accurately verified if and\nonly if both the verdict is correct and retrieved evidence is considered to\nmeet a certain quality threshold. The shared task received 21 submissions, 18\nof which surpassed our baseline. The winning team was TUDA_MAI with an AVeriTeC\nscore of 63%. In this paper we describe the shared task, present the full\nresults, and highlight key takeaways from the shared task.\n","authors":["Michael Schlichtkrull","Yulong Chen","Chenxi Whitehouse","Zhenyun Deng","Mubashara Akhtar","Rami Aly","Zhijiang Guo","Christos Christodoulopoulos","Oana Cocarascu","Arpit Mittal","James Thorne","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2410.23850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23844v1","updated":"2024-10-31T11:50:24Z","published":"2024-10-31T11:50:24Z","title":"Commonsense Knowledge Editing Based on Free-Text in LLMs","summary":"  Knowledge editing technology is crucial for maintaining the accuracy and\ntimeliness of large language models (LLMs) . However, the setting of this task\noverlooks a significant portion of commonsense knowledge based on free-text in\nthe real world, characterized by broad knowledge scope, long content and non\ninstantiation. The editing objects of previous methods (e.g., MEMIT) were\nsingle token or entity, which were not suitable for commonsense knowledge in\nfree-text form. To address the aforementioned challenges, we conducted\nexperiments from two perspectives: knowledge localization and knowledge\nediting. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)\nmethod, revealing the challenges associated with the distribution of\ncommonsense knowledge in MLP and Attention layers, as well as in decentralized\ndistribution. Next, we propose a Dynamics-aware Editing Method(DEM), which\nutilizes a Dynamics-aware Module to locate the parameter positions\ncorresponding to commonsense knowledge, and uses Knowledge Editing Module to\nupdate knowledge. The DEM method fully explores the potential of the MLP and\nAttention layers, and successfully edits commonsense knowledge based on\nfree-text. The experimental results indicate that the DEM can achieve excellent\nediting performance.\n","authors":["Xiusheng Huang","Yequan Wang","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23844v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.23843v1","updated":"2024-10-31T11:49:44Z","published":"2024-10-31T11:49:44Z","title":"Reasons and Solutions for the Decline in Model Performance after Editing","summary":"  Knowledge editing technology has received widespread attention for low-cost\nupdates of incorrect or outdated knowledge in large-scale language models.\nHowever, recent research has found that edited models often exhibit varying\ndegrees of performance degradation. The reasons behind this phenomenon and\npotential solutions have not yet been provided. In order to investigate the\nreasons for the performance decline of the edited model and optimize the\nediting method, this work explores the underlying reasons from both data and\nmodel perspectives. Specifically, 1) from a data perspective, to clarify the\nimpact of data on the performance of editing models, this paper first\nconstructs a Multi-Question Dataset (MQD) to evaluate the impact of different\ntypes of editing data on model performance. The performance of the editing\nmodel is mainly affected by the diversity of editing targets and sequence\nlength, as determined through experiments. 2) From a model perspective, this\narticle explores the factors that affect the performance of editing models. The\nresults indicate a strong correlation between the L1-norm of the editing model\nlayer and the editing accuracy, and clarify that this is an important factor\nleading to the bottleneck of editing performance. Finally, in order to improve\nthe performance of the editing model, this paper further proposes a Dump for\nSequence (D4S) method, which successfully overcomes the previous editing\nbottleneck by reducing the L1-norm of the editing layer, allowing users to\nperform multiple effective edits and minimizing model damage. Our code is\navailable at https://github.com/nlpkeg/D4S.\n","authors":["Xiusheng Huang","Jiaxiang Liu","Yequan Wang","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23843v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.17557v2","updated":"2024-10-31T11:37:49Z","published":"2024-06-25T13:50:56Z","title":"The FineWeb Datasets: Decanting the Web for the Finest Text Data at\n  Scale","summary":"  The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments.\n","authors":["Guilherme Penedo","Hynek Kydlíček","Loubna Ben allal","Anton Lozhkov","Margaret Mitchell","Colin Raffel","Leandro Von Werra","Thomas Wolf"],"pdf_url":"https://arxiv.org/pdf/2406.17557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02370v4","updated":"2024-10-31T11:37:41Z","published":"2024-02-04T06:59:21Z","title":"AutoTimes: Autoregressive Time Series Forecasters via Large Language\n  Models","summary":"  Foundation models of time series have not been fully developed due to the\nlimited availability of time series corpora and the underexploration of\nscalable pre-training. Based on the similar sequential formulation of time\nseries and natural language, increasing research demonstrates the feasibility\nof leveraging large language models (LLM) for time series. Nevertheless, the\ninherent autoregressive property and decoder-only architecture of LLMs have not\nbeen fully considered, resulting in insufficient utilization of LLM abilities.\nTo fully revitalize the general-purpose token transition and multi-step\ngeneration capability of large language models, we propose AutoTimes to\nrepurpose LLMs as autoregressive time series forecasters, which projects time\nseries into the embedding space of language tokens and autoregressively\ngenerates future predictions with arbitrary lengths. Compatible with any\ndecoder-only LLMs, the consequent forecaster exhibits the flexibility of the\nlookback length and scalability with larger LLMs. Further, we formulate time\nseries as prompts, extending the context for prediction beyond the lookback\nwindow, termed in-context forecasting. By introducing LLM-embedded textual\ntimestamps, AutoTimes can utilize chronological information to align\nmultivariate time series. Empirically, AutoTimes achieves state-of-the-art with\n0.1% trainable parameters and over $5\\times$ training/inference speedup\ncompared to advanced LLM-based forecasters. Code is available at this\nrepository: https://github.com/thuml/AutoTimes.\n","authors":["Yong Liu","Guo Qin","Xiangdong Huang","Jianmin Wang","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2402.02370v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23825v1","updated":"2024-10-31T11:14:12Z","published":"2024-10-31T11:14:12Z","title":"GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for\n  Minority Languages","summary":"  The need for large text corpora has increased with the advent of pretrained\nlanguage models and, in particular, the discovery of scaling laws for these\nmodels. Most available corpora have sufficient data only for languages with\nlarge dominant communities. However, there is no corpus available that (i)\ncovers a wide range of minority languages; (ii) is generated by an open-source\nreproducible pipeline; and (iii) is rigorously cleaned from noise, making it\ntrustworthy to use. We present GlotCC, a clean, document-level, 2TB general\ndomain corpus derived from CommonCrawl, covering more than 1000 languages. We\nmake GlotCC and the system used to generate it - including the pipeline,\nlanguage identification model, and filters - available to the research\ncommunity. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1,\nPipeline v. 3.0 https://github.com/cisnlp/GlotCC.\n","authors":["Amir Hossein Kargaran","François Yvon","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2410.23825v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.08126v2","updated":"2024-10-31T11:11:18Z","published":"2024-10-10T17:10:34Z","title":"Mars: Situated Inductive Reasoning in an Open-World Environment","summary":"  Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way.\n","authors":["Xiaojuan Tang","Jiaqi Li","Yitao Liang","Song-chun Zhu","Muhan Zhang","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.08126v2.pdf","comment":"Accepted by NeurIPS 2024 Track Datasets and Benchmarks. Project page:\n  https://marscrafter.github.io/"},{"id":"http://arxiv.org/abs/2409.04181v2","updated":"2024-10-31T11:01:16Z","published":"2024-09-06T10:49:46Z","title":"Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question\n  Answering","summary":"  Advancements in natural language processing have revolutionized the way we\ncan interact with digital information systems, such as databases, making them\nmore accessible. However, challenges persist, especially when accuracy is\ncritical, as in the biomedical domain. A key issue is the hallucination\nproblem, where models generate information unsupported by the underlying data,\npotentially leading to dangerous misinformation. This paper presents a novel\napproach designed to bridge this gap by combining Large Language Models (LLM)\nand Knowledge Graphs (KG) to improve the accuracy and reliability of\nquestion-answering systems, on the example of a biomedical KG. Built on the\nLangChain framework, our method incorporates a query checker that ensures the\nsyntactical and semantic validity of LLM-generated queries, which are then used\nto extract information from a Knowledge Graph, substantially reducing errors\nlike hallucinations. We evaluated the overall performance using a new benchmark\ndataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo\nand llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other\nmodels in generating accurate queries, open-source models like llama3:70b show\npromise with appropriate prompt engineering. To make this approach accessible,\na user-friendly web-based interface has been developed, allowing users to input\nnatural language queries, view generated and corrected Cypher queries, and\nverify the resulting paths for accuracy. Overall, this hybrid approach\neffectively addresses common issues such as data gaps and hallucinations,\noffering a reliable and intuitive solution for question answering systems. The\nsource code for generating the results of this paper and for the user-interface\ncan be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui\n","authors":["Larissa Pusch","Tim O. F. Conrad"],"pdf_url":"https://arxiv.org/pdf/2409.04181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11200v3","updated":"2024-10-31T10:15:06Z","published":"2024-06-17T04:20:02Z","title":"AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning","summary":"  Large language model (LLM) agents have demonstrated impressive capabilities\nin utilizing external tools and knowledge to boost accuracy and reduce\nhallucinations. However, developing prompting techniques that enable LLM agents\nto effectively use these tools and knowledge remains a heuristic and\nlabor-intensive task. Here, we introduce AvaTaR, a novel and automated\nframework that optimizes an LLM agent to effectively leverage provided tools,\nimproving performance on a given task. During optimization, we design a\ncomparator module to iteratively deliver insightful and comprehensive prompts\nto the LLM agent by contrastively reasoning between positive and negative\nexamples sampled from training data. We demonstrate AvaTaR on four complex\nmultimodal retrieval datasets featuring textual, visual, and relational\ninformation, and three general question-answering (QA) datasets. We find AvaTaR\nconsistently outperforms state-of-the-art approaches across all seven tasks,\nexhibiting strong generalization ability when applied to novel cases and\nachieving an average relative improvement of 14% on the Hit@1 metric for the\nretrieval datasets and 13% for the QA datasets. Code and dataset are available\nat https://github.com/zou-group/avatar.\n","authors":["Shirley Wu","Shiyu Zhao","Qian Huang","Kexin Huang","Michihiro Yasunaga","Kaidi Cao","Vassilis N. Ioannidis","Karthik Subbian","Jure Leskovec","James Zou"],"pdf_url":"https://arxiv.org/pdf/2406.11200v3.pdf","comment":"NeurIPS 2024 main conference"},{"id":"http://arxiv.org/abs/2406.06196v3","updated":"2024-10-31T10:14:49Z","published":"2024-06-10T11:50:29Z","title":"LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in\n  Low-Resource and Extinct Languages","summary":"  In this paper, we present the LingOly benchmark, a novel benchmark for\nadvanced reasoning abilities in large language models. Using challenging\nLinguistic Olympiad puzzles, we evaluate (i) capabilities for in-context\nidentification and generalisation of linguistic patterns in very low-resource\nor extinct languages, and (ii) abilities to follow complex task instructions.\nThe LingOly benchmark covers more than 90 mostly low-resource languages,\nminimising issues of data contamination, and contains 1,133 problems across 6\nformats and 5 levels of human difficulty. We assess performance with both\ndirect accuracy and comparison to a no-context baseline to penalise\nmemorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to\nbe challenging, and models perform poorly on the higher difficulty problems. On\nharder problems, even the top model only achieved 38.7% accuracy, a 24.7%\nimprovement over the no-context baseline. Large closed models typically\noutperform open models, and in general, the higher resource the language, the\nbetter the scores. These results indicate, in absence of memorisation, true\nmulti-step out-of-domain reasoning remains a challenge for current language\nmodels.\n","authors":["Andrew M. Bean","Simi Hellsten","Harry Mayne","Jabez Magomere","Ethan A. Chi","Ryan Chi","Scott A. Hale","Hannah Rose Kirk"],"pdf_url":"https://arxiv.org/pdf/2406.06196v3.pdf","comment":"Oral presentation at NeurIPS 2024 Datasets and Benchmarks Track. 10\n  pages, 5 figures, 22 pages supplemental materials"},{"id":"http://arxiv.org/abs/2403.05266v2","updated":"2024-10-31T10:07:54Z","published":"2024-03-08T12:42:36Z","title":"ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models","summary":"  Large language models (LLMs) have achieved unprecedented performances in\nvarious applications, yet evaluating them is still challenging. Existing\nbenchmarks are either manually constructed or are automatic, but lack the\nability to evaluate the thought process of LLMs with arbitrary complexity. We\ncontend that utilizing existing relational databases based on the\nentity-relationship (ER) model is a promising approach for constructing\nbenchmarks as they contain structured knowledge that can be used to question\nLLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational\ndatabases have integrity constraints that can be used to better construct\ncomplex in-depth questions and verify answers: (1) functional dependencies can\nbe used to pinpoint critical keywords that an LLM must know to properly answer\na given question containing certain attribute values; and (2) foreign key\nconstraints can be used to join relations and construct multi-hop questions,\nwhich can be arbitrarily long and used to debug intermediate answers. We thus\npropose ERBench, which uses these integrity constraints to convert any database\ninto an LLM benchmark. ERBench supports continuous evaluation as databases\nchange, multimodal questions, and various prompt engineering techniques. In our\nexperiments, we construct LLM benchmarks using databases of multiple domains\nand make an extensive comparison of contemporary LLMs. We show how ERBench can\nproperly evaluate any LLM by not only checking for answer correctness, but also\neffectively verifying the rationales by looking for the right keywords.\n","authors":["Jio Oh","Soyeon Kim","Junseok Seo","Jindong Wang","Ruochen Xu","Xing Xie","Steven Euijong Whang"],"pdf_url":"https://arxiv.org/pdf/2403.05266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23771v1","updated":"2024-10-31T09:39:28Z","published":"2024-10-31T09:39:28Z","title":"What is Wrong with Perplexity for Long-context Language Modeling?","summary":"  Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.\n","authors":["Lizhe Fang","Yifei Wang","Zhaoyang Liu","Chenheng Zhang","Stefanie Jegelka","Jinyang Gao","Bolin Ding","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23769v1","updated":"2024-10-31T09:33:37Z","published":"2024-10-31T09:33:37Z","title":"The Potential of LLMs in Medical Education: Generating Questions and\n  Answers for Qualification Exams","summary":"  Recent research on large language models (LLMs) has primarily focused on\ntheir adaptation and application in specialized domains. The application of\nLLMs in the medical field is mainly concentrated on tasks such as the\nautomation of medical report generation, summarization, diagnostic reasoning,\nand question-and-answer interactions between doctors and patients. The\nchallenge of becoming a good teacher is more formidable than that of becoming a\ngood student, and this study pioneers the application of LLMs in the field of\nmedical education. In this work, we investigate the extent to which LLMs can\ngenerate medical qualification exam questions and corresponding answers based\non few-shot prompts. Utilizing a real-world Chinese dataset of elderly chronic\ndiseases, we tasked the LLMs with generating open-ended questions and answers\nbased on a subset of sampled admission reports across eight widely used LLMs,\nincluding ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, and\nMistral. Furthermore, we engaged medical experts to manually evaluate these\nopen-ended questions and answers across multiple dimensions. The study found\nthat LLMs, after using few-shot prompts, can effectively mimic real-world\nmedical qualification exam questions, whereas there is room for improvement in\nthe correctness, evidence-based statements, and professionalism of the\ngenerated answers. Moreover, LLMs also demonstrate a decent level of ability to\ncorrect and rectify reference answers. Given the immense potential of\nartificial intelligence in the medical field, the task of generating questions\nand answers for medical qualification exams aimed at medical students, interns\nand residents can be a significant focus of future research.\n","authors":["Yunqi Zhu","Wen Tang","Ying Sun","Xuebing Yang"],"pdf_url":"https://arxiv.org/pdf/2410.23769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03621v2","updated":"2024-10-31T09:11:03Z","published":"2024-09-05T15:33:24Z","title":"Attend First, Consolidate Later: On the Importance of Attention in\n  Different LLM Layers","summary":"  In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally.\n","authors":["Amit Ben-Artzy","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2409.03621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23746v1","updated":"2024-10-31T09:01:25Z","published":"2024-10-31T09:01:25Z","title":"DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios","summary":"  Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating advanced prompt usages, human\nrevisions like word substitutions, and writing errors. Our development of\nDetectRL reveals the strengths and limitations of current SOTA detectors. More\nimportantly, we analyzed the potential impact of writing styles, model types,\nattack methods, the text lengths, and real-world human writing factors on\ndifferent types of detectors. We believe DetectRL could serve as an effective\nbenchmark for assessing detectors in real-world scenarios, evolving with\nadvanced attack methods, thus providing more stressful evaluation to drive the\ndevelopment of more efficient detectors. Data and code are publicly available\nat: https://github.com/NLP2CT/DetectRL.\n","authors":["Junchao Wu","Runzhe Zhan","Derek F. Wong","Shu Yang","Xinyi Yang","Yulin Yuan","Lidia S. Chao"],"pdf_url":"https://arxiv.org/pdf/2410.23746v1.pdf","comment":"Accepted to NeurIPS 2024 Dataset & Benchmarking Track"},{"id":"http://arxiv.org/abs/2410.23743v1","updated":"2024-10-31T08:58:06Z","published":"2024-10-31T08:58:06Z","title":"What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A\n  Gradient Perspective","summary":"  What makes a difference in the post-training of LLMs? We investigate the\ntraining patterns of different layers in large language models (LLMs), through\nthe lens of gradient, when training with different responses and initial\nmodels. We are specifically interested in how fast vs. slow thinking affects\nthe layer-wise gradients, given the recent popularity of training LLMs on\nreasoning paths such as chain-of-thoughts (CoT) and process rewards. In our\nstudy, fast thinking without CoT leads to larger gradients and larger\ndifferences of gradients across layers than slow thinking (Detailed CoT),\nindicating the learning stability brought by the latter. Moreover, pre-trained\nLLMs are less affected by the instability of fast thinking than\ninstruction-tuned LLMs. Additionally, we study whether the gradient patterns\ncan reflect the correctness of responses when training different LLMs using\nslow vs. fast thinking paths. The results show that the gradients of slow\nthinking can distinguish correct and irrelevant reasoning paths. As a\ncomparison, we conduct similar gradient analyses on non-reasoning knowledge\nlearning tasks, on which, however, trivially increasing the response length\ndoes not lead to similar behaviors of slow thinking. Our study strengthens\nfundamental understandings of LLM training and sheds novel insights on its\nefficiency and stability, which pave the way towards building a generalizable\nSystem-2 agent. Our code, data, and gradient statistics can be found in:\nhttps://github.com/MingLiiii/Layer_Gradient.\n","authors":["Ming Li","Yanhong Li","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14405v2","updated":"2024-10-31T08:44:13Z","published":"2024-10-18T12:08:07Z","title":"Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of\n  Language Models for Fact Completion","summary":"  Previous interpretations of language models (LMs) miss important distinctions\nin how these models process factual information. For example, given the query\n\"Astrid Lindgren was born in\" with the corresponding completion \"Sweden\", no\ndifference is made between whether the prediction was based on having the exact\nknowledge of the birthplace of the Swedish author or assuming that a person\nwith a Swedish-sounding name was born in Sweden. In this paper, we investigate\nfour different prediction scenarios for which the LM can be expected to show\ndistinct behaviors. These scenarios correspond to different levels of model\nreliability and types of information being processed - some being less\ndesirable for factual predictions. To facilitate precise interpretations of LMs\nfor fact completion, we propose a model-specific recipe called PrISM for\nconstructing datasets with examples of each scenario based on a set of\ndiagnostic criteria. We apply a popular interpretability method, causal tracing\n(CT), to the four prediction scenarios and find that while CT produces\ndifferent results for each scenario, aggregations over a set of mixed examples\nmay only represent the results from the scenario with the strongest measured\nsignal. In summary, we contribute tools for a more granular study of fact\ncompletion in language models and analyses that provide a more nuanced\nunderstanding of how LMs process fact-related queries.\n","authors":["Denitsa Saynova","Lovisa Hagström","Moa Johansson","Richard Johansson","Marco Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2410.14405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00072v5","updated":"2024-10-31T08:35:42Z","published":"2024-06-21T08:52:11Z","title":"Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback","summary":"  RAG systems face limitations when semantic relevance alone does not guarantee\nimproved generation quality. This issue becomes particularly evident due to the\nsensitivity of large language models (LLMs) to the ordering of few-shot\nprompts, which can affect model performance. To address this challenge,\naligning LLM outputs with human preferences using structured feedback, such as\noptions to copy, regenerate, or dislike, offers a promising method for\nimprovement. This feedback is applied to the entire list of inputs rather than\ngiving specific ratings for individual documents, making it a Listwide Labels\nLearning-to-Rank task.\n  To address this task, we propose Pistis-RAG, a new RAG framework designed\nwith a content-centric approach to better align LLMs with human preferences.\nPistis-RAG effectively utilizes human feedback, enhancing content ranking and\ngeneration quality. To validate our framework, we use public datasets to\nsimulate human feedback, allowing us to evaluate and refine our method\neffectively. Experimental results indicate that Pistis-RAG improves alignment\nwith human preferences relative to the baseline RAG system, showing a 6.06%\nincrease in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy\nmetrics. These results highlight Pistis-RAG's effectiveness in overcoming the\nlimitations associated with traditional RAG approaches.\n","authors":["Yu Bai","Yukai Miao","Li Chen","Dawei Wang","Dan Li","Yanyu Ren","Hongtao Xie","Ce Yang","Xuhui Cai"],"pdf_url":"https://arxiv.org/pdf/2407.00072v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23728v1","updated":"2024-10-31T08:30:55Z","published":"2024-10-31T08:30:55Z","title":"GigaCheck: Detecting LLM-generated Content","summary":"  With the increasing quality and spread of LLM-based assistants, the amount of\nartificially generated content is growing rapidly. In many cases and tasks,\nsuch texts are already indistinguishable from those written by humans, and the\nquality of generation tends to only increase. At the same time, detection\nmethods are developing more slowly, making it challenging to prevent misuse of\nthese technologies.\n  In this work, we investigate the task of generated text detection by\nproposing the GigaCheck. Our research explores two approaches: (i)\ndistinguishing human-written texts from LLM-generated ones, and (ii) detecting\nLLM-generated intervals in Human-Machine collaborative texts. For the first\ntask, our approach utilizes a general-purpose LLM, leveraging its extensive\nlanguage abilities to fine-tune efficiently for the downstream task of\nLLM-generated text detection, achieving high performance even with limited\ndata. For the second task, we propose a novel approach that combines computer\nvision and natural language processing techniques. Specifically, we use a\nfine-tuned general-purpose LLM in conjunction with a DETR-like detection model,\nadapted from computer vision, to localize artificially generated intervals\nwithin text.\n  We evaluate the GigaCheck on five classification datasets with English texts\nand three datasets designed for Human-Machine collaborative text analysis. Our\nresults demonstrate that GigaCheck outperforms previous methods, even in\nout-of-distribution settings, establishing a strong baseline across all\ndatasets.\n","authors":["Irina Tolstykh","Aleksandra Tsybina","Sergey Yakubson","Aleksandr Gordeev","Vladimir Dokholyan","Maksim Kuprashevich"],"pdf_url":"https://arxiv.org/pdf/2410.23728v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.23725v1","updated":"2024-10-31T08:24:37Z","published":"2024-10-31T08:24:37Z","title":"Artificial intelligence to improve clinical coding practice in\n  Scandinavia: a crossover randomized controlled trial","summary":"  \\textbf{Trial design} Crossover randomized controlled trial. \\textbf{Methods}\nAn AI tool, Easy-ICD, was developed to assist clinical coders and was tested\nfor improving both accuracy and time in a user study in Norway and Sweden.\nParticipants were randomly assigned to two groups, and crossed over between\ncoding complex (longer) texts versus simple (shorter) texts, while using our\ntool versus not using our tool. \\textbf{Results} Based on Mann-Whitney U test,\nthe median coding time difference for complex clinical text sequences was 123\nseconds (\\emph{P}\\textless.001, 95\\% CI: 81 to 164), representing a 46\\%\nreduction in median coding time when our tool is used. There was no significant\ntime difference for simpler text sequences. For coding accuracy, the\nimprovement we noted for both complex and simple texts was not significant.\n\\textbf{Conclusions} This study demonstrates the potential of AI to transform\ncommon tasks in clinical workflows, with ostensible positive impacts on work\nefficiencies for complex clinical coding tasks. Further studies within hospital\nworkflows are required before these presumed impacts can be more clearly\nunderstood.\n","authors":["Taridzo Chomutare","Therese Olsen Svenning","Miguel Ángel Tejedor Hernández","Phuong Dinh Ngo","Andrius Budrionis","Kaisa Markljung","Lill Irene Hind","Torbjørn Torsvik","Karl Øyvind Mikalsen","Aleksandar Babic","Hercules Dalianis"],"pdf_url":"https://arxiv.org/pdf/2410.23725v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.03978v3","updated":"2024-10-31T08:11:04Z","published":"2024-07-04T14:50:45Z","title":"Benchmarking Complex Instruction-Following with Multiple Constraints\n  Composition","summary":"  Instruction following is one of the fundamental capabilities of large\nlanguage models (LLMs). As the ability of LLMs is constantly improving, they\nhave been increasingly applied to deal with complex human instructions in\nreal-world scenarios. Therefore, how to evaluate the ability of complex\ninstruction-following of LLMs has become a critical research problem. Existing\nbenchmarks mainly focus on modeling different types of constraints in human\ninstructions while neglecting the composition of different constraints, which\nis an indispensable constituent in complex instructions. To this end, we\npropose ComplexBench, a benchmark for comprehensively evaluating the ability of\nLLMs to follow complex instructions composed of multiple constraints. We\npropose a hierarchical taxonomy for complex instructions, including 4\nconstraint types, 19 constraint dimensions, and 4 composition types, and\nmanually collect a high-quality dataset accordingly. To make the evaluation\nreliable, we augment LLM-based evaluators with rules to effectively verify\nwhether generated texts can satisfy each constraint and composition.\nFurthermore, we obtain the final evaluation score based on the dependency\nstructure determined by different composition types. ComplexBench identifies\nsignificant deficiencies in existing LLMs when dealing with complex\ninstructions with multiple constraints composition.\n","authors":["Bosi Wen","Pei Ke","Xiaotao Gu","Lindong Wu","Hao Huang","Jinfeng Zhou","Wenchuang Li","Binxin Hu","Wendy Gao","Jiaxin Xu","Yiming Liu","Jie Tang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2407.03978v3.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.23703v1","updated":"2024-10-31T07:48:44Z","published":"2024-10-31T07:48:44Z","title":"OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large\n  Language Models","summary":"  Offline evaluation of LLMs is crucial in understanding their capacities,\nthough current methods remain underexplored in existing research. In this work,\nwe focus on the offline evaluation of the chain-of-thought capabilities and\nshow how to optimize LLMs based on the proposed evaluation method. To enable\noffline feedback with rich knowledge and reasoning paths, we use knowledge\ngraphs (e.g., Wikidata5m) to provide feedback on the generated chain of\nthoughts. Due to the heterogeneity between LLM reasoning and KG structures,\ndirect interaction and feedback from KGs on LLM behavior are challenging, as\nthey require accurate entity linking and grounding of LLM-generated chains of\nthought in the KG. To address the above challenge, we propose an offline\nchain-of-thought evaluation framework, OCEAN, which models chain-of-thought\nreasoning in LLMs as an MDP and evaluate the policy's alignment with KG\npreference modeling. To overcome the reasoning heterogeneity and grounding\nproblems, we leverage on-policy KG exploration and RL to model a KG policy that\ngenerates token-level likelihood distributions for LLM-generated\nchain-of-thought reasoning paths, simulating KG reasoning preference. Then we\nincorporate the knowledge-graph feedback on the validity and alignment of the\ngenerated reasoning paths into inverse propensity scores and propose KG-IPS\nestimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS\nestimator and provide a lower bound on its variance. With the off-policy\nevaluated value function, we can directly enable off-policy optimization to\nfurther enhance chain-of-thought alignment. Our empirical study shows that\nOCEAN can be efficiently optimized for generating chain-of-thought reasoning\npaths with higher estimated values without affecting LLMs' general abilities in\ndownstream tasks or their internal knowledge.\n","authors":["Junda Wu","Xintong Li","Ruoyu Wang","Yu Xia","Yuxin Xiong","Jianing Wang","Tong Yu","Xiang Chen","Branislav Kveton","Lina Yao","Jingbo Shang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.23703v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2406.07933v2","updated":"2024-10-31T07:36:39Z","published":"2024-06-12T06:56:20Z","title":"Large Language Model Unlearning via Embedding-Corrupted Prompts","summary":"  Large language models (LLMs) have advanced to encompass extensive knowledge\nacross diverse domains. Yet controlling what a large language model should not\nknow is important for ensuring alignment and thus safe use. However, accurately\nand efficiently unlearning knowledge from an LLM remains challenging due to the\npotential collateral damage caused by the fuzzy boundary between retention and\nforgetting, and the large computational requirements for optimization across\nstate-of-the-art models with hundreds of billions of parameters. In this work,\nwe present \\textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning\nframework for large language models to address both the challenges of knowledge\nentanglement and unlearning efficiency. Instead of relying on the LLM itself to\nunlearn, we enforce an unlearned state during inference by employing a prompt\nclassifier to identify and safeguard prompts to forget. We learn corruptions\nadded to prompt embeddings via zeroth order optimization toward the unlearning\nobjective offline and corrupt prompts flagged by the classifier during\ninference. We find that these embedding-corrupted prompts not only lead to\ndesirable outputs that satisfy the unlearning objective but also closely\napproximate the output from a model that has never been trained on the data\nintended for forgetting. Through extensive experiments on unlearning, we\ndemonstrate the superiority of our method in achieving promising unlearning at\n\\textit{nearly zero side effects} in general domains and domains closely\nrelated to the unlearned ones. Additionally, we highlight the scalability of\nour method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no\nadditional cost as the number of parameters increases. We have made our code\npublicly available at \\url{https://github.com/chrisliu298/llm-unlearn-eco}.\n","authors":["Chris Yuhao Liu","Yaxuan Wang","Jeffrey Flanigan","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07933v2.pdf","comment":"NeurIPS 2024 Poster"},{"id":"http://arxiv.org/abs/2410.21647v2","updated":"2024-10-31T07:31:31Z","published":"2024-10-29T01:21:05Z","title":"Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'","summary":"  Large language models (LLMs) have achieved high accuracy, i.e., more than 90\npass@1, in solving Python coding problems in HumanEval and MBPP. Thus, a\nnatural question is, whether LLMs achieve comparable code completion\nperformance compared to human developers? Unfortunately, one cannot answer this\nquestion using existing manual crafted or simple (e.g., single-line) code\ngeneration benchmarks, since such tasks fail to represent real-world software\ndevelopment tasks. In addition, existing benchmarks often use poor code\ncorrectness metrics, providing misleading conclusions.\n  To address these challenges, we create REPOCOD, a code generation benchmark\nwith 980 problems collected from 11 popular real-world projects, with more than\n58% of them requiring file-level or repository-level context information. In\naddition, REPOCOD has the longest average canonical solution length (331.6\ntokens) and the highest average cyclomatic complexity (9.00) compared to\nexisting benchmarks. Each task in REPOCOD includes 313.5 developerwritten test\ncases on average for better correctness evaluation. In our evaluations of ten\nLLMs, none of the models achieve more than 30 pass@1 on REPOCOD, indicating the\nnecessity of building stronger LLMs that can help developers in real-world\nsoftware development. REPOCOD is available at\nhttps://github.com/ltasset/REPOCOD\n","authors":["Shanchao Liang","Yiran Hu","Nan Jiang","Lin Tan"],"pdf_url":"https://arxiv.org/pdf/2410.21647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23692v1","updated":"2024-10-31T07:30:38Z","published":"2024-10-31T07:30:38Z","title":"Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction","summary":"  Human mobility prediction plays a critical role in applications such as\ndisaster response, urban planning, and epidemic forecasting. Traditional\nmethods often rely on designing crafted, domain-specific models, and typically\nfocus on short-term predictions, which struggle to generalize across diverse\nurban environments. In this study, we introduce Llama-3-8B-Mob, a large\nlanguage model fine-tuned with instruction tuning, for long-term citywide\nmobility prediction -- in a Q&A manner. We validate our approach using\nlarge-scale human mobility data from four metropolitan areas in Japan, focusing\non predicting individual trajectories over the next 15 days. The results\ndemonstrate that Llama-3-8B-Mob excels in modeling long-term human mobility --\nsurpassing the state-of-the-art on multiple prediction metrics. It also\ndisplays strong zero-shot generalization capabilities -- effectively\ngeneralizing to other cities even when fine-tuned only on limited samples from\na single city. Source codes are available at\nhttps://github.com/TANGHULU6/Llama3-8B-Mob.\n","authors":["Peizhi Tang","Chuang Yang","Tong Xing","Xiaohang Xu","Renhe Jiang","Kaoru Sezaki"],"pdf_url":"https://arxiv.org/pdf/2410.23692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17617v3","updated":"2024-10-31T07:23:09Z","published":"2023-12-29T14:25:22Z","title":"Large Language Models for Generative Information Extraction: A Survey","summary":"  Information extraction (IE) aims to extract structural knowledge from plain\nnatural language texts. Recently, generative Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in text understanding and generation. As a\nresult, numerous works have been proposed to integrate LLMs for IE tasks based\non a generative paradigm. To conduct a comprehensive systematic review and\nexploration of LLM efforts for IE tasks, in this study, we survey the most\nrecent advancements in this field. We first present an extensive overview by\ncategorizing these works in terms of various IE subtasks and techniques, and\nthen we empirically analyze the most advanced methods and discover the emerging\ntrend of IE tasks with LLMs. Based on a thorough review conducted, we identify\nseveral insights in technique and promising research directions that deserve\nfurther exploration in future studies. We maintain a public repository and\nconsistently update related works and resources on GitHub\n(\\href{https://github.com/quqxui/Awesome-LLM4IE-Papers}{LLM4IE repository})\n","authors":["Derong Xu","Wei Chen","Wenjun Peng","Chao Zhang","Tong Xu","Xiangyu Zhao","Xian Wu","Yefeng Zheng","Yang Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.17617v3.pdf","comment":"The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40555-y}. You can cite the FCS version"},{"id":"http://arxiv.org/abs/2410.23684v1","updated":"2024-10-31T07:19:44Z","published":"2024-10-31T07:19:44Z","title":"Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in\n  Byte-Level Tokenizers","summary":"  Tokenization is a crucial step that bridges human-readable text with\nmodel-readable discrete tokens. However, recent studies have revealed that\ntokenizers can be exploited to elicit unwanted model behaviors. In this work,\nwe investigate incomplete tokens, i.e., undecodable tokens with stray bytes\nresulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize\nthat such tokens are heavily reliant on their adjacent tokens and are fragile\nwhen paired with unfamiliar tokens. To demonstrate this vulnerability, we\nintroduce improbable bigrams: out-of-distribution combinations of incomplete\ntokens designed to exploit their dependency. Our experiments show that\nimprobable bigrams are significantly prone to hallucinatory behaviors.\nSurprisingly, alternative tokenizations of the same phrases result in\ndrastically lower rates of hallucination (93% reduction in Llama3.1). We\ncaution against the potential vulnerabilities introduced by byte-level BPE\ntokenizers, which may impede the development of trustworthy language models.\n","authors":["Eugene Jang","Kimin Lee","Jin-Woo Chung","Keuntae Park","Seungwon Shin"],"pdf_url":"https://arxiv.org/pdf/2410.23684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21898v2","updated":"2024-10-31T07:13:08Z","published":"2024-10-29T09:42:54Z","title":"A Longitudinal Analysis of Racial and Gender Bias in New York Times and\n  Fox News Images and Articles","summary":"  The manner in which different racial and gender groups are portrayed in news\ncoverage plays a large role in shaping public opinion. As such, understanding\nhow such groups are portrayed in news media is of notable societal value, and\nhas thus been a significant endeavour in both the computer and social sciences.\nYet, the literature still lacks a longitudinal study examining both the\nfrequency of appearance of different racial and gender groups in online news\narticles, as well as the context in which such groups are discussed. To fill\nthis gap, we propose two machine learning classifiers to detect the race and\nage of a given subject. Next, we compile a dataset of 123,337 images and\n441,321 online news articles from New York Times (NYT) and Fox News (Fox), and\nexamine representation through two computational approaches. Firstly, we\nexamine the frequency and prominence of appearance of racial and gender groups\nin images embedded in news articles, revealing that racial and gender\nminorities are largely under-represented, and when they do appear, they are\nfeatured less prominently compared to majority groups. Furthermore, we find\nthat NYT largely features more images of racial minority groups compared to\nFox. Secondly, we examine both the frequency and context with which racial\nminority groups are presented in article text. This reveals the narrow scope in\nwhich certain racial groups are covered and the frequency with which different\ngroups are presented as victims and/or perpetrators in a given conflict. Taken\ntogether, our analysis contributes to the literature by providing two novel\nopen-source classifiers to detect race and age from images, and shedding light\non the racial and gender biases in news articles from venues on opposite ends\nof the American political spectrum.\n","authors":["Hazem Ibrahim","Nouar AlDahoul","Syed Mustafa Ali Abbasi","Fareed Zaffar","Talal Rahwan","Yasir Zaki"],"pdf_url":"https://arxiv.org/pdf/2410.21898v2.pdf","comment":"13 pages, and 11 figures"},{"id":"http://arxiv.org/abs/2406.09136v2","updated":"2024-10-31T07:12:06Z","published":"2024-06-13T14:07:02Z","title":"Chain of Preference Optimization: Improving Chain-of-Thought Reasoning\n  in LLMs","summary":"  The recent development of chain-of-thought (CoT) decoding has enabled large\nlanguage models (LLMs) to generate explicit logical reasoning paths for complex\nproblem-solving. However, research indicates that these paths are not always\ndeliberate and optimal. The tree-of-thought (ToT) method employs tree-searching\nto extensively explore the reasoning space and find better reasoning paths that\nCoT decoding might overlook. This deliberation, however, comes at the cost of\nsignificantly increased inference complexity. In this work, we demonstrate that\nfine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to\nachieve similar or better performance, thereby avoiding the substantial\ninference burden. This is achieved through Chain of Preference Optimization\n(CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths\nwith those of ToT using the inherent preference information in the tree-search\nprocess. Extensive experimental results show that CPO significantly improves\nLLM performance in solving a variety of complex problems, including question\nanswering, fact verification, and arithmetic reasoning, demonstrating its\neffectiveness. Our code is available at https://github.com/sail-sg/CPO.\n","authors":["Xuan Zhang","Chao Du","Tianyu Pang","Qian Liu","Wei Gao","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2406.09136v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23678v1","updated":"2024-10-31T06:58:34Z","published":"2024-10-31T06:58:34Z","title":"Pseudo-Conversation Injection for LLM Goal Hijacking","summary":"  Goal hijacking is a type of adversarial attack on Large Language Models\n(LLMs) where the objective is to manipulate the model into producing a\nspecific, predetermined output, regardless of the user's original input. In\ngoal hijacking, an attacker typically appends a carefully crafted malicious\nsuffix to the user's prompt, which coerces the model into ignoring the user's\noriginal input and generating the target response. In this paper, we introduce\na novel goal hijacking attack method called Pseudo-Conversation Injection,\nwhich leverages the weaknesses of LLMs in role identification within\nconversation contexts. Specifically, we construct the suffix by fabricating\nresponses from the LLM to the user's initial prompt, followed by a prompt for a\nmalicious new task. This leads the model to perceive the initial prompt and\nfabricated response as a completed conversation, thereby executing the new,\nfalsified prompt. Following this approach, we propose three Pseudo-Conversation\nconstruction strategies: Targeted Pseudo-Conversation, Universal\nPseudo-Conversation, and Robust Pseudo-Conversation. These strategies are\ndesigned to achieve effective goal hijacking across various scenarios. Our\nexperiments, conducted on two mainstream LLM platforms including ChatGPT and\nQwen, demonstrate that our proposed method significantly outperforms existing\napproaches in terms of attack effectiveness.\n","authors":["Zheng Chen","Buhui Yao"],"pdf_url":"https://arxiv.org/pdf/2410.23678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11425v2","updated":"2024-10-31T06:34:10Z","published":"2024-03-18T02:42:01Z","title":"Narrative Feature or Structured Feature? A Study of Large Language\n  Models to Identify Cancer Patients at Risk of Heart Failure","summary":"  Cancer treatments are known to introduce cardiotoxicity, negatively impacting\noutcomes and survivorship. Identifying cancer patients at risk of heart failure\n(HF) is critical to improving cancer treatment outcomes and safety. This study\nexamined machine learning (ML) models to identify cancer patients at risk of HF\nusing electronic health records (EHRs), including traditional ML, Time-Aware\nlong short-term memory (T-LSTM), and large language models (LLMs) using novel\nnarrative features derived from the structured medical codes. We identified a\ncancer cohort of 12,806 patients from the University of Florida Health,\ndiagnosed with lung, breast, and colorectal cancers, among which 1,602\nindividuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the\nbest F1 scores, outperforming the traditional support vector machines by 0.397,\nthe T-LSTM deep learning model by 0.077, and a widely used transformer model,\nBERT, by 0.056. The analysis shows that the proposed narrative features\nremarkably increased feature density and improved performance.\n","authors":["Ziyi Chen","Mengyuan Zhang","Mustafa Mohammed Ahmed","Yi Guo","Thomas J. George","Jiang Bian","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11425v2.pdf","comment":"10 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.23668v1","updated":"2024-10-31T06:32:47Z","published":"2024-10-31T06:32:47Z","title":"Kernel Looping: Eliminating Synchronization Boundaries for Peak\n  Inference Performance","summary":"  Token generation speed is critical to power the next wave of AI inference\napplications. GPUs significantly underperform during token generation due to\nsynchronization overheads at kernel boundaries, utilizing only 21% of their\npeak memory bandwidth. While recent dataflow architectures mitigate these\noverheads by enabling aggressive fusion of decoder layers into a single kernel,\nthey too leave performance on the table due to synchronization penalties at\nlayer boundaries.\n  This paper presents kernel looping, a specialized global optimization\ntechnique which exploits an optimization opportunity brought by combining the\nunique layer-level fusion possible in modern dataflow architectures with the\nrepeated layer structure found in language models. Kernel looping eliminates\nsynchronization costs between consecutive calls to the same kernel by\ntransforming these calls into a single call to a modified kernel containing a\npipelined outer loop. We evaluate kernel looping on the SambaNova SN40L\nReconfigurable Dataflow Unit (RDU), a commercial dataflow accelerator for AI.\nExperiments demonstrate that kernel looping speeds up the decode phase of a\nwide array of powerful open-source models by up to 2.2$\\times$ on SN40L. Kernel\nlooping allows scaling of decode performance over multiple SN40L sockets,\nachieving speedups of up to 2.5$\\times$. Finally, kernel looping enables SN40L\nto achieve over 90% of peak performance on 8 and 16 sockets and achieve a\nspeedup of up to 3.7$\\times$ over DGX H100. Kernel looping, as well as the\nmodels evaluated in this paper, are deployed in production in a commercial AI\ninference cloud.\n","authors":["David Koeplinger","Darshan Gandhi","Pushkar Nandkar","Nathan Sheeley","Matheen Musaddiq","Leon Zhang","Reid Goodbar","Matthew Shaffer","Han Wang","Angela Wang","Mingran Wang","Raghu Prabhakar"],"pdf_url":"https://arxiv.org/pdf/2410.23668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23656v1","updated":"2024-10-31T06:13:29Z","published":"2024-10-31T06:13:29Z","title":"Morphological Typology in BPE Subword Productivity and Language Modeling","summary":"  This study investigates the impact of morphological typology on tokenization\nand language modeling performance. We focus on languages with synthetic and\nanalytical morphological structures and examine their productivity when\ntokenized using the byte-pair encoding (BPE) algorithm. We compare the\nperformance of models trained with similar amounts of data in different\nlanguages. Our experiments reveal that languages with synthetic features\nexhibit greater subword regularity and productivity with BPE tokenization and\nachieve better results in language modeling tasks. We also observe that the\ntypological continuum from linguistic theory is reflected in several\nexperiments. These findings suggest a correlation between morphological\ntypology and BPE tokenization efficiency.\n","authors":["Iñigo Parra"],"pdf_url":"https://arxiv.org/pdf/2410.23656v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.02436v3","updated":"2024-10-31T06:09:12Z","published":"2024-03-04T19:33:39Z","title":"How does Architecture Influence the Base Capabilities of Pre-trained\n  Language Models? A Case Study Based on FFN-Wider and MoE Transformers","summary":"  Pre-trained language models have been proven to possess strong base\ncapabilities, which not only excel in in-distribution language modeling but\nalso show powerful abilities in out-of-distribution language modeling, transfer\nlearning and few-shot learning. Unlike existing work focusing on the influence\nof scale on base capabilities, our work examines the influence of architecture\non those. Specifically, our concern is: How does architecture influence the\nbase capabilities of pre-trained language models? In this work, we attempt to\nexplain and reverse the decline in base capabilities caused by the architecture\nof FFN-Wider Transformers, seeking to provide some insights. Through analysis,\nwe found the contribution ratio of Multi-Head Attention (a combination\nfunction) to pre-trained language modeling is a key factor affecting base\ncapabilities. FFN-Wider Transformers reduce the contribution ratio of this\ncombination function, leading to a decline in base capabilities. We confirmed\nthis by experiments and proposed Combination Enhanced Architecture (CEA) to\naddress the decline in base capabilities of such models. Significantly, we\nextended our explanation and CEA to Mixture of Experts (MoE) Transformers. We\nsuccessfully achieved significant improvements in base capabilities on a 14B\nparameter MoE model, demonstrating the practical application value of our work.\nThis also indicates that our analysis has a certain guiding significance for\narchitecture analysis, architecture improvement and architecture design.\n","authors":["Xin Lu","Yanyan Zhao","Bing Qin","Liangyu Huo","Qing Yang","Dongliang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.02436v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.21352v2","updated":"2024-10-31T06:01:26Z","published":"2024-10-28T14:45:01Z","title":"LLMCBench: Benchmarking Large Language Model Compression for Efficient\n  Deployment","summary":"  Although large language models (LLMs) have demonstrated their strong\nintelligence ability, the high demand for computation and storage hinders their\npractical application. To this end, many model compression techniques are\nproposed to increase the efficiency of LLMs. However, current researches only\nvalidate their methods on limited models, datasets, metrics, etc, and still\nlack a comprehensive evaluation under more general scenarios. So it is still a\nquestion of which model compression approach we should use under a specific\ncase. To mitigate this gap, we present the Large Language Model Compression\nBenchmark (LLMCBench), a rigorously designed benchmark with an in-depth\nanalysis for LLM compression algorithms. We first analyze the actual model\nproduction requirements and carefully design evaluation tracks and metrics.\nThen, we conduct extensive experiments and comparison using multiple mainstream\nLLM compression approaches. Finally, we perform an in-depth analysis based on\nthe evaluation and provide useful insight for LLM compression design. We hope\nour LLMCBench can contribute insightful suggestions for LLM compression\nalgorithm design and serve as a foundation for future research. Our code is\navailable at https://github.com/AboveParadise/LLMCBench.\n","authors":["Ge Yang","Changyi He","Jinyang Guo","Jianyu Wu","Yifu Ding","Aishan Liu","Haotong Qin","Pengliang Ji","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21352v2.pdf","comment":"Accepted by NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2409.02795v5","updated":"2024-10-31T05:39:06Z","published":"2024-09-04T15:11:55Z","title":"Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey","summary":"  Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.\n","authors":["Bofei Gao","Feifan Song","Yibo Miao","Zefan Cai","Zhe Yang","Liang Chen","Helan Hu","Runxin Xu","Qingxiu Dong","Ce Zheng","Shanghaoran Quan","Wen Xiao","Ge Zhang","Daoguang Zan","Keming Lu","Bowen Yu","Dayiheng Liu","Zeyu Cui","Jian Yang","Lei Sha","Houfeng Wang","Zhifang Sui","Peiyi Wang","Tianyu Liu","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2409.02795v5.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.11821v3","updated":"2024-10-31T05:19:58Z","published":"2024-02-19T04:29:45Z","title":"Microstructures and Accuracy of Graph Recall by Large Language Models","summary":"  Graphs data is crucial for many applications, and much of it exists in the\nrelations described in textual format. As a result, being able to accurately\nrecall and encode a graph described in earlier text is a basic yet pivotal\nability that LLMs need to demonstrate if they are to perform reasoning tasks\nthat involve graph-structured information. Human performance at graph recall\nhas been studied by cognitive scientists for decades, and has been found to\noften exhibit certain structural patterns of bias that align with human\nhandling of social relationships. To date, however, we know little about how\nLLMs behave in analogous graph recall tasks: do their recalled graphs also\nexhibit certain biased patterns, and if so, how do they compare with humans and\naffect other graph reasoning tasks? In this work, we perform the first\nsystematical study of graph recall by LLMs, investigating the accuracy and\nbiased microstructures (local structural patterns) in their recall. We find\nthat LLMs not only underperform often in graph recall, but also tend to favor\nmore triangles and alternating 2-paths. Moreover, we find that more advanced\nLLMs have a striking dependence on the domain that a real-world graph comes\nfrom -- by yielding the best recall accuracy when the graph is narrated in a\nlanguage style consistent with its original domain.\n","authors":["Yanbang Wang","Hejie Cui","Jon Kleinberg"],"pdf_url":"https://arxiv.org/pdf/2402.11821v3.pdf","comment":"Accepted at NeurIPS 2024; Code available at:\n  https://github.com/Abel0828/llm-graph-recall"},{"id":"http://arxiv.org/abs/2402.02420v3","updated":"2024-10-31T04:50:59Z","published":"2024-02-04T09:36:31Z","title":"Factuality of Large Language Models: A Survey","summary":"  Large language models (LLMs), especially when instruction-tuned for chat,\nhave become part of our daily lives, freeing people from the process of\nsearching, extracting, and integrating information from multiple sources by\noffering a straightforward answer to a variety of questions in a single place.\nUnfortunately, in many cases, LLM responses are factually incorrect, which\nlimits their applicability in real-world scenarios. As a result, research on\nevaluating and improving the factuality of LLMs has attracted a lot of\nattention recently. In this survey, we critically analyze existing work with\nthe aim to identify the major challenges and their associated causes, pointing\nout to potential solutions for improving the factuality of LLMs, and analyzing\nthe obstacles to automated factuality evaluation for open-ended text\ngeneration. We further offer an outlook on where future research should go.\n","authors":["Yuxia Wang","Minghan Wang","Muhammad Arslan Manzoor","Fei Liu","Georgi Georgiev","Rocktim Jyoti Das","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2402.02420v3.pdf","comment":"11 pages, 1 figure and 2 tables"},{"id":"http://arxiv.org/abs/2406.11250v2","updated":"2024-10-31T04:40:26Z","published":"2024-06-17T06:22:20Z","title":"Can Machines Resonate with Humans? Evaluating the Emotional and Empathic\n  Comprehension of LMs","summary":"  Empathy plays a pivotal role in fostering prosocial behavior, often triggered\nby the sharing of personal experiences through narratives. However, modeling\nempathy using NLP approaches remains challenging due to its deep\ninterconnection with human interaction dynamics. Previous approaches, which\ninvolve fine-tuning language models (LMs) on human-annotated empathic datasets,\nhave had limited success. In our pursuit of improving empathy understanding in\nLMs, we propose several strategies, including contrastive learning with masked\nLMs and supervised fine-tuning with large language models. While these methods\nshow improvements over previous methods, the overall results remain\nunsatisfactory. To better understand this trend, we performed an analysis which\nreveals a low agreement among annotators. This lack of consensus hinders\ntraining and highlights the subjective nature of the task. We also explore the\ncultural impact on annotations. To study this, we meticulously collected story\npairs in Urdu language and find that subjectivity in interpreting empathy among\nannotators appears to be independent of cultural background. Our systematic\nexploration of LMs' understanding of empathy reveals substantial opportunities\nfor further investigation in both task formulation and modeling.\n","authors":["Muhammad Arslan Manzoor","Yuxia Wang","Minghan Wang","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2406.11250v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2404.15154v2","updated":"2024-10-31T04:32:26Z","published":"2024-04-22T08:28:13Z","title":"Do not think about pink elephant!","summary":"  Large Models (LMs) have heightened expectations for the potential of general\nAI as they are akin to human intelligence. This paper shows that recent large\nmodels such as Stable Diffusion and DALL-E3 also share the vulnerability of\nhuman intelligence, namely the \"white bear phenomenon\". We investigate the\ncauses of the white bear phenomenon by analyzing their representation space.\nBased on this analysis, we propose a simple prompt-based attack method, which\ngenerates figures prohibited by the LM provider's policy. To counter these\nattacks, we introduce prompt-based defense strategies inspired by cognitive\ntherapy techniques, successfully mitigating attacks by up to 48.22\\%.\n","authors":["Kyomin Hwang","Suyoung Kim","JunHoo Lee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2404.15154v2.pdf","comment":"This paper is accepted in CVPR 2024 Responsible Generative AI\n  Workshop (ReGenAI)"},{"id":"http://arxiv.org/abs/2401.10862v3","updated":"2024-10-31T04:16:12Z","published":"2024-01-19T18:05:34Z","title":"Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs\n  Without Fine-Tuning","summary":"  This paper investigates the impact of model compression on the way Large\nLanguage Models (LLMs) process prompts, particularly concerning jailbreak\nresistance. We show that moderate WANDA pruning can enhance resistance to\njailbreaking attacks without fine-tuning, while maintaining performance on\nstandard benchmarks. To systematically evaluate this safety enhancement, we\nintroduce a dataset of 225 harmful tasks across five categories. Our analysis\nof LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 reveals that pruning\nbenefits correlate with initial model safety levels. We interpret these results\nby examining changes in attention patterns and perplexity shifts, demonstrating\nthat pruned models exhibit sharper attention and increased sensitivity to\nartificial jailbreak constructs. We extend our evaluation to the AdvBench\nharmful behavior tasks and the GCG attack method. We find that LLaMA-2 is much\nsafer on AdvBench prompts than on our dataset when evaluated with manual\njailbreak attempts, and that pruning is effective against both automated\nattacks and manual jailbreaking on Advbench.\n","authors":["Adib Hasan","Ileana Rugina","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2401.10862v3.pdf","comment":"Proceedings of the 7th BlackboxNLP Workshop: Analyzing and\n  Interpreting Neural Networks for NLP"},{"id":"http://arxiv.org/abs/2405.14992v2","updated":"2024-10-31T04:15:40Z","published":"2024-05-23T18:51:47Z","title":"Linking In-context Learning in Transformers to Human Episodic Memory","summary":"  Understanding connections between artificial and biological intelligent\nsystems can reveal fundamental principles of general intelligence. While many\nartificial intelligence models have a neuroscience counterpart, such\nconnections are largely missing in Transformer models and the self-attention\nmechanism. Here, we examine the relationship between interacting attention\nheads and human episodic memory. We focus on induction heads, which contribute\nto in-context learning in Transformer-based large language models (LLMs). We\ndemonstrate that induction heads are behaviorally, functionally, and\nmechanistically similar to the contextual maintenance and retrieval (CMR) model\nof human episodic memory. Our analyses of LLMs pre-trained on extensive text\ndata show that CMR-like heads often emerge in the intermediate and late layers,\nqualitatively mirroring human memory biases. The ablation of CMR-like heads\nsuggests their causal role in in-context learning. Our findings uncover a\nparallel between the computational mechanisms of LLMs and human memory,\noffering valuable insights into both research fields.\n","authors":["Li Ji-An","Corey Y. Zhou","Marcus K. Benna","Marcelo G. Mattar"],"pdf_url":"https://arxiv.org/pdf/2405.14992v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23609v1","updated":"2024-10-31T03:50:15Z","published":"2024-10-31T03:50:15Z","title":"On Positional Bias of Faithfulness for Long-form Summarization","summary":"  Large Language Models (LLMs) often exhibit positional bias in long-context\nsettings, under-attending to information in the middle of inputs. We\ninvestigate the presence of this bias in long-form summarization, its impact on\nfaithfulness, and various techniques to mitigate this bias. To consistently\nevaluate faithfulness, we first compile a benchmark of eight human-annotated\nlong-form summarization datasets and perform a meta-evaluation of faithfulness\nmetrics. We show that LLM-based faithfulness metrics, though effective with\nfull-context inputs, remain sensitive to document order, indicating positional\nbias. Analyzing LLM-generated summaries across six datasets, we find a\n\"U-shaped\" trend in faithfulness, where LLMs faithfully summarize the beginning\nand end of documents but neglect middle content. Perturbing document order\nsimilarly reveals models are less faithful when important documents are placed\nin the middle of the input. We find that this behavior is partly due to\nshifting focus with context length: as context increases, summaries become less\nfaithful, but beyond a certain length, faithfulness improves as the model\nfocuses on the end. Finally, we experiment with different generation techniques\nto reduce positional bias and find that prompting techniques effectively direct\nmodel attention to specific positions, whereas more sophisticated approaches\noffer limited improvements. Our data and code are available in\nhttps://github.com/meetdavidwan/longformfact.\n","authors":["David Wan","Jesse Vig","Mohit Bansal","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2410.23609v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2410.14940v2","updated":"2024-10-31T03:50:03Z","published":"2024-10-19T02:07:33Z","title":"Baichuan Alignment Technical Report","summary":"  We introduce Baichuan Alignment, a detailed analysis of the alignment\ntechniques employed in the Baichuan series of models. This represents the\nindustry's first comprehensive account of alignment methodologies, offering\nvaluable insights for advancing AI research. We investigate the critical\ncomponents that enhance model performance during the alignment process,\nincluding optimization methods, data strategies, capability enhancements, and\nevaluation processes. The process spans three key stages: Prompt Augmentation\nSystem (PAS), Supervised Fine-Tuning (SFT), and Preference Alignment. The\nproblems encountered, the solutions applied, and the improvements made are\nthoroughly recorded.\n  Through comparisons across well-established benchmarks, we highlight the\ntechnological advancements enabled by Baichuan Alignment. Baichuan-Instruct is\nan internal model, while Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct\nversions of the Qwen2-72B and Llama-3-70B base models, optimized through\nBaichuan Alignment. Baichuan-Instruct demonstrates significant improvements in\ncore capabilities, with user experience gains ranging from 17% to 28%, and\nperforms exceptionally well on specialized benchmarks. In open-source benchmark\nevaluations, both Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently\noutperform their respective official instruct versions across nearly all\ndatasets. This report aims to clarify the key technologies behind the alignment\nprocess, fostering a deeper understanding within the community.\nLlama3-PBM-Nova-70B model is available at\nhttps://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.\n","authors":["Mingan Lin","Fan Yang","Yanjun Shen","Haoze Sun","Tianpeng Li","Tao Zhang","Chenzheng Zhu","Tao Zhang","Miao Zheng","Xu Li","Yijie Zhou","Mingyang Chen","Yanzhao Qin","Youquan Li","Hao Liang","Fei Li","Yadong Li","Mang Wang","Guosheng Dong","Kun Fang","Jianhua Xu","Bin Cui","Wentao Zhang","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23605v1","updated":"2024-10-31T03:42:17Z","published":"2024-10-31T03:42:17Z","title":"Dynamic Uncertainty Ranking: Enhancing In-Context Learning for Long-Tail\n  Knowledge in LLMs","summary":"  Large language models (LLMs) can learn vast amounts of knowledge from diverse\ndomains during pre-training. However, long-tail knowledge from specialized\ndomains is often scarce and underrepresented, rarely appearing in the models'\nmemorization. Prior work has shown that in-context learning (ICL) with\nretriever augmentation can help LLMs better capture long-tail knowledge,\nreducing their reliance on pre-trained data. Despite these advances, we observe\nthat LLM predictions for long-tail questions remain uncertain to variations in\nretrieved samples. To take advantage of the uncertainty in ICL for guiding LLM\npredictions toward correct answers on long-tail samples, we propose a\nreinforcement learning-based dynamic uncertainty ranking method for ICL that\naccounts for the varying impact of each retrieved sample on LLM predictions.\nOur approach prioritizes more informative and stable samples while demoting\nmisleading ones, updating rankings based on the feedback from the LLM w.r.t.\neach retrieved sample. To enhance training efficiency and reduce query costs,\nwe introduce a learnable dynamic ranking threshold, adjusted when the model\nencounters negative prediction shifts. Experimental results on various\nquestion-answering datasets from different domains show that our method\noutperforms the best baseline by $2.76\\%$, with a notable $5.96\\%$ boost in\naccuracy on long-tail questions that elude zero-shot inference.\n","authors":["Shuyang Yu","Runxue Bao","Parminder Bhatia","Taha Kass-Hout","Jiayu Zhou","Cao Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.23605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14657v3","updated":"2024-10-31T03:41:03Z","published":"2024-06-20T18:22:59Z","title":"OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization\n  Dataset","summary":"  We introduce OpenDebateEvidence, a comprehensive dataset for argument mining\nand summarization sourced from the American Competitive Debate community. This\ndataset includes over 3.5 million documents with rich metadata, making it one\nof the most extensive collections of debate evidence. OpenDebateEvidence\ncaptures the complexity of arguments in high school and college debates,\nproviding valuable resources for training and evaluation. Our extensive\nexperiments demonstrate the efficacy of fine-tuning state-of-the-art large\nlanguage models for argumentative abstractive summarization across various\nmethods, models, and datasets. By providing this comprehensive resource, we aim\nto advance computational argumentation and support practical applications for\ndebaters, educators, and researchers. OpenDebateEvidence is publicly available\nto support further research and innovation in computational argumentation.\nAccess it here: https://huggingface.co/datasets/Yusuf5/OpenCaselist\n","authors":["Allen Roush","Yusuf Shabazz","Arvind Balaji","Peter Zhang","Stefano Mezza","Markus Zhang","Sanjay Basu","Sriram Vishwanath","Mehdi Fatemi","Ravid Shwartz-Ziv"],"pdf_url":"https://arxiv.org/pdf/2406.14657v3.pdf","comment":"Published to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.23603v1","updated":"2024-10-31T03:37:21Z","published":"2024-10-31T03:37:21Z","title":"Using Multimodal Deep Neural Networks to Disentangle Language from\n  Visual Aesthetics","summary":"  When we experience a visual stimulus as beautiful, how much of that\nexperience derives from perceptual computations we cannot describe versus\nconceptual knowledge we can readily translate into natural language?\nDisentangling perception from language in visually-evoked affective and\naesthetic experiences through behavioral paradigms or neuroimaging is often\nempirically intractable. Here, we circumnavigate this challenge by using linear\ndecoding over the learned representations of unimodal vision, unimodal\nlanguage, and multimodal (language-aligned) deep neural network (DNN) models to\npredict human beauty ratings of naturalistic images. We show that unimodal\nvision models (e.g. SimCLR) account for the vast majority of explainable\nvariance in these ratings. Language-aligned vision models (e.g. SLIP) yield\nsmall gains relative to unimodal vision. Unimodal language models (e.g. GPT2)\nconditioned on visual embeddings to generate captions (via CLIPCap) yield no\nfurther gains. Caption embeddings alone yield less accurate predictions than\nimage and caption embeddings combined (concatenated). Taken together, these\nresults suggest that whatever words we may eventually find to describe our\nexperience of beauty, the ineffable computations of feedforward perception may\nprovide sufficient foundation for that experience.\n","authors":["Colin Conwell","Christopher Hamblin","Chelsea Boccagno","David Mayo","Jesse Cummings","Leyla Isik","Andrei Barbu"],"pdf_url":"https://arxiv.org/pdf/2410.23603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20441v3","updated":"2024-10-31T03:34:33Z","published":"2024-09-30T16:00:34Z","title":"Instance-adaptive Zero-shot Chain-of-Thought Prompting","summary":"  Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism.\n","authors":["Xiaosong Yuan","Chen Shen","Shaotian Yan","Xiaofeng Zhang","Liang Xie","Wenxiao Wang","Renchu Guan","Ying Wang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2409.20441v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.21139v2","updated":"2024-10-31T03:29:15Z","published":"2024-10-28T15:42:45Z","title":"uOttawa at LegalLens-2024: Transformer-based Classification Experiments","summary":"  This paper presents the methods used for LegalLens-2024 shared task, which\nfocused on detecting legal violations within unstructured textual data and\nassociating these violations with potentially affected individuals. The shared\ntask included two subtasks: A) Legal Named Entity Recognition (L-NER) and B)\nLegal Natural Language Inference (L-NLI). For subtask A, we utilized the spaCy\nlibrary, while for subtask B, we employed a combined model incorporating\nRoBERTa and CNN. Our results were 86.3% in the L-NER subtask and 88.25% in the\nL-NLI subtask. Overall, our paper demonstrates the effectiveness of transformer\nmodels in addressing complex tasks in the legal domain. The source code for our\nimplementation is publicly available at\nhttps://github.com/NimaMeghdadi/uOttawa-at-LegalLens-2024-Transformer-based-Classification\n","authors":["Nima Meghdadi","Diana Inkpen"],"pdf_url":"https://arxiv.org/pdf/2410.21139v2.pdf","comment":"Just accepted at the the EMNLP conference"},{"id":"http://arxiv.org/abs/2405.17202v3","updated":"2024-10-31T03:26:21Z","published":"2024-05-27T14:24:47Z","title":"Efficient multi-prompt evaluation of LLMs","summary":"  Most popular benchmarks for comparing LLMs rely on a limited set of prompt\ntemplates, which may not fully capture the LLMs' abilities and can affect the\nreproducibility of results on leaderboards. Many recent works empirically\nverify prompt sensitivity and advocate for changes in LLM evaluation. In this\npaper, we consider the problem of estimating the performance distribution\nacross many prompt variants instead of finding a single prompt to evaluate\nwith. We introduce PromptEval, a method for estimating performance across a\nlarge set of prompts borrowing strength across prompts and examples to produce\naccurate estimates under practical evaluation budgets. The resulting\ndistribution can be used to obtain performance quantiles to construct various\nrobust performance metrics (e.g., top 95% quantile or median). We prove that\nPromptEval consistently estimates the performance distribution and demonstrate\nits efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench\nHard, and LMentry; for example, PromptEval can accurately estimate performance\nquantiles across 100 prompt templates on MMLU with a budget equivalent to two\nsingle-prompt evaluations. Moreover, we show how PromptEval can be useful in\nLLM-as-a-judge and best prompt identification applications.\n","authors":["Felipe Maia Polo","Ronald Xu","Lucas Weber","Mírian Silva","Onkar Bhardwaj","Leshem Choshen","Allysson Flavio Melo de Oliveira","Yuekai Sun","Mikhail Yurochkin"],"pdf_url":"https://arxiv.org/pdf/2405.17202v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2312.04828v3","updated":"2024-10-31T03:21:22Z","published":"2023-12-08T05:01:47Z","title":"HuRef: HUman-REadable Fingerprint for Large Language Models","summary":"  Protecting the copyright of large language models (LLMs) has become crucial\ndue to their resource-intensive training and accompanying carefully designed\nlicenses. However, identifying the original base model of an LLM is challenging\ndue to potential parameter alterations. In this study, we introduce HuRef, a\nhuman-readable fingerprint for LLMs that uniquely identifies the base model\nwithout interfering with training or exposing model parameters to the public.\nWe first observe that the vector direction of LLM parameters remains stable\nafter the model has converged during pretraining, with negligible perturbations\nthrough subsequent training steps, including continued pretraining, supervised\nfine-tuning, and RLHF, which makes it a sufficient condition to identify the\nbase model. The necessity is validated by continuing to train an LLM with an\nextra term to drive away the model parameters' direction and the model becomes\ndamaged. However, this direction is vulnerable to simple attacks like dimension\npermutation or matrix rotation, which significantly change it without affecting\nperformance. To address this, leveraging the Transformer structure, we\nsystematically analyze potential attacks and define three invariant terms that\nidentify an LLM's base model. Due to the potential risk of information leakage,\nwe cannot publish invariant terms directly. Instead, we map them to a Gaussian\nvector using an encoder, then convert it into a natural image using StyleGAN2,\nand finally publish the image. In our black-box setting, all fingerprinting\nsteps are internally conducted by the LLMs owners. To ensure the published\nfingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP).\nExperimental results across various LLMs demonstrate the effectiveness of our\nmethod. The code is available at https://github.com/LUMIA-Group/HuRef.\n","authors":["Boyi Zeng","Lizheng Wang","Yuncong Hu","Yi Xu","Chenghu Zhou","Xinbing Wang","Yu Yu","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2312.04828v3.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2406.08747v2","updated":"2024-10-31T03:16:13Z","published":"2024-06-13T02:08:28Z","title":"StreamBench: Towards Benchmarking Continuous Improvement of Language\n  Agents","summary":"  Recent works have shown that large language model (LLM) agents are able to\nimprove themselves from experience, which is an important ability for\ncontinuous enhancement post-deployment. However, existing benchmarks primarily\nevaluate their innate capabilities and do not assess their ability to improve\nover time. To address this gap, we introduce StreamBench, a pioneering\nbenchmark designed to evaluate the continuous improvement of LLM agents over an\ninput-feedback sequence. StreamBench simulates an online learning environment\nwhere LLMs receive a continuous flow of feedback stream and iteratively enhance\ntheir performance. In addition, we propose several simple yet effective\nbaselines for improving LLMs on StreamBench, and provide a comprehensive\nanalysis to identify critical components that contribute to successful\nstreaming strategies. Our work serves as a stepping stone towards developing\neffective online learning strategies for LLMs, paving the way for more adaptive\nAI systems in streaming scenarios. Source code:\nhttps://github.com/stream-bench/stream-bench. Benchmark website:\nhttps://stream-bench.github.io.\n","authors":["Cheng-Kuang Wu","Zhi Rui Tam","Chieh-Yen Lin","Yun-Nung Chen","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.08747v2.pdf","comment":"NeurIPS 2024 Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2405.17809v3","updated":"2024-10-31T03:11:16Z","published":"2024-05-28T04:11:37Z","title":"TransVIP: Speech to Speech Translation System with Voice and Isochrony\n  Preservation","summary":"  There is a rising interest and trend in research towards directly translating\nspeech from one language to another, known as end-to-end speech-to-speech\ntranslation. However, most end-to-end models struggle to outperform cascade\nmodels, i.e., a pipeline framework by concatenating speech recognition, machine\ntranslation and text-to-speech models. The primary challenges stem from the\ninherent complexities involved in direct translation tasks and the scarcity of\ndata. In this study, we introduce a novel model framework TransVIP that\nleverages diverse datasets in a cascade fashion yet facilitates end-to-end\ninference through joint probability. Furthermore, we propose two separated\nencoders to preserve the speaker's voice characteristics and isochrony from the\nsource speech during the translation process, making it highly suitable for\nscenarios such as video dubbing. Our experiments on the French-English language\npair demonstrate that our model outperforms the current state-of-the-art\nspeech-to-speech translation model.\n","authors":["Chenyang Le","Yao Qian","Dongmei Wang","Long Zhou","Shujie Liu","Xiaofei Wang","Midia Yousefi","Yanmin Qian","Jinyu Li","Sheng Zhao","Michael Zeng"],"pdf_url":"https://arxiv.org/pdf/2405.17809v3.pdf","comment":"Neural Information Processing Systems, poster"},{"id":"http://arxiv.org/abs/2410.23000v2","updated":"2024-10-31T03:04:28Z","published":"2024-10-30T13:29:36Z","title":"Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall","summary":"  Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information.\n","authors":["Zehan Qi","Rongwu Xu","Zhijiang Guo","Cunxiang Wang","Hao Zhang","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.23000v2.pdf","comment":"Accepted to EMNLP'24 (Findings). Camera-ready version"},{"id":"http://arxiv.org/abs/2312.10104v4","updated":"2024-10-31T03:02:43Z","published":"2023-12-15T03:11:03Z","title":"Lever LM: Configuring In-Context Sequence to Lever Large Vision Language\n  Models","summary":"  As Archimedes famously said, ``Give me a lever long enough and a fulcrum on\nwhich to place it, and I shall move the world'', in this study, we propose to\nuse a tiny Language Model (LM), \\eg, a Transformer with 67M parameters, to\nlever much larger Vision-Language Models (LVLMs) with 9B parameters.\nSpecifically, we use this tiny \\textbf{Lever-LM} to configure effective\nin-context demonstration (ICD) sequences to improve the In-Context Learinng\n(ICL) performance of LVLMs. Previous studies show that diverse ICD\nconfigurations like the selection and ordering of the demonstrations heavily\naffect the ICL performance, highlighting the significance of configuring\neffective ICD sequences. Motivated by this and by re-considering the the\nprocess of configuring ICD sequence, we find this is a mirror process of human\nsentence composition and further assume that effective ICD configurations may\ncontain internal statistical patterns that can be captured by Lever-LM. Then a\ndataset with effective ICD sequences is constructed to train Lever-LM. After\ntraining, given novel queries, new ICD sequences are configured by the trained\nLever-LM to solve vision-language tasks through ICL. Experiments show that\nthese ICD sequences can improve the ICL performance of two LVLMs compared with\nsome strong baselines in Visual Question Answering and Image Captioning,\nvalidating that Lever-LM can really capture the statistical patterns for\nlevering LVLMs. The code is available at\n\\url{https://github.com/ForJadeForest/Lever-LM}.\n","authors":["Xu Yang","Yingzhe Peng","Haoxuan Ma","Shuo Xu","Chi Zhang","Yucheng Han","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10104v4.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.03161v2","updated":"2024-10-31T02:56:01Z","published":"2024-09-05T01:36:00Z","title":"MaterialBENCH: Evaluating College-Level Materials Science\n  Problem-Solving Abilities of Large Language Models","summary":"  A college-level benchmark dataset for large language models (LLMs) in the\nmaterials science field, MaterialBENCH, is constructed. This dataset consists\nof problem-answer pairs, based on university textbooks. There are two types of\nproblems: one is the free-response answer type, and the other is the\nmultiple-choice type. Multiple-choice problems are constructed by adding three\nincorrect answers as choices to a correct answer, so that LLMs can choose one\nof the four as a response. Most of the problems for free-response answer and\nmultiple-choice types overlap except for the format of the answers. We also\nconduct experiments using the MaterialBENCH on LLMs, including ChatGPT-3.5,\nChatGPT-4, Bard (at the time of the experiments), and GPT-3.5 and GPT-4 with\nthe OpenAI API. The differences and similarities in the performance of LLMs\nmeasured by the MaterialBENCH are analyzed and discussed. Performance\ndifferences between the free-response type and multiple-choice type in the same\nmodels and the influence of using system massages on multiple-choice problems\nare also studied. We anticipate that MaterialBENCH will encourage further\ndevelopments of LLMs in reasoning abilities to solve more complicated problems\nand eventually contribute to materials research and discovery.\n","authors":["Michiko Yoshitake","Yuta Suzuki","Ryo Igarashi","Yoshitaka Ushiku","Keisuke Nagato"],"pdf_url":"https://arxiv.org/pdf/2409.03161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23584v1","updated":"2024-10-31T02:52:39Z","published":"2024-10-31T02:52:39Z","title":"End-to-End Ontology Learning with Large Language Models","summary":"  Ontologies are useful for automatic machine processing of domain knowledge as\nthey represent it in a structured format. Yet, constructing ontologies requires\nsubstantial manual effort. To automate part of this process, large language\nmodels (LLMs) have been applied to solve various subtasks of ontology learning.\nHowever, this partial ontology learning does not capture the interactions\nbetween subtasks. We address this gap by introducing OLLM, a general and\nscalable method for building the taxonomic backbone of an ontology from\nscratch. Rather than focusing on subtasks, like individual relations between\nentities, we model entire subcomponents of the target ontology by finetuning an\nLLM with a custom regulariser that reduces overfitting on high-frequency\nconcepts. We introduce a novel suite of metrics for evaluating the quality of\nthe generated ontology by measuring its semantic and structural similarity to\nthe ground truth. In contrast to standard metrics, our metrics use deep\nlearning techniques to define more robust distance measures between graphs.\nBoth our quantitative and qualitative results on Wikipedia show that OLLM\noutperforms subtask composition methods, producing more semantically accurate\nontologies while maintaining structural integrity. We further demonstrate that\nour model can be effectively adapted to new domains, like arXiv, needing only a\nsmall number of training examples. Our source code and datasets are available\nat https://github.com/andylolu2/ollm.\n","authors":["Andy Lo","Albert Q. Jiang","Wenda Li","Mateja Jamnik"],"pdf_url":"https://arxiv.org/pdf/2410.23584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23583v1","updated":"2024-10-31T02:51:56Z","published":"2024-10-31T02:51:56Z","title":"BioNCERE: Non-Contrastive Enhancement For Relation Extraction In\n  Biomedical Texts","summary":"  State-of-the-art models for relation extraction (RE) in the biomedical domain\nconsider finetuning BioBERT using classification, but they may suffer from the\nanisotropy problem. Contrastive learning methods can reduce this anisotropy\nphenomena, and also help to avoid class collapse in any classification problem.\nIn the present paper, a new training method called biological non-contrastive\nrelation extraction (BioNCERE) is introduced for relation extraction without\nusing any named entity labels for training to reduce annotation costs. BioNCERE\nuses transfer learning and non-contrastive learning to avoid full or\ndimensional collapse as well as bypass overfitting. It resolves RE in three\nstages by leveraging transfer learning two times. By freezing the weights\nlearned in previous stages in the proposed pipeline and by leveraging\nnon-contrastive learning in the second stage, the model predicts relations\nwithout any knowledge of named entities. Experiments have been done on SemMedDB\nthat are almost similar to State-of-the-art performance on RE without using the\ninformation of named entities.\n","authors":["Farshad Noravesh"],"pdf_url":"https://arxiv.org/pdf/2410.23583v1.pdf","comment":"4 figures, 2 tables, 10 pages"},{"id":"http://arxiv.org/abs/2406.13185v3","updated":"2024-10-31T02:35:03Z","published":"2024-06-19T03:33:45Z","title":"LIVE: Learnable In-Context Vector for Visual Question Answering","summary":"  As language models continue to scale, Large Language Models (LLMs) have\nexhibited emerging capabilities in In-Context Learning (ICL), enabling them to\nsolve language tasks by prefixing a few in-context demonstrations (ICDs) as\ncontext. Inspired by these advancements, researchers have extended these\ntechniques to develop Large Multimodal Models (LMMs) with ICL capabilities.\nHowever, applying ICL usually faces two major challenges: 1) using more ICDs\nwill largely increase the inference time and 2) the performance is sensitive to\nthe selection of ICDs. These challenges are further exacerbated in LMMs due to\nthe integration of multiple data types and the combinational complexity of\nmultimodal ICDs. Recently, to address these challenges, some NLP studies\nintroduce non-learnable In-Context Vectors (ICVs) which extract useful task\ninformation from ICDs into a single vector and then insert it into the LLM to\nhelp solve the corresponding task. However, although useful in simple NLP\ntasks, these non-learnable methods fail to handle complex multimodal tasks like\nVisual Question Answering (VQA). In this study, we propose Learnable In-Context\nVEctor (LIVE) to distill essential task information from demonstrations,\nimproving ICL performance in LMMs. Experiments show that LIVE can significantly\nreduce computational costs while enhancing accuracy in VQA tasks compared to\ntraditional ICL and other non-learnable ICV methods. The code is available at\n\\url{https://github.com/ForJadeForest/LIVE-Learnable-In-Context-Vector}.\n","authors":["Yingzhe Peng","Chenduo Hao","Xu Yang","Jiawei Peng","Xinting Hu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2406.13185v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22335v2","updated":"2024-10-31T02:32:24Z","published":"2024-10-29T01:12:50Z","title":"Efficient Machine Translation with a BiLSTM-Attention Approach","summary":"  With the rapid development of Natural Language Processing (NLP) technology,\nthe accuracy and efficiency of machine translation have become hot topics of\nresearch. This paper proposes a novel Seq2Seq model aimed at improving\ntranslation quality while reducing the storage space required by the model. The\nmodel employs a Bidirectional Long Short-Term Memory network (Bi-LSTM) as the\nencoder to capture the context information of the input sequence; the decoder\nincorporates an attention mechanism, enhancing the model's ability to focus on\nkey information during the translation process. Compared to the current\nmainstream Transformer model, our model achieves superior performance on the\nWMT14 machine translation dataset while maintaining a smaller size.\n  The study first introduces the design principles and innovative points of the\nmodel architecture, followed by a series of experiments to verify the\neffectiveness of the model. The experimental includes an assessment of the\nmodel's performance on different language pairs, as well as comparative\nanalysis with traditional Seq2Seq models. The results show that while\nmaintaining translation accuracy, our model significantly reduces the storage\nrequirements, which is of great significance for translation applications in\nresource-constrained scenarios. our code are available at\nhttps://github.com/mindspore-lab/models/tree/master/research/arxiv_papers/miniformer.\nThanks for the support provided by MindSpore Community.\n","authors":["Yuxu Wu","Yiren Xing"],"pdf_url":"https://arxiv.org/pdf/2410.22335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05197v3","updated":"2024-10-31T02:19:12Z","published":"2024-09-08T19:22:58Z","title":"Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?","summary":"  State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.\n","authors":["Neeladri Bhuiya","Viktor Schlegel","Stefan Winkler"],"pdf_url":"https://arxiv.org/pdf/2409.05197v3.pdf","comment":"15 pages, 3 figures, EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2406.01563v2","updated":"2024-10-31T02:04:53Z","published":"2024-06-03T17:45:41Z","title":"LoFiT: Localized Fine-tuning on LLM Representations","summary":"  Recent work in interpretability shows that large language models (LLMs) can\nbe adapted for new tasks in a learning-free way: it is possible to intervene on\nLLM representations to elicit desired behaviors for alignment. For instance,\nadding certain bias vectors to the outputs of certain attention heads is\nreported to boost the truthfulness of models. In this work, we show that\nlocalized fine-tuning serves as an effective alternative to such representation\nintervention methods. We introduce a framework called Localized Fine-Tuning on\nLLM Representations (LoFiT), which identifies a subset of attention heads that\nare most important for learning a specific task, then trains offset vectors to\nadd to the model's hidden representations at those selected heads. LoFiT\nlocalizes to a sparse set of heads (3%-10%) and learns the offset vectors from\nlimited training data, comparable to the settings used for representation\nintervention. For truthfulness and reasoning tasks, we find that LoFiT's\nintervention vectors are more effective for LLM adaptation than vectors from\nrepresentation intervention methods such as Inference-time Intervention. We\nalso find that the localization step is important: selecting a task-specific\nset of attention heads can lead to higher performance than intervening on heads\nselected for a different task. Finally, across 7 tasks we study, LoFiT achieves\ncomparable performance to other parameter-efficient fine-tuning methods such as\nLoRA, despite modifying 20x-200x fewer parameters than these methods.\n","authors":["Fangcong Yin","Xi Ye","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2406.01563v2.pdf","comment":"NeurIPS 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2407.05674v2","updated":"2024-10-31T02:02:41Z","published":"2024-07-08T07:17:40Z","title":"Coding Reliable LLM-based Integrated Task and Knowledge Agents with\n  GenieWorksheets","summary":"  Large Language Models (LLMs) present an opportunity to create automated\nassistants that can help users navigate complex tasks. However, existing\napproaches have limitations in handling conditional logic, integrating\nknowledge sources, and consistently following instructions. Researchers and\nindustry professionals often employ ad hoc pipelines to construct\nconversational agents. These pipelines aim to maintain context, address failure\ncases, and minimize hallucinations, yet frequently fail to achieve these\nobjectives. To this end, we present Genie - a programmable framework for\ncreating task-oriented conversational agents that are designed to handle\ncomplex user interactions and knowledge queries. Unlike LLMs, Genie provides\nreliable grounded responses, with controllable agent policies through its\nexpressive specification, Genie Worksheet. In contrast to dialog trees, it is\nresilient to diverse user queries, helpful with knowledge sources, and offers\nease of programming policies through its declarative paradigm. The agents built\nusing Genie outperforms the state-of-the-art method on complex logic domains in\nSTARV2 dataset by up to 20.5%. Additionally, through a real-user study\ninvolving 62 participants, we show that Genie beats the GPT-4 with function\ncalling baseline by 21.1%, 20.1%, and 61% on execution accuracy, dialogue act\naccuracy, and goal completion rate, respectively, on three diverse real-world\ndomains\n","authors":["Harshit Joshi","Shicheng Liu","James Chen","Robert Weigle","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2407.05674v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.23555v1","updated":"2024-10-31T01:51:41Z","published":"2024-10-31T01:51:41Z","title":"From Context to Action: Analysis of the Impact of State Representation\n  and Context on the Generalization of Multi-Turn Web Navigation Agents","summary":"  Recent advancements in Large Language Model (LLM)-based frameworks have\nextended their capabilities to complex real-world applications, such as\ninteractive web navigation. These systems, driven by user commands, navigate\nweb browsers to complete tasks through multi-turn dialogues, offering both\ninnovative opportunities and significant challenges. Despite the introduction\nof benchmarks for conversational web navigation, a detailed understanding of\nthe key contextual components that influence the performance of these agents\nremains elusive. This study aims to fill this gap by analyzing the various\ncontextual elements crucial to the functioning of web navigation agents. We\ninvestigate the optimization of context management, focusing on the influence\nof interaction history and web page representation. Our work highlights\nimproved agent performance across out-of-distribution scenarios, including\nunseen websites, categories, and geographic locations through effective context\nmanagement. These findings provide insights into the design and optimization of\nLLM-based agents, enabling more accurate and effective web navigation in\nreal-world applications.\n","authors":["Nalin Tiwary","Vardhan Dongre","Sanil Arun Chawla","Ashwin Lamani","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2410.23555v1.pdf","comment":"10 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2404.04251v3","updated":"2024-10-31T01:39:48Z","published":"2024-04-05T17:57:16Z","title":"Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt\n  Coherence Metrics with T2IScoreScore (TS2)","summary":"  With advances in the quality of text-to-image (T2I) models has come interest\nin benchmarking their prompt faithfulness -- the semantic coherence of\ngenerated images to the prompts they were conditioned on. A variety of T2I\nfaithfulness metrics have been proposed, leveraging advances in cross-modal\nembeddings and vision-language models (VLMs). However, these metrics are not\nrigorously compared and benchmarked, instead presented with correlation to\nhuman Likert scores over a set of easy-to-discriminate images against seemingly\nweak baselines.\n  We introduce T2IScoreScore, a curated set of semantic error graphs containing\na prompt and a set of increasingly erroneous images. These allow us to\nrigorously judge whether a given prompt faithfulness metric can correctly order\nimages with respect to their objective error count and significantly\ndiscriminate between different error nodes, using meta-metric scores derived\nfrom established statistical tests. Surprisingly, we find that the\nstate-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we\ntested fail to significantly outperform simple (and supposedly worse)\nfeature-based metrics like CLIPScore, particularly on a hard subset of\nnaturally-occurring T2I model errors. TS2 will enable the development of better\nT2I prompt faithfulness metrics through more rigorous comparison of their\nconformity to expected orderings and separations under objective criteria.\n","authors":["Michael Saxon","Fatima Jahara","Mahsa Khoshnoodi","Yujie Lu","Aditya Sharma","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2404.04251v3.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.23535v1","updated":"2024-10-31T00:56:08Z","published":"2024-10-31T00:56:08Z","title":"Simulating User Agents for Embodied Conversational-AI","summary":"  Embodied agents designed to assist users with tasks must engage in natural\nlanguage interactions, interpret instructions, execute actions, and communicate\neffectively to resolve issues. However, collecting large-scale, diverse\ndatasets of situated human-robot dialogues to train and evaluate such agents is\nexpensive, labor-intensive, and time-consuming. To address this challenge, we\npropose building a large language model (LLM)-based user agent that can\nsimulate user behavior during interactions with an embodied agent in a virtual\nenvironment. Given a user goal (e.g., make breakfast), at each time step, the\nuser agent may observe\" the robot actions or speak\" to either intervene with\nthe robot or answer questions. Such a user agent assists in improving the\nscalability and efficiency of embodied dialogues dataset generation and is\ncritical for enhancing and evaluating the robot's interaction and task\ncompletion ability, as well as for research in reinforcement learning using AI\nfeedback. We evaluate our user agent's ability to generate human-like behaviors\nby comparing its simulated dialogues with the TEACh dataset. We perform three\nexperiments: zero-shot prompting to predict dialogue acts, few-shot prompting,\nand fine-tuning on the TEACh training subset. Results show the LLM-based user\nagent achieves an F-measure of 42% with zero-shot prompting and 43.4% with\nfew-shot prompting in mimicking human speaking behavior. Through fine-tuning,\nperformance in deciding when to speak remained stable, while deciding what to\nsay improved from 51.1% to 62.5%. These findings showcase the feasibility of\nthe proposed approach for assessing and enhancing the effectiveness of robot\ntask completion through natural language communication.\n","authors":["Daniel Philipov","Vardhan Dongre","Gokhan Tur","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2410.23535v1.pdf","comment":"8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.23528v1","updated":"2024-10-31T00:29:52Z","published":"2024-10-31T00:29:52Z","title":"Large Language Models for Patient Comments Multi-Label Classification","summary":"  Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4o-Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4o-Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses.\n","authors":["Hajar Sakai","Sarah S. Lam","Mohammadsadegh Mikaeili","Joshua Bosire","Franziska Jovin"],"pdf_url":"https://arxiv.org/pdf/2410.23528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23526v1","updated":"2024-10-31T00:18:05Z","published":"2024-10-31T00:18:05Z","title":"LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve\n  Factualness in Large Language Models","summary":"  Large language models (LLMs) have shown remarkable capabilities in various\nnatural language processing tasks, yet they often struggle with maintaining\nfactual accuracy, particularly in knowledge-intensive domains like healthcare.\nThis study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,\na novel approach designed to enhance the factual reliability of LLMs, with a\nfocus on medical question answering (QA). LEAF utilizes a dual strategy to\nenhance the factual accuracy of responses from models such as Llama 3 70B\nInstruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,\nimproves Retrieval-Augmented Generation (RAG) by incorporating fact-checking\nresults to guide the retrieval process without updating model parameters. The\nsecond strategy, Learning from Fact-Checks via Self-Training, involves\nsupervised fine-tuning (SFT) on fact-checked responses or applying Simple\nPreference Optimization (SimPO) with fact-checking as a ranking mechanism, both\nupdating LLM parameters from supervision. These findings suggest that\nintegrating fact-checked responses whether through RAG enhancement or\nself-training enhances the reliability and factual correctness of LLM outputs,\noffering a promising solution for applications where information accuracy is\ncrucial.\n","authors":["Hieu Tran","Junda Wang","Yujan Ting","Weijing Huang","Terrence Chen"],"pdf_url":"https://arxiv.org/pdf/2410.23526v1.pdf","comment":"22 pages, 9 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.24223v1","updated":"2024-10-31T17:59:56Z","published":"2024-10-31T17:59:56Z","title":"URAvatar: Universal Relightable Gaussian Codec Avatars","summary":"  We present a new approach to creating photorealistic and relightable head\navatars from a phone scan with unknown illumination. The reconstructed avatars\ncan be animated and relit in real time with the global illumination of diverse\nenvironments. Unlike existing approaches that estimate parametric reflectance\nparameters via inverse rendering, our approach directly models learnable\nradiance transfer that incorporates global light transport in an efficient\nmanner for real-time rendering. However, learning such a complex light\ntransport that can generalize across identities is non-trivial. A phone scan in\na single environment lacks sufficient information to infer how the head would\nappear in general environments. To address this, we build a universal\nrelightable avatar model represented by 3D Gaussians. We train on hundreds of\nhigh-quality multi-view human scans with controllable point lights.\nHigh-resolution geometric guidance further enhances the reconstruction accuracy\nand generalization. Once trained, we finetune the pretrained model on a phone\nscan using inverse rendering to obtain a personalized relightable avatar. Our\nexperiments establish the efficacy of our design, outperforming existing\napproaches while retaining real-time rendering capability.\n","authors":["Junxuan Li","Chen Cao","Gabriel Schwartz","Rawal Khirodkar","Christian Richardt","Tomas Simon","Yaser Sheikh","Shunsuke Saito"],"pdf_url":"https://arxiv.org/pdf/2410.24223v1.pdf","comment":"SIGGRAPH Asia 2024. Website:\n  https://junxuan-li.github.io/urgca-website/"},{"id":"http://arxiv.org/abs/2410.24221v1","updated":"2024-10-31T17:59:55Z","published":"2024-10-31T17:59:55Z","title":"EgoMimic: Scaling Imitation Learning via Egocentric Video","summary":"  The scale and diversity of demonstration data required for imitation learning\nis a significant challenge. We present EgoMimic, a full-stack framework which\nscales manipulation via human embodiment data, specifically egocentric human\nvideos paired with 3D hand tracking. EgoMimic achieves this through: (1) a\nsystem to capture human embodiment data using the ergonomic Project Aria\nglasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap\nto human data, (3) cross-domain data alignment techniques, and (4) an imitation\nlearning architecture that co-trains on human and robot data. Compared to prior\nworks that only extract high-level intent from human videos, our approach\ntreats human and robot data equally as embodied demonstration data and learns a\nunified policy from both data sources. EgoMimic achieves significant\nimprovement on a diverse set of long-horizon, single-arm and bimanual\nmanipulation tasks over state-of-the-art imitation learning methods and enables\ngeneralization to entirely new scenes. Finally, we show a favorable scaling\ntrend for EgoMimic, where adding 1 hour of additional hand data is\nsignificantly more valuable than 1 hour of additional robot data. Videos and\nadditional information can be found at https://egomimic.github.io/\n","authors":["Simar Kareer","Dhruv Patel","Ryan Punamiya","Pranay Mathur","Shuo Cheng","Chen Wang","Judy Hoffman","Danfei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.24221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24219v1","updated":"2024-10-31T17:59:53Z","published":"2024-10-31T17:59:53Z","title":"Enhancing Motion in Text-to-Video Generation with Decomposed Encoding\n  and Conditioning","summary":"  Despite advancements in Text-to-Video (T2V) generation, producing videos with\nrealistic motion remains challenging. Current models often yield static or\nminimally dynamic outputs, failing to capture complex motions described by\ntext. This issue stems from the internal biases in text encoding, which\noverlooks motions, and inadequate conditioning mechanisms in T2V generation\nmodels. To address this, we propose a novel framework called DEcomposed MOtion\n(DEMO), which enhances motion synthesis in T2V generation by decomposing both\ntext encoding and conditioning into content and motion components. Our method\nincludes a content encoder for static elements and a motion encoder for\ntemporal dynamics, alongside separate content and motion conditioning\nmechanisms. Crucially, we introduce text-motion and video-motion supervision to\nimprove the model's understanding and generation of motion. Evaluations on\nbenchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench\ndemonstrate DEMO's superior ability to produce videos with enhanced motion\ndynamics while maintaining high visual quality. Our approach significantly\nadvances T2V generation by integrating comprehensive motion understanding\ndirectly from textual descriptions. Project page:\nhttps://PR-Ryan.github.io/DEMO-project/\n","authors":["Penghui Ruan","Pichao Wang","Divya Saxena","Jiannong Cao","Yuhui Shi"],"pdf_url":"https://arxiv.org/pdf/2410.24219v1.pdf","comment":"Accepted at NeurIPS 2024, code available at\n  https://github.com/PR-Ryan/DEMO"},{"id":"http://arxiv.org/abs/2410.24218v1","updated":"2024-10-31T17:59:52Z","published":"2024-10-31T17:59:52Z","title":"Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use","summary":"  In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL\n","authors":["Jiajun Xi","Yinong He","Jianing Yang","Yinpei Dai","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2410.24218v1.pdf","comment":"EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL"},{"id":"http://arxiv.org/abs/2410.24214v1","updated":"2024-10-31T17:59:37Z","published":"2024-10-31T17:59:37Z","title":"ARQ: A Mixed-Precision Quantization Framework for Accurate and\n  Certifiably Robust DNNs","summary":"  Mixed precision quantization has become an important technique for enabling\nthe execution of deep neural networks (DNNs) on limited resource computing\nplatforms. Traditional quantization methods have primarily concentrated on\nmaintaining neural network accuracy, either ignoring the impact of quantization\non the robustness of the network, or using only empirical techniques for\nimproving robustness. In contrast, techniques for robustness certification,\nwhich can provide strong guarantees about the robustness of DNNs have not been\nused during quantization due to their high computation cost.\n  This paper introduces ARQ, an innovative mixed-precision quantization method\nthat not only preserves the clean accuracy of the smoothed classifiers but also\nmaintains their certified robustness. ARQ uses reinforcement learning to find\naccurate and robust DNN quantization, while efficiently leveraging randomized\nsmoothing, a popular class of statistical DNN verification algorithms, to guide\nthe search process.\n  We compare ARQ with multiple state-of-the-art quantization techniques on\nseveral DNN architectures commonly used in quantization studies: ResNet-20 on\nCIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. We demonstrate\nthat ARQ consistently performs better than these baselines across all the\nbenchmarks and the input perturbation levels. In many cases, the performance of\nARQ quantized networks can reach that of the original DNN with floating-point\nweights, but with only 1.5% instructions.\n","authors":["Yuchen Yang","Shubham Ugare","Yifan Zhao","Gagandeep Singh","Sasa Misailovic"],"pdf_url":"https://arxiv.org/pdf/2410.24214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24213v1","updated":"2024-10-31T17:59:30Z","published":"2024-10-31T17:59:30Z","title":"Learning Video Representations without Natural Videos","summary":"  In this paper, we show that useful video representations can be learned from\nsynthetic videos and natural images, without incorporating natural videos in\nthe training. We propose a progression of video datasets synthesized by simple\ngenerative processes, that model a growing set of natural video properties\n(e.g. motion, acceleration, and shape transformations). The downstream\nperformance of video models pre-trained on these generated datasets gradually\nincreases with the dataset progression. A VideoMAE model pre-trained on our\nsynthetic videos closes 97.2% of the performance gap on UCF101 action\nclassification between training from scratch and self-supervised pre-training\nfrom natural videos, and outperforms the pre-trained model on HMDB51.\nIntroducing crops of static images to the pre-training stage results in similar\nperformance to UCF101 pre-training and outperforms the UCF101 pre-trained model\non 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the\nlow-level properties of the datasets, we identify correlations between frame\ndiversity, frame similarity to natural data, and downstream performance. Our\napproach provides a more controllable and transparent alternative to video data\ncuration processes for pre-training.\n","authors":["Xueyang Yu","Xinlei Chen","Yossi Gandelsman"],"pdf_url":"https://arxiv.org/pdf/2410.24213v1.pdf","comment":"Project page: https://unicorn53547.github.io/video_syn_rep/"},{"id":"http://arxiv.org/abs/2410.24211v1","updated":"2024-10-31T17:59:01Z","published":"2024-10-31T17:59:01Z","title":"DELTA: Dense Efficient Long-range 3D Tracking for any video","summary":"  Tracking dense 3D motion from monocular videos remains challenging,\nparticularly when aiming for pixel-level precision over long sequences. We\nintroduce \\Approach, a novel method that efficiently tracks every pixel in 3D\nspace, enabling accurate motion estimation across entire videos. Our approach\nleverages a joint global-local attention mechanism for reduced-resolution\ntracking, followed by a transformer-based upsampler to achieve high-resolution\npredictions. Unlike existing methods, which are limited by computational\ninefficiency or sparse tracking, \\Approach delivers dense 3D tracking at scale,\nrunning over 8x faster than previous methods while achieving state-of-the-art\naccuracy. Furthermore, we explore the impact of depth representation on\ntracking performance and identify log-depth as the optimal choice. Extensive\nexperiments demonstrate the superiority of \\Approach on multiple benchmarks,\nachieving new state-of-the-art results in both 2D and 3D dense tracking tasks.\nOur method provides a robust solution for applications requiring fine-grained,\nlong-term motion tracking in 3D space.\n","authors":["Tuan Duc Ngo","Peiye Zhuang","Chuang Gan","Evangelos Kalogerakis","Sergey Tulyakov","Hsin-Ying Lee","Chaoyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24211v1.pdf","comment":"Project Page: https://snap-research.github.io/DELTA/"},{"id":"http://arxiv.org/abs/2406.15349v2","updated":"2024-10-31T17:58:34Z","published":"2024-06-21T17:59:02Z","title":"NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and\n  Benchmarking","summary":"  Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n","authors":["Daniel Dauner","Marcel Hallgarten","Tianyu Li","Xinshuo Weng","Zhiyu Huang","Zetong Yang","Hongyang Li","Igor Gilitschenski","Boris Ivanovic","Marco Pavone","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2406.15349v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.24207v1","updated":"2024-10-31T17:58:22Z","published":"2024-10-31T17:58:22Z","title":"No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse\n  Unposed Images","summary":"  We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D\nscenes parameterized by 3D Gaussians from \\textit{unposed} sparse multi-view\nimages. Our model, trained exclusively with photometric loss, achieves\nreal-time 3D Gaussian reconstruction during inference. To eliminate the need\nfor accurate pose input during reconstruction, we anchor one input view's local\ncamera coordinates as the canonical space and train the network to predict\nGaussian primitives for all views within this space. This approach obviates the\nneed to transform Gaussian primitives from local coordinates into a global\ncoordinate system, thus avoiding errors associated with per-frame Gaussians and\npose estimation. To resolve scale ambiguity, we design and compare various\nintrinsic embedding methods, ultimately opting to convert camera intrinsics\ninto a token embedding and concatenate it with image tokens as input to the\nmodel, enabling accurate scene scale prediction. We utilize the reconstructed\n3D Gaussians for novel view synthesis and pose estimation tasks and propose a\ntwo-stage coarse-to-fine pipeline for accurate pose estimation. Experimental\nresults demonstrate that our pose-free approach can achieve superior novel view\nsynthesis quality compared to pose-required methods, particularly in scenarios\nwith limited input image overlap. For pose estimation, our method, trained\nwithout ground truth depth or explicit matching loss, significantly outperforms\nthe state-of-the-art methods with substantial improvements. This work makes\nsignificant advances in pose-free generalizable 3D reconstruction and\ndemonstrates its applicability to real-world scenarios. Code and trained models\nare available at https://noposplat.github.io/.\n","authors":["Botao Ye","Sifei Liu","Haofei Xu","Xueting Li","Marc Pollefeys","Ming-Hsuan Yang","Songyou Peng"],"pdf_url":"https://arxiv.org/pdf/2410.24207v1.pdf","comment":"Project page: https://noposplat.github.io/"},{"id":"http://arxiv.org/abs/2410.24204v1","updated":"2024-10-31T17:57:07Z","published":"2024-10-31T17:57:07Z","title":"GeoSplatting: Towards Geometry Guided Gaussian Splatting for\n  Physically-based Inverse Rendering","summary":"  We consider the problem of physically-based inverse rendering using 3D\nGaussian Splatting (3DGS) representations. While recent 3DGS methods have\nachieved remarkable results in novel view synthesis (NVS), accurately capturing\nhigh-fidelity geometry, physically interpretable materials and lighting remains\nchallenging, as it requires precise geometry modeling to provide accurate\nsurface normals, along with physically-based rendering (PBR) techniques to\nensure correct material and lighting disentanglement. Previous 3DGS methods\nresort to approximating surface normals, but often struggle with noisy local\ngeometry, leading to inaccurate normal estimation and suboptimal\nmaterial-lighting decomposition. In this paper, we introduce GeoSplatting, a\nnovel hybrid representation that augments 3DGS with explicit geometric guidance\nand differentiable PBR equations. Specifically, we bridge isosurface and 3DGS\ntogether, where we first extract isosurface mesh from a scalar field, then\nconvert it into 3DGS points and formulate PBR equations for them in a fully\ndifferentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,\nenabling precise surface normal modeling, which facilitates the use of PBR\nframeworks for material decomposition. This approach further maintains the\nefficiency and quality of NVS from 3DGS while ensuring accurate geometry from\nthe isosurface. Comprehensive evaluations across diverse datasets demonstrate\nthe superiority of GeoSplatting, consistently outperforming existing methods\nboth quantitatively and qualitatively.\n","authors":["Kai Ye","Chong Gao","Guanbin Li","Wenzheng Chen","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.24204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24203v1","updated":"2024-10-31T17:57:02Z","published":"2024-10-31T17:57:02Z","title":"DiffPano: Scalable and Consistent Text to Panorama Generation with\n  Spherical Epipolar-Aware Diffusion","summary":"  Diffusion-based methods have achieved remarkable achievements in 2D image or\n3D object generation, however, the generation of 3D scenes and even\n$360^{\\circ}$ images remains constrained, due to the limited number of scene\ndatasets, the complexity of 3D scenes themselves, and the difficulty of\ngenerating consistent multi-view images. To address these issues, we first\nestablish a large-scale panoramic video-text dataset containing millions of\nconsecutive panoramic keyframes with corresponding panoramic depths, camera\nposes, and text descriptions. Then, we propose a novel text-driven panoramic\ngeneration framework, termed DiffPano, to achieve scalable, consistent, and\ndiverse panoramic scene generation. Specifically, benefiting from the powerful\ngenerative capabilities of stable diffusion, we fine-tune a single-view\ntext-to-panorama diffusion model with LoRA on the established panoramic\nvideo-text dataset. We further design a spherical epipolar-aware multi-view\ndiffusion model to ensure the multi-view consistency of the generated panoramic\nimages. Extensive experiments demonstrate that DiffPano can generate scalable,\nconsistent, and diverse panoramic images with given unseen text descriptions\nand camera poses.\n","authors":["Weicai Ye","Chenhao Ji","Zheng Chen","Junyao Gao","Xiaoshui Huang","Song-Hai Zhang","Wanli Ouyang","Tong He","Cairong Zhao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24203v1.pdf","comment":"NeurIPS2024, Project: https://github.com/zju3dv/DiffPano; Code:\n  https://github.com/zju3dv/DiffPano"},{"id":"http://arxiv.org/abs/2410.24187v1","updated":"2024-10-31T17:49:44Z","published":"2024-10-31T17:49:44Z","title":"Chasing Better Deep Image Priors between Over- and\n  Under-parameterization","summary":"  Deep Neural Networks (DNNs) are well-known to act as over-parameterized deep\nimage priors (DIP) that regularize various image inverse problems. Meanwhile,\nresearchers also proposed extremely compact, under-parameterized image priors\n(e.g., deep decoder) that are strikingly competent for image restoration too,\ndespite a loss of accuracy. These two extremes push us to think whether there\nexists a better solution in the middle: between over- and under-parameterized\nimage priors, can one identify \"intermediate\" parameterized image priors that\nachieve better trade-offs between performance, efficiency, and even preserving\nstrong transferability? Drawing inspirations from the lottery ticket hypothesis\n(LTH), we conjecture and study a novel \"lottery image prior\" (LIP) by\nexploiting DNN inherent sparsity, stated as: given an over-parameterized\nDNN-based image prior, it will contain a sparse subnetwork that can be trained\nin isolation, to match the original DNN's performance when being applied as a\nprior to various image inverse problems. Our results validate the superiority\nof LIPs: we can successfully locate the LIP subnetworks from over-parameterized\nDIPs at substantial sparsity ranges. Those LIP subnetworks significantly\noutperform deep decoders under comparably compact model sizes (by often fully\npreserving the effectiveness of their over-parameterized counterparts), and\nthey also possess high transferability across different images as well as\nrestoration task types. Besides, we also extend LIP to compressive sensing\nimage reconstruction, where a pre-trained GAN generator is used as the prior\n(in contrast to untrained DIP or deep decoder), and confirm its validity in\nthis setting too. To our best knowledge, this is the first time that LTH is\ndemonstrated to be relevant in the context of inverse problems or image priors.\n","authors":["Qiming Wu","Xiaohan Chen","Yifan Jiang","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24187v1.pdf","comment":"Codes are available at\n  https://github.com/VITA-Group/Chasing-Better-DIPs"},{"id":"http://arxiv.org/abs/2410.24185v1","updated":"2024-10-31T17:48:45Z","published":"2024-10-31T17:48:45Z","title":"DexMimicGen: Automated Data Generation for Bimanual Dexterous\n  Manipulation via Imitation Learning","summary":"  Imitation learning from human demonstrations is an effective means to teach\nrobots manipulation skills. But data acquisition is a major bottleneck in\napplying this paradigm more broadly, due to the amount of cost and human effort\ninvolved. There has been significant interest in imitation learning for\nbimanual dexterous robots, like humanoids. Unfortunately, data collection is\neven more challenging here due to the challenges of simultaneously controlling\nmultiple arms and multi-fingered hands. Automated data generation in simulation\nis a compelling, scalable alternative to fuel this need for data. To this end,\nwe introduce DexMimicGen, a large-scale automated data generation system that\nsynthesizes trajectories from a handful of human demonstrations for humanoid\nrobots with dexterous hands. We present a collection of simulation environments\nin the setting of bimanual dexterous manipulation, spanning a range of\nmanipulation behaviors and different requirements for coordination among the\ntwo arms. We generate 21K demos across these tasks from just 60 source human\ndemos and study the effect of several data generation and policy learning\ndecisions on agent performance. Finally, we present a real-to-sim-to-real\npipeline and deploy it on a real-world humanoid can sorting task. Videos and\nmore are at https://dexmimicgen.github.io/\n","authors":["Zhenyu Jiang","Yuqi Xie","Kevin Lin","Zhenjia Xu","Weikang Wan","Ajay Mandlekar","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.24185v1.pdf","comment":"Project website: https://dexmimicgen.github.io/"},{"id":"http://arxiv.org/abs/2409.06711v2","updated":"2024-10-31T17:48:06Z","published":"2024-08-25T13:14:59Z","title":"Quantized neural network for complex hologram generation","summary":"  Computer-generated holography (CGH) is a promising technology for augmented\nreality displays, such as head-mounted or head-up displays. However, its high\ncomputational demand makes it impractical for implementation. Recent efforts to\nintegrate neural networks into CGH have successfully accelerated computing\nspeed, demonstrating the potential to overcome the trade-off between\ncomputational cost and image quality. Nevertheless, deploying neural\nnetwork-based CGH algorithms on computationally limited embedded systems\nrequires more efficient models with lower computational cost, memory footprint,\nand power consumption. In this study, we developed a lightweight model for\ncomplex hologram generation by introducing neural network quantization.\nSpecifically, we built a model based on tensor holography and quantized it from\n32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Our\nperformance evaluation shows that the proposed INT8 model achieves hologram\nquality comparable to that of the FP32 model while reducing the model size by\napproximately 70% and increasing the speed fourfold. Additionally, we\nimplemented the INT8 model on a system-on-module to demonstrate its\ndeployability on embedded platforms and high power efficiency.\n","authors":["Yutaka Endo","Minoru Oikawa","Timothy D. Wilkinson","Tomoyoshi Shimobaba","Tomoyoshi Ito"],"pdf_url":"https://arxiv.org/pdf/2409.06711v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.04312v2","updated":"2024-10-31T17:47:54Z","published":"2024-06-06T17:56:40Z","title":"ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise\n  Optimization","summary":"  Text-to-Image (T2I) models have made significant advancements in recent\nyears, but they still struggle to accurately capture intricate details\nspecified in complex compositional prompts. While fine-tuning T2I models with\nreward objectives has shown promise, it suffers from \"reward hacking\" and may\nnot generalize well to unseen prompt distributions. In this work, we propose\nReward-based Noise Optimization (ReNO), a novel approach that enhances T2I\nmodels at inference by optimizing the initial noise based on the signal from\none or multiple human preference reward models. Remarkably, solving this\noptimization problem with gradient ascent for 50 iterations yields impressive\nresults on four different one-step models across two competitive benchmarks,\nT2I-CompBench and GenEval. Within a computational budget of 20-50 seconds,\nReNO-enhanced one-step models consistently surpass the performance of all\ncurrent open-source Text-to-Image models. Extensive user studies demonstrate\nthat our model is preferred nearly twice as often compared to the popular SDXL\nmodel and is on par with the proprietary Stable Diffusion 3 with 8B parameters.\nMoreover, given the same computational resources, a ReNO-optimized one-step\nmodel outperforms widely-used open-source models such as SDXL and\nPixArt-$\\alpha$, highlighting the efficiency and effectiveness of ReNO in\nenhancing T2I model performance at inference time. Code is available at\nhttps://github.com/ExplainableML/ReNO.\n","authors":["Luca Eyring","Shyamgopal Karthik","Karsten Roth","Alexey Dosovitskiy","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2406.04312v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2306.01953v3","updated":"2024-10-31T17:47:21Z","published":"2023-06-02T23:29:28Z","title":"Invisible Image Watermarks Are Provably Removable Using Generative AI","summary":"  Invisible watermarks safeguard images' copyrights by embedding hidden\nmessages only detectable by owners. They also prevent people from misusing\nimages, especially those generated by AI models. We propose a family of\nregeneration attacks to remove these invisible watermarks. The proposed attack\nmethod first adds random noise to an image to destroy the watermark and then\nreconstructs the image. This approach is flexible and can be instantiated with\nmany existing image-denoising algorithms and pre-trained generative models such\nas diffusion models. Through formal proofs and extensive empirical evaluations,\nwe demonstrate that pixel-level invisible watermarks are vulnerable to this\nregeneration attack. Our results reveal that, across four different pixel-level\nwatermarking schemes, the proposed method consistently achieves superior\nperformance compared to existing attack techniques, with lower detection rates\nand higher image quality. However, watermarks that keep the image semantically\nsimilar can be an alternative defense against our attacks. Our finding\nunderscores the need for a shift in research/industry emphasis from invisible\nwatermarks to semantic-preserving watermarks. Code is available at\nhttps://github.com/XuandongZhao/WatermarkAttacker\n","authors":["Xuandong Zhao","Kexun Zhang","Zihao Su","Saastha Vasan","Ilya Grishchenko","Christopher Kruegel","Giovanni Vigna","Yu-Xiang Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2306.01953v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24183v1","updated":"2024-10-31T17:46:54Z","published":"2024-10-31T17:46:54Z","title":"Extended Object Tracking and Classification based on Linear Splines","summary":"  This paper introduces a framework based on linear splines for 2-dimensional\nextended object tracking and classification. Unlike state of the art models,\nlinear splines allow to represent extended objects whose contour is an\narbitrarily complex curve. An exact likelihood is derived for the case in which\nnoisy measurements can be scattered from any point on the contour of the\nextended object, while an approximate Monte Carlo likelihood is provided for\nthe case wherein scattering points can be anywhere, i.e. inside or on the\ncontour, on the object surface. Exploiting such likelihood to measure how well\nthe observed data fit a given shape, a suitable estimator is developed. The\nproposed estimator models the extended object in terms of a kinematic state,\nproviding object position and orientation, along with a shape vector,\ncharacterizing object contour and surface. The kinematic state is estimated via\na nonlinear Kalman filter, while the shape vector is estimated via a Bayesian\nclassifier so that classification is implicitly solved during shape estimation.\nNumerical experiments are provided to assess, compared to state of the art\nextended object estimators, the effectiveness of the proposed one.\n","authors":["Matteo Tesori","Giorgio Battistelli","Luigi Chisci"],"pdf_url":"https://arxiv.org/pdf/2410.24183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24181v1","updated":"2024-10-31T17:45:09Z","published":"2024-10-31T17:45:09Z","title":"Federated Black-Box Adaptation for Semantic Segmentation","summary":"  Federated Learning (FL) is a form of distributed learning that allows\nmultiple institutions or clients to collaboratively learn a global model to\nsolve a task. This allows the model to utilize the information from every\ninstitute while preserving data privacy. However, recent studies show that the\npromise of protecting the privacy of data is not upheld by existing methods and\nthat it is possible to recreate the training data from the different\ninstitutions. This is done by utilizing gradients transferred between the\nclients and the global server during training or by knowing the model\narchitecture at the client end. In this paper, we propose a federated learning\nframework for semantic segmentation without knowing the model architecture nor\ntransferring gradients between the client and the server, thus enabling better\nprivacy preservation. We propose BlackFed - a black-box adaptation of neural\nnetworks that utilizes zero order optimization (ZOO) to update the client model\nweights and first order optimization (FOO) to update the server weights. We\nevaluate our approach on several computer vision and medical imaging datasets\nto demonstrate its effectiveness. To the best of our knowledge, this work is\none of the first works in employing federated learning for segmentation, devoid\nof gradients or model information exchange. Code:\nhttps://github.com/JayParanjape/blackfed/tree/master\n","authors":["Jay N. Paranjape","Shameema Sikder","S. Swaroop Vedula","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2410.24181v1.pdf","comment":"Accepted at NEURIPS 2024"},{"id":"http://arxiv.org/abs/2404.13046v2","updated":"2024-10-31T17:39:34Z","published":"2024-04-19T17:59:48Z","title":"MoVA: Adapting Mixture of Vision Experts to Multimodal Context","summary":"  As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM). In the fine-grained stage, we\nelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to\nextract and fuse task-specific knowledge from various experts. This\ncoarse-to-fine paradigm effectively leverages representations from experts\nbased on multimodal context and model expertise, further enhancing the\ngeneralization ability. We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach. Without any bells and whistles, MoVA\ncan achieve significant performance gains over current state-of-the-art methods\nin a wide range of challenging multimodal benchmarks.\n","authors":["Zhuofan Zong","Bingqi Ma","Dazhong Shen","Guanglu Song","Hao Shao","Dongzhi Jiang","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.13046v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.04690v3","updated":"2024-10-31T17:32:26Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\naim to massively improve upon existing infrastructure by providing two new\nmethods for implementing neighborhood attention. We first show that\nneighborhood attention can be represented as a batched GEMM problem, similar to\nstandard attention, and implement it for 1-D and 2-D neighborhood attention.\nThese kernels on average provide 895% and 272% improvement in full precision\nruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood\nattention respectively. We find that aside from being heavily bound by memory\nbandwidth, certain inherent inefficiencies exist in all unfused implementations\nof neighborhood attention, which in most cases undo their theoretical\nefficiency gain. Motivated by the progress made into fused dot-product\nattention kernels, we developed fused neighborhood attention; an adaptation of\nfused dot-product attention kernels that allow fine-grained control over\nattention across different spatial axes. Known for reducing the quadratic time\ncomplexity of self attention to a linear complexity, neighborhood attention can\nnow enjoy a reduced and constant memory footprint, and record-breaking half\nprecision runtime. We observe that our fused implementation successfully\ncircumvents some of the unavoidable inefficiencies in unfused\nimplementations...\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v3.pdf","comment":"To appear in 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.24160v1","updated":"2024-10-31T17:19:03Z","published":"2024-10-31T17:19:03Z","title":"Redefining <Creative> in Dictionary: Towards a Enhanced Semantic\n  Understanding of Creative Generation","summary":"  Creativity, both in human and diffusion models, remains an inherently\nabstract concept; thus, simply adding \"creative\" to a prompt does not yield\nreliable semantic recognition by the model. In this work, we concretize the\nabstract notion of \"creative\" through the TP2O task, which aims to merge two\nunrelated concepts, and introduce CreTok, redefining \"creative\" as the token\n$\\texttt{<CreTok>}$. This redefinition offers a more concrete and universally\nadaptable representation for concept blending. This redefinition occurs\ncontinuously, involving the repeated random sampling of text pairs with\ndifferent concepts and optimizing cosine similarity between target and constant\nprompts. This approach enables $\\texttt{<CreTok>}$ to learn a method for\ncreative concept fusion. Extensive experiments demonstrate that the creative\ncapability enabled by $\\texttt{<CreTok>}$ substantially surpasses recent SOTA\ndiffusion models and achieves superior creative generation. CreTok exhibits\ngreater flexibility and reduced time overhead, as $\\texttt{<CreTok>}$ can\nfunction as a universal token for any concept, facilitating creative generation\nwithout retraining.\n","authors":["Fu Feng","Yucheng Xie","Jing Wang","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2410.24160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00885v2","updated":"2024-10-31T17:12:57Z","published":"2024-06-02T22:40:05Z","title":"Visual place recognition for aerial imagery: A survey","summary":"  Aerial imagery and its direct application to visual localization is an\nessential problem for many Robotics and Computer Vision tasks. While Global\nNavigation Satellite Systems (GNSS) are the standard default solution for\nsolving the aerial localization problem, it is subject to a number of\nlimitations, such as, signal instability or solution unreliability that make\nthis option not so desirable. Consequently, visual geolocalization is emerging\nas a viable alternative. However, adapting Visual Place Recognition (VPR) task\nto aerial imagery presents significant challenges, including weather variations\nand repetitive patterns. Current VPR reviews largely neglect the specific\ncontext of aerial data. This paper introduces a methodology tailored for\nevaluating VPR techniques specifically in the domain of aerial imagery,\nproviding a comprehensive assessment of various methods and their performance.\nHowever, we not only compare various VPR methods, but also demonstrate the\nimportance of selecting appropriate zoom and overlap levels when constructing\nmap tiles to achieve maximum efficiency of VPR algorithms in the case of aerial\nimagery. The code is available on our GitHub repository --\nhttps://github.com/prime-slam/aero-vloc.\n","authors":["Ivan Moskalenko","Anastasiia Kornilova","Gonzalo Ferrer"],"pdf_url":"https://arxiv.org/pdf/2406.00885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24151v1","updated":"2024-10-31T17:09:55Z","published":"2024-10-31T17:09:55Z","title":"Scaling Concept With Text-Guided Diffusion Models","summary":"  Text-guided diffusion models have revolutionized generative tasks by\nproducing high-fidelity content from text descriptions. They have also enabled\nan editing paradigm where concepts can be replaced through text conditioning\n(e.g., a dog to a tiger). In this work, we explore a novel approach: instead of\nreplacing a concept, can we enhance or suppress the concept itself? Through an\nempirical study, we identify a trend where concepts can be decomposed in\ntext-guided diffusion models. Leveraging this insight, we introduce\nScalingConcept, a simple yet effective method to scale decomposed concepts up\nor down in real input without introducing new elements. To systematically\nevaluate our approach, we present the WeakConcept-10 dataset, where concepts\nare imperfect and need to be enhanced. More importantly, ScalingConcept enables\na variety of novel zero-shot applications across image and audio domains,\nincluding tasks such as canonical pose generation and generative sound\nhighlighting or removal.\n","authors":["Chao Huang","Susan Liang","Yunlong Tang","Yapeng Tian","Anurag Kumar","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.24151v1.pdf","comment":"Project page: https://wikichao.github.io/ScalingConcept/"},{"id":"http://arxiv.org/abs/2410.24148v1","updated":"2024-10-31T17:09:19Z","published":"2024-10-31T17:09:19Z","title":"Exploring Vision Language Models for Facial Attribute Recognition:\n  Emotion, Race, Gender, and Age","summary":"  Technologies for recognizing facial attributes like race, gender, age, and\nemotion have several applications, such as surveillance, advertising content,\nsentiment analysis, and the study of demographic trends and social behaviors.\nAnalyzing demographic characteristics based on images and analyzing facial\nexpressions have several challenges due to the complexity of humans' facial\nattributes. Traditional approaches have employed CNNs and various other deep\nlearning techniques, trained on extensive collections of labeled images. While\nthese methods demonstrated effective performance, there remains potential for\nfurther enhancements. In this paper, we propose to utilize vision language\nmodels (VLMs) such as generative pre-trained transformer (GPT), GEMINI, large\nlanguage and vision assistant (LLAVA), PaliGemma, and Microsoft Florence2 to\nrecognize facial attributes such as race, gender, age, and emotion from images\nwith human faces. Various datasets like FairFace, AffectNet, and UTKFace have\nbeen utilized to evaluate the solutions. The results show that VLMs are\ncompetitive if not superior to traditional techniques. Additionally, we propose\n\"FaceScanPaliGemma\"--a fine-tuned PaliGemma model--for race, gender, age, and\nemotion recognition. The results show an accuracy of 81.1%, 95.8%, 80%, and\n59.4% for race, gender, age group, and emotion classification, respectively,\noutperforming pre-trained version of PaliGemma, other VLMs, and SotA methods.\nFinally, we propose \"FaceScanGPT\", which is a GPT-4o model to recognize the\nabove attributes when several individuals are present in the image using a\nprompt engineered for a person with specific facial and/or physical attributes.\nThe results underscore the superior multitasking capability of FaceScanGPT to\ndetect the individual's attributes like hair cut, clothing color, postures,\netc., using only a prompt to drive the detection and recognition tasks.\n","authors":["Nouar AlDahoul","Myles Joshua Toledo Tan","Harishwar Reddy Kasireddy","Yasir Zaki"],"pdf_url":"https://arxiv.org/pdf/2410.24148v1.pdf","comment":"52 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.24144v1","updated":"2024-10-31T17:05:44Z","published":"2024-10-31T17:05:44Z","title":"HoloChrome: Polychromatic Illumination for Speckle Reduction in\n  Holographic Near-Eye Displays","summary":"  Holographic displays hold the promise of providing authentic depth cues,\nresulting in enhanced immersive visual experiences for near-eye applications.\nHowever, current holographic displays are hindered by speckle noise, which\nlimits accurate reproduction of color and texture in displayed images. We\npresent HoloChrome, a polychromatic holographic display framework designed to\nmitigate these limitations. HoloChrome utilizes an ultrafast,\nwavelength-adjustable laser and a dual-Spatial Light Modulator (SLM)\narchitecture, enabling the multiplexing of a large set of discrete wavelengths\nacross the visible spectrum. By leveraging spatial separation in our dual-SLM\nsetup, we independently manipulate speckle patterns across multiple\nwavelengths. This novel approach effectively reduces speckle noise through\nincoherent averaging achieved by wavelength multiplexing. Our method is\ncomplementary to existing speckle reduction techniques, offering a new pathway\nto address this challenge. Furthermore, the use of polychromatic illumination\nbroadens the achievable color gamut compared to traditional three-color primary\nholographic displays.\n  Our simulations and tabletop experiments validate that HoloChrome\nsignificantly reduces speckle noise and expands the color gamut. These\nadvancements enhance the performance of holographic near-eye displays, moving\nus closer to practical, immersive next-generation visual experiences.\n","authors":["Florian Schiffers","Grace Kuo","Nathan Matsuda","Douglas Lanman","Oliver Cossairt"],"pdf_url":"https://arxiv.org/pdf/2410.24144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24139v1","updated":"2024-10-31T17:03:38Z","published":"2024-10-31T17:03:38Z","title":"COSNet: A Novel Semantic Segmentation Network using Enhanced Boundaries\n  in Cluttered Scenes","summary":"  Automated waste recycling aims to efficiently separate the recyclable objects\nfrom the waste by employing vision-based systems. However, the presence of\nvarying shaped objects having different material types makes it a challenging\nproblem, especially in cluttered environments. Existing segmentation methods\nperform reasonably on many semantic segmentation datasets by employing\nmulti-contextual representations, however, their performance is degraded when\nutilized for waste object segmentation in cluttered scenarios. In addition,\nplastic objects further increase the complexity of the problem due to their\ntranslucent nature. To address these limitations, we introduce an efficacious\nsegmentation network, named COSNet, that uses boundary cues along with\nmulti-contextual information to accurately segment the objects in cluttered\nscenes. COSNet introduces novel components including feature sharpening block\n(FSB) and boundary enhancement module (BEM) for enhancing the features and\nhighlighting the boundary information of irregular waste objects in cluttered\nenvironment. Extensive experiments on three challenging datasets including\nZeroWaste-f, SpectralWaste, and ADE20K demonstrate the effectiveness of the\nproposed method. Our COSNet achieves a significant gain of 1.8% on ZeroWaste-f\nand 2.1% on SpectralWaste datasets respectively in terms of mIoU metric.\n","authors":["Muhammad Ali","Mamoona Javaid","Mubashir Noman","Mustansar Fiaz","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2410.24139v1.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2311.00371v2","updated":"2024-10-31T17:01:50Z","published":"2023-11-01T08:53:05Z","title":"Learning Cooperative Trajectory Representations for Motion Forecasting","summary":"  Motion forecasting is an essential task for autonomous driving, and utilizing\ninformation from infrastructure and other vehicles can enhance forecasting\ncapabilities. Existing research mainly focuses on leveraging single-frame\ncooperative information to enhance the limited perception capability of the ego\nvehicle, while underutilizing the motion and interaction context of traffic\nparticipants observed from cooperative devices. In this paper, we propose a\nforecasting-oriented representation paradigm to utilize motion and interaction\nfeatures from cooperative information. Specifically, we present V2X-Graph, a\nrepresentative framework to achieve interpretable and end-to-end trajectory\nfeature fusion for cooperative motion forecasting. V2X-Graph is evaluated on\nV2X-Seq in vehicle-to-infrastructure (V2I) scenarios. To further evaluate on\nvehicle-to-everything (V2X) scenario, we construct the first real-world V2X\nmotion forecasting dataset V2X-Traj, which contains multiple autonomous\nvehicles and infrastructure in every scenario. Experimental results on both\nV2X-Seq and V2X-Traj show the advantage of our method. We hope both V2X-Graph\nand V2X-Traj will benefit the further development of cooperative motion\nforecasting. Find the project at https://github.com/AIR-THU/V2X-Graph.\n","authors":["Hongzhi Ruan","Haibao Yu","Wenxian Yang","Siqi Fan","Zaiqing Nie"],"pdf_url":"https://arxiv.org/pdf/2311.00371v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.01650v3","updated":"2024-10-31T16:53:49Z","published":"2024-01-03T10:07:11Z","title":"De-Confusing Pseudo-Labels in Source-Free Domain Adaptation","summary":"  Source-free domain adaptation aims to adapt a source-trained model to an\nunlabeled target domain without access to the source data. It has attracted\ngrowing attention in recent years, where existing approaches focus on\nself-training that usually includes pseudo-labeling techniques. In this paper,\nwe introduce a novel noise-learning approach tailored to address noise\ndistribution in domain adaptation settings and learn to de-confuse the\npseudo-labels. More specifically, we learn a noise transition matrix of the\npseudo-labels to capture the label corruption of each class and learn the\nunderlying true label distribution. Estimating the noise transition matrix\nenables a better true class-posterior estimation, resulting in better\nprediction accuracy. We demonstrate the effectiveness of our approach when\ncombined with several source-free domain adaptation methods: SHOT, SHOT++, and\nAaD. We obtain state-of-the-art results on three domain adaptation datasets:\nVisDA, DomainNet, and OfficeHome.\n","authors":["Idit Diamant","Amir Rosenfeld","Idan Achituve","Jacob Goldberger","Arnon Netzer"],"pdf_url":"https://arxiv.org/pdf/2401.01650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01903v2","updated":"2024-10-31T16:49:26Z","published":"2024-07-02T03:08:20Z","title":"Text-Aware Diffusion for Policy Learning","summary":"  Training an agent to achieve particular goals or perform desired behaviors is\noften accomplished through reinforcement learning, especially in the absence of\nexpert demonstrations. However, supporting novel goals or behaviors through\nreinforcement learning requires the ad-hoc design of appropriate reward\nfunctions, which quickly becomes intractable. To address this challenge, we\npropose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a\npretrained, frozen text-conditioned diffusion model to compute dense zero-shot\nreward signals for text-aligned policy learning. We hypothesize that\nlarge-scale pretrained generative models encode rich priors that can supervise\na policy to behave not only in a text-aligned manner, but also in alignment\nwith a notion of naturalness summarized from internet-scale training data. In\nour experiments, we demonstrate that TADPoLe is able to learn policies for\nnovel goal-achievement and continuous locomotion behaviors specified by natural\nlanguage, in both Humanoid and Dog environments. The behaviors are learned\nzero-shot without ground-truth rewards or expert demonstrations, and are\nqualitatively more natural according to human evaluation. We further show that\nTADPoLe performs competitively when applied to robotic manipulation tasks in\nthe Meta-World environment, without having access to any in-domain\ndemonstrations.\n","authors":["Calvin Luo","Mandy He","Zilai Zeng","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2407.01903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24116v1","updated":"2024-10-31T16:46:23Z","published":"2024-10-31T16:46:23Z","title":"AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level\n  Classification and Localization","summary":"  Image labeling is a critical bottleneck in the development of computer vision\ntechnologies, often constraining the potential of machine learning models due\nto the time-intensive nature of manual annotations. This work introduces a\nnovel approach that leverages outpainting to address the problem of annotated\ndata scarcity by generating artificial contexts and annotations, significantly\nreducing manual labeling efforts. We apply this technique to a particularly\nacute challenge in autonomous driving, urban planning, and environmental\nmonitoring: the lack of diverse, eye-level vehicle images in desired classes.\nOur dataset comprises AI-generated vehicle images obtained by detecting and\ncropping vehicles from manually selected seed images, which are then outpainted\nonto larger canvases to simulate varied real-world conditions. The outpainted\nimages include detailed annotations, providing high-quality ground truth data.\nAdvanced outpainting techniques and image quality assessments ensure visual\nfidelity and contextual relevance. Augmentation with outpainted vehicles\nimproves overall performance metrics by up to 8\\% and enhances prediction of\nunderrepresented classes by up to 20\\%. This approach, exemplifying outpainting\nas a self-annotating paradigm, presents a solution that enhances dataset\nversatility across multiple domains of machine learning. The code and links to\ndatasets used in this study are available for further research and replication\nat https://github.com/amir-kazemi/aidovecl.\n","authors":["Amir Kazemi","Qurat ul ain Fatima","Volodymyr Kindratenko","Christopher Tessum"],"pdf_url":"https://arxiv.org/pdf/2410.24116v1.pdf","comment":"19 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.24114v1","updated":"2024-10-31T16:44:10Z","published":"2024-10-31T16:44:10Z","title":"Nearest Neighbor Normalization Improves Multimodal Retrieval","summary":"  Multimodal models leverage large-scale pre-training to achieve strong but\nstill imperfect performance on tasks such as image captioning, visual question\nanswering, and cross-modal retrieval. In this paper, we present a simple and\nefficient method for correcting errors in trained contrastive image-text\nretrieval models with no additional training, called Nearest Neighbor\nNormalization (NNN). We show an improvement on retrieval metrics in both text\nretrieval and image retrieval for all of the contrastive models that we tested\n(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used\n(MS-COCO and Flickr30k). NNN requires a reference database, but does not\nrequire any training on this database, and can even increase the retrieval\naccuracy of a model after finetuning.\n","authors":["Neil Chowdhury","Franklin Wang","Sumedh Shenoy","Douwe Kiela","Sarah Schwettmann","Tristan Thrush"],"pdf_url":"https://arxiv.org/pdf/2410.24114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14919v2","updated":"2024-10-31T16:36:14Z","published":"2024-10-19T00:33:51Z","title":"Adversarial Score identity Distillation: Rapidly Surpassing the Teacher\n  in One Step","summary":"  Score identity Distillation (SiD) is a data-free method that has achieved\nstate-of-the-art performance in image generation by leveraging only a\npretrained diffusion model, without requiring any training data. However, the\nultimate performance of SiD is constrained by the accuracy with which the\npretrained model captures the true data scores at different stages of the\ndiffusion process. In this paper, we introduce SiDA (SiD with Adversarial\nLoss), which not only enhances generation quality but also improves\ndistillation efficiency by incorporating real images and adversarial loss. SiDA\nutilizes the encoder from the generator's score network as a discriminator,\nboosting its ability to distinguish between real images and those generated by\nSiD. The adversarial loss is batch-normalized within each GPU and then combined\nwith the original SiD loss. This integration effectively incorporates the\naverage \"fakeness\" per GPU batch into the pixel-based SiD loss, enabling SiDA\nto distill a single-step generator either from scratch or by fine-tuning an\nexisting one. SiDA converges significantly faster than its predecessor when\ntrained from scratch, and swiftly improves upon the original model's\nperformance after an initial warmup period during fine-tuning from a\npre-distilled SiD generator. This one-step adversarial distillation method\nestablishes new benchmarks in generation performance when distilling EDM\ndiffusion models pretrained on CIFAR-10 (32x32) and ImageNet (64x64), achieving\nFID score of 1.110 on ImageNet 64x64. It sets record-low FID scores when\ndistilling EDM2 models trained on ImageNet (512x512), surpassing even the\nlargest teacher model, EDM2-XXL. Our SiDA's results record FID scores of 2.156\nfor EDM2-XS, 1.669 for EDM2-S, 1.488 for EDM2-M, and 1.465 for EDM2-L,\ndemonstrating significant improvements across all model sizes. Our open-source\ncode will be integrated into the SiD codebase.\n","authors":["Mingyuan Zhou","Huangjie Zheng","Yi Gu","Zhendong Wang","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24098v1","updated":"2024-10-31T16:28:49Z","published":"2024-10-31T16:28:49Z","title":"Parameter choices in HaarPSI for IQA with medical images","summary":"  When developing machine learning models, image quality assessment (IQA)\nmeasures are a crucial component for evaluation. However, commonly used IQA\nmeasures have been primarily developed and optimized for natural images. In\nmany specialized settings, such as medical images, this poses an\noften-overlooked problem regarding suitability. In previous studies, the IQA\nmeasure HaarPSI showed promising behavior for natural and medical images.\nHaarPSI is based on Haar wavelet representations and the framework allows\noptimization of two parameters. So far, these parameters have been aligned for\nnatural images. Here, we optimize these parameters for two annotated medical\ndata sets, a photoacoustic and a chest X-Ray data set. We observe that they are\nmore sensitive to the parameter choices than the employed natural images, and\non the other hand both medical data sets lead to similar parameter values when\noptimized. We denote the optimized setting, which improves the performance for\nthe medical images notably, by HaarPSI$_{MED}$. The results suggest that\nadapting common IQA measures within their frameworks for medical images can\nprovide a valuable, generalizable addition to the employment of more specific\ntask-based measures.\n","authors":["Clemens Karner","Janek Gröhl","Ian Selby","Judith Babar","Jake Beckford","Thomas R Else","Timothy J Sadler","Shahab Shahipasand","Arthikkaa Thavakumar","Michael Roberts","James H. F. Rudd","Carola-Bibiane Schönlieb","Jonathan R Weir-McCall","Anna Breger"],"pdf_url":"https://arxiv.org/pdf/2410.24098v1.pdf","comment":"5 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.08570v2","updated":"2024-10-31T16:20:26Z","published":"2024-08-16T07:12:47Z","title":"EraW-Net: Enhance-Refine-Align W-Net for Scene-Associated Driver\n  Attention Estimation","summary":"  Associating driver attention with driving scene across two fields of views\n(FOVs) is a hard cross-domain perception problem, which requires comprehensive\nconsideration of cross-view mapping, dynamic driving scene analysis, and driver\nstatus tracking. Previous methods typically focus on a single view or map\nattention to the scene via estimated gaze, failing to exploit the implicit\nconnection between them. Moreover, simple fusion modules are insufficient for\nmodeling the complex relationships between the two views, making information\nintegration challenging. To address these issues, we propose a novel method for\nend-to-end scene-associated driver attention estimation, called EraW-Net. This\nmethod enhances the most discriminative dynamic cues, refines feature\nrepresentations, and facilitates semantically aligned cross-domain integration\nthrough a W-shaped architecture, termed W-Net. Specifically, a Dynamic Adaptive\nFilter Module (DAF-Module) is proposed to address the challenges of frequently\nchanging driving environments by extracting vital regions. It suppresses the\nindiscriminately recorded dynamics and highlights crucial ones by innovative\njoint frequency-spatial analysis, enhancing the model's ability to parse\ncomplex dynamics. Additionally, to track driver states during non-fixed facial\nposes, we propose a Global Context Sharing Module (GCS-Module) to construct\nrefined feature representations by capturing hierarchical features that adapt\nto various scales of head and eye movements. Finally, W-Net achieves systematic\ncross-view information integration through its \"Encoding-Independent Partial\nDecoding-Fusion Decoding\" structure, addressing semantic misalignment in\nheterogeneous data integration. Experiments demonstrate that the proposed\nmethod robustly and accurately estimates the mapping of driver attention in\nscene on large public datasets.\n","authors":["Jun Zhou","Chunsheng Liu","Faliang Chang","Wenqian Wang","Penghui Hao","Yiming Huang","Zhiqiang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.08570v2.pdf","comment":"13pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.24075v1","updated":"2024-10-31T16:13:55Z","published":"2024-10-31T16:13:55Z","title":"Identifying Spatio-Temporal Drivers of Extreme Events","summary":"  The spatio-temporal relations of impacts of extreme events and their drivers\nin climate data are not fully understood and there is a need of machine\nlearning approaches to identify such spatio-temporal relations from data. The\ntask, however, is very challenging since there are time delays between extremes\nand their drivers, and the spatial response of such drivers is inhomogeneous.\nIn this work, we propose a first approach and benchmarks to tackle this\nchallenge. Our approach is trained end-to-end to predict spatio-temporally\nextremes and spatio-temporally drivers in the physical input variables jointly.\nBy enforcing the network to predict extremes from spatio-temporal binary masks\nof identified drivers, the network successfully identifies drivers that are\ncorrelated with extremes. We evaluate our approach on three newly created\nsynthetic benchmarks, where two of them are based on remote sensing or\nreanalysis climate data, and on two real-world reanalysis datasets. The source\ncode and datasets are publicly available at the project page\nhttps://hakamshams.github.io/IDE.\n","authors":["Mohamad Hakam Shams Eddin","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2410.24075v1.pdf","comment":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.22551v2","updated":"2024-10-31T16:04:48Z","published":"2024-10-29T21:37:03Z","title":"FairSkin: Fair Diffusion for Skin Disease Image Generation","summary":"  Image generation is a prevailing technique for clinical data augmentation for\nadvancing diagnostic accuracy and reducing healthcare disparities. Diffusion\nModel (DM) has become a leading method in generating synthetic medical images,\nbut it suffers from a critical twofold bias: (1) The quality of images\ngenerated for Caucasian individuals is significantly higher, as measured by the\nFrechet Inception Distance (FID). (2) The ability of the downstream-task\nlearner to learn critical features from disease images varies across different\nskin tones. These biases pose significant risks, particularly in skin disease\ndetection, where underrepresentation of certain skin tones can lead to\nmisdiagnosis or neglect of specific conditions. To address these challenges, we\npropose FairSkin, a novel DM framework that mitigates these biases through a\nthree-level resampling mechanism, ensuring fairer representation across racial\nand disease categories. Our approach significantly improves the diversity and\nquality of generated images, contributing to more equitable skin disease\ndetection in clinical settings.\n","authors":["Ruichen Zhang","Yuguang Yao","Zhen Tan","Zhiming Li","Pan Wang","Huan Liu","Jingtong Hu","Sijia Liu","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.22551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24060v1","updated":"2024-10-31T15:57:04Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08557v2","updated":"2024-10-31T15:52:52Z","published":"2023-11-14T21:39:15Z","title":"Low-light Pedestrian Detection in Visible and Infrared Image Feeds:\n  Issues and Challenges","summary":"  Pedestrian detection has become a cornerstone for several high-level tasks,\nincluding autonomous driving, intelligent transportation, and traffic\nsurveillance. There are several works focussed on pedestrian detection using\nvisible images, mainly in the daytime. However, this task is very intriguing\nwhen the environmental conditions change to poor lighting or nighttime.\nRecently, new ideas have been spurred to use alternative sources, such as Far\nInfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light\nconditions. This study reviews recent developments in low-light pedestrian\ndetection approaches. It systematically categorizes and analyses various\nalgorithms from region-based to non-region-based and graph-based learning\nmethodologies by highlighting their methodologies, implementation issues, and\nchallenges. It also outlines the key benchmark datasets that can be used for\nresearch and development of advanced pedestrian detection algorithms,\nparticularly in low-light situations.\n","authors":["Thangarajah Akilan","Hrishikesh Vachhani"],"pdf_url":"https://arxiv.org/pdf/2311.08557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24055v1","updated":"2024-10-31T15:48:36Z","published":"2024-10-31T15:48:36Z","title":"Advanced Predictive Quality Assessment for Ultrasonic Additive\n  Manufacturing with Deep Learning Model","summary":"  Ultrasonic Additive Manufacturing (UAM) employs ultrasonic welding to bond\nsimilar or dissimilar metal foils to a substrate, resulting in solid,\nconsolidated metal components. However, certain processing conditions can lead\nto inter-layer defects, affecting the final product's quality. This study\ndevelops a method to monitor in-process quality using deep learning-based\nconvolutional neural networks (CNNs). The CNN models were evaluated on their\nability to classify samples with and without embedded thermocouples across five\npower levels (300W, 600W, 900W, 1200W, 1500W) using thermal images with\nsupervised labeling. Four distinct CNN classification models were created for\ndifferent scenarios including without (baseline) and with thermocouples, only\nwithout thermocouples across power levels, only with thermocouples across power\nlevels, and combined without and with thermocouples across power levels. The\nmodels achieved 98.29% accuracy on combined baseline and thermocouple images,\n97.10% for baseline images across power levels, 97.43% for thermocouple images,\nand 97.27% for both types across power levels. The high accuracy, above 97%,\ndemonstrates the system's effectiveness in identifying and classifying\nconditions within the UAM process, providing a reliable tool for quality\nassurance and process control in manufacturing environments.\n","authors":["Lokendra Poudel","Sushant Jha","Ryan Meeker","Duy-Nhat Phan","Rahul Bhowmik"],"pdf_url":"https://arxiv.org/pdf/2410.24055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16129v2","updated":"2024-10-31T15:46:45Z","published":"2024-06-23T15:03:35Z","title":"UDHF2-Net: Uncertainty-diffusion-model-based High-Frequency TransFormer\n  Network for Remotely Sensed Imagery Interpretation","summary":"  Remotely sensed imagery interpretation (RSII) faces the three major problems:\n(1) objective representation of spatial distribution patterns; (2) edge\nuncertainty problem caused by downsampling encoder and intrinsic edge noises\n(e.g., mixed pixel and edge occlusion etc.); and (3) false detection problem\ncaused by geometric registration error in change detection. To solve the\naforementioned problems, uncertainty-diffusion-model-based high-Frequency\nTransFormer network (UDHF2-Net) is the first to be proposed, whose\nsuperiorities are as follows: (1) a spatially-stationary-and-non-stationary\nhigh-frequency connection paradigm (SHCP) is proposed to enhance the\ninteraction of spatially frequency-wise stationary and non-stationary features\nto yield high-fidelity edge extraction result. Inspired by HRFormer, SHCP\nproposes high-frequency-wise stream to replace high-resolution-wise stream in\nHRFormer through the whole encoder-decoder process with parallel frequency-wise\nhigh-to-low streams, so it improves the edge extraction accuracy by\ncontinuously remaining high-frequency information; (2) a\nmask-and-geo-knowledge-based uncertainty diffusion module (MUDM), which is a\nself-supervised learning strategy, is proposed to improve the edge accuracy of\nextraction and change detection by gradually removing the simulated spectrum\nnoises based on geo-knowledge and the generated diffused spectrum noises; (3) a\nfrequency-wise semi-pseudo-Siamese UDHF2-Net is the first to be proposed to\nbalance accuracy and complexity for change detection. Besides the\naforementioned spectrum noises in semantic segmentation, MUDM is also a\nself-supervised learning strategy to effectively reduce the edge false change\ndetection from the generated imagery with geometric registration error.\n","authors":["Pengfei Zhang","Chang Li","Yongjun Zhang","Rongjun Qin"],"pdf_url":"https://arxiv.org/pdf/2406.16129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09168v2","updated":"2024-10-31T15:44:36Z","published":"2024-06-13T14:30:35Z","title":"SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image\n  Super-Resolution","summary":"  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes at the cellular\nand subcellular levels. Scanning confocal microscopy allows the capture of\nhigh-quality images from thick three-dimensional (3D) samples, yet suffers from\nwell-known limitations such as photobleaching and phototoxicity of specimens\ncaused by intense light exposure, limiting its applications. Cellular damage\ncan be alleviated by changing imaging parameters to reduce light exposure,\noften at the expense of image quality. Machine/deep learning methods for\nsingle-image super-resolution (SISR) can be applied to restore image quality by\nupscaling lower-resolution (LR) images to yield high-resolution images (HR).\nThese SISR methods have been successfully applied to photo-realistic images due\npartly to the abundance of publicly available data. In contrast, the lack of\npublicly available data partly limits their application and success in scanning\nconfocal microscopy. In this paper, we introduce a large scanning confocal\nmicroscopy dataset named SR-CACO-2 that is comprised of low- and\nhigh-resolution image pairs marked for three different fluorescent markers. It\nallows the evaluation of performance of SISR methods on three different\nupscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell\nline Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured\nwith four resolutions and three markers, forming 9,937 image patches for SISR\nmethods. We provide benchmarking results for 16 state-of-the-art methods of the\nmain SISR families. Results show that these methods have limited success in\nproducing high-resolution textures. The dataset is freely accessible under a\nCreative Commons license (CC BY-NC-SA 4.0). Our dataset, code and pretrained\nweights for SISR methods are available: https://github.com/sbelharbi/sr-caco-2.\n","authors":["Soufiane Belharbi","Mara KM Whitford","Phuong Hoang","Shakeeb Murtaza","Luke McCaffrey","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2406.09168v2.pdf","comment":"27 pages, 15 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24046v1","updated":"2024-10-31T15:42:24Z","published":"2024-10-31T15:42:24Z","title":"Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis","summary":"  This study introduces the Hybrid Multi-modal VGG (HM-VGG) model, a\ncutting-edge deep learning approach for the early diagnosis of glaucoma. The\nHM-VGG model utilizes an attention mechanism to process Visual Field (VF) data,\nenabling the extraction of key features that are vital for identifying early\nsigns of glaucoma. Despite the common reliance on large annotated datasets, the\nHM-VGG model excels in scenarios with limited data, achieving remarkable\nresults with small sample sizes. The model's performance is underscored by its\nhigh metrics in Precision, Accuracy, and F1-Score, indicating its potential for\nreal-world application in glaucoma detection. The paper also discusses the\nchallenges associated with ophthalmic image analysis, particularly the\ndifficulty of obtaining large volumes of annotated data. It highlights the\nimportance of moving beyond single-modality data, such as VF or Optical\nCoherence Tomography (OCT) images alone, to a multimodal approach that can\nprovide a richer, more comprehensive dataset. This integration of different\ndata types is shown to significantly enhance diagnostic accuracy. The HM- VGG\nmodel offers a promising tool for doctors, streamlining the diagnostic process\nand improving patient outcomes. Furthermore, its applicability extends to\ntelemedicine and mobile healthcare, making diagnostic services more accessible.\nThe research presented in this paper is a significant step forward in the field\nof medical image processing and has profound implications for clinical\nophthalmology.\n","authors":["Junliang Du","Yiru Cang","Tong Zhou","Jiacheng Hu","Weijie He"],"pdf_url":"https://arxiv.org/pdf/2410.24046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24037v1","updated":"2024-10-31T15:34:49Z","published":"2024-10-31T15:34:49Z","title":"TPC: Test-time Procrustes Calibration for Diffusion-based Human Image\n  Animation","summary":"  Human image animation aims to generate a human motion video from the inputs\nof a reference human image and a target motion video. Current diffusion-based\nimage animation systems exhibit high precision in transferring human identity\ninto targeted motion, yet they still exhibit irregular quality in their\noutputs. Their optimal precision is achieved only when the physical\ncompositions (i.e., scale and rotation) of the human shapes in the reference\nimage and target pose frame are aligned. In the absence of such alignment,\nthere is a noticeable decline in fidelity and consistency. Especially, in\nreal-world environments, this compositional misalignment commonly occurs,\nposing significant challenges to the practical usage of current systems. To\nthis end, we propose Test-time Procrustes Calibration (TPC), which enhances the\nrobustness of diffusion-based image animation systems by maintaining optimal\nperformance even when faced with compositional misalignment, effectively\naddressing real-world scenarios. The TPC provides a calibrated reference image\nfor the diffusion model, enhancing its capability to understand the\ncorrespondence between human shapes in the reference and target images. Our\nmethod is simple and can be applied to any diffusion-based image animation\nsystem in a model-agnostic manner, improving the effectiveness at test time\nwithout additional training.\n","authors":["Sunjae Yoon","Gwanhyeong Koo","Younghwan Lee","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.24037v1.pdf","comment":"24 pages, 16 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24034v1","updated":"2024-10-31T15:32:14Z","published":"2024-10-31T15:32:14Z","title":"Handwriting Recognition in Historical Documents with Multimodal LLM","summary":"  There is an immense quantity of historical and cultural documentation that\nexists only as handwritten manuscripts. At the same time, performing OCR across\nscripts and different handwriting styles has proven to be an enormously\ndifficult problem relative to the process of digitizing print. While recent\nTransformer based models have achieved relatively strong performance, they rely\nheavily on manually transcribed training data and have difficulty generalizing\nacross writers. Multimodal LLM, such as GPT-4v and Gemini, have demonstrated\neffectiveness in performing OCR and computer vision tasks with few shot\nprompting. In this paper, I evaluate the accuracy of handwritten document\ntranscriptions generated by Gemini against the current state of the art\nTransformer based methods.\n  Keywords: Optical Character Recognition, Multimodal Language Models, Cultural\nPreservation, Mass digitization, Handwriting Recognitio\n","authors":["Lucian Li"],"pdf_url":"https://arxiv.org/pdf/2410.24034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24031v1","updated":"2024-10-31T15:29:51Z","published":"2024-10-31T15:29:51Z","title":"A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems\n  using Disparity Maps","summary":"  Face recognition technologies are increasingly used in various applications,\nyet they are vulnerable to face spoofing attacks. These spoofing attacks often\ninvolve unique 3D structures, such as printed papers or mobile device screens.\nAlthough stereo-depth cameras can detect such attacks effectively, their\nhigh-cost limits their widespread adoption. Conversely, two-sensor systems\nwithout extrinsic calibration offer a cost-effective alternative but are unable\nto calculate depth using stereo techniques. In this work, we propose a method\nto overcome this challenge by leveraging facial attributes to derive disparity\ninformation and estimate relative depth for anti-spoofing purposes, using\nnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coined\nDisparity Model, that incorporates created disparity maps as a third modality\nalongside the two original sensor modalities. We demonstrate the effectiveness\nof the Disparity Model in countering various spoof attacks using a\ncomprehensive dataset collected from the Intel RealSense ID Solution F455. Our\nmethod outperformed existing methods in the literature, achieving an Equal\nError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False\nPositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the\nerrors of the best comparison method, respectively. Additionally, we introduce\na model ensemble that addresses 3D spoof attacks as well, achieving an EER of\n2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a\nstate-of-the-art solution for the challenging task of anti-spoofing in\nnon-calibrated systems that lack depth information.\n","authors":["Ariel Larey","Eyal Rond","Omer Achrack"],"pdf_url":"https://arxiv.org/pdf/2410.24031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24018v1","updated":"2024-10-31T15:20:43Z","published":"2024-10-31T15:20:43Z","title":"Bayesian-guided Label Mapping for Visual Reprogramming","summary":"  Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained\nvision models by adapting their input or output interfaces to solve downstream\ntasks whose labels (i.e., downstream labels) might be totally different from\nthe labels associated with the pretrained models (i.e., pretrained labels).\nWhen adapting the output interface, label mapping methods transform the\npretrained labels to downstream labels by establishing a gradient-free\none-to-one correspondence between the two sets of labels. However, in this\npaper, we reveal that one-to-one mappings may overlook the complex relationship\nbetween pretrained and downstream labels. Motivated by this observation, we\npropose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an\niteratively-updated probabilistic label mapping matrix, with each element\nquantifying a pairwise relationship between pretrained and downstream labels.\nThe assignment of values to the constructed matrix is guided by Bayesian\nconditional probability, considering the joint distribution of the downstream\nlabels and the labels predicted by the pretrained model on downstream samples.\nExperiments conducted on both pretrained vision models (e.g., ResNeXt) and\nvision-language models (e.g., CLIP) demonstrate the superior performance of BLM\nover existing label mapping methods. The success of BLM also offers a\nprobabilistic lens through which to understand and analyze the effectiveness of\nVR. Our code is available at https://github.com/tmlr-group/BayesianLM.\n","authors":["Chengyi Cai","Zesheng Ye","Lei Feng","Jianzhong Qi","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24015v1","updated":"2024-10-31T15:17:14Z","published":"2024-10-31T15:17:14Z","title":"Unveiling Synthetic Faces: How Synthetic Datasets Can Expose Real\n  Identities","summary":"  Synthetic data generation is gaining increasing popularity in different\ncomputer vision applications. Existing state-of-the-art face recognition models\nare trained using large-scale face datasets, which are crawled from the\nInternet and raise privacy and ethical concerns. To address such concerns,\nseveral works have proposed generating synthetic face datasets to train face\nrecognition models. However, these methods depend on generative models, which\nare trained on real face images. In this work, we design a simple yet effective\nmembership inference attack to systematically study if any of the existing\nsynthetic face recognition datasets leak any information from the real data\nused to train the generator model. We provide an extensive study on 6\nstate-of-the-art synthetic face recognition datasets, and show that in all\nthese synthetic datasets, several samples from the original real dataset are\nleaked. To our knowledge, this paper is the first work which shows the leakage\nfrom training data of generator models into the generated synthetic face\nrecognition datasets. Our study demonstrates privacy pitfalls in synthetic face\nrecognition datasets and paves the way for future studies on generating\nresponsible synthetic face datasets.\n","authors":["Hatef Otroshi Shahreza","Sébastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2410.24015v1.pdf","comment":"Accepted in NeurIPS 2024 Workshop on New Frontiers in Adversarial\n  Machine Learning"},{"id":"http://arxiv.org/abs/2410.24010v1","updated":"2024-10-31T15:10:38Z","published":"2024-10-31T15:10:38Z","title":"Re-assembling the past: The RePAIR dataset and benchmark for real world\n  2D and 3D puzzle solving","summary":"  This paper proposes the RePAIR dataset that represents a challenging\nbenchmark to test modern computational and data driven methods for\npuzzle-solving and reassembly tasks. Our dataset has unique properties that are\nuncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and\nfractures are realistic, caused by a collapse of a fresco during a World War II\nbombing at the Pompeii archaeological park. The fragments are also eroded and\nhave missing pieces with irregular shapes and different dimensions, challenging\nfurther the reassembly algorithms. The dataset is multi-modal providing high\nresolution images with characteristic pictorial elements, detailed 3D scans of\nthe fragments and meta-data annotated by the archaeologists. Ground truth has\nbeen generated through several years of unceasing fieldwork, including the\nexcavation and cleaning of each fragment, followed by manual puzzle solving by\narchaeologists of a subset of approx. 1000 pieces among the 16000 available.\nAfter digitizing all the fragments in 3D, a benchmark was prepared to challenge\ncurrent reassembly and puzzle-solving methods that often solve more simplistic\nsynthetic scenarios. The tested baselines show that there clearly exists a gap\nto fill in solving this computationally complex problem.\n","authors":["Theodore Tsesmelis","Luca Palmieri","Marina Khoroshiltseva","Adeela Islam","Gur Elkin","Ofir Itzhak Shahar","Gianluca Scarpellini","Stefano Fiorini","Yaniv Ohayon","Nadav Alali","Sinem Aslan","Pietro Morerio","Sebastiano Vascon","Elena Gravina","Maria Cristina Napolitano","Giuseppe Scarpati","Gabriel Zuchtriegel","Alexandra Spühler","Michel E. Fuchs","Stuart James","Ohad Ben-Shahar","Marcello Pelillo","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2410.24010v1.pdf","comment":"NeurIPS 2024, Track Datasets and Benchmarks, 10 pages"},{"id":"http://arxiv.org/abs/2410.24006v1","updated":"2024-10-31T15:09:36Z","published":"2024-10-31T15:09:36Z","title":"DiffPAD: Denoising Diffusion-based Adversarial Patch Decontamination","summary":"  In the ever-evolving adversarial machine learning landscape, developing\neffective defenses against patch attacks has become a critical challenge,\nnecessitating reliable solutions to safeguard real-world AI systems. Although\ndiffusion models have shown remarkable capacity in image synthesis and have\nbeen recently utilized to counter $\\ell_p$-norm bounded attacks, their\npotential in mitigating localized patch attacks remains largely underexplored.\nIn this work, we propose DiffPAD, a novel framework that harnesses the power of\ndiffusion models for adversarial patch decontamination. DiffPAD first performs\nsuper-resolution restoration on downsampled input images, then adopts\nbinarization, dynamic thresholding scheme and sliding window for effective\nlocalization of adversarial patches. Such a design is inspired by the\ntheoretically derived correlation between patch size and diffusion restoration\nerror that is generalized across diverse patch attack scenarios. Finally,\nDiffPAD applies inpainting techniques to the original input images with the\nestimated patch region being masked. By integrating closed-form solutions for\nsuper-resolution restoration and image inpainting into the conditional reverse\nsampling process of a pre-trained diffusion model, DiffPAD obviates the need\nfor text guidance or fine-tuning. Through comprehensive experiments, we\ndemonstrate that DiffPAD not only achieves state-of-the-art adversarial\nrobustness against patch attacks but also excels in recovering naturalistic\nimages without patch remnants.\n","authors":["Jia Fu","Xiao Zhang","Sepideh Pashami","Fatemeh Rahimian","Anders Holst"],"pdf_url":"https://arxiv.org/pdf/2410.24006v1.pdf","comment":"Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2405.10723v2","updated":"2024-10-31T15:05:50Z","published":"2024-05-17T12:11:58Z","title":"Eddeep: Fast eddy-current distortion correction for diffusion MRI with\n  deep learning","summary":"  Modern diffusion MRI sequences commonly acquire a large number of volumes\nwith diffusion sensitization gradients of differing strengths or directions.\nSuch sequences rely on echo-planar imaging (EPI) to achieve reasonable scan\nduration. However, EPI is vulnerable to off-resonance effects, leading to\ntissue susceptibility and eddy-current induced distortions. The latter is\nparticularly problematic because it causes misalignment between volumes,\ndisrupting downstream modelling and analysis. The essential correction of eddy\ndistortions is typically done post-acquisition, with image registration.\nHowever, this is non-trivial because correspondence between volumes can be\nseverely disrupted due to volume-specific signal attenuations induced by\nvarying directions and strengths of the applied gradients. This challenge has\nbeen successfully addressed by the popular FSL~Eddy tool but at considerable\ncomputational cost. We propose an alternative approach, leveraging recent\nadvances in image processing enabled by deep learning (DL). It consists of two\nconvolutional neural networks: 1) An image translator to restore correspondence\nbetween images; 2) A registration model to align the translated images. Results\ndemonstrate comparable distortion estimates to FSL~Eddy, while requiring only\nmodest training sample sizes. This work, to the best of our knowledge, is the\nfirst to tackle this problem with deep learning. Together with recently\ndeveloped DL-based susceptibility correction techniques, they pave the way for\nreal-time preprocessing of diffusion MRI, facilitating its wider uptake in the\nclinic.\n","authors":["Antoine Legouhy","Ross Callaghan","Whitney Stee","Philippe Peigneux","Hojjat Azadbakht","Hui Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.10723v2.pdf","comment":"accepted in MICCAI 2024 conference"},{"id":"http://arxiv.org/abs/2410.24002v1","updated":"2024-10-31T15:02:16Z","published":"2024-10-31T15:02:16Z","title":"Assessing the Efficacy of Classical and Deep Neuroimaging Biomarkers in\n  Early Alzheimer's Disease Diagnosis","summary":"  Alzheimer's disease (AD) is the leading cause of dementia, and its early\ndetection is crucial for effective intervention, yet current diagnostic methods\noften fall short in sensitivity and specificity. This study aims to detect\nsignificant indicators of early AD by extracting and integrating various\nimaging biomarkers, including radiomics, hippocampal texture descriptors,\ncortical thickness measurements, and deep learning features. We analyze\nstructural magnetic resonance imaging (MRI) scans from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) cohorts, utilizing comprehensive image analysis\nand machine learning techniques. Our results show that combining multiple\nbiomarkers significantly improves detection accuracy. Radiomics and texture\nfeatures emerged as the most effective predictors for early AD, achieving AUCs\nof 0.88 and 0.72 for AD and MCI detection, respectively. Although deep learning\nfeatures proved to be less effective than traditional approaches, incorporating\nage with other biomarkers notably enhanced MCI detection performance.\nAdditionally, our findings emphasize the continued importance of classical\nimaging biomarkers in the face of modern deep-learning approaches, providing a\nrobust framework for early AD diagnosis.\n","authors":["Milla E. Nielsen","Mads Nielsen","Mostafa Mehdipour Ghazi"],"pdf_url":"https://arxiv.org/pdf/2410.24002v1.pdf","comment":"SPIE Medical Imaging (MI25)"},{"id":"http://arxiv.org/abs/2410.24001v1","updated":"2024-10-31T15:02:05Z","published":"2024-10-31T15:02:05Z","title":"ImOV3D: Learning Open-Vocabulary Point Clouds 3D Object Detection from\n  Only 2D Images","summary":"  Open-vocabulary 3D object detection (OV-3Det) aims to generalize beyond the\nlimited number of base categories labeled during the training phase. The\nbiggest bottleneck is the scarcity of annotated 3D data, whereas 2D image\ndatasets are abundant and richly annotated. Consequently, it is intuitive to\nleverage the wealth of annotations in 2D images to alleviate the inherent data\nscarcity in OV-3Det. In this paper, we push the task setup to its limits by\nexploring the potential of using solely 2D images to learn OV-3Det. The major\nchallenges for this setup is the modality gap between training images and\ntesting point clouds, which prevents effective integration of 2D knowledge into\nOV-3Det. To address this challenge, we propose a novel framework ImOV3D to\nleverage pseudo multimodal representation containing both images and point\nclouds (PC) to close the modality gap. The key of ImOV3D lies in flexible\nmodality conversion where 2D images can be lifted into 3D using monocular depth\nestimation and can also be derived from 3D scenes through rendering. This\nallows unifying both training images and testing point clouds into a common\nimage-PC representation, encompassing a wealth of 2D semantic information and\nalso incorporating the depth and structural characteristics of 3D spatial data.\nWe carefully conduct such conversion to minimize the domain gap between\ntraining and test cases. Extensive experiments on two benchmark datasets,\nSUNRGBD and ScanNet, show that ImOV3D significantly outperforms existing\nmethods, even in the absence of ground truth 3D training data. With the\ninclusion of a minimal amount of real 3D data for fine-tuning, the performance\nalso significantly surpasses previous state-of-the-art. Codes and pre-trained\nmodels are released on the https://github.com/yangtiming/ImOV3D.\n","authors":["Timing Yang","Yuanliang Ju","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2410.24001v1.pdf","comment":"Accepted by NeurIPS 2024. Code link\n  https://github.com/yangtiming/ImOV3D"},{"id":"http://arxiv.org/abs/2211.15656v3","updated":"2024-10-31T15:01:41Z","published":"2022-11-28T18:59:02Z","title":"SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation","summary":"  High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion.\n","authors":["Hao Dong","Weihao Gu","Xianjing Zhang","Jintao Xu","Rui Ai","Huimin Lu","Juho Kannala","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2211.15656v3.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2407.03550v2","updated":"2024-10-31T14:51:32Z","published":"2024-07-04T00:07:50Z","title":"CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding","summary":"  The comic domain is rapidly advancing with the development of single-page\nanalysis and synthesis models. However, evaluation metrics and datasets lag\nbehind, often limited to small-scale or single-style test sets. We introduce a\nnovel benchmark, CoMix, designed to evaluate the multi-task capabilities of\nmodels in comic analysis. Unlike existing benchmarks that focus on isolated\ntasks such as object detection or text recognition, CoMix addresses a broader\nrange of tasks including object detection, speaker identification, character\nre-identification, reading order, and multi-modal reasoning tasks like\ncharacter naming and dialogue generation. Our benchmark comprises three\nexisting datasets with expanded annotations to support multi-task evaluation.\nTo mitigate the over-representation of manga-style data, we have incorporated a\nnew dataset of carefully selected American comic-style books, thereby enriching\nthe diversity of comic styles. CoMix is designed to assess pre-trained models\nin zero-shot and limited fine-tuning settings, probing their transfer\ncapabilities across different comic styles and tasks. The validation split of\nthe benchmark is publicly available for research purposes, and an evaluation\nserver for the held-out test split is also provided. Comparative results\nbetween human performance and state-of-the-art models reveal a significant\nperformance gap, highlighting substantial opportunities for advancements in\ncomic understanding. The dataset, baseline models, and code are accessible at\nhttps://github.com/emanuelevivoli/CoMix-dataset. This initiative sets a new\nstandard for comprehensive comic analysis, providing the community with a\ncommon benchmark for evaluation on a large and varied set.\n","authors":["Emanuele Vivoli","Marco Bertini","Dimosthenis Karatzas"],"pdf_url":"https://arxiv.org/pdf/2407.03550v2.pdf","comment":"Accepted at NeurIPS 2024 (D&B)"},{"id":"http://arxiv.org/abs/2410.23991v1","updated":"2024-10-31T14:50:48Z","published":"2024-10-31T14:50:48Z","title":"Localization, balance and affinity: a stronger multifaceted\n  collaborative salient object detector in remote sensing images","summary":"  Despite significant advancements in salient object detection(SOD) in optical\nremote sensing images(ORSI), challenges persist due to the intricate edge\nstructures of ORSIs and the complexity of their contextual relationships.\nCurrent deep learning approaches encounter difficulties in accurately\nidentifying boundary features and lack efficiency in collaboratively modeling\nthe foreground and background by leveraging contextual features. To address\nthese challenges, we propose a stronger multifaceted collaborative salient\nobject detector in ORSIs, termed LBA-MCNet, which incorporates aspects of\nlocalization, balance, and affinity. The network focuses on accurately locating\ntargets, balancing detailed features, and modeling image-level global context\ninformation. Specifically, we design the Edge Feature Adaptive Balancing and\nAdjusting(EFABA) module for precise edge localization, using edge features to\nguide attention to boundaries and preserve spatial details. Moreover, we design\nthe Global Distributed Affinity Learning(GDAL) module to model global context.\nIt captures global context by generating an affinity map from the encoders\nfinal layer, ensuring effective modeling of global patterns. Additionally, deep\nsupervision during deconvolution further enhances feature representation.\nFinally, we compared with 28 state of the art approaches on three publicly\navailable datasets. The results clearly demonstrate the superiority of our\nmethod.\n","authors":["Yakun Xie","Suning Liu","Hongyu Chen","Shaohan Cao","Huixin Zhang","Dejun Feng","Qian Wan","Jun Zhu","Qing Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.23991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15856v2","updated":"2024-10-31T14:48:23Z","published":"2023-12-26T02:50:42Z","title":"SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance\n  Fields","summary":"  Although significant progress has been made in the field of 2D-based\ninteractive editing, fine-grained 3D-based interactive editing remains\nrelatively unexplored. This limitation can be attributed to two main\nchallenges: the lack of an efficient 3D representation robust to different\nmodifications and the absence of an effective 3D interactive segmentation\nmethod. In this paper, we introduce a novel fine-grained interactive 3D\nsegmentation and editing algorithm with radiance fields, which we refer to as\nSERF. Our method entails creating a neural mesh representation by integrating\nmulti-view algorithms with pre-trained 2D models. Building upon this\nrepresentation, we introduce a novel surface rendering technique that preserves\nlocal information and is robust to deformation. Moreover, this representation\nforms the basis for achieving accurate and interactive 3D segmentation without\nrequiring 3D supervision. Harnessing this representation facilitates a range of\ninteractive 3D editing operations, encompassing tasks such as interactive\ngeometry editing and texture painting. Extensive experiments and visualization\nexamples of editing on both real and synthetic data demonstrate the superiority\nof our method on representation quality and editing ability.\n","authors":["Kaichen Zhou","Lanqing Hong","Enze Xie","Yongxin Yang","Zhenguo Li","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.15856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22629v2","updated":"2024-10-31T14:44:44Z","published":"2024-10-30T01:22:37Z","title":"CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable\n  Remote Sensing Semantic Segmentation","summary":"  The field of Remote Sensing Domain Generalization (RSDG) has emerged as a\ncritical and valuable research frontier, focusing on developing models that\ngeneralize effectively across diverse scenarios. Despite the substantial domain\ngaps in RS images that are characterized by variabilities such as location,\nwavelength, and sensor type, research in this area remains underexplored: (1)\nCurrent cross-domain methods primarily focus on Domain Adaptation (DA), which\nadapts models to predefined domains rather than to unseen ones; (2) Few studies\ntargeting the RSDG issue, especially for semantic segmentation tasks, where\nexisting models are developed for specific unknown domains, struggling with\nissues of underfitting on other unknown scenarios; (3) Existing RS foundation\nmodels tend to prioritize in-domain performance over cross-domain\ngeneralization. To this end, we introduce the first vision foundation model for\nRSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong\ncross-domain generalization through a specially designed data-level Earth-Style\nInjection pipeline and a model-level Multi-Task Training pipeline. In addition,\nfor the semantic segmentation task, we have curated an RSDG benchmark\ncomprising 28 cross-domain settings across various regions, spectral bands,\nplatforms, and climates, providing a comprehensive framework for testing the\ngeneralizability of future RSDG models. Extensive experiments on this benchmark\ndemonstrate the superiority of CrossEarth over existing state-of-the-art\nmethods.\n","authors":["Ziyang Gong","Zhixiang Wei","Di Wang","Xianzheng Ma","Hongruixuan Chen","Yuru Jia","Yupeng Deng","Zhenming Ji","Xiangwei Zhu","Naoto Yokoya","Jing Zhang","Bo Du","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.22629v2.pdf","comment":"The codes and models will be available at\n  https://github.com/Cuzyoung/CrossEarth"},{"id":"http://arxiv.org/abs/2410.23988v1","updated":"2024-10-31T14:42:26Z","published":"2024-10-31T14:42:26Z","title":"JEMA: A Joint Embedding Framework for Scalable Co-Learning with\n  Multimodal Alignment","summary":"  This work introduces JEMA (Joint Embedding with Multimodal Alignment), a\nnovel co-learning framework tailored for laser metal deposition (LMD), a\npivotal process in metal additive manufacturing. As Industry 5.0 gains traction\nin industrial applications, efficient process monitoring becomes increasingly\ncrucial. However, limited data and the opaque nature of AI present challenges\nfor its application in an industrial setting. JEMA addresses this challenges by\nleveraging multimodal data, including multi-view images and metadata such as\nprocess parameters, to learn transferable semantic representations. By applying\na supervised contrastive loss function, JEMA enables robust learning and\nsubsequent process monitoring using only the primary modality, simplifying\nhardware requirements and computational overhead. We investigate the\neffectiveness of JEMA in LMD process monitoring, focusing specifically on its\ngeneralization to downstream tasks such as melt pool geometry prediction,\nachieved without extensive fine-tuning. Our empirical evaluation demonstrates\nthe high scalability and performance of JEMA, particularly when combined with\nVision Transformer models. We report an 8% increase in performance in\nmultimodal settings and a 1% improvement in unimodal settings compared to\nsupervised contrastive learning. Additionally, the learned embedding\nrepresentation enables the prediction of metadata, enhancing interpretability\nand making possible the assessment of the added metadata's contributions. Our\nframework lays the foundation for integrating multisensor data with metadata,\nenabling diverse downstream tasks within the LMD domain and beyond.\n","authors":["Joao Sousa","Roya Darabi","Armando Sousa","Frank Brueckner","Luís Paulo Reis","Ana Reis"],"pdf_url":"https://arxiv.org/pdf/2410.23988v1.pdf","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2312.14556v2","updated":"2024-10-31T14:37:59Z","published":"2023-12-22T09:29:45Z","title":"CaptainCook4D: A Dataset for Understanding Errors in Procedural\n  Activities","summary":"  Following step-by-step procedures is an essential component of various\nactivities carried out by individuals in their daily lives. These procedures\nserve as a guiding framework that helps to achieve goals efficiently, whether\nit is assembling furniture or preparing a recipe. However, the complexity and\nduration of procedural activities inherently increase the likelihood of making\nerrors. Understanding such procedural activities from a sequence of frames is a\nchallenging task that demands an accurate interpretation of visual information\nand the ability to reason about the structure of the activity. To this end, we\ncollect a new egocentric 4D dataset, CaptainCook4D, comprising 384 recordings\n(94.5 hours) of people performing recipes in real kitchen environments. This\ndataset consists of two distinct types of activity: one in which participants\nadhere to the provided recipe instructions and another in which they deviate\nand induce errors. We provide 5.3K step annotations and 10K fine-grained action\nannotations and benchmark the dataset for the following tasks: supervised error\nrecognition, multistep localization, and procedure learning\n","authors":["Rohith Peddi","Shivvrat Arya","Bharath Challa","Likhitha Pallapothula","Akshay Vyas","Bhavya Gouripeddi","Jikai Wang","Qifan Zhang","Vasundhara Komaragiri","Eric Ragan","Nicholas Ruozzi","Yu Xiang","Vibhav Gogate"],"pdf_url":"https://arxiv.org/pdf/2312.14556v2.pdf","comment":"Accepted to the 2024 Neural Information Processing Systems Datasets\n  and Benchmarks Track, Project Page:\n  https://captaincook4d.github.io/captain-cook/"},{"id":"http://arxiv.org/abs/2407.12582v2","updated":"2024-10-31T14:37:42Z","published":"2024-07-17T14:09:46Z","title":"Embracing Events and Frames with Hierarchical Feature Refinement Network\n  for Object Detection","summary":"  In frame-based vision, object detection faces substantial performance\ndegradation under challenging conditions due to the limited sensing capability\nof conventional cameras. Event cameras output sparse and asynchronous events,\nproviding a potential solution to solve these problems. However, effectively\nfusing two heterogeneous modalities remains an open issue. In this work, we\npropose a novel hierarchical feature refinement network for event-frame fusion.\nThe core concept is the design of the coarse-to-fine fusion module, denoted as\nthe cross-modality adaptive feature refinement (CAFR) module. In the initial\nphase, the bidirectional cross-modality interaction (BCI) part facilitates\ninformation bridging from two distinct sources. Subsequently, the features are\nfurther refined by aligning the channel-level mean and variance in the two-fold\nadaptive feature refinement (TAFR) part. We conducted extensive experiments on\ntwo benchmarks: the low-resolution PKU-DDD17-Car dataset and the\nhigh-resolution DSEC dataset. Experimental results show that our method\nsurpasses the state-of-the-art by an impressive margin of $\\textbf{8.0}\\%$ on\nthe DSEC dataset. Besides, our method exhibits significantly better robustness\n(\\textbf{69.5}\\% versus \\textbf{38.7}\\%) when introducing 15 different\ncorruption types to the frame images. The code can be found at the link\n(https://github.com/HuCaoFighting/FRN).\n","authors":["Hu Cao","Zehua Zhang","Yan Xia","Xinyi Li","Jiahao Xia","Guang Chen","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2407.12582v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2410.22637v2","updated":"2024-10-31T14:35:31Z","published":"2024-10-30T02:04:23Z","title":"Consistency Diffusion Bridge Models","summary":"  Diffusion models (DMs) have become the dominant paradigm of generative\nmodeling in a variety of domains by learning stochastic processes from noise to\ndata. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\ngenerative modeling that builds stochastic processes between fixed data\nendpoints based on a reference diffusion process, have achieved empirical\nsuccess across tasks with coupled data distribution, such as image-to-image\ntranslation. However, DDBM's sampling process typically requires hundreds of\nnetwork evaluations to achieve decent performance, which may impede their\npractical deployment due to high computational demands. In this work, inspired\nby the recent advance of consistency models in DMs, we tackle this problem by\nlearning the consistency function of the probability-flow ordinary differential\nequation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\nstep given any point on the ODE trajectory. Based on a dedicated general-form\nODE solver, we propose two paradigms: consistency bridge distillation and\nconsistency bridge training, which is flexible to apply on DDBMs with broad\ndesign choices. Experimental results show that our proposed method could sample\n$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\nquality given the same step in various tasks with pixel resolution ranging from\n$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\nas semantic interpolation in the data space.\n","authors":["Guande He","Kaiwen Zheng","Jianfei Chen","Fan Bao","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.22637v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23970v1","updated":"2024-10-31T14:25:55Z","published":"2024-10-31T14:25:55Z","title":"TrAct: Making First-layer Pre-Activations Trainable","summary":"  We consider the training of the first layer of vision models and notice the\nclear relationship between pixel values and gradient update magnitudes: the\ngradients arriving at the weights of a first layer are by definition directly\nproportional to (normalized) input pixel values. Thus, an image with low\ncontrast has a smaller impact on learning than an image with higher contrast,\nand a very bright or very dark image has a stronger impact on the weights than\nan image with moderate brightness. In this work, we propose performing gradient\ndescent on the embeddings produced by the first layer of the model. However,\nswitching to discrete inputs with an embedding layer is not a reasonable option\nfor vision models. Thus, we propose the conceptual procedure of (i) a gradient\ndescent step on first layer activations to construct an activation proposal,\nand (ii) finding the optimal weights of the first layer, i.e., those weights\nwhich minimize the squared distance to the activation proposal. We provide a\nclosed form solution of the procedure and adjust it for robust stochastic\ntraining while computing everything efficiently. Empirically, we find that\nTrAct (Training Activations) speeds up training by factors between 1.25x and 4x\nwhile requiring only a small computational overhead. We demonstrate the utility\nof TrAct with different optimizers for a range of different vision models\nincluding convolutional and transformer architectures.\n","authors":["Felix Petersen","Christian Borgelt","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.23970v1.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23962v1","updated":"2024-10-31T14:14:30Z","published":"2024-10-31T14:14:30Z","title":"Image Synthesis with Class-Aware Semantic Diffusion Models for Surgical\n  Scene Segmentation","summary":"  Surgical scene segmentation is essential for enhancing surgical precision,\nyet it is frequently compromised by the scarcity and imbalance of available\ndata. To address these challenges, semantic image synthesis methods based on\ngenerative adversarial networks and diffusion models have been developed.\nHowever, these models often yield non-diverse images and fail to capture small,\ncritical tissue classes, limiting their effectiveness. In response, we propose\nthe Class-Aware Semantic Diffusion Model (CASDM), a novel approach which\nutilizes segmentation maps as conditions for image synthesis to tackle data\nscarcity and imbalance. Novel class-aware mean squared error and class-aware\nself-perceptual loss functions have been defined to prioritize critical, less\nvisible classes, thereby enhancing image quality and relevance. Furthermore, to\nour knowledge, we are the first to generate multi-class segmentation maps using\ntext prompts in a novel fashion to specify their contents. These maps are then\nused by CASDM to generate surgical scene images, enhancing datasets for\ntraining and validating segmentation models. Our evaluation, which assesses\nboth image quality and downstream segmentation performance, demonstrates the\nstrong effectiveness and generalisability of CASDM in producing realistic\nimage-map pairs, significantly advancing surgical scene segmentation across\ndiverse and challenging datasets.\n","authors":["Yihang Zhou","Rebecca Towning","Zaid Awad","Stamatia Giannarou"],"pdf_url":"https://arxiv.org/pdf/2410.23962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23946v1","updated":"2024-10-31T14:02:40Z","published":"2024-10-31T14:02:40Z","title":"MV-CC: Mask Enhanced Video Model for Remote Sensing Change Caption","summary":"  Remote sensing image change caption (RSICC) aims to provide natural language\ndescriptions for bi-temporal remote sensing images. Since Change Caption (CC)\ntask requires both spatial and temporal features, previous works follow an\nencoder-fusion-decoder architecture. They use an image encoder to extract\nspatial features and the fusion module to integrate spatial features and\nextract temporal features, which leads to increasingly complex manual design of\nthe fusion module. In this paper, we introduce a novel video model-based\nparadigm without design of the fusion module and propose a Mask-enhanced Video\nmodel for Change Caption (MV-CC). Specifically, we use the off-the-shelf video\nencoder to simultaneously extract the temporal and spatial features of\nbi-temporal images. Furthermore, the types of changes in the CC are set based\non specific task requirements, and to enable the model to better focus on the\nregions of interest, we employ masks obtained from the Change Detection (CD)\nmethod to explicitly guide the CC model. Experimental results demonstrate that\nour proposed method can obtain better performance compared with other\nstate-of-the-art RSICC methods. The code is available at\nhttps://github.com/liuruixun/MV-CC.\n","authors":["Ruixun Liu","Kaiyu Li","Jiayi Song","Dongwei Sun","Xiangyong Cao"],"pdf_url":"https://arxiv.org/pdf/2410.23946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03438v2","updated":"2024-10-31T13:41:19Z","published":"2024-10-04T13:52:22Z","title":"Dessie: Disentanglement for Articulated 3D Horse Shape and Pose\n  Estimation from Images","summary":"  In recent years, 3D parametric animal models have been developed to aid in\nestimating 3D shape and pose from images and video. While progress has been\nmade for humans, it's more challenging for animals due to limited annotated\ndata. To address this, we introduce the first method using synthetic data\ngeneration and disentanglement to learn to regress 3D shape and pose. Focusing\non horses, we use text-based texture generation and a synthetic data pipeline\nto create varied shapes, poses, and appearances, learning disentangled spaces.\nOur method, Dessie, surpasses existing 3D horse reconstruction methods and\ngeneralizes to other large animals like zebras, cows, and deer. See the project\nwebsite at: \\url{https://celiali.github.io/Dessie/}.\n","authors":["Ci Li","Yi Yang","Zehang Weng","Elin Hernlund","Silvia Zuffi","Hedvig Kjellström"],"pdf_url":"https://arxiv.org/pdf/2410.03438v2.pdf","comment":"ACCV2024"},{"id":"http://arxiv.org/abs/2410.23931v1","updated":"2024-10-31T13:41:16Z","published":"2024-10-31T13:41:16Z","title":"Manipulating Vehicle 3D Shapes through Latent Space Editing","summary":"  Although 3D object editing has the potential to significantly influence\nvarious industries, recent research in 3D generation and editing has primarily\nfocused on converting text and images into 3D models, often overlooking the\nneed for fine-grained control over the editing of existing 3D objects. This\npaper introduces a framework that employs a pre-trained regressor, enabling\ncontinuous, precise, attribute-specific modifications to both the stylistic and\ngeometric attributes of vehicle 3D models. Our method not only preserves the\ninherent identity of vehicle 3D objects, but also supports multi-attribute\nediting, allowing for extensive customization without compromising the model's\nstructural integrity. Experimental results demonstrate the efficacy of our\napproach in achieving detailed edits on various vehicle 3D models.\n","authors":["JiangDong Miao","Tatsuya Ikeda","Bisser Raytchev","Ryota Mizoguchi","Takenori Hiraoka","Takuji Nakashima","Keigo Shimizu","Toru Higaki","Kazufumi Kaneda"],"pdf_url":"https://arxiv.org/pdf/2410.23931v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.23918v1","updated":"2024-10-31T13:26:11Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04230v2","updated":"2024-10-31T13:18:14Z","published":"2024-06-06T16:30:41Z","title":"M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating\n  Interferometric SAR and Multispectral Data","summary":"  Satellite-based remote sensing has revolutionised the way we address global\nchallenges. Huge quantities of Earth Observation (EO) data are generated by\nsatellite sensors daily, but processing these large datasets for use in ML\npipelines is technically and computationally challenging. While some\npreprocessed Earth observation datasets exist, their content is often limited\nto optical or near-optical wavelength data, which is ineffective at night or in\nadverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing\ntechnique based on microwave length radiation, offers a viable alternative.\nHowever, the application of machine learning to SAR has been limited due to a\nlack of ML-ready data and pipelines, particularly for the full diversity of SAR\ndata, including polarimetry, coherence and interferometry. In this work, we\nintroduce M3LEO, a multi-modal, multi-label Earth observation dataset that\nincludes polarimetric, interferometric, and coherence SAR data derived from\nSentinel-1, alongside multispectral Sentinel-2 imagery and auxiliary data\ndescribing terrain properties such as land use. M3LEO spans approximately 17M\n4x4 km data chips from six diverse geographic regions. The dataset is\ncomplemented by a flexible PyTorch Lightning framework configured using Hydra\nto accommodate its use across diverse ML applications in Earth observation. We\nprovide tools to process any dataset available on popular platforms such as\nGoogle Earth Engine for seamless integration with our framework. We show that\nthe distribution shift in self-supervised embeddings is substantial across\ngeographic regions, even when controlling for terrain properties. Data:\nhuggingface.co/M3LEO, Code: github.com/spaceml-org/M3LEO.\n","authors":["Matthew J Allen","Francisco Dorr","Joseph Alejandro Gallego Mejia","Laura Martínez-Ferrer","Anna Jungbluth","Freddie Kalaitzis","Raúl Ramos-Pollán"],"pdf_url":"https://arxiv.org/pdf/2406.04230v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.23910v1","updated":"2024-10-31T13:13:32Z","published":"2024-10-31T13:13:32Z","title":"Uncertainty Estimation for 3D Object Detection via Evidential Learning","summary":"  3D object detection is an essential task for computer vision applications in\nautonomous vehicles and robotics. However, models often struggle to quantify\ndetection reliability, leading to poor performance on unfamiliar scenes. We\nintroduce a framework for quantifying uncertainty in 3D object detection by\nleveraging an evidential learning loss on Bird's Eye View representations in\nthe 3D detector. These uncertainty estimates require minimal computational\noverhead and are generalizable across different architectures. We demonstrate\nboth the efficacy and importance of these uncertainty estimates on identifying\nout-of-distribution scenes, poorly localized objects, and missing (false\nnegative) detections; our framework consistently improves over baselines by\n10-20% on average. Finally, we integrate this suite of tasks into a system\nwhere a 3D object detector auto-labels driving scenes and our uncertainty\nestimates verify label correctness before the labels are used to train a second\nmodel. Here, our uncertainty-driven verification results in a 1% improvement in\nmAP and a 1-2% improvement in NDS.\n","authors":["Nikita Durasov","Rafid Mahmood","Jiwoong Choi","Marc T. Law","James Lucas","Pascal Fua","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2410.23910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23906v1","updated":"2024-10-31T13:11:09Z","published":"2024-10-31T13:11:09Z","title":"From Web Data to Real Fields: Low-Cost Unsupervised Domain Adaptation\n  for Agricultural Robots","summary":"  In precision agriculture, vision models often struggle with new, unseen\nfields where crops and weeds have been influenced by external factors,\nresulting in compositions and appearances that differ from the learned\ndistribution. This paper aims to adapt to specific fields at low cost using\nUnsupervised Domain Adaptation (UDA). We explore a novel domain shift from a\ndiverse, large pool of internet-sourced data to a small set of data collected\nby a robot at specific locations, minimizing the need for extensive on-field\ndata collection. Additionally, we introduce a novel module -- the Multi-level\nAttention-based Adversarial Discriminator (MAAD) -- which can be integrated at\nthe feature extractor level of any detection model. In this study, we\nincorporate MAAD with CenterNet to simultaneously detect leaf, stem, and vein\ninstances. Our results show significant performance improvements in the\nunlabeled target domain compared to baseline models, with a 7.5% increase in\nobject detection accuracy and a 5.1% improvement in keypoint detection.\n","authors":["Vasileios Tzouras","Lazaros Nalpantidis","Ronja Güldenring"],"pdf_url":"https://arxiv.org/pdf/2410.23906v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2410.23905v1","updated":"2024-10-31T13:10:50Z","published":"2024-10-31T13:10:50Z","title":"Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on\n  Text-modulated Diffusion Model","summary":"  Existing multi-modal image fusion methods fail to address the compound\ndegradations presented in source images, resulting in fusion images plagued by\nnoise, color bias, improper exposure, \\textit{etc}. Additionally, these methods\noften overlook the specificity of foreground objects, weakening the salience of\nthe objects of interest within the fused images. To address these challenges,\nthis study proposes a novel interactive multi-modal image fusion framework\nbased on the text-modulated diffusion model, called Text-DiFuse. First, this\nframework integrates feature-level information integration into the diffusion\nprocess, allowing adaptive degradation removal and multi-modal information\nfusion. This is the first attempt to deeply and explicitly embed information\nfusion within the diffusion process, effectively addressing compound\ndegradation in image fusion. Second, by embedding the combination of the text\nand zero-shot location model into the diffusion fusion process, a\ntext-controlled fusion re-modulation strategy is developed. This enables\nuser-customized text control to improve fusion performance and highlight\nforeground objects in the fused images. Extensive experiments on diverse public\ndatasets show that our Text-DiFuse achieves state-of-the-art fusion performance\nacross various scenarios with complex degradation. Moreover, the semantic\nsegmentation experiment validates the significant enhancement in semantic\nperformance achieved by our text-controlled fusion re-modulation strategy. The\ncode is publicly available at https://github.com/Leiii-Cao/Text-DiFuse.\n","authors":["Hao Zhang","Lei Cao","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2410.23905v1.pdf","comment":"Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2303.15124v2","updated":"2024-10-31T13:10:11Z","published":"2023-03-27T11:56:20Z","title":"Blind Inpainting with Object-aware Discrimination for Artificial Marker\n  Removal","summary":"  Medical images often incorporate doctor-added markers that can hinder\nAI-based diagnosis. This issue highlights the need of inpainting techniques to\nrestore the corrupted visual contents. However, existing methods require manual\nmask annotation as input, limiting the application scenarios. In this paper, we\npropose a novel blind inpainting method that automatically reconstructs visual\ncontents within the corrupted regions without mask input as guidance. Our model\nincludes a blind reconstruction network and an object-aware discriminator for\nadversarial training. The reconstruction network contains two branches that\npredict corrupted regions in images and simultaneously restore the missing\nvisual contents. Leveraging the potent recognition capability of a dense object\ndetector, the object-aware discriminator ensures markers undetectable after\ninpainting. Thus, the restored images closely resemble the clean ones. We\nevaluate our method on three datasets of various medical imaging modalities,\nconfirming better performance over other state-of-the-art methods.\n","authors":["Xuechen Guo","Wenhao Hu","Chiming Ni","Wenhao Chai","Shiyan Li","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23904v1","updated":"2024-10-31T13:06:29Z","published":"2024-10-31T13:06:29Z","title":"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection","summary":"  Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI.\n","authors":["Qinqian Lei","Bo Wang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23904v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.04857v4","updated":"2024-10-31T13:01:13Z","published":"2024-02-07T13:54:56Z","title":"Advancing Video Anomaly Detection: A Concise Review and a New Dataset","summary":"  Video Anomaly Detection (VAD) finds widespread applications in security\nsurveillance, traffic monitoring, industrial monitoring, and healthcare.\nDespite extensive research efforts, there remains a lack of concise reviews\nthat provide insightful guidance for researchers. Such reviews would serve as\nquick references to grasp current challenges, research trends, and future\ndirections. In this paper, we present such a review, examining models and\ndatasets from various perspectives. We emphasize the critical relationship\nbetween model and dataset, where the quality and diversity of datasets\nprofoundly influence model performance, and dataset development adapts to the\nevolving needs of emerging approaches. Our review identifies practical issues,\nincluding the absence of comprehensive datasets with diverse scenarios. To\naddress this, we introduce a new dataset, Multi-Scenario Anomaly Detection\n(MSAD), comprising 14 distinct scenarios captured from various camera views.\nOur dataset has diverse motion patterns and challenging variations, such as\ndifferent lighting and weather conditions, providing a robust foundation for\ntraining superior models. We conduct an in-depth analysis of recent\nrepresentative models using MSAD and highlight its potential in addressing the\nchallenges of detecting anomalies across diverse and evolving surveillance\nscenarios. [Project website: https://msad-dataset.github.io/]\n","authors":["Liyun Zhu","Lei Wang","Arjun Raj","Tom Gedeon","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2402.04857v4.pdf","comment":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2407.08447v2","updated":"2024-10-31T12:58:08Z","published":"2024-07-11T12:41:32Z","title":"WildGaussians: 3D Gaussian Splatting in the Wild","summary":"  While the field of 3D scene reconstruction is dominated by NeRFs due to their\nphotorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,\noffering similar quality with real-time rendering speeds. However, both methods\nprimarily excel with well-controlled 3D scenes, while in-the-wild data -\ncharacterized by occlusions, dynamic objects, and varying illumination -\nremains challenging. NeRFs can adapt to such conditions easily through\nper-image embedding vectors, but 3DGS struggles due to its explicit\nrepresentation and lack of shared parameters. To address this, we introduce\nWildGaussians, a novel approach to handle occlusions and appearance changes\nwith 3DGS. By leveraging robust DINO features and integrating an appearance\nmodeling module within 3DGS, our method achieves state-of-the-art results. We\ndemonstrate that WildGaussians matches the real-time rendering speed of 3DGS\nwhile surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all\nwithin a simple architectural framework.\n","authors":["Jonas Kulhanek","Songyou Peng","Zuzana Kukelova","Marc Pollefeys","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2407.08447v2.pdf","comment":"NeurIPS 2024; Project page: https://wild-gaussians.github.io/"},{"id":"http://arxiv.org/abs/2410.23891v1","updated":"2024-10-31T12:52:52Z","published":"2024-10-31T12:52:52Z","title":"AllClear: A Comprehensive Dataset and Benchmark for Cloud Removal in\n  Satellite Imagery","summary":"  Clouds in satellite imagery pose a significant challenge for downstream\napplications. A major challenge in current cloud removal research is the\nabsence of a comprehensive benchmark and a sufficiently large and diverse\ntraining dataset. To address this problem, we introduce the largest public\ndataset -- $\\textit{AllClear}$ for cloud removal, featuring 23,742 globally\ndistributed regions of interest (ROIs) with diverse land-use patterns,\ncomprising 4 million images in total. Each ROI includes complete temporal\ncaptures from the year 2022, with (1) multi-spectral optical imagery from\nSentinel-2 and Landsat 8/9, (2) synthetic aperture radar (SAR) imagery from\nSentinel-1, and (3) auxiliary remote sensing products such as cloud masks and\nland cover maps. We validate the effectiveness of our dataset by benchmarking\nperformance, demonstrating the scaling law -- the PSNR rises from $28.47$ to\n$33.87$ with $30\\times$ more data, and conducting ablation studies on the\ntemporal length and the importance of individual modalities. This dataset aims\nto provide comprehensive coverage of the Earth's surface and promote better\ncloud removal results.\n","authors":["Hangyu Zhou","Chia-Hsiang Kao","Cheng Perng Phoo","Utkarsh Mall","Bharath Hariharan","Kavita Bala"],"pdf_url":"https://arxiv.org/pdf/2410.23891v1.pdf","comment":"Accepted at NeurIPS 2024 Datasets and Benchmarks Track. Code and data\n  available at https://allclear.cs.cornell.edu/"},{"id":"http://arxiv.org/abs/2410.10356v2","updated":"2024-10-31T12:49:09Z","published":"2024-10-14T10:17:24Z","title":"FasterDiT: Towards Faster Diffusion Transformers Training without\n  Architecture Modification","summary":"  Diffusion Transformers (DiT) have attracted significant attention in\nresearch. However, they suffer from a slow convergence rate. In this paper, we\naim to accelerate DiT training without any architectural modification. We\nidentify the following issues in the training process: firstly, certain\ntraining strategies do not consistently perform well across different data.\nSecondly, the effectiveness of supervision at specific timesteps is limited. In\nresponse, we propose the following contributions: (1) We introduce a new\nperspective for interpreting the failure of the strategies. Specifically, we\nslightly extend the definition of Signal-to-Noise Ratio (SNR) and suggest\nobserving the Probability Density Function (PDF) of SNR to understand the\nessence of the data robustness of the strategy. (2) We conduct numerous\nexperiments and report over one hundred experimental results to empirically\nsummarize a unified accelerating strategy from the perspective of PDF. (3) We\ndevelop a new supervision method that further accelerates the training process\nof DiT. Based on them, we propose FasterDiT, an exceedingly simple and\npracticable design strategy. With few lines of code modifications, it achieves\n2.30 FID on ImageNet 256 resolution at 1000k iterations, which is comparable to\nDiT (2.27 FID) but 7 times faster in training.\n","authors":["Jingfeng Yao","Wang Cheng","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10356v2.pdf","comment":"NeurIPS 2024 (poster); update to camera-ready version"},{"id":"http://arxiv.org/abs/2407.16430v2","updated":"2024-10-31T12:48:05Z","published":"2024-07-23T12:28:59Z","title":"Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution","summary":"  Detecting and rejecting unknown out-of-distribution (OOD) samples is critical\nfor deployed neural networks to void unreliable predictions. In real-world\nscenarios, however, the efficacy of existing OOD detection methods is often\nimpeded by the inherent imbalance of in-distribution (ID) data, which causes\nsignificant performance decline. Through statistical observations, we have\nidentified two common challenges faced by different OOD detectors:\nmisidentifying tail class ID samples as OOD, while erroneously predicting OOD\nsamples as head class from ID. To explain this phenomenon, we introduce a\ngeneralized statistical framework, termed ImOOD, to formulate the OOD detection\nproblem on imbalanced data distribution. Consequently, the theoretical analysis\nreveals that there exists a class-aware bias item between balanced and\nimbalanced OOD detection, which contributes to the performance gap. Building\nupon this finding, we present a unified training-time regularization technique\nto mitigate the bias and boost imbalanced OOD detectors across architecture\ndesigns. Our theoretically grounded method translates into consistent\nimprovements on the representative CIFAR10-LT, CIFAR100-LT, and ImageNet-LT\nbenchmarks against several state-of-the-art OOD detection approaches. Code is\navailable at https://github.com/alibaba/imood.\n","authors":["Kai Liu","Zhihang Fu","Sheng Jin","Chao Chen","Ze Chen","Rongxin Jiang","Fan Zhou","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16430v2.pdf","comment":"This paper has been accepted by NeurIPS 2024. Code is available at\n  https://github.com/alibaba/imood"},{"id":"http://arxiv.org/abs/2408.12282v2","updated":"2024-10-31T12:30:46Z","published":"2024-08-22T10:34:01Z","title":"Subsurface Scattering for 3D Gaussian Splatting","summary":"  3D reconstruction and relighting of objects made from scattering materials\npresent a significant challenge due to the complex light transport beneath the\nsurface. 3D Gaussian Splatting introduced high-quality novel view synthesis at\nreal-time speeds. While 3D Gaussians efficiently approximate an object's\nsurface, they fail to capture the volumetric properties of subsurface\nscattering. We propose a framework for optimizing an object's shape together\nwith the radiance transfer field given multi-view OLAT (one light at a time)\ndata. Our method decomposes the scene into an explicit surface represented as\n3D Gaussians, with a spatially varying BRDF, and an implicit volumetric\nrepresentation of the scattering component. A learned incident light field\naccounts for shadowing. We optimize all parameters jointly via ray-traced\ndifferentiable rendering. Our approach enables material editing, relighting and\nnovel view synthesis at interactive rates. We show successful application on\nsynthetic data and introduce a newly acquired multi-view multi-light dataset of\nobjects in a light-stage setup. Compared to previous work we achieve comparable\nor better results at a fraction of optimization and rendering time while\nenabling detailed control over material attributes. Project page\nhttps://sss.jdihlmann.com/\n","authors":["Jan-Niklas Dihlmann","Arjun Majumdar","Andreas Engelhardt","Raphael Braun","Hendrik P. A. Lensch"],"pdf_url":"https://arxiv.org/pdf/2408.12282v2.pdf","comment":"Project page: https://sss.jdihlmann.com/"},{"id":"http://arxiv.org/abs/2405.15688v2","updated":"2024-10-31T12:24:34Z","published":"2024-05-24T16:27:05Z","title":"UNION: Unsupervised 3D Object Detection using Object Appearance-based\n  Pseudo-Classes","summary":"  Unsupervised 3D object detection methods have emerged to leverage vast\namounts of data without requiring manual labels for training. Recent approaches\nrely on dynamic objects for learning to detect mobile objects but penalize the\ndetections of static instances during training. Multiple rounds of (self)\ntraining are used to add detected static instances to the set of training\ntargets; this procedure to improve performance is computationally expensive. To\naddress this, we propose the method UNION. We use spatial clustering and\nself-supervised scene flow to obtain a set of static and dynamic object\nproposals from LiDAR. Subsequently, object proposals' visual appearances are\nencoded to distinguish static objects in the foreground and background by\nselecting static instances that are visually similar to dynamic objects. As a\nresult, static and dynamic mobile objects are obtained together, and existing\ndetectors can be trained with a single training. In addition, we extend 3D\nobject discovery to detection by using object appearance-based cluster labels\nas pseudo-class labels for training object classification. We conduct extensive\nexperiments on the nuScenes dataset and increase the state-of-the-art\nperformance for unsupervised 3D object discovery, i.e. UNION more than doubles\nthe average precision to 38.4. The code is available at\ngithub.com/TedLentsch/UNION.\n","authors":["Ted Lentsch","Holger Caesar","Dariu M. Gavrila"],"pdf_url":"https://arxiv.org/pdf/2405.15688v2.pdf","comment":"NeurIPS'24"},{"id":"http://arxiv.org/abs/2312.10112v3","updated":"2024-10-31T12:19:37Z","published":"2023-12-15T09:09:25Z","title":"NM-FlowGAN: Modeling sRGB Noise without Paired Images using a Hybrid\n  Approach of Normalizing Flows and GAN","summary":"  Modeling and synthesizing real sRGB noise is crucial for various low-level\nvision tasks, such as building datasets for training image denoising systems.\nThe distribution of real sRGB noise is highly complex and affected by a\nmultitude of factors, making its accurate modeling extremely challenging.\nTherefore, recent studies have proposed methods that employ data-driven\ngenerative models, such as Generative Adversarial Networks (GAN) and\nNormalizing Flows. These studies achieve more accurate modeling of sRGB noise\ncompared to traditional noise modeling methods. However, there are performance\nlimitations due to the inherent characteristics of each generative model. To\naddress this issue, we propose NM-FlowGAN, a hybrid approach that exploits the\nstrengths of both GAN and Normalizing Flows. We combine pixel-wise noise\nmodeling networks based on Normalizing Flows and spatial correlation modeling\nnetworks based on GAN. Specifically, the pixel-wise noise modeling network\nleverages the high training stability of Normalizing Flows to capture noise\ncharacteristics that are affected by a multitude of factors, and the spatial\ncorrelation networks efficiently model pixel-to-pixel relationships. In\nparticular, unlike recent methods that rely on paired noisy images, our method\nsynthesizes noise using clean images and factors that affect noise\ncharacteristics, such as easily obtainable parameters like camera type and ISO\nsettings, making it applicable to various fields where obtaining noisy-clean\nimage pairs is not feasible. In our experiments, our NM-FlowGAN outperforms\nother baselines in the sRGB noise synthesis task. Moreover, the denoising\nneural network trained with synthesized image pairs from our model shows\nsuperior performance compared to other baselines. Our code is available at:\n\\url{https://github.com/YoungJooHan/NM-FlowGAN}.\n","authors":["Young Joo Han","Ha-Jin Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10112v3.pdf","comment":"13 pages, 10 figures, 8 tables"},{"id":"http://arxiv.org/abs/2404.16022v2","updated":"2024-10-31T12:17:39Z","published":"2024-04-24T17:55:33Z","title":"PuLID: Pure and Lightning ID Customization via Contrastive Alignment","summary":"  We propose Pure and Lightning ID customization (PuLID), a novel tuning-free\nID customization method for text-to-image generation. By incorporating a\nLightning T2I branch with a standard diffusion one, PuLID introduces both\ncontrastive alignment loss and accurate ID loss, minimizing disruption to the\noriginal model and ensuring high ID fidelity. Experiments show that PuLID\nachieves superior performance in both ID fidelity and editability. Another\nattractive property of PuLID is that the image elements (e.g., background,\nlighting, composition, and style) before and after the ID insertion are kept as\nconsistent as possible. Codes and models are available at\nhttps://github.com/ToTheBeginning/PuLID\n","authors":["Zinan Guo","Yanze Wu","Zhuowei Chen","Lang Chen","Peng Zhang","Qian He"],"pdf_url":"https://arxiv.org/pdf/2404.16022v2.pdf","comment":"NeurIPS 2024. Codes and models are available at\n  https://github.com/ToTheBeginning/PuLID"},{"id":"http://arxiv.org/abs/2410.23854v1","updated":"2024-10-31T12:04:30Z","published":"2024-10-31T12:04:30Z","title":"Airway Labeling Meets Clinical Applications: Reflecting Topology\n  Consistency and Outliers via Learnable Attentions","summary":"  Accurate airway anatomical labeling is crucial for clinicians to identify and\nnavigate complex bronchial structures during bronchoscopy. Automatic airway\nanatomical labeling is challenging due to significant individual variability\nand anatomical variations. Previous methods are prone to generate inconsistent\npredictions, which is harmful for preoperative planning and intraoperative\nnavigation. This paper aims to address these challenges by proposing a novel\nmethod that enhances topological consistency and improves the detection of\nabnormal airway branches.\n  We propose a novel approach incorporating two modules: the Soft Subtree\nConsistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module\nconstructs a soft subtree to capture clinically relevant topological\nrelationships, allowing for flexible feature aggregation within and across\nsubtrees. The ABS module facilitates the interaction between node features and\nprototypes to distinguish abnormal branches, preventing the erroneous\naggregation of features between normal and abnormal nodes.\n  Evaluated on a challenging dataset characterized by severe airway distortion\nand atrophy, our method achieves superior performance compared to\nstate-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the\nsegmental level and an 83.7% accuracy at the subsegmental level, representing a\n1.4% increase in subsegmental accuracy and a 3.1% increase in topological\nconsistency. Notably, the method demonstrates reliable performance in cases\nwith disease-induced airway deformities, ensuring consistent and accurate\nlabeling.\n","authors":["Chenyu Li","Minghui Zhang","Chuyan Zhang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2410.23854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19370v2","updated":"2024-10-31T12:00:26Z","published":"2024-09-28T14:50:45Z","title":"MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN\n  Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation","summary":"  Segmenting anatomical structures and lesions from ultrasound images\ncontributes to disease assessment. Weakly supervised learning (WSL) based on\nsparse annotation has achieved encouraging performance and demonstrated the\npotential to reduce annotation costs. This study attempts to introduce\nscribble-based WSL into ultrasound image segmentation tasks. However,\nultrasound images often suffer from poor contrast and unclear edges, coupled\nwith insufficient supervison signals for edges, posing challenges to edge\nprediction. Uncertainty modeling has been proven to facilitate models in\ndealing with these issues. Nevertheless, existing uncertainty estimation\nparadigms are not robust enough and often filter out predictions near decision\nboundaries, resulting in unstable edge predictions. Therefore, we propose\nleveraging predictions near decision boundaries effectively. Specifically, we\nintroduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided\nConsistency strategy. This strategy utilizes high-evidence predictions, which\nare more likely to occur near high-density regions, to guide the optimization\nof low-evidence predictions that may appear near decision boundaries.\nFurthermore, the diverse sizes and locations of lesions in ultrasound images\npose a challenge for CNNs with local receptive fields, as they struggle to\nmodel global information. Therefore, we introduce Visual Mamba based on\nstructured state space sequence models, which achieves long-range dependency\nwith linear computational complexity, and we construct a novel hybrid CNN-Mamba\nframework. During training, the collaboration between the CNN branch and the\nMamba branch in the proposed framework draws inspiration from each other based\non the EGC strategy. Experiments demonstrate the competitiveness of the\nproposed method. Dataset and code will be available on\nhttps://github.com/GtLinyer/MambaEviScrib.\n","authors":["Xiaoxiang Han","Xinyu Li","Jiang Shang","Yiman Liu","Keyan Chen","Shugong Xu","Qiaohong Liu","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.19370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12490v2","updated":"2024-10-31T11:42:07Z","published":"2024-10-16T12:13:17Z","title":"Stabilize the Latent Space for Image Autoregressive Modeling: A Unified\n  Perspective","summary":"  Latent-based image generative models, such as Latent Diffusion Models (LDMs)\nand Mask Image Models (MIMs), have achieved notable success in image generation\ntasks. These models typically leverage reconstructive autoencoders like VQGAN\nor VAE to encode pixels into a more compact latent space and learn the data\ndistribution in the latent space instead of directly from pixels. However, this\npractice raises a pertinent question: Is it truly the optimal choice? In\nresponse, we begin with an intriguing observation: despite sharing the same\nlatent space, autoregressive models significantly lag behind LDMs and MIMs in\nimage generation. This finding contrasts sharply with the field of NLP, where\nthe autoregressive model GPT has established a commanding presence. To address\nthis discrepancy, we introduce a unified perspective on the relationship\nbetween latent space and generative models, emphasizing the stability of latent\nspace in image generative modeling. Furthermore, we propose a simple but\neffective discrete image tokenizer to stabilize the latent space for image\ngenerative modeling by applying K-Means on the latent features of\nself-supervised learning models. Experimental results show that image\nautoregressive modeling with our tokenizer (DiGIT) benefits both image\nunderstanding and image generation with the next token prediction principle,\nwhich is inherently straightforward for GPT models but challenging for other\ngenerative models. Remarkably, for the first time, a GPT-style autoregressive\nmodel for images outperforms LDMs, which also exhibits substantial improvement\nakin to GPT when scaling up model size. Our findings underscore the potential\nof an optimized latent space and the integration of discrete tokenization in\nadvancing the capabilities of image generative models. The code is available at\n\\url{https://github.com/DAMO-NLP-SG/DiGIT}.\n","authors":["Yongxin Zhu","Bocheng Li","Hang Zhang","Xin Li","Linli Xu","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.12490v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23836v1","updated":"2024-10-31T11:32:33Z","published":"2024-10-31T11:32:33Z","title":"Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided\n  Mixture-of-Experts","summary":"  This paper introduces Stereo-Talker, a novel one-shot audio-driven human\nvideo synthesis system that generates 3D talking videos with precise lip\nsynchronization, expressive body gestures, temporally consistent\nphoto-realistic quality, and continuous viewpoint control. The process follows\na two-stage approach. In the first stage, the system maps audio input to\nhigh-fidelity motion sequences, encompassing upper-body gestures and facial\nexpressions. To enrich motion diversity and authenticity, large language model\n(LLM) priors are integrated with text-aligned semantic audio features,\nleveraging LLMs' cross-modal generalization power to enhance motion quality. In\nthe second stage, we improve diffusion-based video generation models by\nincorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided\nMoE focuses on view-specific attributes, while a mask-guided MoE enhances\nregion-based rendering stability. Additionally, a mask prediction module is\ndevised to derive human masks from motion data, enhancing the stability and\naccuracy of masks and enabling mask guiding during inference. We also introduce\na comprehensive human video dataset with 2,203 identities, covering diverse\nbody gestures and detailed annotations, facilitating broad generalization. The\ncode, data, and pre-trained models will be released for research purposes.\n","authors":["Xiang Deng","Youxin Pang","Xiaochen Zhao","Chao Xu","Lizhen Wang","Hongjiang Xiao","Shi Yan","Hongwen Zhang","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14927v3","updated":"2024-10-31T11:32:19Z","published":"2024-06-21T07:37:17Z","title":"GIC: Gaussian-Informed Continuum for Physical Property Identification\n  and Simulation","summary":"  This paper studies the problem of estimating physical properties (system\nidentification) through visual observations. To facilitate geometry-aware\nguidance in physical property estimation, we introduce a novel hybrid framework\nthat leverages 3D Gaussian representation to not only capture explicit shapes\nbut also enable the simulated continuum to render object masks as 2D shape\nsurrogates during training. We propose a new dynamic 3D Gaussian framework\nbased on motion factorization to recover the object as 3D Gaussian point sets\nacross different time states. Furthermore, we develop a coarse-to-fine filling\nstrategy to generate the density fields of the object from the Gaussian\nreconstruction, allowing for the extraction of object continuums along with\ntheir surfaces and the integration of Gaussian attributes into these continuum.\nIn addition to the extracted object surfaces, the Gaussian-informed continuum\nalso enables the rendering of object masks during simulations, serving as\n2D-shape guidance for physical property estimation. Extensive experimental\nevaluations demonstrate that our pipeline achieves state-of-the-art performance\nacross multiple benchmarks and metrics. Additionally, we illustrate the\neffectiveness of the proposed method through real-world demonstrations,\nshowcasing its practical utility. Our project page is at\nhttps://jukgei.github.io/project/gic.\n","authors":["Junhao Cai","Yuji Yang","Weihao Yuan","Yisheng He","Zilong Dong","Liefeng Bo","Hui Cheng","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14927v3.pdf","comment":"21 pages, 8 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23835v1","updated":"2024-10-31T11:29:41Z","published":"2024-10-31T11:29:41Z","title":"Counterfactual MRI Data Augmentation using Conditional Denoising\n  Diffusion Generative Models","summary":"  Deep learning (DL) models in medical imaging face challenges in\ngeneralizability and robustness due to variations in image acquisition\nparameters (IAP). In this work, we introduce a novel method using conditional\ndenoising diffusion generative models (cDDGMs) to generate counterfactual\nmagnetic resonance (MR) images that simulate different IAP without altering\npatient anatomy. We demonstrate that using these counterfactual images for data\naugmentation can improve segmentation accuracy, particularly in\nout-of-distribution settings, enhancing the overall generalizability and\nrobustness of DL models across diverse imaging conditions. Our approach shows\npromise in addressing domain and covariate shifts in medical imaging. The code\nis publicly available at https:\n//github.com/pedromorao/Counterfactual-MRI-Data-Augmentation\n","authors":["Pedro Morão","Joao Santinha","Yasna Forghani","Nuno Loução","Pedro Gouveia","Mario A. T. Figueiredo"],"pdf_url":"https://arxiv.org/pdf/2410.23835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07966v3","updated":"2024-10-31T11:25:40Z","published":"2024-09-12T11:53:05Z","title":"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D\n  Facial Animation Synthesis Using VQ-VAE","summary":"  Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).\n","authors":["Sichun Wu","Kazi Injamamul Haque","Zerrin Yumak"],"pdf_url":"https://arxiv.org/pdf/2409.07966v3.pdf","comment":"14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM\n  SIGGRAPH MIG 2024"},{"id":"http://arxiv.org/abs/2410.23834v1","updated":"2024-10-31T11:23:19Z","published":"2024-10-31T11:23:19Z","title":"Denoising Diffusion Models for Anomaly Localization in Medical Images","summary":"  This chapter explores anomaly localization in medical images using denoising\ndiffusion models. After providing a brief methodological background of these\nmodels, including their application to image reconstruction and their\nconditioning using guidance mechanisms, we provide an overview of available\ndatasets and evaluation metrics suitable for their application to anomaly\nlocalization in medical images. In this context, we discuss supervision schemes\nranging from fully supervised segmentation to semi-supervised, weakly\nsupervised, self-supervised, and unsupervised methods, and provide insights\ninto the effectiveness and limitations of these approaches. Furthermore, we\nhighlight open challenges in anomaly localization, including detection bias,\ndomain shift, computational cost, and model interpretability. Our goal is to\nprovide an overview of the current state of the art in the field, outline\nresearch gaps, and highlight the potential of diffusion models for robust\nanomaly localization in medical images.\n","authors":["Cosmin I. Bercea","Philippe C. Cattin","Julia A. Schnabel","Julia Wolleb"],"pdf_url":"https://arxiv.org/pdf/2410.23834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23831v1","updated":"2024-10-31T11:21:21Z","published":"2024-10-31T11:21:21Z","title":"FRoundation: Are Foundation Models Ready for Face Recognition?","summary":"  Foundation models are predominantly trained in an unsupervised or\nself-supervised manner on highly diverse and large-scale datasets, making them\nbroadly applicable to various downstream tasks. In this work, we investigate\nfor the first time whether such models are suitable for the specific domain of\nface recognition. We further propose and demonstrate the adaptation of these\nmodels for face recognition across different levels of data availability.\nExtensive experiments are conducted on multiple foundation models and datasets\nof varying scales for training and fine-tuning, with evaluation on a wide range\nof benchmarks. Our results indicate that, despite their versatility,\npre-trained foundation models underperform in face recognition compared to\nsimilar architectures trained specifically for this task. However, fine-tuning\nfoundation models yields promising results, often surpassing models trained\nfrom scratch when training data is limited. Even with access to large-scale\nface recognition training datasets, fine-tuned foundation models perform\ncomparably to models trained from scratch, but with lower training\ncomputational costs and without relying on the assumption of extensive data\navailability. Our analysis also explores bias in face recognition, with\nslightly higher bias observed in some settings when using foundation models.\n","authors":["Tahar Chettaoui","Naser Damer","Fadi Boutros"],"pdf_url":"https://arxiv.org/pdf/2410.23831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23828v1","updated":"2024-10-31T11:20:13Z","published":"2024-10-31T11:20:13Z","title":"Show Me What and Where has Changed? Question Answering and Grounding for\n  Remote Sensing Change Detection","summary":"  Remote sensing change detection aims to perceive changes occurring on the\nEarth's surface from remote sensing data in different periods, and feed these\nchanges back to humans. However, most existing methods only focus on detecting\nchange regions, lacking the ability to interact with users to identify changes\nthat the users expect. In this paper, we introduce a new task named Change\nDetection Question Answering and Grounding (CDQAG), which extends the\ntraditional change detection task by providing interpretable textual answers\nand intuitive visual evidence. To this end, we construct the first CDQAG\nbenchmark dataset, termed QAG-360K, comprising over 360K triplets of questions,\ntextual answers, and corresponding high-quality visual masks. It encompasses 10\nessential land-cover categories and 8 comprehensive question types, which\nprovides a large-scale and diverse dataset for remote sensing applications.\nBased on this, we present VisTA, a simple yet effective baseline method that\nunifies the tasks of question answering and grounding by delivering both visual\nand textual answers. Our method achieves state-of-the-art results on both the\nclassic CDVQA and the proposed CDQAG datasets. Extensive qualitative and\nquantitative experimental results provide useful insights for the development\nof better CDQAG models, and we hope that our work can inspire further research\nin this important yet underexplored direction. The proposed benchmark dataset\nand method are available at https://github.com/like413/VisTA.\n","authors":["Ke Li","Fuyu Dong","Di Wang","Shaofeng Li","Quan Wang","Xinbo Gao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.23828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01762v4","updated":"2024-10-31T11:11:24Z","published":"2023-05-27T06:00:51Z","title":"Rapid Plug-in Defenders","summary":"  In the realm of daily services, the deployment of deep neural networks\nunderscores the paramount importance of their reliability. However, the\nvulnerability of these networks to adversarial attacks, primarily\nevasion-based, poses a concerning threat to their functionality. Common methods\nfor enhancing robustness involve heavy adversarial training or leveraging\nlearned knowledge from clean data, both necessitating substantial computational\nresources. This inherent time-intensive nature severely limits the agility of\nlarge foundational models to swiftly counter adversarial perturbations. To\naddress this challenge, this paper focuses on the Rapid Plug-in Defender\n(RaPiD) problem, aiming to rapidly counter adversarial perturbations without\naltering the deployed model. Drawing inspiration from the generalization and\nthe universal computation ability of pre-trained transformer models, we propose\na novel method termed CeTaD (Considering Pre-trained Transformers as Defenders)\nfor RaPiD, optimized for efficient computation. CeTaD strategically fine-tunes\nthe normalization layer parameters within the defender using a limited set of\nclean and adversarial examples. Our evaluation centers on assessing CeTaD's\neffectiveness, transferability, and the impact of different components in\nscenarios involving one-shot adversarial examples. The proposed method is\ncapable of rapidly adapting to various attacks and different application\nscenarios without altering the target model and clean training data. We also\nexplore the influence of varying training data conditions on CeTaD's\nperformance. Notably, CeTaD exhibits adaptability across differentiable service\nmodels and proves the potential of continuous learning.\n","authors":["Kai Wu","Yujian Betterest Li","Jian Lou","Xiaoyu Zhang","Handing Wang","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2306.01762v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23822v1","updated":"2024-10-31T11:07:26Z","published":"2024-10-31T11:07:26Z","title":"Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models\n  for Medical Visual Grounding","summary":"  Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review.\n","authors":["Jinlong He","Pengfei Li","Gang Liu","Shenjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.23822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23820v1","updated":"2024-10-31T11:05:09Z","published":"2024-10-31T11:05:09Z","title":"Disentangling Disentangled Representations: Towards Improved Latent\n  Units via Diffusion Models","summary":"  Disentangled representation learning (DRL) aims to break down observed data\ninto core intrinsic factors for a profound understanding of the data. In\nreal-world scenarios, manually defining and labeling these factors are\nnon-trivial, making unsupervised methods attractive. Recently, there have been\nlimited explorations of utilizing diffusion models (DMs), which are already\nmainstream in generative modeling, for unsupervised DRL. They implement their\nown inductive bias to ensure that each latent unit input to the DM expresses\nonly one distinct factor. In this context, we design Dynamic Gaussian Anchoring\nto enforce attribute-separated latent units for more interpretable DRL. This\nunconventional inductive bias explicitly delineates the decision boundaries\nbetween attributes while also promoting the independence among latent units.\nAdditionally, we also propose Skip Dropout technique, which easily modifies the\ndenoising U-Net to be more DRL-friendly, addressing its uncooperative nature\nwith the disentangling feature extractor. Our methods, which carefully consider\nthe latent unit semantics and the distinct DM structure, enhance the\npracticality of DM-based disentangled representations, demonstrating\nstate-of-the-art disentanglement performance on both synthetic and real data,\nas well as advantages in downstream tasks.\n","authors":["Youngjun Jun","Jiwoo Park","Kyobin Choo","Tae Eun Choi","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.23820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23806v1","updated":"2024-10-31T10:46:11Z","published":"2024-10-31T10:46:11Z","title":"Human Action Recognition (HAR) Using Skeleton-based Quantum Spatial\n  Temporal Relative Transformer Network: ST-RTR","summary":"  Quantum Human Action Recognition (HAR) is an interesting research area in\nhuman-computer interaction used to monitor the activities of elderly and\ndisabled individuals affected by physical and mental health. In the recent era,\nskeleton-based HAR has received much attention because skeleton data has shown\nthat it can handle changes in striking, body size, camera views, and complex\nbackgrounds. One key characteristic of ST-GCN is automatically learning spatial\nand temporal patterns from skeleton sequences. It has some limitations, as this\nmethod only works for short-range correlation due to its limited receptive\nfield. Consequently, understanding human action requires long-range\ninterconnection. To address this issue, we developed a quantum spatial-temporal\nrelative transformer ST-RTR model. The ST-RTR includes joint and relay nodes,\nwhich allow efficient communication and data transmission within the network.\nThese nodes help to break the inherent spatial and temporal skeleton\ntopologies, which enables the model to understand long-range human action\nbetter. Furthermore, we combine quantum ST-RTR with a fusion model for further\nperformance improvements. To assess the performance of the quantum ST-RTR\nmethod, we conducted experiments on three skeleton-based HAR benchmarks: NTU\nRGB+D 60, NTU RGB+D 120, and UAV-Human. It boosted CS and CV by 2.11 % and\n1.45% on NTU RGB+D 60, 1.25% and 1.05% on NTU RGB+D 120. On UAV-Human datasets,\naccuracy improved by 2.54%. The experimental outcomes explain that the proposed\nST-RTR model significantly improves action recognition associated with the\nstandard ST-GCN method.\n","authors":["Faisal Mehmood","Enqing Chen","Touqeer Abbas","Samah M. Alzanin"],"pdf_url":"https://arxiv.org/pdf/2410.23806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23800v1","updated":"2024-10-31T10:35:59Z","published":"2024-10-31T10:35:59Z","title":"SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild","summary":"  Self-occlusion is common when capturing people in the wild, where the\nperformer do not follow predefined motion scripts. This challenges existing\nmonocular human reconstruction systems that assume full body visibility. We\nintroduce Self-Occluded Avatar Recovery (SOAR), a method for complete human\nreconstruction from partial observations where parts of the body are entirely\nunobserved. SOAR leverages structural normal prior and generative diffusion\nprior to address such an ill-posed reconstruction problem. For structural\nnormal prior, we model human with an reposable surfel model with well-defined\nand easily readable shapes. For generative diffusion prior, we perform an\ninitial reconstruction and refine it using score distillation. On various\nbenchmarks, we show that SOAR performs favorably than state-of-the-art\nreconstruction and generation methods, and on-par comparing to concurrent\nworks. Additional video results and code are available at\nhttps://soar-avatar.github.io/.\n","authors":["Zhuoyang Pan","Angjoo Kanazawa","Hang Gao"],"pdf_url":"https://arxiv.org/pdf/2410.23800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23788v1","updated":"2024-10-31T10:13:05Z","published":"2024-10-31T10:13:05Z","title":"EDT: An Efficient Diffusion Transformer Framework Inspired by Human-like\n  Sketching","summary":"  Transformer-based Diffusion Probabilistic Models (DPMs) have shown more\npotential than CNN-based DPMs, yet their extensive computational requirements\nhinder widespread practical applications. To reduce the computation budget of\ntransformer-based DPMs, this work proposes the Efficient Diffusion Transformer\n(EDT) framework. The framework includes a lightweight-design diffusion model\narchitecture, and a training-free Attention Modulation Matrix and its\nalternation arrangement in EDT inspired by human-like sketching. Additionally,\nwe propose a token relation-enhanced masking training strategy tailored\nexplicitly for EDT to augment its token relation learning capability. Our\nextensive experiments demonstrate the efficacy of EDT. The EDT framework\nreduces training and inference costs and surpasses existing transformer-based\ndiffusion models in image synthesis performance, thereby achieving a\nsignificant overall enhancement. With lower FID, EDT-S, EDT-B, and EDT-XL\nattained speed-ups of 3.93x, 2.84x, and 1.92x respectively in the training\nphase, and 2.29x, 2.29x, and 2.22x respectively in inference, compared to the\ncorresponding sizes of MDTv2. The source code is released at\nhttps://github.com/xinwangChen/EDT.\n","authors":["Xinwang Chen","Ning Liu","Yichen Zhu","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2410.23788v1.pdf","comment":"Xinwang Chen and Ning Liu are with equal contributions. This paper\n  has been accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.07085v2","updated":"2024-10-31T10:06:16Z","published":"2024-06-11T09:22:39Z","title":"CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor\n  Segmentation","summary":"  Existing promptable segmentation methods in the medical imaging field\nprimarily consider either textual or visual prompts to segment relevant\nobjects, yet they often fall short when addressing anomalies in medical images,\nlike tumors, which may vary greatly in shape, size, and appearance. Recognizing\nthe complexity of medical scenarios and the limitations of textual or visual\nprompts, we propose a novel dual-prompt schema that leverages the complementary\nstrengths of visual and textual prompts for segmenting various organs and\ntumors. Specifically, we introduce CAT, an innovative model that Coordinates\nAnatomical prompts derived from 3D cropped images with Textual prompts enriched\nby medical domain knowledge. The model architecture adopts a general\nquery-based design, where prompt queries facilitate segmentation queries for\nmask prediction. To synergize two types of prompts within a unified framework,\nwe implement a ShareRefiner, which refines both segmentation and prompt queries\nwhile disentangling the two types of prompts. Trained on a consortium of 10\npublic CT datasets, CAT demonstrates superior performance in multiple\nsegmentation tasks. Further validation on a specialized in-house dataset\nreveals the remarkable capacity of segmenting tumors across multiple cancer\nstages. This approach confirms that coordinating multimodal prompts is a\npromising avenue for addressing complex scenarios in the medical domain.\n","authors":["Zhongzhen Huang","Yankai Jiang","Rongzhao Zhang","Shaoting Zhang","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10897v2","updated":"2024-10-31T10:01:59Z","published":"2024-07-15T16:46:14Z","title":"Optical Diffusion Models for Image Generation","summary":"  Diffusion models generate new samples by progressively decreasing the noise\nfrom the initially provided random distribution. This inference procedure\ngenerally utilizes a trained neural network numerous times to obtain the final\noutput, creating significant latency and energy consumption on digital\nelectronic hardware such as GPUs. In this study, we demonstrate that the\npropagation of a light beam through a semi-transparent medium can be programmed\nto implement a denoising diffusion model on image samples. This framework\nprojects noisy image patterns through passive diffractive optical layers, which\ncollectively only transmit the predicted noise term in the image. The optical\ntransparent layers, which are trained with an online training approach,\nbackpropagating the error to the analytical model of the system, are passive\nand kept the same across different steps of denoising. Hence this method\nenables high-speed image generation with minimal power consumption, benefiting\nfrom the bandwidth and energy efficiency of optical information processing.\n","authors":["Ilker Oguz","Niyazi Ulas Dinc","Mustafa Yildirim","Junjie Ke","Innfarn Yoo","Qifei Wang","Feng Yang","Christophe Moser","Demetri Psaltis"],"pdf_url":"https://arxiv.org/pdf/2407.10897v2.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.23782v1","updated":"2024-10-31T09:55:32Z","published":"2024-10-31T09:55:32Z","title":"Video Token Merging for Long-form Video Understanding","summary":"  As the scale of data and models for video understanding rapidly expand,\nhandling long-form video input in transformer-based models presents a practical\nchallenge. Rather than resorting to input sampling or token dropping, which may\nresult in information loss, token merging shows promising results when used in\ncollaboration with transformers. However, the application of token merging for\nlong-form video processing is not trivial. We begin with the premise that token\nmerging should not rely solely on the similarity of video tokens; the saliency\nof tokens should also be considered. To address this, we explore various video\ntoken merging strategies for long-form video classification, starting with a\nsimple extension of image token merging, moving to region-concentrated merging,\nand finally proposing a learnable video token merging (VTM) algorithm that\ndynamically merges tokens based on their saliency. Extensive experimental\nresults show that we achieve better or comparable performances on the LVU,\nCOIN, and Breakfast datasets. Moreover, our approach significantly reduces\nmemory costs by 84% and boosts throughput by approximately 6.89 times compared\nto baseline algorithms.\n","authors":["Seon-Ho Lee","Jue Wang","Zhikang Zhang","David Fan","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.23782v1.pdf","comment":"21 pages, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23780v1","updated":"2024-10-31T09:53:21Z","published":"2024-10-31T09:53:21Z","title":"Driving by the Rules: A Benchmark for Integrating Traffic Sign\n  Regulations into Vectorized HD Map","summary":"  Ensuring adherence to traffic sign regulations is essential for both human\nand autonomous vehicle navigation. While current benchmark datasets concentrate\non lane perception or basic traffic sign recognition, they often overlook the\nintricate task of integrating these regulations into lane operations.\nAddressing this gap, we introduce MapDR, a novel dataset designed for the\nextraction of Driving Rules from traffic signs and their association with\nvectorized, locally perceived HD Maps. MapDR features over 10,000 annotated\nvideo clips that capture the intricate correlation between traffic sign\nregulations and lanes. We define two pivotal sub-tasks: 1) Rule Extraction from\nTraffic Sign, which accurately deciphers regulatory instructions, and 2)\nRule-Lane Correspondence Reasoning, which aligns these rules with their\nrespective lanes. Built upon this benchmark, we provide a multimodal solution\nthat offers a strong baseline for advancing autonomous driving technologies. It\nfills a critical gap in the integration of traffic sign rules, contributing to\nthe development of reliable autonomous navigation systems.\n","authors":["Xinyuan Chang","Maixuan Xue","Xinran Liu","Zheng Pan","Xing Wei"],"pdf_url":"https://arxiv.org/pdf/2410.23780v1.pdf","comment":"27 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.23775v1","updated":"2024-10-31T09:45:00Z","published":"2024-10-31T09:45:00Z","title":"In-Context LoRA for Diffusion Transformers","summary":"  Recent research arXiv:2410.15027 has explored the use of diffusion\ntransformers (DiTs) for task-agnostic image generation by simply concatenating\nattention tokens across images. However, despite substantial computational\nresources, the fidelity of the generated images remains suboptimal. In this\nstudy, we reevaluate and streamline this framework by hypothesizing that\ntext-to-image DiTs inherently possess in-context generation capabilities,\nrequiring only minimal tuning to activate them. Through diverse task\nexperiments, we qualitatively demonstrate that existing text-to-image DiTs can\neffectively perform in-context generation without any tuning. Building on this\ninsight, we propose a remarkably simple pipeline to leverage the in-context\nabilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint\ncaptioning of multiple images, and (3) apply task-specific LoRA tuning using\nsmall datasets (e.g., $20\\sim 100$ samples) instead of full-parameter tuning\nwith large datasets. We name our models In-Context LoRA (IC-LoRA). This\napproach requires no modifications to the original DiT models, only changes to\nthe training data. Remarkably, our pipeline generates high-fidelity image sets\nthat better adhere to prompts. While task-specific in terms of tuning data, our\nframework remains task-agnostic in architecture and pipeline, offering a\npowerful tool for the community and providing valuable insights for further\nresearch on product-level task-agnostic generation systems. We release our\ncode, data, and models at https://github.com/ali-vilab/In-Context-LoRA\n","authors":["Lianghua Huang","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Huanzhang Dou","Chen Liang","Yutong Feng","Yu Liu","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23775v1.pdf","comment":"Project page: https://ali-vilab.github.io/In-Context-Lora-Page/"},{"id":"http://arxiv.org/abs/2410.14790v2","updated":"2024-10-31T09:34:45Z","published":"2024-10-02T13:42:38Z","title":"SSL-NBV: A Self-Supervised-Learning-Based Next-Best-View algorithm for\n  Efficient 3D Plant Reconstruction by a Robot","summary":"  The 3D reconstruction of plants is challenging due to their complex shape\ncausing many occlusions. Next-Best-View (NBV) methods address this by\niteratively selecting new viewpoints to maximize information gain (IG).\nDeep-learning-based NBV (DL-NBV) methods demonstrate higher computational\nefficiency over classic voxel-based NBV approaches but current methods require\nextensive training using ground-truth plant models, making them impractical for\nreal-world plants. These methods, moreover, rely on offline training with\npre-collected data, limiting adaptability in changing agricultural\nenvironments. This paper proposes a self-supervised learning-based NBV method\n(SSL-NBV) that uses a deep neural network to predict the IG for candidate\nviewpoints. The method allows the robot to gather its own training data during\ntask execution by comparing new 3D sensor data to the earlier gathered data and\nby employing weakly-supervised learning and experience replay for efficient\nonline learning. Comprehensive evaluations were conducted in simulation and\nreal-world environments using cross-validation. The results showed that SSL-NBV\nrequired fewer views for plant reconstruction than non-NBV methods and was over\n800 times faster than a voxel-based method. SSL-NBV reduced training\nannotations by over 90% compared to a baseline DL-NBV. Furthermore, SSL-NBV\ncould adapt to novel scenarios through online fine-tuning. Also using real\nplants, the results showed that the proposed method can learn to effectively\nplan new viewpoints for 3D plant reconstruction. Most importantly, SSL-NBV\nautomated the entire network training and uses continuous online learning,\nallowing it to operate in changing agricultural environments.\n","authors":["Jianchao Ci","Eldert J. van Henten","Xin Wang","Akshay K. Burusa","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2410.14790v2.pdf","comment":"22 pages, 11 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.23767v1","updated":"2024-10-31T09:29:55Z","published":"2024-10-31T09:29:55Z","title":"Open-Set 3D object detection in LiDAR data as an Out-of-Distribution\n  problem","summary":"  3D Object Detection from LiDAR data has achieved industry-ready performance\nin controlled environments through advanced deep learning methods. However,\nthese neural network models are limited by a finite set of inlier object\ncategories. Our work redefines the open-set 3D Object Detection problem in\nLiDAR data as an Out-Of-Distribution (OOD) problem to detect outlier objects.\nThis approach brings additional information in comparison with traditional\nobject detection. We establish a comparative benchmark and show that two-stage\nOOD methods, notably autolabelling, show promising results for 3D OOD Object\nDetection. Our contributions include setting a rigorous evaluation protocol by\nexamining the evaluation of hyperparameters and evaluating strategies for\ngenerating additional data to train an OOD-aware 3D object detector. This\ncomprehensive analysis is essential for developing robust 3D object detection\nsystems that can perform reliably in diverse and unpredictable real-world\nscenarios.\n","authors":["Louis Soum-Fontez","Jean-Emmanuel Deschaud","François Goulette"],"pdf_url":"https://arxiv.org/pdf/2410.23767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23758v1","updated":"2024-10-31T09:25:02Z","published":"2024-10-31T09:25:02Z","title":"Reverse Attitude Statistics Based Star Map Identification Method","summary":"  The star tracker is generally affected by the atmospheric background light\nand the aerodynamic environment when working in near space, which results in\nmissing stars or false stars. Moreover, high-speed maneuvering may cause star\ntrailing, which reduces the accuracy of the star position. To address the\nchallenges for starmap identification, a reverse attitude statistics based\nmethod is proposed to handle position noise, false stars, and missing stars.\nConversely to existing methods which match before solving for attitude, this\nmethod introduces attitude solving into the matching process, and obtains the\nfinal match and the correct attitude simultaneously by frequency statistics.\nFirstly, based on stable angular distance features, the initial matching is\nobtained by utilizing spatial hash indexing. Then, the dual-vector attitude\ndetermination is introduced to calculate potential attitude. Finally, the star\npairs are accurately matched by applying a frequency statistics filtering\nmethod. In addition, Bayesian optimization is employed to find optimal\nparameters under the impact of noises, which is able to enhance the algorithm\nperformance further. In this work, the proposed method is validated in\nsimulation, field test and on-orbit experiment. Compared with the\nstate-of-the-art, the identification rate is improved by more than 14.3%, and\nthe solving time is reduced by over 28.5%.\n","authors":["Shunmei Dong","Qinglong Wang","Haiqing Wang","Qianqian Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23758v1.pdf","comment":"10 pages, 17figures, 4 tables, 4663 words, submitted to IEEE Sensors\n  Journal"},{"id":"http://arxiv.org/abs/2403.09400v3","updated":"2024-10-31T09:21:29Z","published":"2024-03-14T13:50:44Z","title":"ConDiSR: Contrastive Disentanglement and Style Regularization for Single\n  Domain Generalization","summary":"  Medical data often exhibits distribution shifts, which cause test-time\nperformance degradation for deep learning models trained using standard\nsupervised learning pipelines. This challenge is addressed in the field of\nDomain Generalization (DG) with the sub-field of Single Domain Generalization\n(SDG) being specifically interesting due to the privacy- or logistics-related\nissues often associated with medical data. Existing disentanglement-based SDG\nmethods heavily rely on structural information embedded in segmentation masks,\nhowever classification labels do not provide such dense information. This work\nintroduces a novel SDG method aimed at medical image classification that\nleverages channel-wise contrastive disentanglement. It is further enhanced with\nreconstruction-based style regularization to ensure extraction of distinct\nstyle and structure feature representations. We evaluate our method on the\ncomplex task of multicenter histopathology image classification, comparing it\nagainst state-of-the-art (SOTA) SDG baselines. Results demonstrate that our\nmethod surpasses the SOTA by a margin of 1% in average accuracy while also\nshowing more stable performance. This study highlights the importance and\nchallenges of exploring SDG frameworks in the context of the classification\ntask. The code is publicly available at\nhttps://github.com/BioMedIA-MBZUAI/ConDiSR\n","authors":["Aleksandr Matsun","Numan Saeed","Fadillah Adamsyah Maani","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.09400v3.pdf","comment":"A flaw was found in the results acquisition"},{"id":"http://arxiv.org/abs/2410.23751v1","updated":"2024-10-31T09:11:56Z","published":"2024-10-31T09:11:56Z","title":"EXACFS -- A CIL Method to mitigate Catastrophic Forgetting","summary":"  Deep neural networks (DNNS) excel at learning from static datasets but\nstruggle with continual learning, where data arrives sequentially. Catastrophic\nforgetting, the phenomenon of forgetting previously learned knowledge, is a\nprimary challenge. This paper introduces EXponentially Averaged Class-wise\nFeature Significance (EXACFS) to mitigate this issue in the class incremental\nlearning (CIL) setting. By estimating the significance of model features for\neach learned class using loss gradients, gradually aging the significance\nthrough the incremental tasks and preserving the significant features through a\ndistillation loss, EXACFS effectively balances remembering old knowledge\n(stability) and learning new knowledge (plasticity). Extensive experiments on\nCIFAR-100 and ImageNet-100 demonstrate EXACFS's superior performance in\npreserving stability while acquiring plasticity.\n","authors":["S Balasubramanian","M Sai Subramaniam","Sai Sriram Talasu","P Yedu Krishna","Manepalli Pranav Phanindra Sai","Ravi Mukkamala","Darshan Gera"],"pdf_url":"https://arxiv.org/pdf/2410.23751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13495v2","updated":"2024-10-31T09:11:37Z","published":"2024-06-19T12:35:02Z","title":"DF40: Toward Next-Generation Deepfake Detection","summary":"  We propose a new comprehensive benchmark to revolutionize the current\ndeepfake detection field to the next generation. Predominantly, existing works\nidentify top-notch detection algorithms and models by adhering to the common\npractice: training detectors on one specific dataset (e.g., FF++) and testing\nthem on other prevalent deepfake datasets. This protocol is often regarded as a\n\"golden compass\" for navigating SoTA detectors. But can these stand-out\n\"winners\" be truly applied to tackle the myriad of realistic and diverse\ndeepfakes lurking in the real world? If not, what underlying factors contribute\nto this gap? In this work, we found the dataset (both train and test) can be\nthe \"primary culprit\" due to: (1) forgery diversity: Deepfake techniques are\ncommonly referred to as both face forgery and entire image synthesis. Most\nexisting datasets only contain partial types of them, with limited forgery\nmethods implemented; (2) forgery realism: The dominated training dataset, FF++,\ncontains out-of-date forgery techniques from the past four years. \"Honing\nskills\" on these forgeries makes it difficult to guarantee effective detection\ngeneralization toward nowadays' SoTA deepfakes; (3) evaluation protocol: Most\ndetection works perform evaluations on one type, which hinders the development\nof universal deepfake detectors. To address this dilemma, we construct a highly\ndiverse deepfake detection dataset called DF40, which comprises 40 distinct\ndeepfake techniques. We then conduct comprehensive evaluations using 4 standard\nevaluation protocols and 8 representative detection methods, resulting in over\n2,000 evaluations. Through these evaluations, we provide an extensive analysis\nfrom various perspectives, leading to 7 new insightful findings. We also open\nup 4 valuable yet previously underexplored research questions to inspire future\nworks. Our project page is https://github.com/YZY-stack/DF40.\n","authors":["Zhiyuan Yan","Taiping Yao","Shen Chen","Yandan Zhao","Xinghe Fu","Junwei Zhu","Donghao Luo","Chengjie Wang","Shouhong Ding","Yunsheng Wu","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.13495v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2108.05080 by other authors"},{"id":"http://arxiv.org/abs/2405.14702v2","updated":"2024-10-31T09:08:48Z","published":"2024-05-23T15:37:06Z","title":"G3: An Effective and Adaptive Framework for Worldwide Geolocalization\n  Using Large Multi-Modality Models","summary":"  Worldwide geolocalization aims to locate the precise location at the\ncoordinate level of photos taken anywhere on the Earth. It is very challenging\ndue to 1) the difficulty of capturing subtle location-aware visual semantics,\nand 2) the heterogeneous geographical distribution of image data. As a result,\nexisting studies have clear limitations when scaled to a worldwide context.\nThey may easily confuse distant images with similar visual contents, or cannot\nadapt to various locations worldwide with different amounts of relevant data.\nTo resolve these limitations, we propose G3, a novel framework based on\nRetrieval-Augmented Generation (RAG). In particular, G3 consists of three\nsteps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to\noptimize both retrieval and generation phases of worldwide geolocalization.\nDuring Geo-alignment, our solution jointly learns expressive multi-modal\nrepresentations for images, GPS and textual descriptions, which allows us to\ncapture location-aware semantics for retrieving nearby images for a given\nquery. During Geo-diversification, we leverage a prompt ensembling method that\nis robust to inconsistent retrieval performance for different image queries.\nFinally, we combine both retrieved and generated GPS candidates in\nGeo-verification for location prediction. Experiments on two well-established\ndatasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other\nstate-of-the-art methods. Our code and data are available online for\nreproduction.\n","authors":["Pengyue Jia","Yiding Liu","Xiaopeng Li","Yuhao Wang","Yantong Du","Xiao Han","Xuetao Wei","Shuaiqiang Wang","Dawei Yin","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.14702v2.pdf","comment":"Accepted to NeurIPS2024"},{"id":"http://arxiv.org/abs/2312.05508v3","updated":"2024-10-31T09:04:41Z","published":"2023-12-09T09:08:03Z","title":"Improving Adversarial Robust Fairness via Anti-Bias Soft Label\n  Distillation","summary":"  Adversarial Training (AT) has been widely proved to be an effective method to\nimprove the adversarial robustness against adversarial examples for Deep Neural\nNetworks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD)\nhas demonstrated its superior performance in improving the robustness of small\nstudent models with the guidance of large teacher models. However, both AT and\nARD encounter the robust fairness problem: these models exhibit strong\nrobustness when facing part of classes (easy class), but weak robustness when\nfacing others (hard class). In this paper, we give an in-depth analysis of the\npotential factors and argue that the smoothness degree of samples' soft labels\nfor different classes (i.e., hard class or easy class) will affect the robust\nfairness of DNNs from both empirical observation and theoretical analysis.\nBased on the above finding, we propose an Anti-Bias Soft Label Distillation\n(ABSLD) method to mitigate the adversarial robust fairness problem within the\nframework of Knowledge Distillation (KD). Specifically, ABSLD adaptively\nreduces the student's error risk gap between different classes to achieve\nfairness by adjusting the class-wise smoothness degree of samples' soft labels\nduring the training process, and the smoothness degree of soft labels is\ncontrolled by assigning different temperatures in KD to different classes.\nExtensive experiments demonstrate that ABSLD outperforms state-of-the-art AT,\nARD, and robust fairness methods in the comprehensive metric (Normalized\nStandard Deviation) of robustness and fairness.\n","authors":["Shiji Zhao","Ranjie Duan","Xizhe Wang","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2312.05508v3.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.23744v1","updated":"2024-10-31T08:59:34Z","published":"2024-10-31T08:59:34Z","title":"EchoNarrator: Generating natural text explanations for ejection fraction\n  predictions","summary":"  Ejection fraction (EF) of the left ventricle (LV) is considered as one of the\nmost important measurements for diagnosing acute heart failure and can be\nestimated during cardiac ultrasound acquisition. While recent successes in deep\nlearning research successfully estimate EF values, the proposed models often\nlack an explanation for the prediction. However, providing clear and intuitive\nexplanations for clinical measurement predictions would increase the trust of\ncardiologists in these models. In this paper, we explore predicting EF\nmeasurements with Natural Language Explanation (NLE). We propose a model that\nin a single forward pass combines estimation of the LV contour over multiple\nframes, together with a set of modules and routines for computing various\nmotion and shape attributes that are associated with ejection fraction. It then\nfeeds the attributes into a large language model to generate text that helps to\nexplain the network's outcome in a human-like manner. We provide experimental\nevaluation of our explanatory output, as well as EF prediction, and show that\nour model can provide EF comparable to state-of-the-art together with\nmeaningful and accurate natural language explanation to the prediction. The\nproject page can be found at https://github.com/guybenyosef/EchoNarrator .\n","authors":["Sarina Thomas","Qing Cao","Anna Novikova","Daria Kulikova","Guy Ben-Yosef"],"pdf_url":"https://arxiv.org/pdf/2410.23744v1.pdf","comment":"accepted for MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.15223v3","updated":"2024-10-31T08:58:08Z","published":"2024-05-24T05:29:12Z","title":"iVideoGPT: Interactive VideoGPTs are Scalable World Models","summary":"  World models empower model-based agents to interactively explore, reason, and\nplan within imagined environments for real-world decision-making. However, the\nhigh demand for interactivity poses challenges in harnessing recent\nadvancements in video generative models for developing world models at scale.\nThis work introduces Interactive VideoGPT (iVideoGPT), a scalable\nautoregressive transformer framework that integrates multimodal signals--visual\nobservations, actions, and rewards--into a sequence of tokens, facilitating an\ninteractive experience of agents via next-token prediction. iVideoGPT features\na novel compressive tokenization technique that efficiently discretizes\nhigh-dimensional visual observations. Leveraging its scalable architecture, we\nare able to pre-train iVideoGPT on millions of human and robotic manipulation\ntrajectories, establishing a versatile foundation that is adaptable to serve as\ninteractive world models for a wide range of downstream tasks. These include\naction-conditioned video prediction, visual planning, and model-based\nreinforcement learning, where iVideoGPT achieves competitive performance\ncompared with state-of-the-art methods. Our work advances the development of\ninteractive general world models, bridging the gap between generative video\nmodels and practical model-based reinforcement learning applications. Code and\npre-trained models are available at https://thuml.github.io/iVideoGPT.\n","authors":["Jialong Wu","Shaofeng Yin","Ningya Feng","Xu He","Dong Li","Jianye Hao","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2405.15223v3.pdf","comment":"NeurIPS 2024. Code is available at project website:\n  https://thuml.github.io/iVideoGPT"},{"id":"http://arxiv.org/abs/2410.23742v1","updated":"2024-10-31T08:58:00Z","published":"2024-10-31T08:58:00Z","title":"Scaled Inverse Graphics: Efficiently Learning Large Sets of 3D Scenes","summary":"  While the field of inverse graphics has been witnessing continuous growth,\ntechniques devised thus far predominantly focus on learning individual scene\nrepresentations. In contrast, learning large sets of scenes has been a\nconsiderable bottleneck in NeRF developments, as repeatedly applying inverse\ngraphics on a sequence of scenes, though essential for various applications,\nremains largely prohibitive in terms of resource costs. We introduce a\nframework termed \"scaled inverse graphics\", aimed at efficiently learning large\nsets of scene representations, and propose a novel method to this end. It\noperates in two stages: (i) training a compression model on a subset of scenes,\nthen (ii) training NeRF models on the resulting smaller representations,\nthereby reducing the optimization space per new scene. In practice, we compact\nthe representation of scenes by learning NeRFs in a latent space to reduce the\nimage resolution, and sharing information across scenes to reduce NeRF\nrepresentation complexity. We experimentally show that our method presents both\nthe lowest training time and memory footprint in scaled inverse graphics\ncompared to other methods applied independently on each scene. Our codebase is\npublicly available as open-source. Our project page can be found at\nhttps://scaled-ig.github.io .\n","authors":["Karim Kassab","Antoine Schnepf","Jean-Yves Franceschi","Laurent Caraffa","Flavian Vasile","Jeremie Mary","Andrew Comport","Valérie Gouet-Brunet"],"pdf_url":"https://arxiv.org/pdf/2410.23742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23738v1","updated":"2024-10-31T08:54:23Z","published":"2024-10-31T08:54:23Z","title":"MLLA-UNet: Mamba-like Linear Attention in an Efficient U-Shape Model for\n  Medical Image Segmentation","summary":"  Recent advancements in medical imaging have resulted in more complex and\ndiverse images, with challenges such as high anatomical variability, blurred\ntissue boundaries, low organ contrast, and noise. Traditional segmentation\nmethods struggle to address these challenges, making deep learning approaches,\nparticularly U-shaped architectures, increasingly prominent. However, the\nquadratic complexity of standard self-attention makes Transformers\ncomputationally prohibitive for high-resolution images. To address these\nchallenges, we propose MLLA-UNet (Mamba-Like Linear Attention UNet), a novel\narchitecture that achieves linear computational complexity while maintaining\nhigh segmentation accuracy through its innovative combination of linear\nattention and Mamba-inspired adaptive mechanisms, complemented by an efficient\nsymmetric sampling structure for enhanced feature processing. Our architecture\neffectively preserves essential spatial features while capturing long-range\ndependencies at reduced computational complexity. Additionally, we introduce a\nnovel sampling strategy for multi-scale feature fusion. Experiments demonstrate\nthat MLLA-UNet achieves state-of-the-art performance on six challenging\ndatasets with 24 different segmentation tasks, including but not limited to\nFLARE22, AMOS CT, and ACDC, with an average DSC of 88.32%. These results\nunderscore the superiority of MLLA-UNet over existing methods. Our\ncontributions include the novel 2D segmentation architecture and its empirical\nvalidation. The code is available via https://github.com/csyfjiang/MLLA-UNet.\n","authors":["Yufeng Jiang","Zongxi Li","Xiangyan Chen","Haoran Xie","Jing Cai"],"pdf_url":"https://arxiv.org/pdf/2410.23738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23736v1","updated":"2024-10-31T08:49:05Z","published":"2024-10-31T08:49:05Z","title":"MoTaDual: Modality-Task Dual Alignment for Enhanced Zero-shot Composed\n  Image Retrieval","summary":"  Composed Image Retrieval (CIR) is a challenging vision-language task,\nutilizing bi-modal (image+text) queries to retrieve target images. Despite the\nimpressive performance of supervised CIR, the dependence on costly,\nmanually-labeled triplets limits its scalability and zero-shot capability. To\naddress this issue, zero-shot composed image retrieval (ZS-CIR) is presented\nalong with projection-based approaches. However, such methods face two major\nproblems, i.e., task discrepancy between pre-training (image $\\leftrightarrow$\ntext) and inference (image+text $\\rightarrow$ image), and modality discrepancy.\nThe latter pertains to approaches based on text-only projection training due to\nthe necessity of feature extraction from the reference image during inference.\nIn this paper, we propose a two-stage framework to tackle both discrepancies.\nFirst, to ensure efficiency and scalability, a textual inversion network is\npre-trained on large-scale caption datasets. Subsequently, we put forward\nModality-Task Dual Alignment (MoTaDual) as the second stage, where\nlarge-language models (LLMs) generate triplet data for fine-tuning, and\nadditionally, prompt learning is introduced in a multi-modal context to\neffectively alleviate both modality and task discrepancies. The experimental\nresults show that our MoTaDual achieves the state-of-the-art performance across\nfour widely used ZS-CIR benchmarks, while maintaining low training time and\ncomputational cost. The code will be released soon.\n","authors":["Haiwen Li","Fei Su","Zhicheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.23736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08845v4","updated":"2024-10-31T08:38:26Z","published":"2024-06-13T06:09:22Z","title":"Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing\n  Reliability,Reproducibility, and Practicality","summary":"  Recent text-to-video (T2V) technology advancements, as demonstrated by models\nsuch as Gen2, Pika, and Sora, have significantly broadened its applicability\nand popularity. Despite these strides, evaluating these models poses\nsubstantial challenges. Primarily, due to the limitations inherent in automatic\nmetrics, manual evaluation is often considered a superior method for assessing\nT2V generation. However, existing manual evaluation protocols face\nreproducibility, reliability, and practicality issues. To address these\nchallenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE)\nprotocol, a comprehensive and standardized protocol for T2V models. The T2VHE\nprotocol includes well-defined metrics, thorough annotator training, and an\neffective dynamic evaluation module. Experimental results demonstrate that this\nprotocol not only ensures high-quality annotations but can also reduce\nevaluation costs by nearly 50\\%. We will open-source the entire setup of the\nT2VHE protocol, including the complete protocol workflow, the dynamic\nevaluation component details, and the annotation interface code. This will help\ncommunities establish more sophisticated human assessment protocols.\n","authors":["Tianle Zhang","Langtian Ma","Yuchen Yan","Yuchen Zhang","Kai Wang","Yue Yang","Ziyao Guo","Wenqi Shao","Yang You","Yu Qiao","Ping Luo","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08845v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16666v4","updated":"2024-10-31T08:37:22Z","published":"2024-04-25T15:06:58Z","title":"PhyRecon: Physically Plausible Neural Scene Reconstruction","summary":"  We address the issue of physical implausibility in multi-view neural\nreconstruction. While implicit representations have gained popularity in\nmulti-view 3D reconstruction, previous work struggles to yield physically\nplausible results, limiting their utility in domains requiring rigorous\nphysical accuracy. This lack of plausibility stems from the absence of physics\nmodeling in existing methods and their inability to recover intricate\ngeometrical structures. In this paper, we introduce PHYRECON, the first\napproach to leverage both differentiable rendering and differentiable physics\nsimulation to learn implicit surface representations. PHYRECON features a novel\ndifferentiable particle-based physical simulator built on neural implicit\nrepresentations. Central to this design is an efficient transformation between\nSDF-based implicit representations and explicit surface points via our proposed\nSurface Points Marching Cubes (SP-MC), enabling differentiable learning with\nboth rendering and physical losses. Additionally, PHYRECON models both\nrendering and physical uncertainty to identify and compensate for inconsistent\nand inaccurate monocular geometric priors. The physical uncertainty further\nfacilitates physics-guided pixel sampling to enhance the learning of slender\nstructures. By integrating these techniques, our model supports differentiable\njoint modeling of appearance, geometry, and physics. Extensive experiments\ndemonstrate that PHYRECON significantly improves the reconstruction quality.\nOur results also exhibit superior physical stability in physical simulators,\nwith at least a 40% improvement across all datasets, paving the way for future\nphysics-based applications.\n","authors":["Junfeng Ni","Yixin Chen","Bohan Jing","Nan Jiang","Bin Wang","Bo Dai","Puhao Li","Yixin Zhu","Song-Chun Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2404.16666v4.pdf","comment":"NeurIPS'24. Project page: https://phyrecon.github.io/"},{"id":"http://arxiv.org/abs/2410.23730v1","updated":"2024-10-31T08:33:30Z","published":"2024-10-31T08:33:30Z","title":"An Empirical Analysis of GPT-4V's Performance on Fashion Aesthetic\n  Evaluation","summary":"  Fashion aesthetic evaluation is the task of estimating how well the outfits\nworn by individuals in images suit them. In this work, we examine the zero-shot\nperformance of GPT-4V on this task for the first time. We show that its\npredictions align fairly well with human judgments on our datasets, and also\nfind that it struggles with ranking outfits in similar colors. The code is\navailable at https://github.com/st-tech/gpt4v-fashion-aesthetic-evaluation.\n","authors":["Yuki Hirakawa","Takashi Wada","Kazuya Morishita","Ryotaro Shimizu","Takuya Furusawa","Sai Htaung Kham","Yuki Saito"],"pdf_url":"https://arxiv.org/pdf/2410.23730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13453v2","updated":"2024-10-31T08:29:51Z","published":"2022-03-25T05:27:28Z","title":"Model LEGO: Creating Models Like Disassembling and Assembling Building\n  Blocks","summary":"  With the rapid development of deep learning, the increasing complexity and\nscale of parameters make training a new model increasingly resource-intensive.\nIn this paper, we start from the classic convolutional neural network (CNN) and\nexplore a paradigm that does not require training to obtain new models. Similar\nto the birth of CNN inspired by receptive fields in the biological visual\nsystem, we draw inspiration from the information subsystem pathways in the\nbiological visual system and propose Model Disassembling and Assembling (MDA).\nDuring model disassembling, we introduce the concept of relative contribution\nand propose a component locating technique to extract task-aware components\nfrom trained CNN classifiers. For model assembling, we present the alignment\npadding strategy and parameter scaling strategy to construct a new model\ntailored for a specific task, utilizing the disassembled task-aware components.\nThe entire process is akin to playing with LEGO bricks, enabling arbitrary\nassembly of new models, and providing a novel perspective for model creation\nand reuse. Extensive experiments showcase that task-aware components\ndisassembled from CNN classifiers or new models assembled using these\ncomponents closely match or even surpass the performance of the baseline,\ndemonstrating its promising results for model reuse. Furthermore, MDA exhibits\ndiverse potential applications, with comprehensive experiments exploring model\ndecision route analysis, model compression, knowledge distillation, and more.\nThe code is available at https://github.com/jiaconghu/Model-LEGO.\n","authors":["Jiacong Hu","Jing Gao","Jingwen Ye","Yang Gao","Xingen Wang","Zunlei Feng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2203.13453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22733v2","updated":"2024-10-31T08:26:18Z","published":"2024-10-30T06:39:27Z","title":"ETO:Efficient Transformer-based Local Feature Matching by Organizing\n  Multiple Homography Hypotheses","summary":"  We tackle the efficiency problem of learning local feature matching. Recent\nadvancements have given rise to purely CNN-based and transformer-based\napproaches, each augmented with deep learning techniques. While CNN-based\nmethods often excel in matching speed, transformer-based methods tend to\nprovide more accurate matches. We propose an efficient transformer-based\nnetwork architecture for local feature matching. This technique is built on\nconstructing multiple homography hypotheses to approximate the continuous\ncorrespondence in the real world and uni-directional cross-attention to\naccelerate the refinement. On the YFCC100M dataset, our matching accuracy is\ncompetitive with LoFTR, a state-of-the-art transformer-based architecture,\nwhile the inference speed is boosted to 4 times, even outperforming the\nCNN-based methods. Comprehensive evaluations on other open datasets such as\nMegadepth, ScanNet, and HPatches demonstrate our method's efficacy,\nhighlighting its potential to significantly enhance a wide array of downstream\napplications.\n","authors":["Junjie Ni","Guofeng Zhang","Guanglin Li","Yijin Li","Xinyang Liu","Zhaoyang Huang","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2410.22733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04555v2","updated":"2024-10-31T08:25:08Z","published":"2024-02-07T03:19:02Z","title":"FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language\n  Foundation Models","summary":"  Semantic mapping based on the supervised object detectors is sensitive to\nimage distribution. In real-world environments, the object detection and\nsegmentation performance can lead to a major drop, preventing the use of\nsemantic mapping in a wider domain. On the other hand, the development of\nvision-language foundation models demonstrates a strong zero-shot\ntransferability across data distribution. It provides an opportunity to\nconstruct generalizable instance-aware semantic maps. Hence, this work explores\nhow to boost instance-aware semantic mapping from object detection generated\nfrom foundation models. We propose a probabilistic label fusion method to\npredict close-set semantic classes from open-set label measurements. An\ninstance refinement module merges the over-segmented instances caused by\ninconsistent segmentation. We integrate all the modules into a unified semantic\nmapping system. Reading a sequence of RGB-D input, our work incrementally\nreconstructs an instance-aware semantic map. We evaluate the zero-shot\nperformance of our method in ScanNet and SceneNN datasets. Our method achieves\n40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation\ntask. It outperforms the traditional semantic mapping method significantly.\n","authors":["Chuhao Liu","Ke Wang","Jieqi Shi","Zhijian Qiao","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2402.04555v2.pdf","comment":"Published in IEEE RAL"},{"id":"http://arxiv.org/abs/2312.10175v3","updated":"2024-10-31T08:10:48Z","published":"2023-12-15T19:57:07Z","title":"UniAR: A Unified model for predicting human Attention and Responses on\n  visual content","summary":"  Progress in human behavior modeling involves understanding both implicit,\nearly-stage perceptual behavior, such as human attention, and explicit,\nlater-stage behavior, such as subjective preferences or likes. Yet most prior\nresearch has focused on modeling implicit and explicit human behavior in\nisolation; and often limited to a specific type of visual content. We propose\nUniAR -- a unified model of human attention and preference behavior across\ndiverse visual content. UniAR leverages a multimodal transformer to predict\nsubjective feedback, such as satisfaction or aesthetic quality, along with the\nunderlying human attention or interaction heatmaps and viewing order. We train\nUniAR on diverse public datasets spanning natural images, webpages, and graphic\ndesigns, and achieve SOTA performance on multiple benchmarks across various\nimage domains and behavior modeling tasks. Potential applications include\nproviding instant feedback on the effectiveness of UIs/visual content, and\nenabling designers and content-creation models to optimize their creation for\nhuman-centric improvements.\n","authors":["Peizhao Li","Junfeng He","Gang Li","Rachit Bhargava","Shaolei Shen","Nachiappan Valliappan","Youwei Liang","Hongxiang Gu","Venky Ramachandran","Golnaz Farhadi","Yang Li","Kai J Kohlhoff","Vidhya Navalpakkam"],"pdf_url":"https://arxiv.org/pdf/2312.10175v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23718v1","updated":"2024-10-31T08:08:54Z","published":"2024-10-31T08:08:54Z","title":"GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian\n  Splatting","summary":"  3D Gaussian Splatting (3DGS) has become a crucial method for acquiring 3D\nassets. To protect the copyright of these assets, digital watermarking\ntechniques can be applied to embed ownership information discreetly within 3DGS\nmodels. However, existing watermarking methods for meshes, point clouds, and\nimplicit radiance fields cannot be directly applied to 3DGS models, as 3DGS\nmodels use explicit 3D Gaussians with distinct structures and do not rely on\nneural networks. Naively embedding the watermark on a pre-trained 3DGS can\ncause obvious distortion in rendered images. In our work, we propose an\nuncertainty-based method that constrains the perturbation of model parameters\nto achieve invisible watermarking for 3DGS. At the message decoding stage, the\ncopyright messages can be reliably extracted from both 3D Gaussians and 2D\nrendered images even under various forms of 3D and 2D distortions. We conduct\nextensive experiments on the Blender, LLFF and MipNeRF-360 datasets to validate\nthe effectiveness of our proposed method, demonstrating state-of-the-art\nperformance on both message decoding accuracy and view synthesis quality.\n","authors":["Xiufeng Huang","Ruiqi Li","Yiu-ming Cheung","Ka Chun Cheung","Simon See","Renjie Wan"],"pdf_url":"https://arxiv.org/pdf/2410.23718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22725v2","updated":"2024-10-31T08:08:07Z","published":"2024-10-30T06:17:20Z","title":"One Prompt to Verify Your Models: Black-Box Text-to-Image Models\n  Verification via Non-Transferable Adversarial Attacks","summary":"  Recently, the success of Text-to-Image (T2I) models has led to the rise of\nnumerous third-party platforms, which claim to provide cheaper API services and\nmore flexibility in model options. However, this also raises a new security\nconcern: Are these third-party services truly offering the models they claim?\nTo address this problem, we propose the first T2I model verification method\nnamed Text-to-Image Model Verification via Non-Transferable Adversarial Attacks\n(TVN). The non-transferability of adversarial examples means that these\nexamples are only effective on a target model and ineffective on other models,\nthereby allowing for the verification of the target model. TVN utilizes the\nNon-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize the cosine\nsimilarity of a prompt's text encoding, generating non-transferable adversarial\nprompts. By calculating the CLIP-text scores between the non-transferable\nadversarial prompts without perturbations and the images, we can verify if the\nmodel matches the claimed target model, based on a 3-sigma threshold. The\nexperiments showed that TVN performed well in both closed-set and open-set\nscenarios, achieving a verification accuracy of over 90\\%. Moreover, the\nadversarial prompts generated by TVN significantly reduced the CLIP-text scores\nof the target model, while having little effect on other models.\n","authors":["Ji Guo","Wenbo Jiang","Rui Zhang","Guoming Lu","Hongwei Li"],"pdf_url":"https://arxiv.org/pdf/2410.22725v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20568v2","updated":"2024-10-31T07:43:52Z","published":"2024-10-27T19:45:15Z","title":"Detection of adrenal anomalous findings in spinal CT images using multi\n  model graph aggregation","summary":"  Low back pain is the symptom that is the second most frequently reported to\nprimary care physicians, effecting 50 to 80 percent of the population in a\nlifetime, resulting in multiple referrals of patients suffering from back\nproblems, to CT and MRI scans, which are then examined by radiologists. The\nradiologists examining these spinal scans naturally focus on spinal pathologies\nand might miss other types of abnormalities, and in particular, abdominal ones,\nsuch as malignancies. Nevertheless, the patients whose spine was scanned might\nas well have malignant and other abdominal pathologies. Thus, clinicians have\nsuggested the need for computerized assistance and decision support in\nscreening spinal scans for additional abnormalities. In the current study, We\nhave addressed the important case of detecting suspicious lesions in the\nadrenal glands as an example for the overall methodology we have developed. A\npatient CT scan is integrated from multiple slices with an axial orientation.\nOur method determines whether a patient has an abnormal adrenal gland, and\nlocalises the abnormality if it exists. Our method is composed of three deep\nlearning models; each model has a different task for achieving the final goal.\nWe call our compound method the Multi Model Graph Aggregation MMGA method. The\nnovelty in this study is twofold. First, the use, for an important screening\ntask, of CT scans that are originally focused and tuned for imaging the spine,\nwhich were acquired from patients with potential spinal disorders, for\ndetection of a totally different set of abnormalities such as abdominal Adrenal\nglands pathologies. Second, we have built a complex pipeline architecture\ncomposed from three deep learning models that can be utilized for other organs\n(such as the pancreas or the kidney), or for similar applications, but using\nother types of imaging, such as MRI.\n","authors":["Shabalin Carmel","Shenkman Israel","Shelef Ilan","Ben-Arie Gal","Alex Geftler","Shahar Yuval"],"pdf_url":"https://arxiv.org/pdf/2410.20568v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10667v2","updated":"2024-10-31T07:43:14Z","published":"2024-04-16T15:43:22Z","title":"VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time","summary":"  We introduce VASA, a framework for generating lifelike talking faces with\nappealing visual affective skills (VAS) given a single static image and a\nspeech audio clip. Our premiere model, VASA-1, is capable of not only\ngenerating lip movements that are exquisitely synchronized with the audio, but\nalso producing a large spectrum of facial nuances and natural head motions that\ncontribute to the perception of authenticity and liveliness. The core\ninnovations include a holistic facial dynamics and head movement generation\nmodel that works in a face latent space, and the development of such an\nexpressive and disentangled face latent space using videos. Through extensive\nexperiments including evaluation on a set of new metrics, we show that our\nmethod significantly outperforms previous methods along various dimensions\ncomprehensively. Our method not only delivers high video quality with realistic\nfacial and head dynamics but also supports the online generation of 512x512\nvideos at up to 40 FPS with negligible starting latency. It paves the way for\nreal-time engagements with lifelike avatars that emulate human conversational\nbehaviors.\n","authors":["Sicheng Xu","Guojun Chen","Yu-Xiao Guo","Jiaolong Yang","Chong Li","Zhenyu Zang","Yizhong Zhang","Xin Tong","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2404.10667v2.pdf","comment":"NeurIPS 2024 (Oral) Camera ready. Project webpage:\n  https://www.microsoft.com/en-us/research/project/vasa-1/"},{"id":"http://arxiv.org/abs/2410.23698v1","updated":"2024-10-31T07:41:13Z","published":"2024-10-31T07:41:13Z","title":"Aggregate-and-Adapt Natural Language Prompts for Downstream\n  Generalization of CLIP","summary":"  Large pretrained vision-language models like CLIP have shown promising\ngeneralization capability, but may struggle in specialized domains (e.g.,\nsatellite imagery) or fine-grained classification (e.g., car models) where the\nvisual concepts are unseen or under-represented during pretraining. Prompt\nlearning offers a parameter-efficient finetuning framework that can adapt CLIP\nto downstream tasks even when limited annotation data are available. In this\npaper, we improve prompt learning by distilling the textual knowledge from\nnatural language prompts (either human- or LLM-generated) to provide rich\npriors for those under-represented concepts. We first obtain a prompt\n``summary'' aligned to each input image via a learned prompt aggregator. Then\nwe jointly train a prompt generator, optimized to produce a prompt embedding\nthat stays close to the aggregated summary while minimizing task loss at the\nsame time. We dub such prompt embedding as Aggregate-and-Adapted Prompt\nEmbedding (AAPE). AAPE is shown to be able to generalize to different\ndownstream data distributions and tasks, including vision-language\nunderstanding tasks (e.g., few-shot classification, VQA) and generation tasks\n(image captioning) where AAPE achieves competitive performance. We also show\nAAPE is particularly helpful to handle non-canonical and OOD examples.\nFurthermore, AAPE learning eliminates LLM-based inference cost as required by\nbaselines, and scales better with data and LLM model size.\n","authors":["Chen Huang","Skyler Seto","Samira Abnar","David Grangier","Navdeep Jaitly","Josh Susskind"],"pdf_url":"https://arxiv.org/pdf/2410.23698v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.09774v2","updated":"2024-10-31T07:40:40Z","published":"2024-09-15T15:46:03Z","title":"Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization","summary":"  Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications.\n","authors":["Haoyuan Sun","Bo Xia","Yongzhe Chang","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09774v2.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2410.23690v1","updated":"2024-10-31T07:25:39Z","published":"2024-10-31T07:25:39Z","title":"XRDSLAM: A Flexible and Modular Framework for Deep Learning based SLAM","summary":"  In this paper, we propose a flexible SLAM framework, XRDSLAM. It adopts a\nmodular code design and a multi-process running mechanism, providing highly\nreusable foundational modules such as unified dataset management, 3d\nvisualization, algorithm configuration, and metrics evaluation. It can help\ndevelopers quickly build a complete SLAM system, flexibly combine different\nalgorithm modules, and conduct standardized benchmarking for accuracy and\nefficiency comparison. Within this framework, we integrate several\nstate-of-the-art SLAM algorithms with different types, including NeRF and 3DGS\nbased SLAM, and even odometry or reconstruction algorithms, which demonstrates\nthe flexibility and extensibility. We also conduct a comprehensive comparison\nand evaluation of these integrated algorithms, analyzing the characteristics of\neach. Finally, we contribute all the code, configuration and data to the\nopen-source community, which aims to promote the widespread research and\ndevelopment of SLAM technology within the open-source ecosystem.\n","authors":["Xiaomeng Wang","Nan Wang","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.23690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21991v2","updated":"2024-10-31T07:24:06Z","published":"2024-10-29T12:22:07Z","title":"From Explicit Rules to Implicit Reasoning in an Interpretable Violence\n  Monitoring System","summary":"  Recently, research based on pre-trained models has demonstrated outstanding\nperformance in violence surveillance tasks. However, these black-box systems\nface challenges regarding explainability during training and inference\nprocesses. An important question is how to incorporate explicit knowledge into\nthese implicit models, thereby designing expert-driven and interpretable\nviolence surveillance systems. This paper proposes a new paradigm for weakly\nsupervised violence monitoring (WSVM) called Rule base Violence monitoring\n(RuleVM). The proposed RuleVM uses a dual-branch structure for different\ndesigns for images and text. One of the branches is called the implicit branch,\nwhich uses only visual features for coarse-grained binary classification. In\nthis branch, image feature extraction is divided into two channels: one\nresponsible for extracting scene frames and the other focusing on extracting\nactions. The other branch is called the explicit branch, which utilizes\nlanguage-image alignment to perform fine-grained classification. For the\nlanguage channel design in the explicit branch, the proposed RuleCLIP uses the\nstate-of-the-art YOLO-World model to detect objects and actions in video\nframes, and association rules are identified through data mining methods as\ndescriptions of the video. Leveraging the dual-branch architecture, RuleVM\nachieves interpretable coarse-grained and fine-grained violence surveillance.\nExtensive experiments were conducted on two commonly used benchmarks, and the\nresults show that RuleCLIP achieved the best performance in both coarse-grained\nand fine-grained detection, significantly outperforming existing\nstate-of-the-art methods. Moreover, interpretability experiments uncovered some\ninteresting rules, such as the observation that as the number of people\nincreases, the risk level of violent behavior also rises.\n","authors":["Wen-Dong Jiang","Chih-Yung Chang","Hsiang-Chuan Chang","Diptendu Sinha Roy"],"pdf_url":"https://arxiv.org/pdf/2410.21991v2.pdf","comment":"12 pages,7 figures"},{"id":"http://arxiv.org/abs/2410.23687v1","updated":"2024-10-31T07:22:51Z","published":"2024-10-31T07:22:51Z","title":"Adversarial Attacks of Vision Tasks in the Past 10 Years: A Survey","summary":"  Adversarial attacks, which manipulate input data to undermine model\navailability and integrity, pose significant security threats during machine\nlearning inference. With the advent of Large Vision-Language Models (LVLMs),\nnew attack vectors, such as cognitive bias, prompt injection, and jailbreak\ntechniques, have emerged. Understanding these attacks is crucial for developing\nmore robust systems and demystifying the inner workings of neural networks.\nHowever, existing reviews often focus on attack classifications and lack\ncomprehensive, in-depth analysis. The research community currently needs: 1)\nunified insights into adversariality, transferability, and generalization; 2)\ndetailed evaluations of existing methods; 3) motivation-driven attack\ncategorizations; and 4) an integrated perspective on both traditional and LVLM\nattacks. This article addresses these gaps by offering a thorough summary of\ntraditional and LVLM adversarial attacks, emphasizing their connections and\ndistinctions, and providing actionable insights for future research.\n","authors":["Chiyu Zhang","Xiaogang Xu","Jiafei Wu","Zhe Liu","Lu Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21898v2","updated":"2024-10-31T07:13:08Z","published":"2024-10-29T09:42:54Z","title":"A Longitudinal Analysis of Racial and Gender Bias in New York Times and\n  Fox News Images and Articles","summary":"  The manner in which different racial and gender groups are portrayed in news\ncoverage plays a large role in shaping public opinion. As such, understanding\nhow such groups are portrayed in news media is of notable societal value, and\nhas thus been a significant endeavour in both the computer and social sciences.\nYet, the literature still lacks a longitudinal study examining both the\nfrequency of appearance of different racial and gender groups in online news\narticles, as well as the context in which such groups are discussed. To fill\nthis gap, we propose two machine learning classifiers to detect the race and\nage of a given subject. Next, we compile a dataset of 123,337 images and\n441,321 online news articles from New York Times (NYT) and Fox News (Fox), and\nexamine representation through two computational approaches. Firstly, we\nexamine the frequency and prominence of appearance of racial and gender groups\nin images embedded in news articles, revealing that racial and gender\nminorities are largely under-represented, and when they do appear, they are\nfeatured less prominently compared to majority groups. Furthermore, we find\nthat NYT largely features more images of racial minority groups compared to\nFox. Secondly, we examine both the frequency and context with which racial\nminority groups are presented in article text. This reveals the narrow scope in\nwhich certain racial groups are covered and the frequency with which different\ngroups are presented as victims and/or perpetrators in a given conflict. Taken\ntogether, our analysis contributes to the literature by providing two novel\nopen-source classifiers to detect race and age from images, and shedding light\non the racial and gender biases in news articles from venues on opposite ends\nof the American political spectrum.\n","authors":["Hazem Ibrahim","Nouar AlDahoul","Syed Mustafa Ali Abbasi","Fareed Zaffar","Talal Rahwan","Yasir Zaki"],"pdf_url":"https://arxiv.org/pdf/2410.21898v2.pdf","comment":"13 pages, and 11 figures"},{"id":"http://arxiv.org/abs/2410.12419v2","updated":"2024-10-31T07:11:29Z","published":"2024-10-16T10:04:22Z","title":"Mind the Context: Attention-Guided Weak-to-Strong Consistency for\n  Enhanced Semi-Supervised Medical Image Segmentation","summary":"  Medical image segmentation is a pivotal step in diagnostic and therapeutic\nprocesses, relying on high-quality annotated data that is often challenging and\ncostly to obtain. Semi-supervised learning offers a promising approach to\nenhance model performance by leveraging unlabeled data. Although weak-to-strong\nconsistency is a prevalent method in semi-supervised image segmentation, there\nis a scarcity of research on perturbation strategies specifically tailored for\nsemi-supervised medical image segmentation tasks. To address this challenge,\nthis paper introduces a simple yet efficient semi-supervised learning framework\nnamed Attention-Guided weak-to-strong Consistency Match (AIGCMatch). The\nAIGCMatch framework incorporates attention-guided perturbation strategies at\nboth the image and feature levels to achieve weak-to-strong consistency\nregularization. This method not only preserves the structural information of\nmedical images but also enhances the model's ability to process complex\nsemantic information. Extensive experiments conducted on the ACDC and ISIC-2017\ndatasets have validated the effectiveness of AIGCMatch. Our method achieved a\n90.4\\% Dice score in the 7-case scenario on the ACDC dataset, surpassing the\nstate-of-the-art methods and demonstrating its potential and efficacy in\nclinical settings. Additionally, on the ISIC-2017 dataset, we significantly\noutperformed our baseline, indicating the robustness and generalizability of\nAIGCMatch across different medical image segmentation tasks.\n","authors":["Yuxuan Cheng","Chenxi Shao","Jie Ma","Guoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22817v2","updated":"2024-10-31T07:07:27Z","published":"2024-10-30T08:51:29Z","title":"Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View\n  Synthesis","summary":"  Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from\nsparse-view observations in a feed-forward inference manner, eliminating the\nneed for scene-specific retraining required in conventional 3DGS. However,\nexisting methods rely heavily on epipolar priors, which can be unreliable in\ncomplex realworld scenes, particularly in non-overlapping and occluded regions.\nIn this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based\nmodel for generalizable novel view synthesis that operates independently of\nepipolar line constraints. To enhance multiview feature extraction with 3D\nperception, we employ a selfsupervised Vision Transformer (ViT) with cross-view\ncompletion pre-training on large-scale datasets. Additionally, we introduce an\nIterative Cross-view Gaussians Alignment method to ensure consistent depth\nscales across different views. Our eFreeSplat represents an innovative approach\nfor generalizable novel view synthesis. Different from the existing pure\ngeometry-free methods, eFreeSplat focuses more on achieving epipolar-free\nfeature matching and encoding by providing 3D priors through cross-view\npretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks\nusing the RealEstate10K and ACID datasets. Extensive experiments demonstrate\nthat eFreeSplat surpasses state-of-the-art baselines that rely on epipolar\npriors, achieving superior geometry reconstruction and novel view synthesis\nquality. Project page: https://tatakai1.github.io/efreesplat/.\n","authors":["Zhiyuan Min","Yawei Luo","Jianwen Sun","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.22817v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23677v1","updated":"2024-10-31T06:55:57Z","published":"2024-10-31T06:55:57Z","title":"Wide Two-Layer Networks can Learn from Adversarial Perturbations","summary":"  Adversarial examples have raised several open questions, such as why they can\ndeceive classifiers and transfer between different models. A prevailing\nhypothesis to explain these phenomena suggests that adversarial perturbations\nappear as random noise but contain class-specific features. This hypothesis is\nsupported by the success of perturbation learning, where classifiers trained\nsolely on adversarial examples and the corresponding incorrect labels\ngeneralize well to correctly labeled test data. Although this hypothesis and\nperturbation learning are effective in explaining intriguing properties of\nadversarial examples, their solid theoretical foundation is limited. In this\nstudy, we theoretically explain the counterintuitive success of perturbation\nlearning. We assume wide two-layer networks and the results hold for any data\ndistribution. We prove that adversarial perturbations contain sufficient\nclass-specific features for networks to generalize from them. Moreover, the\npredictions of classifiers trained on mislabeled adversarial examples coincide\nwith those of classifiers trained on correctly labeled clean samples. The code\nis available at https://github.com/s-kumano/perturbation-learning.\n","authors":["Soichiro Kumano","Hiroshi Kera","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2410.23677v1.pdf","comment":"NeurIPS24"},{"id":"http://arxiv.org/abs/2410.23676v1","updated":"2024-10-31T06:55:24Z","published":"2024-10-31T06:55:24Z","title":"Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach","summary":"  Web-scale visual entity recognition, the task of associating images with\ntheir corresponding entities within vast knowledge bases like Wikipedia,\npresents significant challenges due to the lack of clean, large-scale training\ndata. In this paper, we propose a novel methodology to curate such a dataset,\nleveraging a multimodal large language model (LLM) for label verification,\nmetadata generation, and rationale explanation. Instead of relying on the\nmultimodal LLM to directly annotate data, which we found to be suboptimal, we\nprompt it to reason about potential candidate entity labels by accessing\nadditional contextually relevant information (such as Wikipedia), resulting in\nmore accurate annotations. We further use the multimodal LLM to enrich the\ndataset by generating question-answer pairs and a grounded finegrained textual\ndescription (referred to as \"rationale\") that explains the connection between\nimages and their assigned entities. Experiments demonstrate that models trained\non this automatically curated data achieve state-of-the-art performance on\nweb-scale visual entity recognition tasks (e.g. +6.9% improvement in OVEN\nentity task), underscoring the importance of high-quality training data in this\ndomain.\n","authors":["Mathilde Caron","Alireza Fathi","Cordelia Schmid","Ahmet Iscen"],"pdf_url":"https://arxiv.org/pdf/2410.23676v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.12138v3","updated":"2024-10-31T06:38:27Z","published":"2024-02-19T13:38:15Z","title":"Perceiving Longer Sequences With Bi-Directional Cross-Attention\n  Transformers","summary":"  We present a novel bi-directional Transformer architecture (BiXT) which\nscales linearly with input size in terms of computational cost and memory\nconsumption, but does not suffer the drop in performance or limitation to only\none input modality seen with other efficient Transformer-based approaches. BiXT\nis inspired by the Perceiver architectures but replaces iterative attention\nwith an efficient bi-directional cross-attention module in which input tokens\nand latent variables attend to each other simultaneously, leveraging a\nnaturally emerging attention-symmetry between the two. This approach unlocks a\nkey bottleneck experienced by Perceiver-like architectures and enables the\nprocessing and interpretation of both semantics ('what') and location ('where')\nto develop alongside each other over multiple layers -- allowing its direct\napplication to dense and instance-based tasks alike. By combining efficiency\nwith the generality and performance of a full Transformer architecture, BiXT\ncan process longer sequences like point clouds, text or images at higher\nfeature resolutions and achieves competitive performance across a range of\ntasks like point cloud part segmentation, semantic image segmentation, image\nclassification, hierarchical sequence modeling and document retrieval. Our\nexperiments demonstrate that BiXT models outperform larger competitors by\nleveraging longer sequences more efficiently on vision tasks like\nclassification and segmentation, and perform on par with full Transformer\nvariants on sequence modeling and document retrieval -- but require $28\\%$\nfewer FLOPs and are up to $8.4\\times$ faster.\n","authors":["Markus Hiller","Krista A. Ehinger","Tom Drummond"],"pdf_url":"https://arxiv.org/pdf/2402.12138v3.pdf","comment":"Accepted at NeurIPS 2024; Code and models will be available at\n  https://github.com/mrkshllr/BiXT"},{"id":"http://arxiv.org/abs/2410.23663v1","updated":"2024-10-31T06:26:00Z","published":"2024-10-31T06:26:00Z","title":"DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake\n  Detection","summary":"  With the advancement of deepfake generation techniques, the importance of\ndeepfake detection in protecting multimedia content integrity has become\nincreasingly obvious. Recently, temporal inconsistency clues have been explored\nto improve the generalizability of deepfake video detection. According to our\nobservation, the temporal artifacts of forged videos in terms of motion\ninformation usually exhibits quite distinct inconsistency patterns along\nhorizontal and vertical directions, which could be leveraged to improve the\ngeneralizability of detectors. In this paper, a transformer-based framework for\nDiffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits\ndirectional inconsistencies for deepfake video detection. Specifically, DIP\nbegins with a spatiotemporal encoder to represent spatiotemporal information. A\ndirectional inconsistency decoder is adopted accordingly, where direction-aware\nattention and inconsistency diffusion are incorporated to explore potential\ninconsistency patterns and jointly learn the inherent relationships. In\naddition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to\ncontrast spatiotemporally augmented sample pairs and prevent the model from\noverfitting nonessential forgery artifacts. Extensive experiments on several\npublic datasets demonstrate that our method could effectively identify\ndirectional forgery clues and achieve state-of-the-art performance.\n","authors":["Fan Nie","Jiangqun Ni","Jian Zhang","Bin Zhang","Weizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.23663v1.pdf","comment":"13 pages, accepted with IEEE Trans. on Multimedia"},{"id":"http://arxiv.org/abs/2410.23658v1","updated":"2024-10-31T06:17:16Z","published":"2024-10-31T06:17:16Z","title":"GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring","summary":"  To train a deblurring network, an appropriate dataset with paired blurry and\nsharp images is essential. Existing datasets collect blurry images either\nsynthetically by aggregating consecutive sharp frames or using sophisticated\ncamera systems to capture real blur. However, these methods offer limited\ndiversity in blur types (blur trajectories) or require extensive human effort\nto reconstruct large-scale datasets, failing to fully reflect real-world blur\nscenarios. To address this, we propose GS-Blur, a dataset of synthesized\nrealistic blurry images created using a novel approach. To this end, we first\nreconstruct 3D scenes from multi-view images using 3D Gaussian Splatting\n(3DGS), then render blurry images by moving the camera view along the randomly\ngenerated motion trajectories. By adopting various camera trajectories in\nreconstructing our GS-Blur, our dataset contains realistic and diverse types of\nblur, offering a large-scale dataset that generalizes well to real-world blur.\nUsing GS-Blur with various deblurring methods, we demonstrate its ability to\ngeneralize effectively compared to previous synthetic or real blur datasets,\nshowing significant improvements in deblurring performance.\n","authors":["Dongwoo Lee","Joonkyu Park","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2410.23658v1.pdf","comment":"Accepted at NeurIPS 2024 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.11187v2","updated":"2024-10-31T06:09:11Z","published":"2024-10-15T02:04:05Z","title":"Multiview Scene Graph","summary":"  A proper scene representation is central to the pursuit of spatial\nintelligence where agents can robustly reconstruct and efficiently understand\n3D scenes. A scene representation is either metric, such as landmark maps in 3D\nreconstruction, 3D bounding boxes in object detection, or voxel grids in\noccupancy prediction, or topological, such as pose graphs with loop closures in\nSLAM or visibility graphs in SfM. In this work, we propose to build Multiview\nScene Graphs (MSG) from unposed images, representing a scene topologically with\ninterconnected place and object nodes. The task of building MSG is challenging\nfor existing representation learning methods since it needs to jointly address\nboth visual place recognition, object detection, and object association from\nimages with limited fields of view and potentially large viewpoint changes. To\nevaluate any method tackling this task, we developed an MSG dataset and\nannotation based on a public 3D dataset. We also propose an evaluation metric\nbased on the intersection-over-union score of MSG edges. Moreover, we develop a\nnovel baseline method built on mainstream pretrained vision models, combining\nvisual place recognition and object association into one Transformer decoder\narchitecture. Experiments demonstrate that our method has superior performance\ncompared to existing relevant baselines.\n","authors":["Juexiao Zhang","Gao Zhu","Sihang Li","Xinhao Liu","Haorui Song","Xinran Tang","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2410.11187v2.pdf","comment":"To be published in NeurIPS 2024. Website at\n  https://ai4ce.github.io/MSG/"},{"id":"http://arxiv.org/abs/2405.14325v3","updated":"2024-10-31T05:47:33Z","published":"2024-05-23T08:55:20Z","title":"Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised\n  Anomaly Detection","summary":"  Recent studies highlighted a practical setting of unsupervised anomaly\ndetection (UAD) that builds a unified model for multi-class images, serving as\nan alternative to the conventional one-class-one-model setup. Despite various\nadvancements addressing this challenging task, the detection performance under\nthe multi-class setting still lags far behind state-of-the-art class-separated\nmodels. Our research aims to bridge this substantial performance gap. In this\npaper, we introduce a minimalistic reconstruction-based anomaly detection\nframework, namely Dinomaly, which leverages pure Transformer architectures\nwithout relying on complex designs, additional modules, or specialized tricks.\nGiven this powerful framework consisted of only Attentions and MLPs, we found\nfour simple components that are essential to multi-class anomaly detection: (1)\nFoundation Transformers that extracts universal and discriminative features,\n(2) Noisy Bottleneck where pre-existing Dropouts do all the noise injection\ntricks, (3) Linear Attention that naturally cannot focus, and (4) Loose\nReconstruction that does not force layer-to-layer and point-by-point\nreconstruction. Extensive experiments are conducted across popular anomaly\ndetection benchmarks including MVTec-AD, VisA, and Real-IAD. Our proposed\nDinomaly achieves impressive image-level AUROC of 99.6%, 98.7%, and 89.3% on\nthe three datasets respectively, which is not only superior to state-of-the-art\nmulti-class UAD methods, but also achieves the most advanced class-separated\nUAD records.\n","authors":["Jia Guo","Shuai Lu","Weihang Zhang","Fang Chen","Hongen Liao","Huiqi Li"],"pdf_url":"https://arxiv.org/pdf/2405.14325v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14596v3","updated":"2024-10-31T05:38:39Z","published":"2024-06-20T17:45:02Z","title":"VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs","summary":"  Large-scale generative language and vision-language models excel in\nin-context learning for decision making. However, they require high-quality\nexemplar demonstrations to be included in their context window. In this work,\nwe ask: Can LLMs and VLMs generate their own examples from generic, sub-optimal\ndemonstrations? We propose In-Context Abstraction Learning (ICAL), a method\nthat builds a memory of multimodal experience from sub-optimal demonstrations\nand human feedback. Given a task demonstration that may contain inefficiencies\nor mistakes, a VLM abstracts the trajectory into a generalized program by\ncorrecting inefficient actions and annotating cognitive abstractions: causal\nrelationships, object state changes, temporal subgoals, and task-relevant\nvisual elements. These abstractions are iteratively improved through human\nfeedback while the agent attempts to execute the trajectory. The resulting\nexamples, when used as exemplars in the prompt, significantly improve\ndecision-making in retrieval-augmented LLM and VLM agents. Moreover, as the\nagent's library of examples grows, it becomes more efficient, relying less on\nhuman feedback and requiring fewer environment interactions per demonstration.\nOur ICAL agent surpasses the state-of-the-art in dialogue-based instruction\nfollowing in TEACh, multimodal web agents in VisualWebArena, and action\nanticipation in Ego4D. In TEACh, we achieve a 12.6% improvement in\ngoal-condition success. In VisualWebArena, our task success rate improves over\nthe SOTA from 14.3% to 22.7% using GPT4V. In Ego4D action forecasting, we\nimprove over few-shot GPT-4V and remain competitive with supervised models. We\nshow finetuning our retrieval-augmented in-context agent yields additional\nimprovements. Our approach significantly reduces reliance on manual prompt\nengineering and consistently outperforms in-context learning from action plans\nthat lack such abstractions.\n","authors":["Gabriel Sarch","Lawrence Jang","Michael J. Tarr","William W. Cohen","Kenneth Marino","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2406.14596v3.pdf","comment":"Project website: http://ical-learning.github.io/"},{"id":"http://arxiv.org/abs/2410.23642v1","updated":"2024-10-31T05:29:18Z","published":"2024-10-31T05:29:18Z","title":"Novel Clinical-Grade Prostate Cancer Detection and Grading Model:\n  Development and Prospective Validation Using Real World Data, with\n  Performance Assessment on IHC Requested Cases","summary":"  Artificial intelligence may assist healthcare systems in meeting increasing\ndemand for pathology services while maintaining diagnostic quality and reducing\nturnaround time and costs. We aimed to investigate the performance of an\ninstitutionally developed system for prostate cancer detection, grading, and\nworkflow optimization and to contrast this with commercial alternatives. From\nAugust 2021 to March 2023, we scanned 21,396 slides from 1,147 patients with\npositive biopsies. We developed models for cancer detection, grading, and\nscreening of equivocal cases for IHC ordering. We compared a task-specific\nmodel trained using the PANDA dataset of prostate cancer biopsies with one\nbuilt using features extracted by the general-purpose histology foundation\nmodel, UNI and compare their performance in an unfiltered prospectively\ncollected dataset that reflects our patient population (1737 slides,95\npatients). We evaluated the contributions of a bespoke model designed to\nimprove sensitivity in detecting small cancer foci and scoring of broader\npatterns observed at lower resolution. We found high concordance between the\ndeveloped systems and pathologist reference in detection (AUC 98.5, sensitivity\n95.0, and specificity 97.8), ISUP grading (quadratic Cohen's kappa 0.869),\ngrade group 3 or higher (AUC 97.5, sensitivity 94.9, specificity 96.6) and\ncomparable to published data from commercial systems. Screening could reduce\nIHC ordering for equivocal cases by 44.5% with an overall error rate of 1.8%\n(1.4% false positive, 0.4% false negative rates). Institutions like academic\nmedical centers that have high scanning volumes and report abstraction\ncapabilities can develop accurate computational pathology models for internal\nuse. These models have the potential to aid in quality control role and to\nimprove workflow in the pathology lab to help meet future challenges in\nprostate cancer diagnosis.\n","authors":["Ramin Nateghi","Ruoji Zhou","Madeline Saft","Marina Schnauss","Clayton Neill","Ridwan Alam","Nicole Handa","Mitchell Huang","Eric V Li","Jeffery A Goldstein","Edward M Schaeffer","Menatalla Nadim","Fattaneh Pourakpour","Bogdan Isaila","Christopher Felicelli","Vikas Mehta","Behtash G Nezami","Ashley Ross","Ximing Yang","Lee AD Cooper"],"pdf_url":"https://arxiv.org/pdf/2410.23642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23641v1","updated":"2024-10-31T05:27:58Z","published":"2024-10-31T05:27:58Z","title":"Recovering Complete Actions for Cross-dataset Skeleton Action\n  Recognition","summary":"  Despite huge progress in skeleton-based action recognition, its\ngeneralizability to different domains remains a challenging issue. In this\npaper, to solve the skeleton action generalization problem, we present a\nrecover-and-resample augmentation framework based on a novel complete action\nprior. We observe that human daily actions are confronted with temporal\nmismatch across different datasets, as they are usually partial observations of\ntheir complete action sequences. By recovering complete actions and resampling\nfrom these full sequences, we can generate strong augmentations for unseen\ndomains. At the same time, we discover the nature of general action\ncompleteness within large datasets, indicated by the per-frame diversity over\ntime. This allows us to exploit two assets of transferable knowledge that can\nbe shared across action samples and be helpful for action completion: boundary\nposes for determining the action start, and linear temporal transforms for\ncapturing global action patterns. Therefore, we formulate the recovering stage\nas a two-step stochastic action completion with boundary pose-conditioned\nextrapolation followed by smooth linear transforms. Both the boundary poses and\nlinear transforms can be efficiently learned from the whole dataset via\nclustering. We validate our approach on a cross-dataset setting with three\nskeleton action datasets, outperforming other domain generalization approaches\nby a considerable margin.\n","authors":["Hanchao Liu","Yujiang Li","Tai-Jiang Mu","Shi-Min Hu"],"pdf_url":"https://arxiv.org/pdf/2410.23641v1.pdf","comment":"accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.19137v3","updated":"2024-10-31T05:22:26Z","published":"2024-03-28T04:15:58Z","title":"CLAP4CLIP: Continual Learning with Probabilistic Finetuning for\n  Vision-Language Models","summary":"  Continual learning (CL) aims to help deep neural networks learn new knowledge\nwhile retaining what has been learned. Owing to their powerful\ngeneralizability, pre-trained vision-language models such as Contrastive\nLanguage-Image Pre-training (CLIP) have lately gained traction as practical CL\ncandidates. However, the domain mismatch between the pre-training and the\ndownstream CL tasks often calls for finetuning of the CLIP on the latter. Most\nexisting finetuning methods exhibit deterministic nature. This makes them\noverlook the many possible interactions across the input modalities and deems\nthem unsafe for high-risk tasks requiring reliable uncertainty estimation. To\naddress these, our work proposes Continual LeArning with Probabilistic\nfinetuning (CLAP) - a probabilistic modeling framework over visual-guided text\nfeatures per task, thus providing more calibrated CL finetuning. Unlike recent\ndata-hungry anti-forgetting CL techniques, CLAP alleviates forgetting by\nexploiting the rich pre-trained knowledge of CLIP for weight initialization and\ndistribution regularization of task-specific parameters. Cooperating with the\ndiverse range of existing prompting methods, CLAP can surpass the predominant\ndeterministic finetuning approaches for CL with CLIP. We conclude with\nout-of-the-box applications of superior uncertainty estimation abilities of\nCLAP including novel data detection and exemplar selection within the existing\nCL setups. Our code is available at\n\\url{https://github.com/srvCodes/clap4clip}.\n","authors":["Saurav Jha","Dong Gong","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.19137v3.pdf","comment":"Accepted as a poster at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.14430v3","updated":"2024-10-31T05:14:31Z","published":"2024-05-23T11:00:07Z","title":"PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers\n  Inference","summary":"  This paper presents PipeFusion, an innovative parallel methodology to tackle\nthe high latency issues associated with generating high-resolution images using\ndiffusion transformers (DiTs) models. PipeFusion partitions images into patches\nand the model layers across multiple GPUs. It employs a patch-level pipeline\nparallel strategy to orchestrate communication and computation efficiently. By\ncapitalizing on the high similarity between inputs from successive diffusion\nsteps, PipeFusion reuses one-step stale feature maps to provide context for the\ncurrent pipeline step. This approach notably reduces communication costs\ncompared to existing DiTs inference parallelism, including tensor parallel,\nsequence parallel and DistriFusion. PipeFusion also exhibits superior memory\nefficiency, because it can distribute model parameters across multiple devices,\nmaking it more suitable for DiTs with large parameter sizes, such as Flux.1.\nExperimental results demonstrate that PipeFusion achieves state-of-the-art\nperformance on 8xL40 PCIe GPUs for Pixart, Stable-Diffusion 3 and Flux.1\nmodels.Our Source code is available at https://github.com/xdit-project/xDiT.\n","authors":["Jiarui Fang","Jinzhe Pan","Jiannan Wang","Aoyu Li","Xibo Sun"],"pdf_url":"https://arxiv.org/pdf/2405.14430v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23629v1","updated":"2024-10-31T04:42:43Z","published":"2024-10-31T04:42:43Z","title":"Posture-Informed Muscular Force Learning for Robust Hand Pressure\n  Estimation","summary":"  We present PiMForce, a novel framework that enhances hand pressure estimation\nby leveraging 3D hand posture information to augment forearm surface\nelectromyography (sEMG) signals. Our approach utilizes detailed spatial\ninformation from 3D hand poses in conjunction with dynamic muscle activity from\nsEMG to enable accurate and robust whole-hand pressure measurements under\ndiverse hand-object interactions. We also developed a multimodal data\ncollection system that combines a pressure glove, an sEMG armband, and a\nmarkerless finger-tracking module. We created a comprehensive dataset from 21\nparticipants, capturing synchronized data of hand posture, sEMG signals, and\nexerted hand pressure across various hand postures and hand-object interaction\nscenarios using our collection system. Our framework enables precise hand\npressure estimation in complex and natural interaction scenarios. Our approach\nsubstantially mitigates the limitations of traditional sEMG-based or\nvision-based methods by integrating 3D hand posture information with sEMG\nsignals. Video demos, data, and code are available online.\n","authors":["Kyungjin Seo","Junghoon Seo","Hanseok Jeong","Sangpil Kim","Sang Ho Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.23629v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23628v1","updated":"2024-10-31T04:34:28Z","published":"2024-10-31T04:34:28Z","title":"Cycle-Constrained Adversarial Denoising Convolutional Network for PET\n  Image Denoising: Multi-Dimensional Validation on Large Datasets with Reader\n  Study and Real Low-Dose Data","summary":"  Positron emission tomography (PET) is a critical tool for diagnosing tumors\nand neurological disorders but poses radiation risks to patients, particularly\nto sensitive populations. While reducing injected radiation dose mitigates this\nrisk, it often compromises image quality. To reconstruct full-dose-quality\nimages from low-dose scans, we propose a Cycle-constrained Adversarial\nDenoising Convolutional Network (Cycle-DCN). This model integrates a noise\npredictor, two discriminators, and a consistency network, and is optimized\nusing a combination of supervised loss, adversarial loss, cycle consistency\nloss, identity loss, and neighboring Structural Similarity Index (SSIM) loss.\nExperiments were conducted on a large dataset consisting of raw PET brain data\nfrom 1,224 patients, acquired using a Siemens Biograph Vision PET/CT scanner.\nEach patient underwent a 120-seconds brain scan. To simulate low-dose PET\nconditions, images were reconstructed from shortened scan durations of 30, 12,\nand 5 seconds, corresponding to 1/4, 1/10, and 1/24 of the full-dose\nacquisition, respectively, using a custom-developed GPU-based image\nreconstruction software. The results show that Cycle-DCN significantly improves\naverage Peak Signal-to-Noise Ratio (PSNR), SSIM, and Normalized Root Mean\nSquare Error (NRMSE) across three dose levels, with improvements of up to 56%,\n35%, and 71%, respectively. Additionally, it achieves contrast-to-noise ratio\n(CNR) and Edge Preservation Index (EPI) values that closely align with\nfull-dose images, effectively preserving image details, tumor shape, and\ncontrast, while resolving issues with blurred edges. The results of reader\nstudies indicated that the images restored by Cycle-DCN consistently received\nthe highest ratings from nuclear medicine physicians, highlighting their strong\nclinical relevance.\n","authors":["Yucun Hou","Fenglin Zhan","Xin Cheng","Chenxi Li","Ziquan Yuan","Runze Liao","Haihao Wang","Jianlang Hua","Jing Wu","Jianyong Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.23628v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2410.23623v1","updated":"2024-10-31T04:20:47Z","published":"2024-10-31T04:20:47Z","title":"On Learning Multi-Modal Forgery Representation for Diffusion Generated\n  Video Detection","summary":"  Large numbers of synthesized videos from diffusion models pose threats to\ninformation security and authenticity, leading to an increasing demand for\ngenerated content detection. However, existing video-level detection algorithms\nprimarily focus on detecting facial forgeries and often fail to identify\ndiffusion-generated content with a diverse range of semantics. To advance the\nfield of video forensics, we propose an innovative algorithm named Multi-Modal\nDetection(MM-Det) for detecting diffusion-generated videos. MM-Det utilizes the\nprofound perceptual and comprehensive abilities of Large Multi-modal Models\n(LMMs) by generating a Multi-Modal Forgery Representation (MMFR) from LMM's\nmulti-modal space, enhancing its ability to detect unseen forgery content.\nBesides, MM-Det leverages an In-and-Across Frame Attention (IAFA) mechanism for\nfeature augmentation in the spatio-temporal domain. A dynamic fusion strategy\nhelps refine forgery representations for the fusion. Moreover, we construct a\ncomprehensive diffusion video dataset, called Diffusion Video Forensics (DVF),\nacross a wide range of forgery videos. MM-Det achieves state-of-the-art\nperformance in DVF, demonstrating the effectiveness of our algorithm. Both\nsource code and DVF are available at https://github.com/SparkleXFantasy/MM-Det.\n","authors":["Xiufeng Song","Xiao Guo","Jiache Zhang","Qirui Li","Lei Bai","Xiaoming Liu","Guangtao Zhai","Xiaohong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23623v1.pdf","comment":"10 pages, 9 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.24200v1","updated":"2024-10-31T17:55:36Z","published":"2024-10-31T17:55:36Z","title":"Length-Induced Embedding Collapse in Transformer-based Models","summary":"  Text embeddings enable various applications, but their performance\ndeteriorates on longer texts. In this paper, we find that the performance\ndegradation is due to a phenomenon called Length Collapse, where longer text\nembeddings collapse into a narrow space. This collapse results in a\ndistributional inconsistency between embeddings of different text lengths,\nultimately hurting the performance of downstream tasks. Theoretically, by\nconsidering the self-attention mechanism inherently functions as a low-pass\nfilter, we prove that long sequences increase the attenuation rate of the\nlow-pass filter effect of the self-attention mechanism. With layers going\ndeeper, excessive low-pass filtering causes the token signals to retain only\ntheir Direct-Current (DC) component, which means the input token feature maps\nwill collapse into a narrow space, especially in long texts. Based on the above\nanalysis, we propose to mitigate the undesirable length collapse limitation by\nintroducing a temperature in softmax(), which achieves a higher low-filter\nattenuation rate. The tuning-free method, called TempScale, can be plugged into\nmultiple transformer-based embedding models. Empirically, we demonstrate that\nTempScale can improve existing embedding models, especially on long text\ninputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text\nEmbedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from\nLongEmbed, which specifically focuses on long context retrieval.\n","authors":["Yuqi Zhou","Sunhao Dai","Zhanshuo Cao","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.24200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19073v2","updated":"2024-10-31T15:58:23Z","published":"2024-05-29T13:31:12Z","title":"An engine not a camera: Measuring performative power of online search","summary":"  The power of digital platforms is at the center of major ongoing policy and\nregulatory efforts. To advance existing debates, we designed and executed an\nexperiment to measure the performative power of online search providers.\nInstantiated in our setting, performative power quantifies the ability of a\nsearch engine to steer web traffic by rearranging results. To operationalize\nthis definition we developed a browser extension that performs unassuming\nrandomized experiments in the background. These randomized experiments emulate\nupdates to the search algorithm and identify the causal effect of different\ncontent arrangements on clicks. Analyzing tens of thousands of clicks, we\ndiscuss what our robust quantitative findings say about the power of online\nsearch engines, using the Google Shopping antitrust investigation as a case\nstudy. More broadly, we envision our work to serve as a blueprint for how the\nrecent definition of performative power can help integrate quantitative\ninsights from online experiments with future investigations into the economic\npower of digital platforms.\n","authors":["Celestine Mendler-Dünner","Gabriele Carovano","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2405.19073v2.pdf","comment":"to appear at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23879v1","updated":"2024-10-31T12:40:38Z","published":"2024-10-31T12:40:38Z","title":"Investigating Bias in Political Search Query Suggestions by Relative\n  Comparison with LLMs","summary":"  Search query suggestions affect users' interactions with search engines,\nwhich then influences the information they encounter. Thus, bias in search\nquery suggestions can lead to exposure to biased search results and can impact\nopinion formation. This is especially critical in the political domain.\nDetecting and quantifying bias in web search engines is difficult due to its\ntopic dependency, complexity, and subjectivity. The lack of context and\nphrasality of query suggestions emphasizes this problem. In a multi-step\napproach, we combine the benefits of large language models, pairwise\ncomparison, and Elo-based scoring to identify and quantify bias in English\nsearch query suggestions. We apply our approach to the U.S. political news\ndomain and compare bias in Google and Bing.\n","authors":["Fabian Haak","Björn Engelmann","Christin Katharina Kreutz","Philipp Schaer"],"pdf_url":"https://arxiv.org/pdf/2410.23879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23851v1","updated":"2024-10-31T12:01:51Z","published":"2024-10-31T12:01:51Z","title":"Leveraging Large Language Models for Medical Information Extraction and\n  Query Generation","summary":"  This paper introduces a system that integrates large language models (LLMs)\ninto the clinical trial retrieval process, enhancing the effectiveness of\nmatching patients with eligible trials while maintaining information privacy\nand allowing expert oversight. We evaluate six LLMs for query generation,\nfocusing on open-source and relatively small models that require minimal\ncomputational resources. Our evaluation includes two closed-source and four\nopen-source models, with one specifically trained in the medical field and five\ngeneral-purpose models. We compare the retrieval effectiveness achieved by\nLLM-generated queries against those created by medical experts and\nstate-of-the-art methods from the literature. Our findings indicate that the\nevaluated models reach retrieval effectiveness on par with or greater than\nexpert-created queries. The LLMs consistently outperform standard baselines and\nother approaches in the literature. The best performing LLMs exhibit fast\nresponse times, ranging from 1.7 to 8 seconds, and generate a manageable number\nof query terms (15-63 on average), making them suitable for practical\nimplementation. Our overall findings suggest that leveraging small, open-source\nLLMs for clinical trials retrieval can balance performance, computational\nefficiency, and real-world applicability in medical settings.\n","authors":["Georgios Peikos","Pranav Kasela","Gabriella Pasi"],"pdf_url":"https://arxiv.org/pdf/2410.23851v1.pdf","comment":"Accepted in WI-IAT '24"},{"id":"http://arxiv.org/abs/2410.23842v1","updated":"2024-10-31T11:49:16Z","published":"2024-10-31T11:49:16Z","title":"Auditing Google's Search Algorithm: Measuring News Diversity Across\n  Brazil, the UK, and the US","summary":"  This study examines the influence of Google's search algorithm on news\ndiversity by analyzing search results in Brazil, the UK, and the US. It\nexplores how Google's system preferentially favors a limited number of news\noutlets. Utilizing algorithm auditing techniques, the research measures source\nconcentration with the Herfindahl-Hirschman Index (HHI) and Gini coefficient,\nrevealing significant concentration trends. The study underscores the\nimportance of conducting horizontal analyses across multiple search queries, as\nfocusing solely on individual results pages may obscure these patterns. Factors\nsuch as popularity, political bias, and recency were evaluated for their impact\non news rankings. Findings indicate a slight leftward bias in search outcomes\nand a preference for popular, often national outlets. This bias, combined with\na tendency to prioritize recent content, suggests that Google's algorithm may\nreinforce existing media inequalities. By analyzing the largest dataset to date\n-- 221,863 search results -- this research provides comprehensive, longitudinal\ninsights into how algorithms shape public access to diverse news sources.\n","authors":["Raphael Hernandes","Giulio Corsi"],"pdf_url":"https://arxiv.org/pdf/2410.23842v1.pdf","comment":"21 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.23841v1","updated":"2024-10-31T11:47:21Z","published":"2024-10-31T11:47:21Z","title":"Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models","summary":"  Instruction-following capabilities in large language models (LLMs) have\nsignificantly progressed, enabling more complex user interactions through\ndetailed prompts. However, retrieval systems have not matched these advances,\nmost of them still relies on traditional lexical and semantic matching\ntechniques that fail to fully capture user intent. Recent efforts have\nintroduced instruction-aware retrieval models, but these primarily focus on\nintrinsic content relevance, which neglects the importance of customized\npreferences for broader document-level attributes. This study evaluates the\ninstruction-following capabilities of various retrieval models beyond content\nrelevance, including LLM-based dense retrieval and reranking models. We develop\nInfoSearch, a novel retrieval evaluation benchmark spanning six document-level\nattributes: Audience, Keyword, Format, Language, Length, and Source, and\nintroduce novel metrics -- Strict Instruction Compliance Ratio (SICR) and\nWeighted Instruction Sensitivity Evaluation (WISE) to accurately assess the\nmodels' responsiveness to instructions. Our findings reveal that while\nreranking models generally surpass retrieval models in instruction following,\nthey still face challenges in handling certain attributes. Moreover, although\ninstruction fine-tuning and increased model size lead to better performance,\nmost models fall short of achieving comprehensive instruction compliance as\nassessed by our benchmark.\n","authors":["Jianqun Zhou","Yuanlei Zheng","Wei Chen","Qianqian Zheng","Zeyuan Shang","Wei Zhang","Rui Meng","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.23841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21488v2","updated":"2024-10-31T11:45:00Z","published":"2024-07-31T09:52:53Z","title":"Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing\n  the Upper Bound of Generative Retrieval","summary":"  Generative retrieval (GR) has emerged as a transformative paradigm in search\nand recommender systems, leveraging numeric-based identifier representations to\nenhance efficiency and generalization. Notably, methods like TIGER employing\nResidual Quantization-based Semantic Identifiers (RQ-SID), have shown\nsignificant promise in e-commerce scenarios by effectively managing item IDs.\nHowever, a critical issue termed the \"\\textbf{Hourglass}\" phenomenon, occurs in\nRQ-SID, where intermediate codebook tokens become overly concentrated,\nhindering the full utilization of generative retrieval methods. This paper\nanalyses and addresses this problem by identifying data sparsity and\nlong-tailed distribution as the primary causes. Through comprehensive\nexperiments and detailed ablation studies, we analyze the impact of these\nfactors on codebook utilization and data distribution. Our findings reveal that\nthe \"Hourglass\" phenomenon substantially impacts the performance of RQ-SID in\ngenerative retrieval. We propose effective solutions to mitigate this issue,\nthereby significantly enhancing the effectiveness of generative retrieval in\nreal-world E-commerce applications.\n","authors":["Zhirui Kuai","Zuxu Chen","Huimu Wang","Mingming Li","Dadong Miao","Binbin Wang","Xusong Chen","Li Kuang","Yuxing Han","Jiaxing Wang","Guoyu Tang","Lin Liu","Songlin Wang","Jingwei Zhuo"],"pdf_url":"https://arxiv.org/pdf/2407.21488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15187v2","updated":"2024-10-31T10:10:28Z","published":"2024-06-21T14:29:39Z","title":"UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world\n  Document Analysis","summary":"  The use of Retrieval-Augmented Generation (RAG) has improved Large Language\nModels (LLMs) in collaborating with external data, yet significant challenges\nexist in real-world scenarios. In areas such as academic literature and finance\nquestion answering, data are often found in raw text and tables in HTML or PDF\nformats, which can be lengthy and highly unstructured. In this paper, we\nintroduce a benchmark suite, namely Unstructured Document Analysis (UDA), that\ninvolves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We\nrevisit popular LLM- and RAG-based solutions for document analysis and evaluate\nthe design choices and answer qualities across multiple document domains and\ndiverse query types. Our evaluation yields interesting findings and highlights\nthe importance of data parsing and retrieval. We hope our benchmark can shed\nlight and better serve real-world document analysis applications. The benchmark\nsuite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.\n","authors":["Yulong Hui","Yao Lu","Huanchen Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.15187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23757v1","updated":"2024-10-31T09:24:22Z","published":"2024-10-31T09:24:22Z","title":"Identify Then Recommend: Towards Unsupervised Group Recommendation","summary":"  Group Recommendation (GR), which aims to recommend items to groups of users,\nhas become a promising and practical direction for recommendation systems. This\npaper points out two issues of the state-of-the-art GR models. (1) The\npre-defined and fixed number of user groups is inadequate for real-time\nindustrial recommendation systems, where the group distribution can shift\ndynamically. (2) The training schema of existing GR methods is supervised,\nnecessitating expensive user-group and group-item labels, leading to\nsignificant annotation costs. To this end, we present a novel unsupervised\ngroup recommendation framework named \\underline{I}dentify \\underline{T}hen\n\\underline{R}ecommend (\\underline{ITR}), where it first identifies the user\ngroups in an unsupervised manner even without the pre-defined number of groups,\nand then two pre-text tasks are designed to conduct self-supervised group\nrecommendation. Concretely, at the group identification stage, we first\nestimate the adaptive density of each user point, where areas with higher\ndensities are more likely to be recognized as group centers. Then, a heuristic\nmerge-and-split strategy is designed to discover the user groups and decision\nboundaries. Subsequently, at the self-supervised learning stage, the\npull-and-repulsion pre-text task is proposed to optimize the user-group\ndistribution. Besides, the pseudo group recommendation pre-text task is\ndesigned to assist the recommendations. Extensive experiments demonstrate the\nsuperiority and effectiveness of ITR on both user recommendation (e.g., 22.22\\%\nNDCG@5 $\\uparrow$) and group recommendation (e.g., 22.95\\% NDCG@5 $\\uparrow$).\nFurthermore, we deploy ITR on the industrial recommender and achieve promising\nresults.\n","authors":["Yue Liu","Shihao Zhu","Tianyuan Yang","Jian Ma","Wenliang Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.23757v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2401.05975v4","updated":"2024-10-31T09:14:56Z","published":"2024-01-11T15:22:55Z","title":"End-to-end Learnable Clustering for Intent Learning in Recommendation","summary":"  Intent learning, which aims to learn users' intents for user understanding\nand item recommendation, has become a hot research spot in recent years.\nHowever, existing methods suffer from complex and cumbersome alternating\noptimization, limiting performance and scalability. To this end, we propose a\nnovel intent learning method termed \\underline{ELCRec}, by unifying behavior\nrepresentation learning into an \\underline{E}nd-to-end \\underline{L}earnable\n\\underline{C}lustering framework, for effective and efficient\n\\underline{Rec}ommendation. Concretely, we encode user behavior sequences and\ninitialize the cluster centers (latent intents) as learnable neurons. Then, we\ndesign a novel learnable clustering module to separate different cluster\ncenters, thus decoupling users' complex intents. Meanwhile, it guides the\nnetwork to learn intents from behaviors by forcing behavior embeddings close to\ncluster centers. This allows simultaneous optimization of recommendation and\nclustering via mini-batch data. Moreover, we propose intent-assisted\ncontrastive learning by using cluster centers as self-supervision signals,\nfurther enhancing mutual promotion. Both experimental results and theoretical\nanalyses demonstrate the superiority of ELCRec from six perspectives. Compared\nto the runner-up, ELCRec improves NDCG@5 by 8.9\\% and reduces computational\ncosts by 22.5\\% on the Beauty dataset. Furthermore, due to the scalability and\nuniversal applicability, we deploy this method on the industrial recommendation\nsystem with 130 million page views and achieve promising results. The codes are\navailable on GitHub (https://github.com/yueliu1999/ELCRec). A collection\n(papers, codes, datasets) of deep group recommendation/intent learning methods\nis available on GitHub\n(https://github.com/yueliu1999/Awesome-Deep-Group-Recommendation).\n","authors":["Yue Liu","Shihao Zhu","Jun Xia","Yingwei Ma","Jian Ma","Xinwang Liu","Shengju Yu","Kejun Zhang","Wenliang Zhong"],"pdf_url":"https://arxiv.org/pdf/2401.05975v4.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2410.23736v1","updated":"2024-10-31T08:49:05Z","published":"2024-10-31T08:49:05Z","title":"MoTaDual: Modality-Task Dual Alignment for Enhanced Zero-shot Composed\n  Image Retrieval","summary":"  Composed Image Retrieval (CIR) is a challenging vision-language task,\nutilizing bi-modal (image+text) queries to retrieve target images. Despite the\nimpressive performance of supervised CIR, the dependence on costly,\nmanually-labeled triplets limits its scalability and zero-shot capability. To\naddress this issue, zero-shot composed image retrieval (ZS-CIR) is presented\nalong with projection-based approaches. However, such methods face two major\nproblems, i.e., task discrepancy between pre-training (image $\\leftrightarrow$\ntext) and inference (image+text $\\rightarrow$ image), and modality discrepancy.\nThe latter pertains to approaches based on text-only projection training due to\nthe necessity of feature extraction from the reference image during inference.\nIn this paper, we propose a two-stage framework to tackle both discrepancies.\nFirst, to ensure efficiency and scalability, a textual inversion network is\npre-trained on large-scale caption datasets. Subsequently, we put forward\nModality-Task Dual Alignment (MoTaDual) as the second stage, where\nlarge-language models (LLMs) generate triplet data for fine-tuning, and\nadditionally, prompt learning is introduced in a multi-modal context to\neffectively alleviate both modality and task discrepancies. The experimental\nresults show that our MoTaDual achieves the state-of-the-art performance across\nfour widely used ZS-CIR benchmarks, while maintaining low training time and\ncomputational cost. The code will be released soon.\n","authors":["Haiwen Li","Fei Su","Zhicheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.23736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00072v5","updated":"2024-10-31T08:35:42Z","published":"2024-06-21T08:52:11Z","title":"Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback","summary":"  RAG systems face limitations when semantic relevance alone does not guarantee\nimproved generation quality. This issue becomes particularly evident due to the\nsensitivity of large language models (LLMs) to the ordering of few-shot\nprompts, which can affect model performance. To address this challenge,\naligning LLM outputs with human preferences using structured feedback, such as\noptions to copy, regenerate, or dislike, offers a promising method for\nimprovement. This feedback is applied to the entire list of inputs rather than\ngiving specific ratings for individual documents, making it a Listwide Labels\nLearning-to-Rank task.\n  To address this task, we propose Pistis-RAG, a new RAG framework designed\nwith a content-centric approach to better align LLMs with human preferences.\nPistis-RAG effectively utilizes human feedback, enhancing content ranking and\ngeneration quality. To validate our framework, we use public datasets to\nsimulate human feedback, allowing us to evaluate and refine our method\neffectively. Experimental results indicate that Pistis-RAG improves alignment\nwith human preferences relative to the baseline RAG system, showing a 6.06%\nincrease in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy\nmetrics. These results highlight Pistis-RAG's effectiveness in overcoming the\nlimitations associated with traditional RAG approaches.\n","authors":["Yu Bai","Yukai Miao","Li Chen","Dawei Wang","Dan Li","Yanyu Ren","Hongtao Xie","Ce Yang","Xuhui Cai"],"pdf_url":"https://arxiv.org/pdf/2407.00072v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23715v1","updated":"2024-10-31T08:03:13Z","published":"2024-10-31T08:03:13Z","title":"Towards Cross-Modal Text-Molecule Retrieval with Better Modality\n  Alignment","summary":"  Cross-modal text-molecule retrieval model aims to learn a shared feature\nspace of the text and molecule modalities for accurate similarity calculation,\nwhich facilitates the rapid screening of molecules with specific properties and\nactivities in drug design. However, previous works have two main defects.\nFirst, they are inadequate in capturing modality-shared features considering\nthe significant gap between text sequences and molecule graphs. Second, they\nmainly rely on contrastive learning and adversarial training for cross-modality\nalignment, both of which mainly focus on the first-order similarity, ignoring\nthe second-order similarity that can capture more structural information in the\nembedding space. To address these issues, we propose a novel cross-modal\ntext-molecule retrieval model with two-fold improvements. Specifically, on the\ntop of two modality-specific encoders, we stack a memory bank based feature\nprojector that contain learnable memory vectors to extract modality-shared\nfeatures better. More importantly, during the model training, we calculate four\nkinds of similarity distributions (text-to-text, text-to-molecule,\nmolecule-to-molecule, and molecule-to-text similarity distributions) for each\ninstance, and then minimize the distance between these similarity distributions\n(namely second-order similarity losses) to enhance cross-modal alignment.\nExperimental results and analysis strongly demonstrate the effectiveness of our\nmodel. Particularly, our model achieves SOTA performance, outperforming the\npreviously-reported best result by 6.4%.\n","authors":["Jia Song","Wanru Zhuang","Yujie Lin","Liang Zhang","Chunyan Li","Jinsong Su","Song He","Xiaochen Bo"],"pdf_url":"https://arxiv.org/pdf/2410.23715v1.pdf","comment":"BIBM 2024 regular paper"},{"id":"http://arxiv.org/abs/2410.23683v1","updated":"2024-10-31T07:19:22Z","published":"2024-10-31T07:19:22Z","title":"Unveiling User Satisfaction and Creator Productivity Trade-Offs in\n  Recommendation Platforms","summary":"  On User-Generated Content (UGC) platforms, recommendation algorithms\nsignificantly impact creators' motivation to produce content as they compete\nfor algorithmically allocated user traffic. This phenomenon subtly shapes the\nvolume and diversity of the content pool, which is crucial for the platform's\nsustainability. In this work, we demonstrate, both theoretically and\nempirically, that a purely relevance-driven policy with low exploration\nstrength boosts short-term user satisfaction but undermines the long-term\nrichness of the content pool. In contrast, a more aggressive exploration policy\nmay slightly compromise user satisfaction but promote higher content creation\nvolume. Our findings reveal a fundamental trade-off between immediate user\nsatisfaction and overall content production on UGC platforms. Building on this\nfinding, we propose an efficient optimization method to identify the optimal\nexploration strength, balancing user and creator engagement. Our model can\nserve as a pre-deployment audit tool for recommendation algorithms on UGC\nplatforms, helping to align their immediate objectives with sustainable,\nlong-term goals.\n","authors":["Fan Yao","Yiming Liao","Jingzhou Liu","Shaoliang Nie","Qifan Wang","Haifeng Xu","Hongning Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11611v3","updated":"2024-10-31T07:09:38Z","published":"2024-08-21T13:39:21Z","title":"DTN: Deep Multiple Task-specific Feature Interactions Network for\n  Multi-Task Recommendation","summary":"  Neural-based multi-task learning (MTL) has been successfully applied to many\nrecommendation applications. However, these MTL models (e.g., MMoE, PLE) did\nnot consider feature interaction during the optimization, which is crucial for\ncapturing complex high-order features and has been widely used in ranking\nmodels for real-world recommender systems. Moreover, through feature importance\nanalysis across various tasks in MTL, we have observed an interesting\ndivergence phenomenon that the same feature can have significantly different\nimportance across different tasks in MTL. To address these issues, we propose\nDeep Multiple Task-specific Feature Interactions Network (DTN) with a novel\nmodel structure design. DTN introduces multiple diversified task-specific\nfeature interaction methods and task-sensitive network in MTL networks,\nenabling the model to learn task-specific diversified feature interaction\nrepresentations, which improves the efficiency of joint representation learning\nin a general setup. We applied DTN to our company's real-world E-commerce\nrecommendation dataset, which consisted of over 6.3 billion samples, the\nresults demonstrated that DTN significantly outperformed state-of-the-art MTL\nmodels. Moreover, during online evaluation of DTN in a large-scale E-commerce\nrecommender system, we observed a 3.28% in clicks, a 3.10% increase in orders\nand a 2.70% increase in GMV (Gross Merchandise Value) compared to the\nstate-of-the-art MTL models. Finally, extensive offline experiments conducted\non public benchmark datasets demonstrate that DTN can be applied to various\nscenarios beyond recommendations, enhancing the performance of ranking models.\n","authors":["Yaowen Bi","Yuteng Lian","Jie Cui","Jun Liu","Peijian Wang","Guanghui Li","Xuejun Chen","Jinglin Zhao","Hao Wen","Jing Zhang","Zhaoqi Zhang","Wenzhuo Song","Yang Sun","Weiwei Zhang","Mingchen Cai","Jian Dong","Guanxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.11611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11821v3","updated":"2024-10-31T05:19:58Z","published":"2024-02-19T04:29:45Z","title":"Microstructures and Accuracy of Graph Recall by Large Language Models","summary":"  Graphs data is crucial for many applications, and much of it exists in the\nrelations described in textual format. As a result, being able to accurately\nrecall and encode a graph described in earlier text is a basic yet pivotal\nability that LLMs need to demonstrate if they are to perform reasoning tasks\nthat involve graph-structured information. Human performance at graph recall\nhas been studied by cognitive scientists for decades, and has been found to\noften exhibit certain structural patterns of bias that align with human\nhandling of social relationships. To date, however, we know little about how\nLLMs behave in analogous graph recall tasks: do their recalled graphs also\nexhibit certain biased patterns, and if so, how do they compare with humans and\naffect other graph reasoning tasks? In this work, we perform the first\nsystematical study of graph recall by LLMs, investigating the accuracy and\nbiased microstructures (local structural patterns) in their recall. We find\nthat LLMs not only underperform often in graph recall, but also tend to favor\nmore triangles and alternating 2-paths. Moreover, we find that more advanced\nLLMs have a striking dependence on the domain that a real-world graph comes\nfrom -- by yielding the best recall accuracy when the graph is narrated in a\nlanguage style consistent with its original domain.\n","authors":["Yanbang Wang","Hejie Cui","Jon Kleinberg"],"pdf_url":"https://arxiv.org/pdf/2402.11821v3.pdf","comment":"Accepted at NeurIPS 2024; Code available at:\n  https://github.com/Abel0828/llm-graph-recall"},{"id":"http://arxiv.org/abs/2307.02147v4","updated":"2024-10-31T02:54:38Z","published":"2023-07-05T09:42:51Z","title":"Recommendation Unlearning via Influence Function","summary":"  Recommendation unlearning is an emerging task to serve users for erasing\nunusable data (e.g., some historical behaviors) from a well-trained recommender\nmodel. Existing methods process unlearning requests by fully or partially\nretraining the model after removing the unusable data. However, these methods\nare impractical due to the high computation cost of full retraining and the\nhighly possible performance damage of partial training. In this light, a\ndesired recommendation unlearning method should obtain a similar model as full\nretraining in a more efficient manner, i.e., achieving complete, efficient and\nharmless unlearning.\n  In this work, we propose a new Influence Function-based Recommendation\nUnlearning (IFRU) framework, which efficiently updates the model without\nretraining by estimating the influence of the unusable data on the model via\nthe influence function. In the light that recent recommender models use\nhistorical data for both the constructions of the optimization loss and the\ncomputational graph (e.g., neighborhood aggregation), IFRU jointly estimates\nthe direct influence of unusable data on optimization loss and the spillover\ninfluence on the computational graph to pursue complete unlearning.\nFurthermore, we propose an importance-based pruning algorithm to reduce the\ncost of the influence function. IFRU is harmless and applicable to mainstream\ndifferentiable models. Extensive experiments demonstrate that IFRU achieves\nmore than 250 times acceleration compared to retraining-based methods with\nrecommendation performance comparable to full retraining. Codes are avaiable at\nhttps://github.com/baiyimeng/IFRU.\n","authors":["Yang Zhang","Zhiyu Hu","Yimeng Bai","Jiancan Wu","Qifan Wang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2307.02147v4.pdf","comment":"Accepted by ACM TORS"},{"id":"http://arxiv.org/abs/2411.00262v1","updated":"2024-10-31T23:41:09Z","published":"2024-10-31T23:41:09Z","title":"Content Aware Analysis of Scholarly Networks: A Case Study on CORD19\n  Dataset","summary":"  This paper investigates the relationships among key elements of scientific\nresearch network, namely articles, researchers, and journals. We introduce a\nnovel approach to use semantic information through the HITS algorithm based\npropagation of topic information in the network. The topic information is\nderived by using the Named Entity Recognition and Entity Linkage. In our case,\nMedCAT is used to extract the topics from the CORD19 Dataset, which is a corpus\nof academic articles about COVID-19 and coronavirus scientific network. Our\napproach focuses on the COVID-19 domain, utilizing the CORD-19 dataset to\ndemonstrate the efficacy of integrating topic-related information within the\ncitation framework. Through the application of a hybrid HITS algorithm, we show\nthat incorporating topic data significantly influences article rankings,\nrevealing deeper insights into the structure of the academic community.\n","authors":["Mehmet Emre Akbulut","Yusuf Erdem Nacar"],"pdf_url":"https://arxiv.org/pdf/2411.00262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12036v2","updated":"2024-10-31T23:08:03Z","published":"2024-08-21T23:42:06Z","title":"Reasoning and Tools for Human-Level Forecasting","summary":"  Language models (LMs) trained on web-scale datasets are largely successful\ndue to their ability to memorize large amounts of training data, even if only\npresent in a few examples. These capabilities are often desirable in evaluation\non tasks such as question answering but raise questions about whether these\nmodels can exhibit genuine reasoning or succeed only at mimicking patterns from\nthe training data. This distinction is particularly salient in forecasting\ntasks, where the answer is not present in the training data, and the model must\nreason to make logical deductions. We present Reasoning and Tools for\nForecasting (RTF), a framework of reasoning-and-acting (ReAct) agents that can\ndynamically retrieve updated information and run numerical simulation with\nequipped tools. We evaluate our model with questions from competitive\nforecasting platforms and demonstrate that our method is competitive with and\ncan outperform human predictions. This suggests that LMs, with the right tools,\ncan indeed think and adapt like humans, offering valuable insights for\nreal-world decision-making.\n","authors":["Elvis Hsieh","Preston Fu","Jonathan Chen"],"pdf_url":"https://arxiv.org/pdf/2408.12036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14094v2","updated":"2024-10-31T21:26:01Z","published":"2024-07-19T07:58:26Z","title":"User-Creator Feature Polarization in Recommender Systems with Dual\n  Influence","summary":"  Recommender systems serve the dual purpose of presenting relevant content to\nusers and helping content creators reach their target audience. The dual nature\nof these systems naturally influences both users and creators: users'\npreferences are affected by the items they are recommended, while creators may\nbe incentivized to alter their content to attract more users. We define a\nmodel, called user-creator feature dynamics, to capture the dual influence of\nrecommender systems. We prove that a recommender system with dual influence is\nguaranteed to polarize, causing diversity loss in the system. We then\ninvestigate, both theoretically and empirically, approaches for mitigating\npolarization and promoting diversity in recommender systems. Unexpectedly, we\nfind that common diversity-promoting approaches do not work in the presence of\ndual influence, while relevancy-optimizing methods like top-$k$ truncation can\nprevent polarization and improve diversity of the system.\n","authors":["Tao Lin","Kun Jin","Andrew Estornell","Xiaoying Zhang","Yiling Chen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14094v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.08571v2","updated":"2024-10-31T20:30:51Z","published":"2024-07-11T14:59:17Z","title":"Multi-Group Proportional Representation in Retrieval","summary":"  Image search and retrieval tasks can perpetuate harmful stereotypes, erase\ncultural identities, and amplify social disparities. Current approaches to\nmitigate these representational harms balance the number of retrieved items\nacross population groups defined by a small number of (often binary)\nattributes. However, most existing methods overlook intersectional groups\ndetermined by combinations of group attributes, such as gender, race, and\nethnicity. We introduce Multi-Group Proportional Representation (MPR), a novel\nmetric that measures representation across intersectional groups. We develop\npractical methods for estimating MPR, provide theoretical guarantees, and\npropose optimization algorithms to ensure MPR in retrieval. We demonstrate that\nexisting methods optimizing for equal and proportional representation metrics\nmay fail to promote MPR. Crucially, our work shows that optimizing MPR yields\nmore proportional representation across multiple intersectional groups\nspecified by a rich function class, often with minimal compromise in retrieval\naccuracy.\n","authors":["Alex Oesterling","Claudio Mayrink Verdun","Carol Xuan Long","Alexander Glynn","Lucas Monteiro Paes","Sajani Vithana","Martina Cardone","Flavio P. Calmon"],"pdf_url":"https://arxiv.org/pdf/2407.08571v2.pdf","comment":"48 pages, 33 figures. Accepted as poster at NeurIPS 2024. Code can be\n  found at\n  https://github.com/alex-oesterling/multigroup-proportional-representation"},{"id":"http://arxiv.org/abs/2411.00188v1","updated":"2024-10-31T20:15:14Z","published":"2024-10-31T20:15:14Z","title":"Building Multi-Agent Copilot towards Autonomous Agricultural Data\n  Management and Analysis","summary":"  Current agricultural data management and analysis paradigms are to large\nextent traditional, in which data collecting, curating, integration, loading,\nstoring, sharing and analyzing still involve too much human effort and\nknow-how. The experts, researchers and the farm operators need to understand\nthe data and the whole process of data management pipeline to make fully use of\nthe data. The essential problem of the traditional paradigm is the lack of a\nlayer of orchestrational intelligence which can understand, organize and\ncoordinate the data processing utilities to maximize data management and\nanalysis outcome. The emerging reasoning and tool mastering abilities of large\nlanguage models (LLM) make it a potentially good fit to this position, which\nhelps a shift from the traditional user-driven paradigm to AI-driven paradigm.\nIn this paper, we propose and explore the idea of a LLM based copilot for\nautonomous agricultural data management and analysis. Based on our previously\ndeveloped platform of Agricultural Data Management and Analytics (ADMA), we\nbuild a proof-of-concept multi-agent system called ADMA Copilot, which can\nunderstand user's intent, makes plans for data processing pipeline and\naccomplishes tasks automatically, in which three agents: a LLM based\ncontroller, an input formatter and an output formatter collaborate together.\nDifferent from existing LLM based solutions, by defining a meta-program graph,\nour work decouples control flow and data flow to enhance the predictability of\nthe behaviour of the agents. Experiments demonstrates the intelligence,\nautonomy, efficacy, efficiency, extensibility, flexibility and privacy of our\nsystem. Comparison is also made between ours and existing systems to show the\nsuperiority and potential of our system.\n","authors":["Yu Pan","Jianxin Sun","Hongfeng Yu","Joe Luck","Geng Bai","Nipuna Chamara","Yufeng Ge","Tala Awada"],"pdf_url":"https://arxiv.org/pdf/2411.00188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19302v3","updated":"2024-10-31T19:37:07Z","published":"2024-03-28T10:40:22Z","title":"Generating Multi-Aspect Queries for Conversational Search","summary":"  Conversational information seeking (CIS) systems aim to model the user's\ninformation need within the conversational context and retrieve the relevant\ninformation. One major approach to modeling the conversational context aims to\nrewrite the user utterance in the conversation to represent the information\nneed independently. Recent work has shown the benefit of expanding the\nrewritten utterance with relevant terms. In this work, we hypothesize that\nbreaking down the information of an utterance into multi-aspect rewritten\nqueries can lead to more effective retrieval performance. This is more evident\nin more complex utterances that require gathering evidence from various\ninformation sources, where a single query rewrite or query representation\ncannot capture the complexity of the utterance. To test this hypothesis, we\nconduct extensive experiments on five widely used CIS datasets where we\nleverage LLMs to generate multi-aspect queries to represent the information\nneed for each utterance in multiple query rewrites. We show that, for most of\nthe utterances, the same retrieval model would perform better with more than\none rewritten query by 85% in terms of nDCG@3. We further propose a\nmulti-aspect query generation and retrieval framework, called MQ4CS. Our\nextensive experiments show that MQ4CS outperforms the state-of-the-art query\nrewriting methods. We make our code and our new dataset of generated\nmulti-aspect queries publicly available.\n","authors":["Zahra Abbasiantaeb","Simon Lupart","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2403.19302v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00163v1","updated":"2024-10-31T19:11:26Z","published":"2024-10-31T19:11:26Z","title":"PSL: Rethinking and Improving Softmax Loss from Pairwise Perspective for\n  Recommendation","summary":"  Softmax Loss (SL) is widely applied in recommender systems (RS) and has\ndemonstrated effectiveness. This work analyzes SL from a pairwise perspective,\nrevealing two significant limitations: 1) the relationship between SL and\nconventional ranking metrics like DCG is not sufficiently tight; 2) SL is\nhighly sensitive to false negative instances. Our analysis indicates that these\nlimitations are primarily due to the use of the exponential function. To\naddress these issues, this work extends SL to a new family of loss functions,\ntermed Pairwise Softmax Loss (PSL), which replaces the exponential function in\nSL with other appropriate activation functions. While the revision is minimal,\nwe highlight three merits of PSL: 1) it serves as a tighter surrogate for DCG\nwith suitable activation functions; 2) it better balances data contributions;\nand 3) it acts as a specific BPR loss enhanced by Distributionally Robust\nOptimization (DRO). We further validate the effectiveness and robustness of PSL\nthrough empirical experiments. The code is available at\nhttps://github.com/Tiny-Snow/IR-Benchmark.\n","authors":["Weiqin Yang","Jiawei Chen","Xin Xin","Sheng Zhou","Binbin Hu","Yan Feng","Chun Chen","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2411.00163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13959v2","updated":"2024-10-31T18:38:37Z","published":"2024-10-17T18:34:43Z","title":"FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven\n  Question Answering Pipeline","summary":"  Financial decision-making hinges on the analysis of relevant information\nembedded in the enormous volume of documents in the financial domain. To\naddress this challenge, we developed FinQAPT, an end-to-end pipeline that\nstreamlines the identification of relevant financial reports based on a query,\nextracts pertinent context, and leverages Large Language Models (LLMs) to\nperform downstream tasks. To evaluate the pipeline, we experimented with\nvarious techniques to optimize the performance of each module using the FinQA\ndataset. We introduced a novel clustering-based negative sampling technique to\nenhance context extraction and a novel prompting method called Dynamic N-shot\nPrompting to boost the numerical question-answering capabilities of LLMs. At\nthe module level, we achieved state-of-the-art accuracy on FinQA, attaining an\naccuracy of 80.6%. However, at the pipeline level, we observed decreased\nperformance due to challenges in extracting relevant context from financial\nreports. We conducted a detailed error analysis of each module and the\nend-to-end pipeline, pinpointing specific challenges that must be addressed to\ndevelop a robust solution for handling complex financial tasks.\n","authors":["Kuldeep Singh","Simerjot Kaur","Charese Smiley"],"pdf_url":"https://arxiv.org/pdf/2410.13959v2.pdf","comment":"Accepted in ICAIF 2024, 8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2411.00137v1","updated":"2024-10-31T18:35:03Z","published":"2024-10-31T18:35:03Z","title":"Cost-Aware Query Policies in Active Learning for Efficient Autonomous\n  Robotic Exploration","summary":"  In missions constrained by finite resources, efficient data collection is\ncritical. Informative path planning, driven by automated decision-making,\noptimizes exploration by reducing the costs associated with accurate\ncharacterization of a target in an environment. Previous implementations of\nactive learning did not consider the action cost for regression problems or\nonly considered the action cost for classification problems. This paper\nanalyzes an AL algorithm for Gaussian Process regression while incorporating\naction cost. The algorithm's performance is compared on various regression\nproblems to include terrain mapping on diverse simulated surfaces along metrics\nof root mean square error, samples and distance until convergence, and model\nvariance upon convergence. The cost-dependent acquisition policy doesn't\norganically optimize information gain over distance. Instead, the traditional\nuncertainty metric with a distance constraint best minimizes root-mean-square\nerror over trajectory distance. This studys impact is to provide insight into\nincorporating action cost with AL methods to optimize exploration under\nrealistic mission constraints.\n","authors":["Sapphira Akins","Hans Mertens","Frances Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.00137v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.24222v1","updated":"2024-10-31T17:59:56Z","published":"2024-10-31T17:59:56Z","title":"Robust Gaussian Processes via Relevance Pursuit","summary":"  Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range.\n","authors":["Sebastian Ament","Elizabeth Santorella","David Eriksson","Ben Letham","Maximilian Balandat","Eytan Bakshy"],"pdf_url":"https://arxiv.org/pdf/2410.24222v1.pdf","comment":"NeurIPS 2024 Article"},{"id":"http://arxiv.org/abs/2410.24220v1","updated":"2024-10-31T17:59:53Z","published":"2024-10-31T17:59:53Z","title":"Bridging Geometric States via Geometric Diffusion Bridge","summary":"  The accurate prediction of geometric state evolution in complex systems is\ncritical for advancing scientific domains such as quantum chemistry and\nmaterial modeling. Traditional experimental and computational methods face\nchallenges in terms of environmental constraints and computational demands,\nwhile current deep learning approaches still fall short in terms of precision\nand generality. In this work, we introduce the Geometric Diffusion Bridge\n(GDB), a novel generative modeling framework that accurately bridges initial\nand target geometric states. GDB leverages a probabilistic approach to evolve\ngeometric state distributions, employing an equivariant diffusion bridge\nderived by a modified version of Doob's $h$-transform for connecting geometric\nstates. This tailored diffusion process is anchored by initial and target\ngeometric states as fixed endpoints and governed by equivariant transition\nkernels. Moreover, trajectory data can be seamlessly leveraged in our GDB\nframework by using a chain of equivariant diffusion bridges, providing a more\ndetailed and accurate characterization of evolution dynamics. Theoretically, we\nconduct a thorough examination to confirm our framework's ability to preserve\njoint distributions of geometric states and capability to completely model the\nunderlying dynamics inducing trajectory distributions with negligible error.\nExperimental evaluations across various real-world scenarios show that GDB\nsurpasses existing state-of-the-art approaches, opening up a new pathway for\naccurately bridging geometric states and tackling crucial scientific challenges\nwith improved accuracy and applicability.\n","authors":["Shengjie Luo","Yixian Xu","Di He","Shuxin Zheng","Tie-Yan Liu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24220v1.pdf","comment":"33 pages, 5 tables; NeurIPS 2024 Camera Ready version"},{"id":"http://arxiv.org/abs/2410.24218v1","updated":"2024-10-31T17:59:52Z","published":"2024-10-31T17:59:52Z","title":"Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use","summary":"  In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL\n","authors":["Jiajun Xi","Yinong He","Jianing Yang","Yinpei Dai","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2410.24218v1.pdf","comment":"EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL"},{"id":"http://arxiv.org/abs/2410.24216v1","updated":"2024-10-31T17:59:46Z","published":"2024-10-31T17:59:46Z","title":"CaAdam: Improving Adam optimizer using connection aware methods","summary":"  We introduce a new method inspired by Adam that enhances convergence speed\nand achieves better loss function minima. Traditional optimizers, including\nAdam, apply uniform or globally adjusted learning rates across neural networks\nwithout considering their architectural specifics. This architecture-agnostic\napproach is deeply embedded in most deep learning frameworks, where optimizers\nare implemented as standalone modules without direct access to the network's\nstructural information. For instance, in popular frameworks like Keras or\nPyTorch, optimizers operate solely on gradients and parameters, without\nknowledge of layer connectivity or network topology. Our algorithm, CaAdam,\nexplores this overlooked area by introducing connection-aware optimization\nthrough carefully designed proxies of architectural information. We propose\nmultiple scaling methodologies that dynamically adjust learning rates based on\neasily accessible structural properties such as layer depth, connection counts,\nand gradient distributions. This approach enables more granular optimization\nwhile working within the constraints of current deep learning frameworks.\nEmpirical evaluations on standard datasets (e.g., CIFAR-10, Fashion MNIST) show\nthat our method consistently achieves faster convergence and higher accuracy\ncompared to standard Adam optimizer, demonstrating the potential benefits of\nincorporating architectural awareness in optimization strategies.\n","authors":["Remi Genet","Hugo Inzirillo"],"pdf_url":"https://arxiv.org/pdf/2410.24216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24214v1","updated":"2024-10-31T17:59:37Z","published":"2024-10-31T17:59:37Z","title":"ARQ: A Mixed-Precision Quantization Framework for Accurate and\n  Certifiably Robust DNNs","summary":"  Mixed precision quantization has become an important technique for enabling\nthe execution of deep neural networks (DNNs) on limited resource computing\nplatforms. Traditional quantization methods have primarily concentrated on\nmaintaining neural network accuracy, either ignoring the impact of quantization\non the robustness of the network, or using only empirical techniques for\nimproving robustness. In contrast, techniques for robustness certification,\nwhich can provide strong guarantees about the robustness of DNNs have not been\nused during quantization due to their high computation cost.\n  This paper introduces ARQ, an innovative mixed-precision quantization method\nthat not only preserves the clean accuracy of the smoothed classifiers but also\nmaintains their certified robustness. ARQ uses reinforcement learning to find\naccurate and robust DNN quantization, while efficiently leveraging randomized\nsmoothing, a popular class of statistical DNN verification algorithms, to guide\nthe search process.\n  We compare ARQ with multiple state-of-the-art quantization techniques on\nseveral DNN architectures commonly used in quantization studies: ResNet-20 on\nCIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. We demonstrate\nthat ARQ consistently performs better than these baselines across all the\nbenchmarks and the input perturbation levels. In many cases, the performance of\nARQ quantized networks can reach that of the original DNN with floating-point\nweights, but with only 1.5% instructions.\n","authors":["Yuchen Yang","Shubham Ugare","Yifan Zhao","Gagandeep Singh","Sasa Misailovic"],"pdf_url":"https://arxiv.org/pdf/2410.24214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24210v1","updated":"2024-10-31T17:58:41Z","published":"2024-10-31T17:58:41Z","title":"TabM: Advancing Tabular Deep Learning with Parameter-Efficient\n  Ensembling","summary":"  Deep learning architectures for supervised learning on tabular data range\nfrom simple multilayer perceptrons (MLP) to sophisticated Transformers and\nretrieval-augmented methods. This study highlights a major, yet so far\noverlooked opportunity for substantially improving tabular MLPs: namely,\nparameter-efficient ensembling -- a paradigm for implementing an ensemble of\nmodels as one model producing multiple predictions. We start by developing TabM\n-- a simple model based on MLP and our variations of BatchEnsemble (an existing\ntechnique). Then, we perform a large-scale evaluation of tabular DL\narchitectures on public benchmarks in terms of both task performance and\nefficiency, which renders the landscape of tabular DL in a new light.\nGenerally, we show that MLPs, including TabM, form a line of stronger and more\npractical models compared to attention- and retrieval-based architectures. In\nparticular, we find that TabM demonstrates the best performance among tabular\nDL models. Lastly, we conduct an empirical analysis on the ensemble-like nature\nof TabM. For example, we observe that the multiple predictions of TabM are weak\nindividually, but powerful collectively. Overall, our work brings an impactful\ntechnique to tabular DL, analyses its behaviour, and advances the\nperformance-efficiency trade-off with TabM -- a simple and powerful baseline\nfor researchers and practitioners.\n","authors":["Yury Gorishniy","Akim Kotelnikov","Artem Babenko"],"pdf_url":"https://arxiv.org/pdf/2410.24210v1.pdf","comment":"Code: https://github.com/yandex-research/tabm"},{"id":"http://arxiv.org/abs/2406.15349v2","updated":"2024-10-31T17:58:34Z","published":"2024-06-21T17:59:02Z","title":"NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and\n  Benchmarking","summary":"  Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n","authors":["Daniel Dauner","Marcel Hallgarten","Tianyu Li","Xinshuo Weng","Zhiyu Huang","Zetong Yang","Hongyang Li","Igor Gilitschenski","Boris Ivanovic","Marco Pavone","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2406.15349v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.24206v1","updated":"2024-10-31T17:58:13Z","published":"2024-10-31T17:58:13Z","title":"Understanding Optimization in Deep Learning with Central Flows","summary":"  Optimization in deep learning remains poorly understood, even in the simple\nsetting of deterministic (i.e. full-batch) training. A key difficulty is that\nmuch of an optimizer's behavior is implicitly determined by complex oscillatory\ndynamics, referred to as the \"edge of stability.\" The main contribution of this\npaper is to show that an optimizer's implicit behavior can be explicitly\ncaptured by a \"central flow:\" a differential equation which models the\ntime-averaged optimization trajectory. We show that these flows can empirically\npredict long-term optimization trajectories of generic neural networks with a\nhigh degree of numerical accuracy. By interpreting these flows, we reveal for\nthe first time 1) the precise sense in which RMSProp adapts to the local loss\nlandscape, and 2) an \"acceleration via regularization\" mechanism, wherein\nadaptive optimizers implicitly navigate towards low-curvature regions in which\nthey can take larger steps. This mechanism is key to the efficacy of these\nadaptive optimizers. Overall, we believe that central flows constitute a\npromising tool for reasoning about optimization in deep learning.\n","authors":["Jeremy M. Cohen","Alex Damian","Ameet Talwalkar","Zico Kolter","Jason D. Lee"],"pdf_url":"https://arxiv.org/pdf/2410.24206v1.pdf","comment":"first two authors contributed equally; author order determined by\n  coin flip"},{"id":"http://arxiv.org/abs/2410.24198v1","updated":"2024-10-31T17:55:13Z","published":"2024-10-31T17:55:13Z","title":"SelfCodeAlign: Self-Alignment for Code Generation","summary":"  Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.\n","authors":["Yuxiang Wei","Federico Cassano","Jiawei Liu","Yifeng Ding","Naman Jain","Zachary Mueller","Harm de Vries","Leandro von Werra","Arjun Guha","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24198v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24185v1","updated":"2024-10-31T17:48:45Z","published":"2024-10-31T17:48:45Z","title":"DexMimicGen: Automated Data Generation for Bimanual Dexterous\n  Manipulation via Imitation Learning","summary":"  Imitation learning from human demonstrations is an effective means to teach\nrobots manipulation skills. But data acquisition is a major bottleneck in\napplying this paradigm more broadly, due to the amount of cost and human effort\ninvolved. There has been significant interest in imitation learning for\nbimanual dexterous robots, like humanoids. Unfortunately, data collection is\neven more challenging here due to the challenges of simultaneously controlling\nmultiple arms and multi-fingered hands. Automated data generation in simulation\nis a compelling, scalable alternative to fuel this need for data. To this end,\nwe introduce DexMimicGen, a large-scale automated data generation system that\nsynthesizes trajectories from a handful of human demonstrations for humanoid\nrobots with dexterous hands. We present a collection of simulation environments\nin the setting of bimanual dexterous manipulation, spanning a range of\nmanipulation behaviors and different requirements for coordination among the\ntwo arms. We generate 21K demos across these tasks from just 60 source human\ndemos and study the effect of several data generation and policy learning\ndecisions on agent performance. Finally, we present a real-to-sim-to-real\npipeline and deploy it on a real-world humanoid can sorting task. Videos and\nmore are at https://dexmimicgen.github.io/\n","authors":["Zhenyu Jiang","Yuqi Xie","Kevin Lin","Zhenjia Xu","Weikang Wan","Ajay Mandlekar","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.24185v1.pdf","comment":"Project website: https://dexmimicgen.github.io/"},{"id":"http://arxiv.org/abs/2410.24184v1","updated":"2024-10-31T17:47:01Z","published":"2024-10-31T17:47:01Z","title":"Group Crosscoders for Mechanistic Analysis of Symmetry","summary":"  We introduce group crosscoders, an extension of crosscoders that\nsystematically discover and analyse symmetrical features in neural networks.\nWhile neural networks often develop equivariant representations without\nexplicit architectural constraints, understanding these emergent symmetries has\ntraditionally relied on manual analysis. Group crosscoders automate this\nprocess by performing dictionary learning across transformed versions of inputs\nunder a symmetry group. Applied to InceptionV1's mixed3b layer using the\ndihedral group $\\mathrm{D}_{32}$, our method reveals several key insights:\nFirst, it naturally clusters features into interpretable families that\ncorrespond to previously hypothesised feature types, providing more precise\nseparation than standard sparse autoencoders. Second, our transform block\nanalysis enables the automatic characterisation of feature symmetries,\nrevealing how different geometric features (such as curves versus lines)\nexhibit distinct patterns of invariance and equivariance. These results\ndemonstrate that group crosscoders can provide systematic insights into how\nneural networks represent symmetry, offering a promising new tool for\nmechanistic interpretability.\n","authors":["Liv Gorton"],"pdf_url":"https://arxiv.org/pdf/2410.24184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24178v1","updated":"2024-10-31T17:43:53Z","published":"2024-10-31T17:43:53Z","title":"AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal\n  Properties","summary":"  Anomaly detection is widely used for identifying critical errors and\nsuspicious behaviors, but current methods lack interpretability. We leverage\ncommon properties of existing methods and recent advances in generative models\nto introduce counterfactual explanations for anomaly detection. Given an input,\nwe generate its counterfactual as a diffusion-based repair that shows what a\nnon-anomalous version should have looked like. A key advantage of this approach\nis that it enables a domain-independent formal specification of explainability\ndesiderata, offering a unified framework for generating and evaluating\nexplanations. We demonstrate the effectiveness of our anomaly explainability\nframework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI)\nanomaly datasets. The code used for the experiments is accessible at:\nhttps://github.com/xjiae/arpro.\n","authors":["Xiayan Ji","Anton Xue","Eric Wong","Oleg Sokolsky","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2410.24178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24177v1","updated":"2024-10-31T17:43:13Z","published":"2024-10-31T17:43:13Z","title":"DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models","summary":"  Spoken language models (SLMs) have gained increasing attention with\nadvancements in text-based, decoder-only language models. SLMs process text and\nspeech, enabling simultaneous speech understanding and generation. This paper\npresents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to\nimprove speech tokenization by bridging audio signals and SLM tokens. DC-Spin\nextracts speaker-invariant tokens rich in phonetic information and resilient to\ninput variations, enhancing zero-shot SLM tasks and speech resynthesis. We\npropose a chunk-wise approach to enable streamable DC-Spin without retraining\nand degradation. Comparisons of tokenization methods (self-supervised and\nneural audio codecs), model scalability, and downstream task proxies show that\ntokens easily modeled by an n-gram LM or aligned with phonemes offer strong\nperformance, providing insights for designing speech tokenizers for SLMs.\n","authors":["Heng-Jui Chang","Hongyu Gong","Changhan Wang","James Glass","Yu-An Chung"],"pdf_url":"https://arxiv.org/pdf/2410.24177v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.24169v1","updated":"2024-10-31T17:35:57Z","published":"2024-10-31T17:35:57Z","title":"The Importance of Being Scalable: Improving the Speed and Accuracy of\n  Neural Network Interatomic Potentials Across Chemical Domains","summary":"  Scaling has been critical in improving model performance and generalization\nin machine learning. It involves how a model's performance changes with\nincreases in model size or input data, as well as how efficiently computational\nresources are utilized to support this growth. Despite successes in other\nareas, the study of scaling in Neural Network Interatomic Potentials (NNIPs)\nremains limited. NNIPs act as surrogate models for ab initio quantum mechanical\ncalculations. The dominant paradigm here is to incorporate many physical domain\nconstraints into the model, such as rotational equivariance. We contend that\nthese complex constraints inhibit the scaling ability of NNIPs, and are likely\nto lead to performance plateaus in the long run. In this work, we take an\nalternative approach and start by systematically studying NNIP scaling\nstrategies. Our findings indicate that scaling the model through attention\nmechanisms is efficient and improves model expressivity. These insights\nmotivate us to develop an NNIP architecture designed for scalability: the\nEfficiently Scaled Attention Interatomic Potential (EScAIP). EScAIP leverages a\nmulti-head self-attention formulation within graph neural networks, applying\nattention at the neighbor-level representations. Implemented with\nhighly-optimized attention GPU kernels, EScAIP achieves substantial gains in\nefficiency--at least 10x faster inference, 5x less memory usage--compared to\nexisting NNIPs. EScAIP also achieves state-of-the-art performance on a wide\nrange of datasets including catalysts (OC20 and OC22), molecules (SPICE), and\nmaterials (MPTrj). We emphasize that our approach should be thought of as a\nphilosophy rather than a specific model, representing a proof-of-concept for\ndeveloping general-purpose NNIPs that achieve better expressivity through\nscaling, and continue to scale efficiently with increased computational\nresources and training data.\n","authors":["Eric Qu","Aditi S. Krishnapriyan"],"pdf_url":"https://arxiv.org/pdf/2410.24169v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.04690v3","updated":"2024-10-31T17:32:26Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\naim to massively improve upon existing infrastructure by providing two new\nmethods for implementing neighborhood attention. We first show that\nneighborhood attention can be represented as a batched GEMM problem, similar to\nstandard attention, and implement it for 1-D and 2-D neighborhood attention.\nThese kernels on average provide 895% and 272% improvement in full precision\nruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood\nattention respectively. We find that aside from being heavily bound by memory\nbandwidth, certain inherent inefficiencies exist in all unfused implementations\nof neighborhood attention, which in most cases undo their theoretical\nefficiency gain. Motivated by the progress made into fused dot-product\nattention kernels, we developed fused neighborhood attention; an adaptation of\nfused dot-product attention kernels that allow fine-grained control over\nattention across different spatial axes. Known for reducing the quadratic time\ncomplexity of self attention to a linear complexity, neighborhood attention can\nnow enjoy a reduced and constant memory footprint, and record-breaking half\nprecision runtime. We observe that our fused implementation successfully\ncircumvents some of the unavoidable inefficiencies in unfused\nimplementations...\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v3.pdf","comment":"To appear in 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2406.10796v2","updated":"2024-10-31T17:29:37Z","published":"2024-06-16T03:45:03Z","title":"Ab Initio Structure Solutions from Nanocrystalline Powder Diffraction\n  Data","summary":"  A major challenge in materials science is the determination of the structure\nof nanometer sized objects. Here we present a novel approach that uses a\ngenerative machine learning model based on diffusion processes that is trained\non 45,229 known structures. The model factors both the measured diffraction\npattern as well as relevant statistical priors on the unit cell of atomic\ncluster structures. Conditioned only on the chemical formula and the\ninformation-scarce finite-size broadened powder diffraction pattern, we find\nthat our model, PXRDnet, can successfully solve simulated nanocrystals as small\nas 10 angstroms across 200 materials of varying symmetry and complexity,\nincluding structures from all seven crystal systems. We show that our model can\nsuccessfully and verifiably determine structural candidates four out of five\ntimes, with average error among these candidates being only 7% (as measured by\npost-Rietveld refinement R-factor). Furthermore, PXRDnet is capable of solving\nstructures from noisy diffraction patterns gathered in real-world experiments.\nWe suggest that data driven approaches, bootstrapped from theoretical\nsimulation, will ultimately provide a path towards determining the structure of\npreviously unsolved nano-materials.\n","authors":["Gabe Guo","Tristan Saidi","Maxwell Terban","Michele Valsecchi","Simon JL Billinge","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2406.10796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24166v1","updated":"2024-10-31T17:28:41Z","published":"2024-10-31T17:28:41Z","title":"Approaches to human activity recognition via passive radar","summary":"  The thesis explores novel methods for Human Activity Recognition (HAR) using\npassive radar with a focus on non-intrusive Wi-Fi Channel State Information\n(CSI) data. Traditional HAR approaches often use invasive sensors like cameras\nor wearables, raising privacy issues. This study leverages the non-intrusive\nnature of CSI, using Spiking Neural Networks (SNN) to interpret signal\nvariations caused by human movements. These networks, integrated with symbolic\nreasoning frameworks such as DeepProbLog, enhance the adaptability and\ninterpretability of HAR systems. SNNs offer reduced power consumption, ideal\nfor privacy-sensitive applications. Experimental results demonstrate SNN-based\nneurosymbolic models achieve high accuracy making them a promising alternative\nfor HAR across various domains.\n","authors":["Christian Bresciani","Federico Cerutti","Marco Cominelli"],"pdf_url":"https://arxiv.org/pdf/2410.24166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24164v1","updated":"2024-10-31T17:22:30Z","published":"2024-10-31T17:22:30Z","title":"$π_0$: A Vision-Language-Action Flow Model for General Robot Control","summary":"  Robot learning holds tremendous promise to unlock the full potential of\nflexible, general, and dexterous robot systems, as well as to address some of\nthe deepest questions in artificial intelligence. However, bringing robot\nlearning to the level of generality required for effective real-world systems\nfaces major obstacles in terms of data, generalization, and robustness. In this\npaper, we discuss how generalist robot policies (i.e., robot foundation models)\ncan address these challenges, and how we can design effective generalist robot\npolicies for complex and highly dexterous tasks. We propose a novel flow\nmatching architecture built on top of a pre-trained vision-language model (VLM)\nto inherit Internet-scale semantic knowledge. We then discuss how this model\ncan be trained on a large and diverse dataset from multiple dexterous robot\nplatforms, including single-arm robots, dual-arm robots, and mobile\nmanipulators. We evaluate our model in terms of its ability to perform tasks in\nzero shot after pre-training, follow language instructions from people and from\na high-level VLM policy, and its ability to acquire new skills via fine-tuning.\nOur results cover a wide variety of tasks, such as laundry folding, table\ncleaning, and assembling boxes.\n","authors":["Kevin Black","Noah Brown","Danny Driess","Adnan Esmail","Michael Equi","Chelsea Finn","Niccolo Fusai","Lachy Groom","Karol Hausman","Brian Ichter","Szymon Jakubczak","Tim Jones","Liyiming Ke","Sergey Levine","Adrian Li-Bell","Mohith Mothukuri","Suraj Nair","Karl Pertsch","Lucy Xiaoyang Shi","James Tanner","Quan Vuong","Anna Walling","Haohuan Wang","Ury Zhilinsky"],"pdf_url":"https://arxiv.org/pdf/2410.24164v1.pdf","comment":"See project website for videos:\n  https://physicalintelligence.company/blog/pi0"},{"id":"http://arxiv.org/abs/2410.24162v1","updated":"2024-10-31T17:20:13Z","published":"2024-10-31T17:20:13Z","title":"Conformalized Prediction of Post-Fault Voltage Trajectories Using\n  Pre-trained and Finetuned Attention-Driven Neural Operators","summary":"  This paper proposes a new data-driven methodology for predicting intervals of\npost-fault voltage trajectories in power systems. We begin by introducing the\nQuantile Attention-Fourier Deep Operator Network (QAF-DeepONet), designed to\ncapture the complex dynamics of voltage trajectories and reliably estimate\nquantiles of the target trajectory without any distributional assumptions. The\nproposed operator regression model maps the observed portion of the voltage\ntrajectory to its unobserved post-fault trajectory. Our methodology employs a\npre-training and fine-tuning process to address the challenge of limited data\navailability. To ensure data privacy in learning the pre-trained model, we use\nmerging via federated learning with data from neighboring buses, enabling the\nmodel to learn the underlying voltage dynamics from such buses without directly\nsharing their data. After pre-training, we fine-tune the model with data from\nthe target bus, allowing it to adapt to unique dynamics and operating\nconditions. Finally, we integrate conformal prediction into the fine-tuned\nmodel to ensure coverage guarantees for the predicted intervals. We evaluated\nthe performance of the proposed methodology using the New England 39-bus test\nsystem considering detailed models of voltage and frequency controllers. Two\nmetrics, Prediction Interval Coverage Probability (PICP) and Prediction\nInterval Normalized Average Width (PINAW), are used to numerically assess the\nmodel's performance in predicting intervals. The results show that the proposed\napproach offers practical and reliable uncertainty quantification in predicting\nthe interval of post-fault voltage trajectories.\n","authors":["Amirhossein Mollaali","Gabriel Zufferey","Gonzalo Constante-Flores","Christian Moya","Can Li","Guang Lin","Meng Yue"],"pdf_url":"https://arxiv.org/pdf/2410.24162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12747v2","updated":"2024-10-31T17:18:16Z","published":"2024-06-18T16:07:33Z","title":"TSI-Bench: Benchmarking Time Series Imputation","summary":"  Effective imputation is a crucial preprocessing step for time series\nanalysis. Despite the development of numerous deep learning algorithms for time\nseries imputation, the community lacks standardized and comprehensive benchmark\nplatforms to effectively evaluate imputation performance across different\nsettings. Moreover, although many deep learning forecasting algorithms have\ndemonstrated excellent performance, whether their modelling achievements can be\ntransferred to time series imputation tasks remains unexplored. To bridge these\ngaps, we develop TSI-Bench, the first (to our knowledge) comprehensive\nbenchmark suite for time series imputation utilizing deep learning techniques.\nThe TSI-Bench pipeline standardizes experimental settings to enable fair\nevaluation of imputation algorithms and identification of meaningful insights\ninto the influence of domain-appropriate missing rates and patterns on model\nperformance. Furthermore, TSI-Bench innovatively provides a systematic paradigm\nto tailor time series forecasting algorithms for imputation purposes. Our\nextensive study across 34,804 experiments, 28 algorithms, and 8 datasets with\ndiverse missingness scenarios demonstrates TSI-Bench's effectiveness in diverse\ndownstream tasks and potential to unlock future directions in time series\nimputation research and analysis. All source code and experiment logs are\nreleased at https://github.com/WenjieDu/AwesomeImputation.\n","authors":["Wenjie Du","Jun Wang","Linglong Qian","Yiyuan Yang","Zina Ibrahim","Fanxing Liu","Zepu Wang","Haoxin Liu","Zhiyuan Zhao","Yingjie Zhou","Wenjia Wang","Kaize Ding","Yuxuan Liang","B. Aditya Prakash","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.12747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22382v2","updated":"2024-10-31T17:12:27Z","published":"2024-10-29T12:54:55Z","title":"Debiasing Alternative Data for Credit Underwriting Using Causal\n  Inference","summary":"  Alternative data provides valuable insights for lenders to evaluate a\nborrower's creditworthiness, which could help expand credit access to\nunderserved groups and lower costs for borrowers. But some forms of alternative\ndata have historically been excluded from credit underwriting because it could\nact as an illegal proxy for a protected class like race or gender, causing\nredlining. We propose a method for applying causal inference to a supervised\nmachine learning model to debias alternative data so that it might be used for\ncredit underwriting. We demonstrate how our algorithm can be used against a\npublic credit dataset to improve model accuracy across different racial groups,\nwhile providing theoretically robust nondiscrimination guarantees.\n","authors":["Chris Lam"],"pdf_url":"https://arxiv.org/pdf/2410.22382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24153v1","updated":"2024-10-31T17:10:57Z","published":"2024-10-31T17:10:57Z","title":"Dense Associative Memory Through the Lens of Random Features","summary":"  Dense Associative Memories are high storage capacity variants of the Hopfield\nnetworks that are capable of storing a large number of memory patterns in the\nweights of the network of a given size. Their common formulations typically\nrequire storing each pattern in a separate set of synaptic weights, which leads\nto the increase of the number of synaptic weights when new patterns are\nintroduced. In this work we propose an alternative formulation of this class of\nmodels using random features, commonly used in kernel methods. In this\nformulation the number of network's parameters remains fixed. At the same time,\nnew memories can be added to the network by modifying existing weights. We show\nthat this novel network closely approximates the energy function and dynamics\nof conventional Dense Associative Memories and shares their desirable\ncomputational properties.\n","authors":["Benjamin Hoover","Duen Horng Chau","Hendrik Strobelt","Parikshit Ram","Dmitry Krotov"],"pdf_url":"https://arxiv.org/pdf/2410.24153v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24145v1","updated":"2024-10-31T17:05:52Z","published":"2024-10-31T17:05:52Z","title":"Conformal prediction of circular data","summary":"  Split conformal prediction techniques are applied to regression problems with\ncircular responses by introducing a suitable conformity score, leading to\nprediction sets with adaptive arc length and finite-sample coverage guarantees\nfor any circular predictive model under exchangeable data. Leveraging the high\nperformance of existing predictive models designed for linear responses, we\nanalyze a general projection procedure that converts any linear response\nregression model into one suitable for circular responses. When random forests\nserve as basis models in this projection procedure, we harness the out-of-bag\ndynamics to eliminate the necessity for a separate calibration sample in the\nconstruction of prediction sets. For synthetic and real datasets the resulting\nprojected random forests model produces more efficient out-of-bag conformal\nprediction sets, with shorter median arc length, when compared to the split\nconformal prediction sets generated by two existing alternative models.\n","authors":["Paulo C. Marques F.","Rinaldo Artes","Helton Graziadei"],"pdf_url":"https://arxiv.org/pdf/2410.24145v1.pdf","comment":"7 pages; 4 figures"},{"id":"http://arxiv.org/abs/2402.07963v3","updated":"2024-10-31T17:05:49Z","published":"2024-02-12T10:32:47Z","title":"SPO: Sequential Monte Carlo Policy Optimisation","summary":"  Leveraging planning during learning and decision-making is central to the\nlong-term development of intelligent agents. Recent works have successfully\ncombined tree-based search methods and self-play learning mechanisms to this\nend. However, these methods typically face scaling challenges due to the\nsequential nature of their search. While practical engineering solutions can\npartly overcome this, they often result in a negative impact on performance. In\nthis paper, we introduce SPO: Sequential Monte Carlo Policy Optimisation, a\nmodel-based reinforcement learning algorithm grounded within the Expectation\nMaximisation (EM) framework. We show that SPO provides robust policy\nimprovement and efficient scaling properties. The sample-based search makes it\ndirectly applicable to both discrete and continuous action spaces without\nmodifications. We demonstrate statistically significant improvements in\nperformance relative to model-free and model-based baselines across both\ncontinuous and discrete environments. Furthermore, the parallel nature of SPO's\nsearch enables effective utilisation of hardware accelerators, yielding\nfavourable scaling laws.\n","authors":["Matthew V Macfarlane","Edan Toledo","Donal Byrne","Paul Duckworth","Alexandre Laterre"],"pdf_url":"https://arxiv.org/pdf/2402.07963v3.pdf","comment":"Accepted to NeurIPS 2024. 34 pages, 3 main figures"},{"id":"http://arxiv.org/abs/2402.18551v2","updated":"2024-10-31T17:01:45Z","published":"2024-02-28T18:34:53Z","title":"Implicit Optimization Bias of Next-Token Prediction in Linear Models","summary":"  We initiate an investigation into the optimization properties of next-token\nprediction (NTP), the dominant training paradigm for modern language models.\nSpecifically, we study the structural properties of the solutions selected by\ngradient-based optimizers among the many possible minimizers of the NTP\nobjective. By framing NTP as cross-entropy minimization across distinct\ncontexts, each tied with a sparse conditional probability distribution across a\nfinite vocabulary of tokens, we introduce \"NTP-separability conditions\" that\nenable reaching the data-entropy lower bound. With this setup, and focusing on\nlinear models with fixed context embeddings, we characterize the optimization\nbias of gradient descent (GD): Within the data subspace defined by the sparsity\npatterns of distinct contexts, GD selects parameters that equate the logits'\ndifferences of in-support tokens to their log-odds. In the orthogonal subspace,\nthe GD parameters diverge in norm and select the direction that maximizes a\nmargin specific to NTP. These findings extend previous research on implicit\nbias in one-hot classification to the NTP setting, highlighting key differences\nand prompting further research into the optimization and generalization\nproperties of NTP, irrespective of the specific architecture used to generate\nthe context embeddings.\n","authors":["Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2402.18551v2.pdf","comment":"v2: fixed typos and writing in various parts; updated figures and\n  future-work section"},{"id":"http://arxiv.org/abs/2407.01079v3","updated":"2024-10-31T16:59:13Z","published":"2024-07-01T08:34:40Z","title":"On Statistical Rates and Provably Efficient Criteria of Latent Diffusion\n  Transformers (DiTs)","summary":"  We investigate the statistical and computational limits of latent Diffusion\nTransformers (DiTs) under the low-dimensional linear latent space assumption.\nStatistically, we study the universal approximation and sample complexity of\nthe DiTs score function, as well as the distribution recovery property of the\ninitial data. Specifically, under mild data assumptions, we derive an\napproximation error bound for the score network of latent DiTs, which is\nsub-linear in the latent space dimension. Additionally, we derive the\ncorresponding sample complexity bound and show that the data distribution\ngenerated from the estimated score function converges toward a proximate area\nof the original one. Computationally, we characterize the hardness of both\nforward inference and backward computation of latent DiTs, assuming the Strong\nExponential Time Hypothesis (SETH). For forward inference, we identify\nefficient criteria for all possible latent DiTs inference algorithms and\nshowcase our theory by pushing the efficiency toward almost-linear time\ninference. For backward computation, we leverage the low-rank structure within\nthe gradient computation of DiTs training for possible algorithmic speedup.\nSpecifically, we show that such speedup achieves almost-linear time latent DiTs\ntraining by casting the DiTs gradient as a series of chained low-rank\napproximations with bounded error. Under the low-dimensional assumption, we\nshow that the statistical rates and the computational efficiency are all\ndominated by the dimension of the subspace, suggesting that latent DiTs have\nthe potential to bypass the challenges associated with the high dimensionality\nof initial data.\n","authors":["Jerry Yao-Chieh Hu","Weimin Wu","Zhao Song","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2407.01079v3.pdf","comment":"Accepted at NeurIPS 2024. v3 updated to camera-ready version with\n  many typos fixed; v2 fixed typos, added Fig. 1 and added clarifications"},{"id":"http://arxiv.org/abs/2406.03636v4","updated":"2024-10-31T16:54:30Z","published":"2024-06-05T22:16:19Z","title":"Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource\n  Programming and Formal Languages","summary":"  Recent advances in large language models (LLMs) for code applications have\ndemonstrated remarkable zero-shot fluency and instruction following on\nchallenging code related tasks ranging from test case generation to\nself-repair. Unsurprisingly, however, models struggle to compose syntactically\nvalid programs in programming languages unrepresented in pre-training, referred\nto as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial\nsettings, including domain-specific languages for internal tools, tool-chains\nfor legacy languages, and formal verification frameworks. Inspired by a\ntechnique called natural programming elicitation, we propose designing an\nintermediate language that LLMs \"naturally\" know how to use and which can be\nautomatically compiled to a target VLPL. When LLMs generate code that lies\noutside of this intermediate language, we use compiler techniques to repair the\ncode into programs in the intermediate language. Overall, we introduce\n\\emph{synthetic programming elicitation and compilation} (SPEAC), an approach\nthat enables LLMs to generate syntactically valid code even for VLPLs. We\nempirically evaluate the performance of SPEAC in a case study for the UCLID5\nformal verification language and find that, compared to existing retrieval and\nfine-tuning baselines, SPEAC produces syntactically correct programs more\nfrequently and without sacrificing semantic correctness.\n","authors":["Federico Mora","Justin Wong","Haley Lepe","Sahil Bhatia","Karim Elmaaroufi","George Varghese","Joseph E. Gonzalez","Elizabeth Polgreen","Sanjit A. Seshia"],"pdf_url":"https://arxiv.org/pdf/2406.03636v4.pdf","comment":"14 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.24128v1","updated":"2024-10-31T16:53:20Z","published":"2024-10-31T16:53:20Z","title":"Q-learning for Quantile MDPs: A Decomposition, Performance, and\n  Convergence Analysis","summary":"  In Markov decision processes (MDPs), quantile risk measures such as\nValue-at-Risk are a standard metric for modeling RL agents' preferences for\ncertain outcomes. This paper proposes a new Q-learning algorithm for quantile\noptimization in MDPs with strong convergence and performance guarantees. The\nalgorithm leverages a new, simple dynamic program (DP) decomposition for\nquantile MDPs. Compared with prior work, our DP decomposition requires neither\nknown transition probabilities nor solving complex saddle point equations and\nserves as a suitable foundation for other model-free RL algorithms. Our\nnumerical results in tabular domains show that our Q-learning algorithm\nconverges to its DP variant and outperforms earlier algorithms.\n","authors":["Jia Lin Hau","Erick Delage","Esther Derman","Mohammad Ghavamzadeh","Marek Petrik"],"pdf_url":"https://arxiv.org/pdf/2410.24128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01903v2","updated":"2024-10-31T16:49:26Z","published":"2024-07-02T03:08:20Z","title":"Text-Aware Diffusion for Policy Learning","summary":"  Training an agent to achieve particular goals or perform desired behaviors is\noften accomplished through reinforcement learning, especially in the absence of\nexpert demonstrations. However, supporting novel goals or behaviors through\nreinforcement learning requires the ad-hoc design of appropriate reward\nfunctions, which quickly becomes intractable. To address this challenge, we\npropose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a\npretrained, frozen text-conditioned diffusion model to compute dense zero-shot\nreward signals for text-aligned policy learning. We hypothesize that\nlarge-scale pretrained generative models encode rich priors that can supervise\na policy to behave not only in a text-aligned manner, but also in alignment\nwith a notion of naturalness summarized from internet-scale training data. In\nour experiments, we demonstrate that TADPoLe is able to learn policies for\nnovel goal-achievement and continuous locomotion behaviors specified by natural\nlanguage, in both Humanoid and Dog environments. The behaviors are learned\nzero-shot without ground-truth rewards or expert demonstrations, and are\nqualitatively more natural according to human evaluation. We further show that\nTADPoLe performs competitively when applied to robotic manipulation tasks in\nthe Meta-World environment, without having access to any in-domain\ndemonstrations.\n","authors":["Calvin Luo","Mandy He","Zilai Zeng","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2407.01903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07451v2","updated":"2024-10-31T16:48:40Z","published":"2024-06-11T16:57:48Z","title":"An Optimism-based Approach to Online Evaluation of Generative Models","summary":"  Existing frameworks for evaluating and comparing generative models typically\ntarget an offline setting, where the evaluator has access to full batches of\ndata produced by the models. However, in many practical scenarios, the goal is\nto identify the best model using the fewest generated samples to minimize the\ncosts of querying data from the models. Such an online comparison is\nchallenging with current offline assessment methods. In this work, we propose\nan online evaluation framework to find the generative model that maximizes a\nstandard assessment score among a group of available models. Our method uses an\noptimism-based multi-armed bandit framework to identify the model producing\ndata with the highest evaluation score, quantifying the quality and diversity\nof generated data. Specifically, we study the online assessment of generative\nmodels based on the Fr\\'echet Inception Distance (FID) and Inception Score (IS)\nmetrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper\nconfidence bound approach in online learning. We prove sub-linear regret bounds\nfor these algorithms and present numerical results on standard image datasets,\ndemonstrating their effectiveness in identifying the score-maximizing\ngenerative model.\n","authors":["Xiaoyan Hu","Ho-fung Leung","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2406.07451v2.pdf","comment":"arXiv version"},{"id":"http://arxiv.org/abs/2410.24117v1","updated":"2024-10-31T16:46:52Z","published":"2024-10-31T16:46:52Z","title":"Repository-Level Compositional Code Translation and Validation","summary":"  Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans translated the entire repository of\nthese projects consisting of 6899 source code fragments. 99.1% of the\ntranslated code fragments are syntactically correct, and AlphaTrans validates\nthe translations' runtime behavior and functional correctness for 25.8%. On\naverage, the integrated translation and validation take 36 hours to translate a\nproject, showing its scalability in practice. For the syntactically or\nsemantically incorrect translations, AlphaTrans generates a report including\nexisting translation, stack trace, test errors, or assertion failures. We\nprovided these artifacts to two developers to fix the translation bugs in four\nprojects. They were able to fix the issues in 20.1 hours on average and achieve\nall passing tests.\n","authors":["Ali Reza Ibrahimzada","Kaiyao Ke","Mrigank Pawagi","Muhammad Salman Abid","Rangeet Pan","Saurabh Sinha","Reyhaneh Jabbarvand"],"pdf_url":"https://arxiv.org/pdf/2410.24117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24116v1","updated":"2024-10-31T16:46:23Z","published":"2024-10-31T16:46:23Z","title":"AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level\n  Classification and Localization","summary":"  Image labeling is a critical bottleneck in the development of computer vision\ntechnologies, often constraining the potential of machine learning models due\nto the time-intensive nature of manual annotations. This work introduces a\nnovel approach that leverages outpainting to address the problem of annotated\ndata scarcity by generating artificial contexts and annotations, significantly\nreducing manual labeling efforts. We apply this technique to a particularly\nacute challenge in autonomous driving, urban planning, and environmental\nmonitoring: the lack of diverse, eye-level vehicle images in desired classes.\nOur dataset comprises AI-generated vehicle images obtained by detecting and\ncropping vehicles from manually selected seed images, which are then outpainted\nonto larger canvases to simulate varied real-world conditions. The outpainted\nimages include detailed annotations, providing high-quality ground truth data.\nAdvanced outpainting techniques and image quality assessments ensure visual\nfidelity and contextual relevance. Augmentation with outpainted vehicles\nimproves overall performance metrics by up to 8\\% and enhances prediction of\nunderrepresented classes by up to 20\\%. This approach, exemplifying outpainting\nas a self-annotating paradigm, presents a solution that enhances dataset\nversatility across multiple domains of machine learning. The code and links to\ndatasets used in this study are available for further research and replication\nat https://github.com/amir-kazemi/aidovecl.\n","authors":["Amir Kazemi","Qurat ul ain Fatima","Volodymyr Kindratenko","Christopher Tessum"],"pdf_url":"https://arxiv.org/pdf/2410.24116v1.pdf","comment":"19 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2402.07307v3","updated":"2024-10-31T16:39:11Z","published":"2024-02-11T21:12:21Z","title":"Self-Calibrating Conformal Prediction","summary":"  In machine learning, model calibration and predictive inference are essential\nfor producing reliable predictions and quantifying uncertainty to support\ndecision-making. Recognizing the complementary roles of point and interval\npredictions, we introduce Self-Calibrating Conformal Prediction, a method that\ncombines Venn-Abers calibration and conformal prediction to deliver calibrated\npoint predictions alongside prediction intervals with finite-sample validity\nconditional on these predictions. To achieve this, we extend the original\nVenn-Abers procedure from binary classification to regression. Our theoretical\nframework supports analyzing conformal prediction methods that involve\ncalibrating model predictions and subsequently constructing conditionally valid\nprediction intervals on the same data, where the conditioning set or conformity\nscores may depend on the calibrated predictions. Real-data experiments show\nthat our method improves interval efficiency through model calibration and\noffers a practical alternative to feature-conditional validity.\n","authors":["Lars van der Laan","Ahmed M. Alaa"],"pdf_url":"https://arxiv.org/pdf/2402.07307v3.pdf","comment":"Accepted to Neurips 2024. Preprint previously titled Self-Consistent\n  Conformal Prediction.\n  https://openreview.net/forum?id=BJ6HkT7qIk&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2024%2FConference%2FAuthors%23your-submissions)"},{"id":"http://arxiv.org/abs/2410.24108v1","updated":"2024-10-31T16:38:51Z","published":"2024-10-31T16:38:51Z","title":"Reinforcement Learning Gradients as Vitamin for Online Finetuning\n  Decision Transformers","summary":"  Decision Transformers have recently emerged as a new and compelling paradigm\nfor offline Reinforcement Learning (RL), completing a trajectory in an\nautoregressive way. While improvements have been made to overcome initial\nshortcomings, online finetuning of decision transformers has been surprisingly\nunder-explored. The widely adopted state-of-the-art Online Decision Transformer\n(ODT) still struggles when pretrained with low-reward offline data. In this\npaper, we theoretically analyze the online-finetuning of the decision\ntransformer, showing that the commonly used Return-To-Go (RTG) that's far from\nthe expected return hampers the online fine-tuning process. This problem,\nhowever, is well-addressed by the value function and advantage of standard RL\nalgorithms. As suggested by our analysis, in our experiments, we hence find\nthat simply adding TD3 gradients to the finetuning process of ODT effectively\nimproves the online finetuning performance of ODT, especially if ODT is\npretrained with low-reward offline data. These findings provide new directions\nto further improve decision transformers.\n","authors":["Kai Yan","Alexander G. Schwing","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24108v1.pdf","comment":"Accepted as NeurIPS 2024 spotlight. 33 pages, 26 figures"},{"id":"http://arxiv.org/abs/2410.24106v1","updated":"2024-10-31T16:37:25Z","published":"2024-10-31T16:37:25Z","title":"On Sampling Strategies for Spectral Model Sharding","summary":"  The problem of heterogeneous clients in federated learning has recently drawn\na lot of attention. Spectral model sharding, i.e., partitioning the model\nparameters into low-rank matrices based on the singular value decomposition,\nhas been one of the proposed solutions for more efficient on-device training in\nsuch settings. In this work, we present two sampling strategies for such\nsharding, obtained as solutions to specific optimization problems. The first\nproduces unbiased estimators of the original weights, while the second aims to\nminimize the squared approximation error. We discuss how both of these\nestimators can be incorporated in the federated learning loop and practical\nconsiderations that arise during local training. Empirically, we demonstrate\nthat both of these methods can lead to improved performance on various commonly\nused datasets.\n","authors":["Denis Korzhenkov","Christos Louizos"],"pdf_url":"https://arxiv.org/pdf/2410.24106v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.15892v3","updated":"2024-10-31T16:36:27Z","published":"2024-07-22T01:52:30Z","title":"Mini-Sequence Transformer: Optimizing Intermediate Memory for Long\n  Sequences Training","summary":"  We introduce Mini-Sequence Transformer (MsT), a simple and effective\nmethodology for highly efficient and accurate LLM training with extremely long\nsequences. MsT partitions input sequences and iteratively processes\nmini-sequences to reduce intermediate memory usage. Integrated with activation\nrecomputation, it enables significant memory savings in both forward and\nbackward passes. In experiments with the Llama3-8B model, with MsT, we measure\nno degradation in throughput or convergence even with 12x longer sequences than\nstandard implementations. MsT is fully general, implementation-agnostic, and\nrequires minimal code changes to integrate with existing LLM training\nframeworks. Integrated with the huggingface library, MsT successfully extends\nthe maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x.\n","authors":["Cheng Luo","Jiawei Zhao","Zhuoming Chen","Beidi Chen","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2407.15892v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14919v2","updated":"2024-10-31T16:36:14Z","published":"2024-10-19T00:33:51Z","title":"Adversarial Score identity Distillation: Rapidly Surpassing the Teacher\n  in One Step","summary":"  Score identity Distillation (SiD) is a data-free method that has achieved\nstate-of-the-art performance in image generation by leveraging only a\npretrained diffusion model, without requiring any training data. However, the\nultimate performance of SiD is constrained by the accuracy with which the\npretrained model captures the true data scores at different stages of the\ndiffusion process. In this paper, we introduce SiDA (SiD with Adversarial\nLoss), which not only enhances generation quality but also improves\ndistillation efficiency by incorporating real images and adversarial loss. SiDA\nutilizes the encoder from the generator's score network as a discriminator,\nboosting its ability to distinguish between real images and those generated by\nSiD. The adversarial loss is batch-normalized within each GPU and then combined\nwith the original SiD loss. This integration effectively incorporates the\naverage \"fakeness\" per GPU batch into the pixel-based SiD loss, enabling SiDA\nto distill a single-step generator either from scratch or by fine-tuning an\nexisting one. SiDA converges significantly faster than its predecessor when\ntrained from scratch, and swiftly improves upon the original model's\nperformance after an initial warmup period during fine-tuning from a\npre-distilled SiD generator. This one-step adversarial distillation method\nestablishes new benchmarks in generation performance when distilling EDM\ndiffusion models pretrained on CIFAR-10 (32x32) and ImageNet (64x64), achieving\nFID score of 1.110 on ImageNet 64x64. It sets record-low FID scores when\ndistilling EDM2 models trained on ImageNet (512x512), surpassing even the\nlargest teacher model, EDM2-XXL. Our SiDA's results record FID scores of 2.156\nfor EDM2-XS, 1.669 for EDM2-S, 1.488 for EDM2-M, and 1.465 for EDM2-L,\ndemonstrating significant improvements across all model sizes. Our open-source\ncode will be integrated into the SiD codebase.\n","authors":["Mingyuan Zhou","Huangjie Zheng","Yi Gu","Zhendong Wang","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24105v1","updated":"2024-10-31T16:34:03Z","published":"2024-10-31T16:34:03Z","title":"Matchmaker: Self-Improving Large Language Model Programs for Schema\n  Matching","summary":"  Schema matching -- the task of finding matches between attributes across\ndisparate data sources with different tables and hierarchies -- is critical for\ncreating interoperable machine learning (ML)-ready data. Addressing this\nfundamental data-centric problem has wide implications, especially in domains\nlike healthcare, finance and e-commerce -- but also has the potential to\nbenefit ML models more generally, by increasing the data available for ML model\ntraining. However, schema matching is a challenging ML task due to\nstructural/hierarchical and semantic heterogeneity between different schemas.\nPrevious ML approaches to automate schema matching have either required\nsignificant labeled data for model training, which is often unrealistic or\nsuffer from poor zero-shot performance. To this end, we propose Matchmaker - a\ncompositional language model program for schema matching, comprised of\ncandidate generation, refinement and confidence scoring. Matchmaker also\nself-improves in a zero-shot manner without the need for labeled demonstrations\nvia a novel optimization approach, which constructs synthetic in-context\ndemonstrations to guide the language model's reasoning process. Empirically, we\ndemonstrate on real-world medical schema matching benchmarks that Matchmaker\noutperforms previous ML-based approaches, highlighting its potential to\naccelerate data integration and interoperability of ML-ready data.\n","authors":["Nabeel Seedat","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2410.24105v1.pdf","comment":"Accepted to NeurIPS 2024, GenAI for Health Workshop and Table\n  Representation Learning Workshop"},{"id":"http://arxiv.org/abs/2410.24104v1","updated":"2024-10-31T16:33:40Z","published":"2024-10-31T16:33:40Z","title":"Clustering to Minimize Cluster-Aware Norm Objectives","summary":"  We initiate the study of the following general clustering problem. We seek to\npartition a given set $P$ of data points into $k$ clusters by finding a set $X$\nof $k$ centers and assigning each data point to one of the centers. The cost of\na cluster, represented by a center $x\\in X$, is a monotone, symmetric norm $f$\n(inner norm) of the vector of distances of points assigned to $x$. The goal is\nto minimize a norm $g$ (outer norm) of the vector of cluster costs. This\nproblem, which we call $(f,g)$-Clustering, generalizes many fundamental\nclustering problems such as $k$-Center, $k$-Median , Min-Sum of Radii, and\nMin-Load $k$-Clustering . A recent line of research (Chakrabarty, Swamy\n[STOC'19]) studies norm objectives that are oblivious to the cluster structure\nsuch as $k$-Median and $k$-Center. In contrast, our problem models\ncluster-aware objectives including Min-Sum of Radii and Min-Load\n$k$-Clustering.\n  Our main results are as follows. First, we design a constant-factor\napproximation algorithm for $(\\textsf{top}_\\ell,\\mathcal{L}_1)$-Clustering\nwhere the inner norm ($\\textsf{top}_\\ell$) sums over the $\\ell$ largest\ndistances. Second, we design a constant-factor approximation\\ for\n$(\\mathcal{L}_\\infty,\\textsf{Ord})$-Clustering where the outer norm is a convex\ncombination of $\\textsf{top}_\\ell$ norms (ordered weighted norm).\n","authors":["Martin G. Herold","Evangelos Kipouridis","Joachim Spoerhase"],"pdf_url":"https://arxiv.org/pdf/2410.24104v1.pdf","comment":"accepted at SODA 2025"},{"id":"http://arxiv.org/abs/2410.24100v1","updated":"2024-10-31T16:30:08Z","published":"2024-10-31T16:30:08Z","title":"Benchmark Data Repositories for Better Benchmarking","summary":"  In machine learning research, it is common to evaluate algorithms via their\nperformance on standard benchmark datasets. While a growing body of work\nestablishes guidelines for -- and levies criticisms at -- data and benchmarking\npractices in machine learning, comparatively less attention has been paid to\nthe data repositories where these datasets are stored, documented, and shared.\nIn this paper, we analyze the landscape of these $\\textit{benchmark data\nrepositories}$ and the role they can play in improving benchmarking. This role\nincludes addressing issues with both datasets themselves (e.g.,\nrepresentational harms, construct validity) and the manner in which evaluation\nis carried out using such datasets (e.g., overemphasis on a few datasets and\nmetrics, lack of reproducibility). To this end, we identify and discuss a set\nof considerations surrounding the design and use of benchmark data\nrepositories, with a focus on improving benchmarking practices in machine\nlearning.\n","authors":["Rachel Longjohn","Markelle Kelly","Sameer Singh","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2410.24100v1.pdf","comment":"Accepted to NeurIPS Datasets and Benchmarks 2024"},{"id":"http://arxiv.org/abs/2403.11574v2","updated":"2024-10-31T16:29:10Z","published":"2024-03-18T08:50:30Z","title":"Offline Multitask Representation Learning for Reinforcement Learning","summary":"  We study offline multitask representation learning in reinforcement learning\n(RL), where a learner is provided with an offline dataset from different tasks\nthat share a common representation and is asked to learn the shared\nrepresentation. We theoretically investigate offline multitask low-rank RL, and\npropose a new algorithm called MORL for offline multitask representation\nlearning. Furthermore, we examine downstream RL in reward-free, offline and\nonline scenarios, where a new task is introduced to the agent that shares the\nsame representation as the upstream offline tasks. Our theoretical results\ndemonstrate the benefits of using the learned representation from the upstream\noffline task instead of directly learning the representation of the low-rank\nmodel.\n","authors":["Haque Ishfaq","Thanh Nguyen-Tang","Songtao Feng","Raman Arora","Mengdi Wang","Ming Yin","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2403.11574v2.pdf","comment":"Accepted to 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.24096v1","updated":"2024-10-31T16:28:33Z","published":"2024-10-31T16:28:33Z","title":"Progressive Safeguards for Safe and Model-Agnostic Reinforcement\n  Learning","summary":"  In this paper we propose a formal, model-agnostic meta-learning framework for\nsafe reinforcement learning. Our framework is inspired by how parents safeguard\ntheir children across a progression of increasingly riskier tasks, imparting a\nsense of safety that is carried over from task to task. We model this as a\nmeta-learning process where each task is synchronized with a safeguard that\nmonitors safety and provides a reward signal to the agent. The safeguard is\nimplemented as a finite-state machine based on a safety specification; the\nreward signal is formally shaped around this specification. The safety\nspecification and its corresponding safeguard can be arbitrarily complex and\nnon-Markovian, which adds flexibility to the training process and\nexplainability to the learned policy. The design of the safeguard is manual but\nit is high-level and model-agnostic, which gives rise to an end-to-end safe\nlearning approach with wide applicability, from pixel-level game control to\nlanguage model fine-tuning. Starting from a given set of safety specifications\n(tasks), we train a model such that it can adapt to new specifications using\nonly a small number of training samples. This is made possible by our method\nfor efficiently transferring safety bias between tasks, which effectively\nminimizes the number of safety violations. We evaluate our framework in a\nMinecraft-inspired Gridworld, a VizDoom game environment, and an LLM\nfine-tuning application. Agents trained with our approach achieve near-minimal\nsafety violations, while baselines are shown to underperform.\n","authors":["Nabil Omi","Hosein Hasanbeig","Hiteshi Sharma","Sriram K. Rajamani","Siddhartha Sen"],"pdf_url":"https://arxiv.org/pdf/2410.24096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24091v1","updated":"2024-10-31T16:22:53Z","published":"2024-10-31T16:22:53Z","title":"3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing","summary":"  Tactile and visual perception are both crucial for humans to perform\nfine-grained interactions with their environment. Developing similar\nmulti-modal sensing capabilities for robots can significantly enhance and\nexpand their manipulation skills. This paper introduces \\textbf{3D-ViTac}, a\nmulti-modal sensing and learning system designed for dexterous bimanual\nmanipulation. Our system features tactile sensors equipped with dense sensing\nunits, each covering an area of 3$mm^2$. These sensors are low-cost and\nflexible, providing detailed and extensive coverage of physical contacts,\neffectively complementing visual information. To integrate tactile and visual\ndata, we fuse them into a unified 3D representation space that preserves their\n3D structures and spatial relationships. The multi-modal representation can\nthen be coupled with diffusion policies for imitation learning. Through\nconcrete hardware experiments, we demonstrate that even low-cost robots can\nperform precise manipulations and significantly outperform vision-only\npolicies, particularly in safe interactions with fragile items and executing\nlong-horizon tasks involving in-hand manipulation. Our project page is\navailable at \\url{https://binghao-huang.github.io/3D-ViTac/}.\n","authors":["Binghao Huang","Yixuan Wang","Xinyi Yang","Yiyue Luo","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2410.24091v1.pdf","comment":"Accepted at Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2410.24089v1","updated":"2024-10-31T16:21:41Z","published":"2024-10-31T16:21:41Z","title":"Demystifying Linear MDPs and Novel Dynamics Aggregation Framework","summary":"  In this work, we prove that, in linear MDPs, the feature dimension $d$ is\nlower bounded by $S/U$ in order to aptly represent transition probabilities,\nwhere $S$ is the size of the state space and $U$ is the maximum size of\ndirectly reachable states. Hence, $d$ can still scale with $S$ depending on the\ndirect reachability of the environment. To address this limitation of linear\nMDPs, we propose a novel structural aggregation framework based on dynamics,\nnamed as the \"dynamics aggregation\". For this newly proposed framework, we\ndesign a provably efficient hierarchical reinforcement learning algorithm in\nlinear function approximation that leverages aggregated sub-structures. Our\nproposed algorithm exhibits statistical efficiency, achieving a regret of $\n\\tilde{O} ( d_{\\psi}^{3/2} H^{3/2}\\sqrt{ N T} )$, where $d_{\\psi}$ represents\nthe feature dimension of aggregated subMDPs and $N$ signifies the number of\naggregated subMDPs. We establish that the condition $d_{\\psi}^3 N \\ll d^{3}$ is\nreadily met in most real-world environments with hierarchical structures,\nenabling a substantial improvement in the regret bound compared to LSVI-UCB,\nwhich enjoys a regret of $ \\tilde{O} (d^{3/2} H^{3/2} \\sqrt{ T})$. To the best\nof our knowledge, this work presents the first HRL algorithm with linear\nfunction approximation that offers provable guarantees.\n","authors":["Joongkyu Lee","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2410.24089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24087v1","updated":"2024-10-31T16:20:04Z","published":"2024-10-31T16:20:04Z","title":"In-Context Fine-Tuning for Time-Series Foundation Models","summary":"  Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain.\n","authors":["Abhimanyu Das","Matthew Faw","Rajat Sen","Yichen Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.24087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24079v1","updated":"2024-10-31T16:16:18Z","published":"2024-10-31T16:16:18Z","title":"Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects\n  Models","summary":"  Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and\noften requires advanced sampling techniques like Markov chain Monte Carlo\n(MCMC). A common approach is to write the model in a probabilistic programming\nlanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there are\nmany ways a user can transform a model that make inference more or less\nefficient. In particular, marginalizing some variables can greatly improve\ninference but is difficult for users to do manually. We develop an algorithm to\neasily marginalize random effects in LMMs. A naive approach introduces cubic\ntime operations within an inference algorithm like HMC, but we reduce the\nrunning time to linear using fast linear algebra techniques. We show that\nmarginalization is always beneficial when applicable and highlight improvements\nin various models, especially ones from cognitive sciences.\n","authors":["Jinlin Lai","Daniel Sheldon","Justin Domke"],"pdf_url":"https://arxiv.org/pdf/2410.24079v1.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2409.17446v2","updated":"2024-10-31T16:16:00Z","published":"2024-09-26T00:38:18Z","title":"Efficient Federated Learning against Heterogeneous and Non-stationary\n  Client Unavailability","summary":"  Addressing intermittent client availability is critical for the real-world\ndeployment of federated learning algorithms. Most prior work either overlooks\nthe potential non-stationarity in the dynamics of client unavailability or\nrequires substantial memory/computation overhead. We study federated learning\nin the presence of heterogeneous and non-stationary client availability, which\nmay occur when the deployment environments are uncertain, or the clients are\nmobile. The impacts of heterogeneity and non-stationarity on client\nunavailability can be significant, as we illustrate using FedAvg, the most\nwidely adopted federated learning algorithm. We propose FedAPM, which includes\nnovel algorithmic structures that (i) compensate for missed computations due to\nunavailability with only $O(1)$ additional memory and computation with respect\nto standard FedAvg, and (ii) evenly diffuse local updates within the federated\nlearning system through implicit gossiping, despite being agnostic to\nnon-stationary dynamics. We show that FedAPM converges to a stationary point of\neven non-convex objectives while achieving the desired linear speedup property.\nWe corroborate our analysis with numerical experiments over diversified client\nunavailability dynamics on real-world data sets.\n","authors":["Ming Xiang","Stratis Ioannidis","Edmund Yeh","Carlee Joe-Wong","Lili Su"],"pdf_url":"https://arxiv.org/pdf/2409.17446v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17518v2","updated":"2024-10-31T16:10:42Z","published":"2024-10-23T02:50:05Z","title":"Univariate Conditional Variational Autoencoder for Morphogenic Patterns\n  Design in Frontal Polymerization-Based Manufacturing","summary":"  Under some initial and boundary conditions, the rapid reaction-thermal\ndiffusion process taking place during frontal polymerization (FP) destabilizes\nthe planar mode of front propagation, leading to spatially varying, complex\nhierarchical patterns in thermoset polymeric materials. Although modern\nreaction-diffusion models can predict the patterns resulting from unstable FP,\nthe inverse design of patterns, which aims to retrieve process conditions that\nproduce a desired pattern, remains an open challenge due to the non-unique and\nnon-intuitive mapping between process conditions and manufactured patterns. In\nthis work, we propose a probabilistic generative model named univariate\nconditional variational autoencoder (UcVAE) for the inverse design of\nhierarchical patterns in FP-based manufacturing. Unlike the cVAE, which encodes\nboth the design space and the design target, the UcVAE encodes only the design\nspace. In the encoder of the UcVAE, the number of training parameters is\nsignificantly reduced compared to the cVAE, resulting in a shorter training\ntime while maintaining comparable performance. Given desired pattern images,\nthe trained UcVAE can generate multiple process condition solutions that\nproduce high-fidelity hierarchical patterns.\n","authors":["Qibang Liu","Pengfei Cai","Diab Abueidda","Sagar Vyas","Seid Koric","Rafael Gomez-Bombarelli","Philippe Geubelle"],"pdf_url":"https://arxiv.org/pdf/2410.17518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24071v1","updated":"2024-10-31T16:07:22Z","published":"2024-10-31T16:07:22Z","title":"Local Linearity: the Key for No-regret Reinforcement Learning in\n  Continuous MDPs","summary":"  Achieving the no-regret property for Reinforcement Learning (RL) problems in\ncontinuous state and action-space environments is one of the major open\nproblems in the field. Existing solutions either work under very specific\nassumptions or achieve bounds that are vacuous in some regimes. Furthermore,\nmany structural assumptions are known to suffer from a provably unavoidable\nexponential dependence on the time horizon $H$ in the regret, which makes any\npossible solution unfeasible in practice. In this paper, we identify local\nlinearity as the feature that makes Markov Decision Processes (MDPs) both\nlearnable (sublinear regret) and feasible (regret that is polynomial in $H$).\nWe define a novel MDP representation class, namely Locally Linearizable MDPs,\ngeneralizing other representation classes like Linear MDPs and MDPS with low\ninherent Belmman error. Then, i) we introduce Cinderella, a no-regret algorithm\nfor this general representation class, and ii) we show that all known learnable\nand feasible MDP families are representable in this class. We first show that\nall known feasible MDPs belong to a family that we call Mildly Smooth MDPs.\nThen, we show how any mildly smooth MDP can be represented as a Locally\nLinearizable MDP by an appropriate choice of representation. This way,\nCinderella is shown to achieve state-of-the-art regret bounds for all\npreviously known (and some new) continuous MDPs for which RL is learnable and\nfeasible.\n","authors":["Davide Maran","Alberto Maria Metelli","Matteo Papini","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2410.24071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24070v1","updated":"2024-10-31T16:07:21Z","published":"2024-10-31T16:07:21Z","title":"Dynamical similarity analysis uniquely captures how computations develop\n  in RNNs","summary":"  Methods for analyzing representations in neural systems are increasingly\npopular tools in neuroscience and mechanistic interpretability. Measures\ncomparing neural activations across conditions, architectures, and species give\nscalable ways to understand information transformation within different neural\nnetworks. However, recent findings show that some metrics respond to spurious\nsignals, leading to misleading results. Establishing benchmark test cases is\nthus essential for identifying the most reliable metric and potential\nimprovements. We propose that compositional learning in recurrent neural\nnetworks (RNNs) can provide a test case for dynamical representation alignment\nmetrics. Implementing this case allows us to evaluate if metrics can identify\nrepresentations that develop throughout learning and determine if\nrepresentations identified by metrics reflect the network's actual\ncomputations. Building both attractor and RNN based test cases, we show that\nthe recently proposed Dynamical Similarity Analysis (DSA) is more noise robust\nand reliably identifies behaviorally relevant representations compared to prior\nmetrics (Procrustes, CKA). We also demonstrate how such test cases can extend\nbeyond metric evaluation to study new architectures. Specifically, testing DSA\nin modern (Mamba) state space models suggests that these models, unlike RNNs,\nmay not require changes in recurrent dynamics due to their expressive hidden\nstates. Overall, we develop test cases that showcase how DSA's enhanced ability\nto detect dynamical motifs makes it highly effective for identifying ongoing\ncomputations in RNNs and revealing how networks learn tasks.\n","authors":["Quentin Guilhot","Jascha Achterberg","Michał Wójcik","Rui Ponte Costa"],"pdf_url":"https://arxiv.org/pdf/2410.24070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19238v2","updated":"2024-10-31T16:06:22Z","published":"2024-06-27T15:01:53Z","title":"Revealing Fine-Grained Values and Opinions in Large Language Models","summary":"  Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.\n","authors":["Dustin Wright","Arnav Arora","Nadav Borenstein","Srishti Yadav","Serge Belongie","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2406.19238v2.pdf","comment":"Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables"},{"id":"http://arxiv.org/abs/2312.02119v3","updated":"2024-10-31T15:57:42Z","published":"2023-12-04T18:49:23Z","title":"Tree of Attacks: Jailbreaking Black-Box LLMs Automatically","summary":"  While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard.\n","authors":["Anay Mehrotra","Manolis Zampetakis","Paul Kassianik","Blaine Nelson","Hyrum Anderson","Yaron Singer","Amin Karbasi"],"pdf_url":"https://arxiv.org/pdf/2312.02119v3.pdf","comment":"Accepted for presentation at NeurIPS 2024. Code:\n  https://github.com/RICommunity/TAP"},{"id":"http://arxiv.org/abs/2402.10095v3","updated":"2024-10-31T15:57:39Z","published":"2024-02-15T16:49:42Z","title":"Classification Diffusion Models: Revitalizing Density Ratio Estimation","summary":"  A prominent family of methods for learning data distributions relies on\ndensity ratio estimation (DRE), where a model is trained to $\\textit{classify}$\nbetween data samples and samples from some reference distribution. DRE-based\nmodels can directly output the likelihood for any given input, a highly desired\nproperty that is lacking in most generative techniques. Nevertheless, to date,\nDRE methods have failed in accurately capturing the distributions of complex\nhigh-dimensional data, like images, and have thus been drawing reduced research\nattention in recent years. In this work we present $\\textit{classification\ndiffusion models}$ (CDMs), a DRE-based generative method that adopts the\nformalism of denoising diffusion models (DDMs) while making use of a classifier\nthat predicts the level of noise added to a clean signal. Our method is based\non an analytical connection that we derive between the MSE-optimal denoiser for\nremoving white Gaussian noise and the cross-entropy-optimal classifier for\npredicting the noise level. Our method is the first DRE-based technique that\ncan successfully generate images beyond the MNIST dataset. Furthermore, it can\noutput the likelihood of any input in a single forward pass, achieving\nstate-of-the-art negative log likelihood (NLL) among methods with this\nproperty. Code is available on the project's webpage in\nhttps://shaharYadin.github.io/CDM/ .\n","authors":["Shahar Yadin","Noam Elata","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2402.10095v3.pdf","comment":"Accepted for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24060v1","updated":"2024-10-31T15:57:04Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24059v1","updated":"2024-10-31T15:56:50Z","published":"2024-10-31T15:56:50Z","title":"Identifying General Mechanism Shifts in Linear Causal Representations","summary":"  We consider the linear causal representation learning setting where we\nobserve a linear mixing of $d$ unknown latent factors, which follow a linear\nstructural causal model. Recent work has shown that it is possible to recover\nthe latent factors as well as the underlying structural causal model over them,\nup to permutation and scaling, provided that we have at least $d$ environments,\neach of which corresponds to perfect interventions on a single latent node\n(factor). After this powerful result, a key open problem faced by the community\nhas been to relax these conditions: allow for coarser than perfect single-node\ninterventions, and allow for fewer than $d$ of them, since the number of latent\nfactors $d$ could be very large. In this work, we consider precisely such a\nsetting, where we allow a smaller than $d$ number of environments, and also\nallow for very coarse interventions that can very coarsely \\textit{change the\nentire causal graph over the latent factors}. On the flip side, we relax what\nwe wish to extract to simply the \\textit{list of nodes that have shifted\nbetween one or more environments}. We provide a surprising identifiability\nresult that it is indeed possible, under some very mild standard assumptions,\nto identify the set of shifted nodes. Our identifiability proof moreover is a\nconstructive one: we explicitly provide necessary and sufficient conditions for\na node to be a shifted node, and show that we can check these conditions given\nobserved data. Our algorithm lends itself very naturally to the sample setting\nwhere instead of just interventional distributions, we are provided datasets of\nsamples from each of these distributions. We corroborate our results on both\nsynthetic experiments as well as an interesting psychometric dataset. The code\ncan be found at https://github.com/TianyuCodings/iLCS.\n","authors":["Tianyu Chen","Kevin Bello","Francesco Locatello","Bryon Aragam","Pradeep Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2410.24059v1.pdf","comment":"NeuIPS 2024"},{"id":"http://arxiv.org/abs/2402.01093v2","updated":"2024-10-31T15:56:08Z","published":"2024-02-02T01:45:18Z","title":"Need a Small Specialized Language Model? Plan Early!","summary":"  Large language models are versatile tools but are not suitable for small\ninference budgets. Small models have more efficient inference, but their lower\ncapacity means that their performance can be good only if one limits their\nscope to a specialized domain. This paper explores how to get good specialized\nsmall language models using a large, generic, pretraining set and a limited\namount of specialized data. We consider two scenarios, depending on whether (i)\none can afford pretraining a model for each specialization task, or (ii) one\nwants to cheaply adapt a single pretrained model for each task. In the first\nscenario, we propose an effective solution based on importance sampling: we\nresample the pretraining set to imitate the specialization data and train a\nsmall model on it. In the second scenario, we propose a novel architecture,\nprojected networks (PN). PN is a large network whose parameters can be linearly\nprojected into a small network for specialization. For both scenarios, we\ndemonstrate the empirical effectiveness of our solutions across various\ndomains, training set sizes, and training budgets.\n","authors":["David Grangier","Angelos Katharopoulos","Pierre Ablin","Awni Hannun"],"pdf_url":"https://arxiv.org/pdf/2402.01093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24058v1","updated":"2024-10-31T15:56:06Z","published":"2024-10-31T15:56:06Z","title":"Natural gradient and parameter estimation for quantum Boltzmann machines","summary":"  Thermal states play a fundamental role in various areas of physics, and they\nare becoming increasingly important in quantum information science, with\napplications related to semi-definite programming, quantum Boltzmann machine\nlearning, Hamiltonian learning, and the related task of estimating the\nparameters of a Hamiltonian. Here we establish formulas underlying the basic\ngeometry of parameterized thermal states, and we delineate quantum algorithms\nfor estimating the values of these formulas. More specifically, we prove\nformulas for the Fisher--Bures and Kubo--Mori information matrices of\nparameterized thermal states, and our quantum algorithms for estimating their\nmatrix elements involve a combination of classical sampling, Hamiltonian\nsimulation, and the Hadamard test. These results have applications in\ndeveloping a natural gradient descent algorithm for quantum Boltzmann machine\nlearning, which takes into account the geometry of thermal states, and in\nestablishing fundamental limitations on the ability to estimate the parameters\nof a Hamiltonian, when given access to thermal-state samples. For the latter\ntask, and for the special case of estimating a single parameter, we sketch an\nalgorithm that realizes a measurement that is asymptotically optimal for the\nestimation task. We finally stress that the natural gradient descent algorithm\ndeveloped here can be used for any machine learning problem that employs the\nquantum Boltzmann machine ansatz.\n","authors":["Dhrumil Patel","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2410.24058v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.18946v2","updated":"2024-10-31T15:53:15Z","published":"2024-09-27T17:46:05Z","title":"Unconditional stability of a recurrent neural circuit implementing\n  divisive normalization","summary":"  Stability in recurrent neural models poses a significant challenge,\nparticularly in developing biologically plausible neurodynamical models that\ncan be seamlessly trained. Traditional cortical circuit models are notoriously\ndifficult to train due to expansive nonlinearities in the dynamical system,\nleading to an optimization problem with nonlinear stability constraints that\nare difficult to impose. Conversely, recurrent neural networks (RNNs) excel in\ntasks involving sequential data but lack biological plausibility and\ninterpretability. In this work, we address these challenges by linking dynamic\ndivisive normalization (DN) to the stability of ORGaNICs, a biologically\nplausible recurrent cortical circuit model that dynamically achieves DN and\nthat has been shown to simulate a wide range of neurophysiological phenomena.\nBy using the indirect method of Lyapunov, we prove the remarkable property of\nunconditional local stability for an arbitrary-dimensional ORGaNICs circuit\nwhen the recurrent weight matrix is the identity. We thus connect ORGaNICs to a\nsystem of coupled damped harmonic oscillators, which enables us to derive the\ncircuit's energy function, providing a normative principle of what the circuit,\nand individual neurons, aim to accomplish. Further, for a generic recurrent\nweight matrix, we prove the stability of the 2D model and demonstrate\nempirically that stability holds in higher dimensions. Finally, we show that\nORGaNICs can be trained by backpropagation through time without gradient\nclipping/scaling, thanks to its intrinsic stability property and adaptive time\nconstants, which address the problems of exploding, vanishing, and oscillating\ngradients. By evaluating the model's performance on RNN benchmarks, we find\nthat ORGaNICs outperform alternative neurodynamical models on static image\nclassification tasks and perform comparably to LSTMs on sequential tasks.\n","authors":["Shivang Rawat","David J. Heeger","Stefano Martiniani"],"pdf_url":"https://arxiv.org/pdf/2409.18946v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08557v2","updated":"2024-10-31T15:52:52Z","published":"2023-11-14T21:39:15Z","title":"Low-light Pedestrian Detection in Visible and Infrared Image Feeds:\n  Issues and Challenges","summary":"  Pedestrian detection has become a cornerstone for several high-level tasks,\nincluding autonomous driving, intelligent transportation, and traffic\nsurveillance. There are several works focussed on pedestrian detection using\nvisible images, mainly in the daytime. However, this task is very intriguing\nwhen the environmental conditions change to poor lighting or nighttime.\nRecently, new ideas have been spurred to use alternative sources, such as Far\nInfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light\nconditions. This study reviews recent developments in low-light pedestrian\ndetection approaches. It systematically categorizes and analyses various\nalgorithms from region-based to non-region-based and graph-based learning\nmethodologies by highlighting their methodologies, implementation issues, and\nchallenges. It also outlines the key benchmark datasets that can be used for\nresearch and development of advanced pedestrian detection algorithms,\nparticularly in low-light situations.\n","authors":["Thangarajah Akilan","Hrishikesh Vachhani"],"pdf_url":"https://arxiv.org/pdf/2311.08557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16978v2","updated":"2024-10-31T15:50:21Z","published":"2024-05-27T09:21:40Z","title":"OSLO: One-Shot Label-Only Membership Inference Attacks","summary":"  We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs),\nwhich accurately infer a given sample's membership in a target model's training\nset with high precision using just \\emph{a single query}, where the target\nmodel only returns the predicted hard label. This is in contrast to\nstate-of-the-art label-only attacks which require $\\sim6000$ queries, yet get\nattack precisions lower than OSLO's. OSLO leverages transfer-based black-box\nadversarial attacks. The core idea is that a member sample exhibits more\nresistance to adversarial perturbations than a non-member. We compare OSLO\nagainst state-of-the-art label-only attacks and demonstrate that, despite\nrequiring only one query, our method significantly outperforms previous attacks\nin terms of precision and true positive rate (TPR) under the same false\npositive rates (FPR). For example, compared to previous label-only MIAs, OSLO\nachieves a TPR that is at least 7$\\times$ higher under a 1\\% FPR and at least\n22$\\times$ higher under a 0.1\\% FPR on CIFAR100 for a ResNet18 model. We\nevaluated multiple defense mechanisms against OSLO.\n","authors":["Yuefeng Peng","Jaechul Roh","Subhransu Maji","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2405.16978v2.pdf","comment":"To appear at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24055v1","updated":"2024-10-31T15:48:36Z","published":"2024-10-31T15:48:36Z","title":"Advanced Predictive Quality Assessment for Ultrasonic Additive\n  Manufacturing with Deep Learning Model","summary":"  Ultrasonic Additive Manufacturing (UAM) employs ultrasonic welding to bond\nsimilar or dissimilar metal foils to a substrate, resulting in solid,\nconsolidated metal components. However, certain processing conditions can lead\nto inter-layer defects, affecting the final product's quality. This study\ndevelops a method to monitor in-process quality using deep learning-based\nconvolutional neural networks (CNNs). The CNN models were evaluated on their\nability to classify samples with and without embedded thermocouples across five\npower levels (300W, 600W, 900W, 1200W, 1500W) using thermal images with\nsupervised labeling. Four distinct CNN classification models were created for\ndifferent scenarios including without (baseline) and with thermocouples, only\nwithout thermocouples across power levels, only with thermocouples across power\nlevels, and combined without and with thermocouples across power levels. The\nmodels achieved 98.29% accuracy on combined baseline and thermocouple images,\n97.10% for baseline images across power levels, 97.43% for thermocouple images,\nand 97.27% for both types across power levels. The high accuracy, above 97%,\ndemonstrates the system's effectiveness in identifying and classifying\nconditions within the UAM process, providing a reliable tool for quality\nassurance and process control in manufacturing environments.\n","authors":["Lokendra Poudel","Sushant Jha","Ryan Meeker","Duy-Nhat Phan","Rahul Bhowmik"],"pdf_url":"https://arxiv.org/pdf/2410.24055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24054v1","updated":"2024-10-31T15:48:34Z","published":"2024-10-31T15:48:34Z","title":"EigenVI: score-based variational inference with orthogonal function\n  expansions","summary":"  We develop EigenVI, an eigenvalue-based approach for black-box variational\ninference (BBVI). EigenVI constructs its variational approximations from\northogonal function expansions. For distributions over $\\mathbb{R}^D$, the\nlowest order term in these expansions provides a Gaussian variational\napproximation, while higher-order terms provide a systematic way to model\nnon-Gaussianity. These approximations are flexible enough to model complex\ndistributions (multimodal, asymmetric), but they are simple enough that one can\ncalculate their low-order moments and draw samples from them. EigenVI can also\nmodel other types of random variables (e.g., nonnegative, bounded) by\nconstructing variational approximations from different families of orthogonal\nfunctions. Within these families, EigenVI computes the variational\napproximation that best matches the score function of the target distribution\nby minimizing a stochastic estimate of the Fisher divergence. Notably, this\noptimization reduces to solving a minimum eigenvalue problem, so that EigenVI\neffectively sidesteps the iterative gradient-based optimizations that are\nrequired for many other BBVI algorithms. (Gradient-based methods can be\nsensitive to learning rates, termination criteria, and other tunable\nhyperparameters.) We use EigenVI to approximate a variety of target\ndistributions, including a benchmark suite of Bayesian models from posteriordb.\nOn these distributions, we find that EigenVI is more accurate than existing\nmethods for Gaussian BBVI.\n","authors":["Diana Cai","Chirag Modi","Charles C. Margossian","Robert M. Gower","David M. Blei","Lawrence K. Saul"],"pdf_url":"https://arxiv.org/pdf/2410.24054v1.pdf","comment":"25 pages, 9 figures. Advances in Neural Information Processing\n  Systems (NeurIPS), 2024"},{"id":"http://arxiv.org/abs/2410.24052v1","updated":"2024-10-31T15:47:50Z","published":"2024-10-31T15:47:50Z","title":"Attention is All You Need to Optimize Wind Farm Operations and\n  Maintenance","summary":"  Operations and maintenance (O&M) is a fundamental problem in wind energy\nsystems with far reaching implications for reliability and profitability.\nOptimizing O&M is a multi-faceted decision optimization problem that requires a\ncareful balancing act across turbine level failure risks, operational revenues,\nand maintenance crew logistics. The resulting O&M problems are typically solved\nusing large-scale mixed integer programming (MIP) models, which yield\ncomputationally challenging problems that require either long-solution times,\nor heuristics to reach a solution. To address this problem, we introduce a\nnovel decision-making framework for wind farm O&M that builds on a multi-head\nattention (MHA) models, an emerging artificial intelligence methods that are\nspecifically designed to learn in rich and complex problem settings. The\ndevelopment of proposed MHA framework incorporates a number of modeling\ninnovations that allows explicit embedding of MIP models within an MHA\nstructure. The proposed MHA model (i) significantly reduces the solution time\nfrom hours to seconds, (ii) guarantees feasibility of the proposed solutions\nconsidering complex constraints that are omnipresent in wind farm O&M, (iii)\nresults in significant solution quality compared to the conventional MIP\nformulations, and (iv) exhibits significant transfer learning capability across\ndifferent problem settings.\n","authors":["Iman Kazemian","Murat Yildirim","Paritosh Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.24052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24050v1","updated":"2024-10-31T15:46:10Z","published":"2024-10-31T15:46:10Z","title":"A Visual Case Study of the Training Dynamics in Neural Networks","summary":"  This paper introduces a visual sandbox designed to explore the training\ndynamics of a small-scale transformer model, with the embedding dimension\nconstrained to $d=2$. This restriction allows for a comprehensive\ntwo-dimensional visualization of each layer's dynamics. Through this approach,\nwe gain insights into training dynamics, circuit transferability, and the\ncauses of loss spikes, including those induced by the high curvature of\nnormalization layers. We propose strategies to mitigate these spikes,\ndemonstrating how good visualization facilitates the design of innovative ideas\nof practical interest. Additionally, we believe our sandbox could assist\ntheoreticians in assessing essential training dynamics mechanisms and\nintegrating them into future theories. The code is available at\nhttps://github.com/facebookresearch/pal.\n","authors":["Ambroise Odonnat","Wassim Bouaziz","Vivien Cabannes"],"pdf_url":"https://arxiv.org/pdf/2410.24050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09168v2","updated":"2024-10-31T15:44:36Z","published":"2024-06-13T14:30:35Z","title":"SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image\n  Super-Resolution","summary":"  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes at the cellular\nand subcellular levels. Scanning confocal microscopy allows the capture of\nhigh-quality images from thick three-dimensional (3D) samples, yet suffers from\nwell-known limitations such as photobleaching and phototoxicity of specimens\ncaused by intense light exposure, limiting its applications. Cellular damage\ncan be alleviated by changing imaging parameters to reduce light exposure,\noften at the expense of image quality. Machine/deep learning methods for\nsingle-image super-resolution (SISR) can be applied to restore image quality by\nupscaling lower-resolution (LR) images to yield high-resolution images (HR).\nThese SISR methods have been successfully applied to photo-realistic images due\npartly to the abundance of publicly available data. In contrast, the lack of\npublicly available data partly limits their application and success in scanning\nconfocal microscopy. In this paper, we introduce a large scanning confocal\nmicroscopy dataset named SR-CACO-2 that is comprised of low- and\nhigh-resolution image pairs marked for three different fluorescent markers. It\nallows the evaluation of performance of SISR methods on three different\nupscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell\nline Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured\nwith four resolutions and three markers, forming 9,937 image patches for SISR\nmethods. We provide benchmarking results for 16 state-of-the-art methods of the\nmain SISR families. Results show that these methods have limited success in\nproducing high-resolution textures. The dataset is freely accessible under a\nCreative Commons license (CC BY-NC-SA 4.0). Our dataset, code and pretrained\nweights for SISR methods are available: https://github.com/sbelharbi/sr-caco-2.\n","authors":["Soufiane Belharbi","Mara KM Whitford","Phuong Hoang","Shakeeb Murtaza","Luke McCaffrey","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2406.09168v2.pdf","comment":"27 pages, 15 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.18457v3","updated":"2024-10-31T15:44:22Z","published":"2024-05-28T16:58:37Z","title":"Improving Linear System Solvers for Hyperparameter Optimisation in\n  Iterative Gaussian Processes","summary":"  Scaling hyperparameter optimisation to very large datasets remains an open\nproblem in the Gaussian process community. This paper focuses on iterative\nmethods, which use linear system solvers, like conjugate gradients, alternating\nprojections or stochastic gradient descent, to construct an estimate of the\nmarginal likelihood gradient. We discuss three key improvements which are\napplicable across solvers: (i) a pathwise gradient estimator, which reduces the\nrequired number of solver iterations and amortises the computational cost of\nmaking predictions, (ii) warm starting linear system solvers with the solution\nfrom the previous step, which leads to faster solver convergence at the cost of\nnegligible bias, (iii) early stopping linear system solvers after a limited\ncomputational budget, which synergises with warm starting, allowing solver\nprogress to accumulate over multiple marginal likelihood steps. These\ntechniques provide speed-ups of up to $72\\times$ when solving to tolerance, and\ndecrease the average residual norm by up to $7\\times$ when stopping early.\n","authors":["Jihao Andreas Lin","Shreyas Padhy","Bruno Mlodozeniec","Javier Antorán","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2405.18457v3.pdf","comment":"Advances in Neural Information Processing Systems 2024"},{"id":"http://arxiv.org/abs/2311.08376v3","updated":"2024-10-31T15:42:51Z","published":"2023-11-14T18:41:28Z","title":"Ensemble sampling for linear bandits: small ensembles suffice","summary":"  We provide the first useful and rigorous analysis of ensemble sampling for\nthe stochastic linear bandit setting. In particular, we show that, under\nstandard assumptions, for a $d$-dimensional stochastic linear bandit with an\ninteraction horizon $T$, ensemble sampling with an ensemble of size of order\n$\\smash{d \\log T}$ incurs regret at most of the order $\\smash{(d \\log T)^{5/2}\n\\sqrt{T}}$. Ours is the first result in any structured setting not to require\nthe size of the ensemble to scale linearly with $T$ -- which defeats the\npurpose of ensemble sampling -- while obtaining near $\\smash{\\sqrt{T}}$ order\nregret. Ours is also the first result that allows infinite action sets.\n","authors":["David Janz","Alexander E. Litvak","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2311.08376v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24046v1","updated":"2024-10-31T15:42:24Z","published":"2024-10-31T15:42:24Z","title":"Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis","summary":"  This study introduces the Hybrid Multi-modal VGG (HM-VGG) model, a\ncutting-edge deep learning approach for the early diagnosis of glaucoma. The\nHM-VGG model utilizes an attention mechanism to process Visual Field (VF) data,\nenabling the extraction of key features that are vital for identifying early\nsigns of glaucoma. Despite the common reliance on large annotated datasets, the\nHM-VGG model excels in scenarios with limited data, achieving remarkable\nresults with small sample sizes. The model's performance is underscored by its\nhigh metrics in Precision, Accuracy, and F1-Score, indicating its potential for\nreal-world application in glaucoma detection. The paper also discusses the\nchallenges associated with ophthalmic image analysis, particularly the\ndifficulty of obtaining large volumes of annotated data. It highlights the\nimportance of moving beyond single-modality data, such as VF or Optical\nCoherence Tomography (OCT) images alone, to a multimodal approach that can\nprovide a richer, more comprehensive dataset. This integration of different\ndata types is shown to significantly enhance diagnostic accuracy. The HM- VGG\nmodel offers a promising tool for doctors, streamlining the diagnostic process\nand improving patient outcomes. Furthermore, its applicability extends to\ntelemedicine and mobile healthcare, making diagnostic services more accessible.\nThe research presented in this paper is a significant step forward in the field\nof medical image processing and has profound implications for clinical\nophthalmology.\n","authors":["Junliang Du","Yiru Cang","Tong Zhou","Jiacheng Hu","Weijie He"],"pdf_url":"https://arxiv.org/pdf/2410.24046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22114v2","updated":"2024-10-31T15:34:35Z","published":"2024-10-29T15:16:02Z","title":"Policy Gradient for Robust Markov Decision Processes","summary":"  We develop a generic policy gradient method with the global optimality\nguarantee for robust Markov Decision Processes (MDPs). While policy gradient\nmethods are widely used for solving dynamic decision problems due to their\nscalable and efficient nature, adapting these methods to account for model\nambiguity has been challenging, often making it impractical to learn robust\npolicies. This paper introduces a novel policy gradient method, Double-Loop\nRobust Policy Mirror Descent (DRPMD), for solving robust MDPs. DRPMD employs a\ngeneral mirror descent update rule for the policy optimization with adaptive\ntolerance per iteration, guaranteeing convergence to a globally optimal policy.\nWe provide a comprehensive analysis of DRPMD, including new convergence results\nunder both direct and softmax parameterizations, and provide novel insights\ninto the inner problem solution through Transition Mirror Ascent (TMA).\nAdditionally, we propose innovative parametric transition kernels for both\ndiscrete and continuous state-action spaces, broadening the applicability of\nour approach. Empirical results validate the robustness and global convergence\nof DRPMD across various challenging robust MDP settings.\n","authors":["Qiuhao Wang","Shaohang Xu","Chin Pang Ho","Marek Petrik"],"pdf_url":"https://arxiv.org/pdf/2410.22114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24035v1","updated":"2024-10-31T15:32:32Z","published":"2024-10-31T15:32:32Z","title":"State- and context-dependent robotic manipulation and grasping via\n  uncertainty-aware imitation learning","summary":"  Generating context-adaptive manipulation and grasping actions is a\nchallenging problem in robotics. Classical planning and control algorithms tend\nto be inflexible with regard to parameterization by external variables such as\nobject shapes. In contrast, Learning from Demonstration (LfD) approaches, due\nto their nature as function approximators, allow for introducing external\nvariables to modulate policies in response to the environment. In this paper,\nwe utilize this property by introducing an LfD approach to acquire\ncontext-dependent grasping and manipulation strategies. We treat the problem as\na kernel-based function approximation, where the kernel inputs include generic\ncontext variables describing task-dependent parameters such as the object\nshape. We build on existing work on policy fusion with uncertainty\nquantification to propose a state-dependent approach that automatically returns\nto demonstrations, avoiding unpredictable behavior while smoothly adapting to\ncontext changes. The approach is evaluated against the LASA handwriting dataset\nand on a real 7-DoF robot in two scenarios: adaptation to slippage while\ngrasping and manipulating a deformable food item.\n","authors":["Tim R. Winter","Ashok M. Sundaram","Werner Friedl","Maximo A. Roa","Freek Stulp","João Silvério"],"pdf_url":"https://arxiv.org/pdf/2410.24035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10490v2","updated":"2024-10-31T15:28:49Z","published":"2024-06-15T04:03:12Z","title":"Active, anytime-valid risk controlling prediction sets","summary":"  Rigorously establishing the safety of black-box machine learning models\nconcerning critical risk measures is important for providing guarantees about\nmodel behavior. Recently, Bates et. al. (JACM '24) introduced the notion of a\nrisk controlling prediction set (RCPS) for producing prediction sets that are\nstatistically guaranteed low risk from machine learning models. Our method\nextends this notion to the sequential setting, where we provide guarantees even\nwhen the data is collected adaptively, and ensures that the risk guarantee is\nanytime-valid, i.e., simultaneously holds at all time steps. Further, we\npropose a framework for constructing RCPSes for active labeling, i.e., allowing\none to use a labeling policy that chooses whether to query the true label for\neach received data point and ensures that the expected proportion of data\npoints whose labels are queried are below a predetermined label budget. We also\ndescribe how to use predictors (i.e., the machine learning model for which we\nprovide risk control guarantees) to further improve the utility of our RCPSes\nby estimating the expected risk conditioned on the covariates. We characterize\nthe optimal choices of label policy and predictor under a fixed label budget\nand show a regret result that relates the estimation error of the optimal\nlabeling policy and predictor to the wealth process that underlies our RCPSes.\nLastly, we present practical ways of formulating label policies and empirically\nshow that our label policies use fewer labels to reach higher utility than\nnaive baseline labeling strategies on both simulations and real data.\n","authors":["Ziyu Xu","Nikos Karampatziakis","Paul Mineiro"],"pdf_url":"https://arxiv.org/pdf/2406.10490v2.pdf","comment":"22 pages, 3 figures. Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24029v1","updated":"2024-10-31T15:28:26Z","published":"2024-10-31T15:28:26Z","title":"Joint Training for Selective Prediction","summary":"  Classifier models are prevalent in natural language processing (NLP), often\nwith high accuracy. Yet in real world settings, human-in-the-loop systems can\nfoster trust in model outputs and even higher performance. Selective Prediction\n(SP) methods determine when to adopt a classifier's output versus defer to a\nhuman. Previous SP approaches have addressed how to improve softmax as a\nmeasure of model confidence, or have developed separate confidence estimators.\nOne previous method involves learning a deferral model based on engineered\nfeatures. We introduce a novel joint-training approach that simultaneously\noptimizes learned representations used by the classifier module and a learned\ndeferral policy. Our results on four classification tasks demonstrate that\njoint training not only leads to better SP outcomes over two strong baselines,\nbut also improves the performance of both modules.\n","authors":["Zhaohui Li","Rebecca J. Passonneau"],"pdf_url":"https://arxiv.org/pdf/2410.24029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24028v1","updated":"2024-10-31T15:28:22Z","published":"2024-10-31T15:28:22Z","title":"AdaFlow: Opportunistic Inference on Asynchronous Mobile Data with\n  Generalized Affinity Control","summary":"  The rise of mobile devices equipped with numerous sensors, such as LiDAR and\ncameras, has spurred the adoption of multi-modal deep intelligence for\ndistributed sensing tasks, such as smart cabins and driving assistance.\nHowever, the arrival times of mobile sensory data vary due to modality size and\nnetwork dynamics, which can lead to delays (if waiting for slower data) or\naccuracy decline (if inference proceeds without waiting). Moreover, the\ndiversity and dynamic nature of mobile systems exacerbate this challenge. In\nresponse, we present a shift to \\textit{opportunistic} inference for\nasynchronous distributed multi-modal data, enabling inference as soon as\npartial data arrives. While existing methods focus on optimizing modality\nconsistency and complementarity, known as modal affinity, they lack a\n\\textit{computational} approach to control this affinity in open-world mobile\nenvironments. AdaFlow pioneers the formulation of structured cross-modality\naffinity in mobile contexts using a hierarchical analysis-based normalized\nmatrix. This approach accommodates the diversity and dynamics of modalities,\ngeneralizing across different types and numbers of inputs. Employing an\naffinity attention-based conditional GAN (ACGAN), AdaFlow facilitates flexible\ndata imputation, adapting to various modalities and downstream tasks without\nretraining. Experiments show that AdaFlow significantly reduces inference\nlatency by up to 79.9\\% and enhances accuracy by up to 61.9\\%, outperforming\nstatus quo approaches.\n","authors":["Fenmin Wu","Sicong Liu","Kehao Zhu","Xiaochen Li","Bin Guo","Zhiwen Yu","Hongkai Wen","Xiangrui Xu","Lehao Wang","Xiangyu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10881v2","updated":"2024-10-31T15:27:40Z","published":"2024-04-16T20:01:10Z","title":"Differentially Private Optimization with Sparse Gradients","summary":"  Motivated by applications of large embedding models, we study differentially\nprivate (DP) optimization problems under sparsity of individual gradients. We\nstart with new near-optimal bounds for the classic mean estimation problem but\nwith sparse data, improving upon existing algorithms particularly for the\nhigh-dimensional regime. Building on this, we obtain pure- and approximate-DP\nalgorithms with almost optimal rates for stochastic convex optimization with\nsparse gradients; the former represents the first nearly dimension-independent\nrates for this problem. Finally, we study the approximation of stationary\npoints for the empirical loss in approximate-DP optimization and obtain rates\nthat depend on sparsity instead of dimension, modulo polylogarithmic factors.\n","authors":["Badih Ghazi","Cristóbal Guzmán","Pritish Kamath","Ravi Kumar","Pasin Manurangsi"],"pdf_url":"https://arxiv.org/pdf/2404.10881v2.pdf","comment":"Minor corrections and re-structuring of the presentation"},{"id":"http://arxiv.org/abs/2410.24023v1","updated":"2024-10-31T15:23:34Z","published":"2024-10-31T15:23:34Z","title":"Approximate attention with MLP: a pruning strategy for attention-based\n  model in multivariate time series forecasting","summary":"  Attention-based architectures have become ubiquitous in time series\nforecasting tasks, including spatio-temporal (STF) and long-term time series\nforecasting (LTSF). Yet, our understanding of the reasons for their\neffectiveness remains limited. This work proposes a new way to understand\nself-attention networks: we have shown empirically that the entire attention\nmechanism in the encoder can be reduced to an MLP formed by feedforward,\nskip-connection, and layer normalization operations for temporal and/or spatial\nmodeling in multivariate time series forecasting. Specifically, the Q, K, and V\nprojection, the attention score calculation, the dot-product between the\nattention score and the V, and the final projection can be removed from the\nattention-based networks without significantly degrading the performance that\nthe given network remains the top-tier compared to other SOTA methods. For\nspatio-temporal networks, the MLP-replace-attention network achieves a\nreduction in FLOPS of $62.579\\%$ with a loss in performance less than $2.5\\%$;\nfor LTSF, a reduction in FLOPs of $42.233\\%$ with a loss in performance less\nthan $2\\%$.\n","authors":["Suhan Guo","Jiahong Deng","Yi Wei","Hui Dou","Furao Shen","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.24023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16510v2","updated":"2024-10-31T15:23:13Z","published":"2023-03-29T07:36:54Z","title":"Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms\n  for Optimization under Orthogonality Constraints","summary":"  Orthogonality constraints naturally appear in many machine learning problems,\nfrom principal component analysis to robust neural network training. They are\nusually solved using Riemannian optimization algorithms, which minimize the\nobjective function while enforcing the constraint. However, enforcing the\northogonality constraint can be the most time-consuming operation in such\nalgorithms. Recently, Ablin & Peyr\\'e (2022) proposed the landing algorithm, a\nmethod with cheap iterations that does not enforce the orthogonality\nconstraints but is attracted towards the manifold in a smooth manner. This\narticle provides new practical and theoretical developments for the landing\nalgorithm. First, the method is extended to the Stiefel manifold, the set of\nrectangular orthogonal matrices. We also consider stochastic and variance\nreduction algorithms when the cost function is an average of many functions. We\ndemonstrate that all these methods have the same rate of convergence as their\nRiemannian counterparts that exactly enforce the constraint, and converge to\nthe manifold. Finally, our experiments demonstrate the promise of our approach\nto an array of machine-learning problems that involve orthogonality\nconstraints.\n","authors":["Pierre Ablin","Simon Vary","Bin Gao","P. -A. Absil"],"pdf_url":"https://arxiv.org/pdf/2303.16510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24022v1","updated":"2024-10-31T15:22:03Z","published":"2024-10-31T15:22:03Z","title":"SFM-Protein: Integrative Co-evolutionary Pre-training for Advanced\n  Protein Sequence Representation","summary":"  Proteins, essential to biological systems, perform functions intricately\nlinked to their three-dimensional structures. Understanding the relationship\nbetween protein structures and their amino acid sequences remains a core\nchallenge in protein modeling. While traditional protein foundation models\nbenefit from pre-training on vast unlabeled datasets, they often struggle to\ncapture critical co-evolutionary information, which evolutionary-based methods\nexcel at. In this study, we introduce a novel pre-training strategy for protein\nfoundation models that emphasizes the interactions among amino acid residues to\nenhance the extraction of both short-range and long-range co-evolutionary\nfeatures from sequence data. Trained on a large-scale protein sequence dataset,\nour model demonstrates superior generalization ability, outperforming\nestablished baselines of similar size, including the ESM model, across diverse\ndownstream tasks. Experimental results confirm the model's effectiveness in\nintegrating co-evolutionary information, marking a significant step forward in\nprotein sequence-based modeling.\n","authors":["Liang He","Peiran Jin","Yaosen Min","Shufang Xie","Lijun Wu","Tao Qin","Xiaozhuan Liang","Kaiyuan Gao","Yuliang Jiang","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24018v1","updated":"2024-10-31T15:20:43Z","published":"2024-10-31T15:20:43Z","title":"Bayesian-guided Label Mapping for Visual Reprogramming","summary":"  Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained\nvision models by adapting their input or output interfaces to solve downstream\ntasks whose labels (i.e., downstream labels) might be totally different from\nthe labels associated with the pretrained models (i.e., pretrained labels).\nWhen adapting the output interface, label mapping methods transform the\npretrained labels to downstream labels by establishing a gradient-free\none-to-one correspondence between the two sets of labels. However, in this\npaper, we reveal that one-to-one mappings may overlook the complex relationship\nbetween pretrained and downstream labels. Motivated by this observation, we\npropose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an\niteratively-updated probabilistic label mapping matrix, with each element\nquantifying a pairwise relationship between pretrained and downstream labels.\nThe assignment of values to the constructed matrix is guided by Bayesian\nconditional probability, considering the joint distribution of the downstream\nlabels and the labels predicted by the pretrained model on downstream samples.\nExperiments conducted on both pretrained vision models (e.g., ResNeXt) and\nvision-language models (e.g., CLIP) demonstrate the superior performance of BLM\nover existing label mapping methods. The success of BLM also offers a\nprobabilistic lens through which to understand and analyze the effectiveness of\nVR. Our code is available at https://github.com/tmlr-group/BayesianLM.\n","authors":["Chengyi Cai","Zesheng Ye","Lei Feng","Jianzhong Qi","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24016v1","updated":"2024-10-31T15:18:37Z","published":"2024-10-31T15:18:37Z","title":"Maximum Entropy Hindsight Experience Replay","summary":"  Hindsight experience replay (HER) is well-known to accelerate goal-based\nreinforcement learning (RL). While HER is generally applied to off-policy RL\nalgorithms, we previously showed that HER can also accelerate on-policy\nalgorithms, such as proximal policy optimization (PPO), for goal-based\nPredator-Prey environments. Here, we show that we can improve the previous\nPPO-HER algorithm by selectively applying HER in a principled manner.\n","authors":["Douglas C. Crowder","Matthew L. Trappett","Darrien M. McKenzie","Frances S. Chance"],"pdf_url":"https://arxiv.org/pdf/2410.24016v1.pdf","comment":"11 pages, 11 Figures"},{"id":"http://arxiv.org/abs/2410.24012v1","updated":"2024-10-31T15:13:11Z","published":"2024-10-31T15:13:11Z","title":"Diffusion Twigs with Loop Guidance for Conditional Graph Generation","summary":"  We introduce a novel score-based diffusion framework named Twigs that\nincorporates multiple co-evolving flows for enriching conditional generation\ntasks. Specifically, a central or trunk diffusion process is associated with a\nprimary variable (e.g., graph structure), and additional offshoot or stem\nprocesses are dedicated to dependent variables (e.g., graph properties or\nlabels). A new strategy, which we call loop guidance, effectively orchestrates\nthe flow of information between the trunk and the stem processes during\nsampling. This approach allows us to uncover intricate interactions and\ndependencies, and unlock new generative capabilities. We provide extensive\nexperiments to demonstrate strong performance gains of the proposed method over\ncontemporary baselines in the context of conditional graph generation,\nunderscoring the potential of Twigs in challenging generative tasks such as\ninverse molecular design and molecular optimization.\n","authors":["Giangiacomo Mercatali","Yogesh Verma","Andre Freitas","Vikas Garg"],"pdf_url":"https://arxiv.org/pdf/2410.24012v1.pdf","comment":"NeurIPS 2024. Code is available at\n  https://github.com/Aalto-QuML/Diffusion_twigs"},{"id":"http://arxiv.org/abs/2410.24006v1","updated":"2024-10-31T15:09:36Z","published":"2024-10-31T15:09:36Z","title":"DiffPAD: Denoising Diffusion-based Adversarial Patch Decontamination","summary":"  In the ever-evolving adversarial machine learning landscape, developing\neffective defenses against patch attacks has become a critical challenge,\nnecessitating reliable solutions to safeguard real-world AI systems. Although\ndiffusion models have shown remarkable capacity in image synthesis and have\nbeen recently utilized to counter $\\ell_p$-norm bounded attacks, their\npotential in mitigating localized patch attacks remains largely underexplored.\nIn this work, we propose DiffPAD, a novel framework that harnesses the power of\ndiffusion models for adversarial patch decontamination. DiffPAD first performs\nsuper-resolution restoration on downsampled input images, then adopts\nbinarization, dynamic thresholding scheme and sliding window for effective\nlocalization of adversarial patches. Such a design is inspired by the\ntheoretically derived correlation between patch size and diffusion restoration\nerror that is generalized across diverse patch attack scenarios. Finally,\nDiffPAD applies inpainting techniques to the original input images with the\nestimated patch region being masked. By integrating closed-form solutions for\nsuper-resolution restoration and image inpainting into the conditional reverse\nsampling process of a pre-trained diffusion model, DiffPAD obviates the need\nfor text guidance or fine-tuning. Through comprehensive experiments, we\ndemonstrate that DiffPAD not only achieves state-of-the-art adversarial\nrobustness against patch attacks but also excels in recovering naturalistic\nimages without patch remnants.\n","authors":["Jia Fu","Xiao Zhang","Sepideh Pashami","Fatemeh Rahimian","Anders Holst"],"pdf_url":"https://arxiv.org/pdf/2410.24006v1.pdf","comment":"Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2410.24005v1","updated":"2024-10-31T15:06:16Z","published":"2024-10-31T15:06:16Z","title":"Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models","summary":"  The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm.\n","authors":["Paulius Rauba","Nabeel Seedat","Max Ruiz Luyten","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2410.24005v1.pdf","comment":"Presented at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024). *Rauba & Seedat contributed equally"},{"id":"http://arxiv.org/abs/2405.15010v2","updated":"2024-10-31T15:03:06Z","published":"2024-05-23T19:29:38Z","title":"Parameter-free Clipped Gradient Descent Meets Polyak","summary":"  Gradient descent and its variants are de facto standard algorithms for\ntraining machine learning models. As gradient descent is sensitive to its\nhyperparameters, we need to tune the hyperparameters carefully using a grid\nsearch. However, the method is time-consuming, particularly when multiple\nhyperparameters exist. Therefore, recent studies have analyzed parameter-free\nmethods that adjust the hyperparameters on the fly. However, the existing work\nis limited to investigations of parameter-free methods for the stepsize, and\nparameter-free methods for other hyperparameters have not been explored. For\ninstance, although the gradient clipping threshold is a crucial hyperparameter\nin addition to the stepsize for preventing gradient explosion issues, none of\nthe existing studies have investigated parameter-free methods for clipped\ngradient descent. Therefore, in this study, we investigate the parameter-free\nmethods for clipped gradient descent. Specifically, we propose Inexact Polyak\nStepsize, which converges to the optimal solution without any hyperparameters\ntuning, and its convergence rate is asymptotically independent of $L$ under\n$L$-smooth and $(L_0, L_1)$-smooth assumptions of the loss function, similar to\nthat of clipped gradient descent with well-tuned hyperparameters. We\nnumerically validated our convergence results using a synthetic function and\ndemonstrated the effectiveness of our proposed methods using LSTM, Nano-GPT,\nand T5.\n","authors":["Yuki Takezawa","Han Bao","Ryoma Sato","Kenta Niwa","Makoto Yamada"],"pdf_url":"https://arxiv.org/pdf/2405.15010v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23996v1","updated":"2024-10-31T14:57:31Z","published":"2024-10-31T14:57:31Z","title":"An Information Criterion for Controlled Disentanglement of Multimodal\n  Data","summary":"  Multimodal representation learning seeks to relate and decompose information\ninherent in multiple modalities. By disentangling modality-specific information\nfrom information that is shared across modalities, we can improve\ninterpretability and robustness and enable downstream tasks such as the\ngeneration of counterfactual outcomes. Separating the two types of information\nis challenging since they are often deeply entangled in many real-world\napplications. We propose Disentangled Self-Supervised Learning\n(DisentangledSSL), a novel self-supervised approach for learning disentangled\nrepresentations. We present a comprehensive analysis of the optimality of each\ndisentangled representation, particularly focusing on the scenario not covered\nin prior work where the so-called Minimum Necessary Information (MNI) point is\nnot attainable. We demonstrate that DisentangledSSL successfully learns shared\nand modality-specific features on multiple synthetic and real-world datasets\nand consistently outperforms baselines on various downstream tasks, including\nprediction tasks for vision-language data, as well as molecule-phenotype\nretrieval tasks for biological data.\n","authors":["Chenyu Wang","Sharut Gupta","Xinyi Zhang","Sana Tonekaboni","Stefanie Jegelka","Tommi Jaakkola","Caroline Uhler"],"pdf_url":"https://arxiv.org/pdf/2410.23996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01158v3","updated":"2024-10-31T14:53:43Z","published":"2023-06-01T21:31:59Z","title":"Heterogeneous Knowledge for Augmented Modular Reinforcement Learning","summary":"  Existing modular Reinforcement Learning (RL) architectures are generally\nbased on reusable components, also allowing for \"plug-and-play\" integration.\nHowever, these modules are homogeneous in nature - in fact, they essentially\nprovide policies obtained via RL through the maximization of individual reward\nfunctions. Consequently, such solutions still lack the ability to integrate and\nprocess multiple types of information (i.e., heterogeneous knowledge\nrepresentations), such as rules, sub-goals, and skills from various sources. In\nthis paper, we discuss several practical examples of heterogeneous knowledge\nand propose Augmented Modular Reinforcement Learning (AMRL) to address these\nlimitations. Our framework uses a selector to combine heterogeneous modules and\nseamlessly incorporate different types of knowledge representations and\nprocessing mechanisms. Our results demonstrate the performance and efficiency\nimprovements, also in terms of generalization, that can be achieved by\naugmenting traditional modular RL with heterogeneous knowledge sources and\nprocessing mechanisms. Finally, we examine the safety, robustness, and\ninterpretability issues stemming from the introduction of knowledge\nheterogeneity.\n","authors":["Lorenz Wolf","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2306.01158v3.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.00002v3","updated":"2024-10-31T14:52:28Z","published":"2024-04-09T14:08:06Z","title":"Kermut: Composite kernel regression for protein variant effects","summary":"  Reliable prediction of protein variant effects is crucial for both protein\noptimization and for advancing biological understanding. For practical use in\nprotein engineering, it is important that we can also provide reliable\nuncertainty estimates for our predictions, and while prediction accuracy has\nseen much progress in recent years, uncertainty metrics are rarely reported. We\nhere provide a Gaussian process regression model, Kermut, with a novel\ncomposite kernel for modeling mutation similarity, which obtains\nstate-of-the-art performance for supervised protein variant effect prediction\nwhile also offering estimates of uncertainty through its posterior. An analysis\nof the quality of the uncertainty estimates demonstrates that our model\nprovides meaningful levels of overall calibration, but that instance-specific\nuncertainty calibration remains more challenging.\n","authors":["Peter Mørch Groth","Mads Herbert Kerrn","Lars Olsen","Jesper Salomon","Wouter Boomsma"],"pdf_url":"https://arxiv.org/pdf/2407.00002v3.pdf","comment":"Accepted to NeurIPS 2024 as Spotlight"},{"id":"http://arxiv.org/abs/2410.23994v1","updated":"2024-10-31T14:52:01Z","published":"2024-10-31T14:52:01Z","title":"Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using\n  Discrete State Space Diffusion Model","summary":"  Sequential recommendation (SR) aims to predict items that users may be\ninterested in based on their historical behavior sequences. We revisit SR from\na novel information-theoretic perspective and find that conventional sequential\nmodeling methods fail to adequately capture the randomness and unpredictability\nof user behavior. Inspired by fuzzy information processing theory, this paper\nintroduces the DDSR model, which uses fuzzy sets of interaction sequences to\novercome the limitations and better capture the evolution of users' real\ninterests. Formally based on diffusion transition processes in discrete state\nspaces, which is unlike common diffusion models such as DDPM that operate in\ncontinuous domains. It is better suited for discrete data, using structured\ntransitions instead of arbitrary noise introduction to avoid information loss.\nAdditionally, to address the inefficiency of matrix transformations due to the\nvast discrete space, we use semantic labels derived from quantization or RQ-VAE\nto replace item IDs, enhancing efficiency and improving cold start issues.\nTesting on three public benchmark datasets shows that DDSR outperforms existing\nstate-of-the-art methods in various settings, demonstrating its potential and\neffectiveness in handling SR tasks.\n","authors":["Wenjia Xie","Hao Wang","Luankang Zhang","Rui Zhou","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.23994v1.pdf","comment":"NeurIPS'2024, 10 pages"},{"id":"http://arxiv.org/abs/2410.23992v1","updated":"2024-10-31T14:51:09Z","published":"2024-10-31T14:51:09Z","title":"Ada-MSHyper: Adaptive Multi-Scale Hypergraph Transformer for Time Series\n  Forecasting","summary":"  Although transformer-based methods have achieved great success in multi-scale\ntemporal pattern interaction modeling, two key challenges limit their further\ndevelopment: (1) Individual time points contain less semantic information, and\nleveraging attention to model pair-wise interactions may cause the information\nutilization bottleneck. (2) Multiple inherent temporal variations (e.g.,\nrising, falling, and fluctuating) entangled in temporal patterns. To this end,\nwe propose Adaptive Multi-Scale Hypergraph Transformer (Ada-MSHyper) for time\nseries forecasting. Specifically, an adaptive hypergraph learning module is\ndesigned to provide foundations for modeling group-wise interactions, then a\nmulti-scale interaction module is introduced to promote more comprehensive\npattern interactions at different scales. In addition, a node and hyperedge\nconstraint mechanism is introduced to cluster nodes with similar semantic\ninformation and differentiate the temporal variations within each scales.\nExtensive experiments on 11 real-world datasets demonstrate that Ada-MSHyper\nachieves state-of-the-art performance, reducing prediction errors by an average\nof 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and\nultra-long-range time series forecasting, respectively. Code is available at\nhttps://github.com/shangzongjiang/Ada-MSHyper.\n","authors":["Zongjiang Shang","Ling Chen","Binqing wu","Dongliang Cui"],"pdf_url":"https://arxiv.org/pdf/2410.23992v1.pdf","comment":"Accepted by NeurIPS, 21 pages, and 8 figures"},{"id":"http://arxiv.org/abs/2409.17692v2","updated":"2024-10-31T14:38:27Z","published":"2024-09-26T09:57:16Z","title":"MIO: A Foundation Model on Multimodal Tokens","summary":"  In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.\n","authors":["Zekun Wang","King Zhu","Chunpu Xu","Wangchunshu Zhou","Jiaheng Liu","Yibo Zhang","Jiashuo Wang","Ning Shi","Siyu Li","Yizhi Li","Haoran Que","Zhaoxiang Zhang","Yuanxing Zhang","Ge Zhang","Ke Xu","Jie Fu","Wenhao Huang"],"pdf_url":"https://arxiv.org/pdf/2409.17692v2.pdf","comment":"Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO"},{"id":"http://arxiv.org/abs/2403.03333v3","updated":"2024-10-31T14:37:27Z","published":"2024-03-05T21:34:23Z","title":"Federated Learning over Connected Modes","summary":"  Statistical heterogeneity in federated learning poses two major challenges:\nslow global training due to conflicting gradient signals, and the need of\npersonalization for local distributions. In this work, we tackle both\nchallenges by leveraging recent advances in \\emph{linear mode connectivity} --\nidentifying a linearly connected low-loss region in the parameter space of\nneural networks, which we call solution simplex. We propose federated learning\nover connected modes (\\textsc{Floco}), where clients are assigned local\nsubregions in this simplex based on their gradient signals, and together learn\nthe shared global solution simplex. This allows personalization of the client\nmodels to fit their local distributions within the degrees of freedom in the\nsolution simplex and homogenizes the update signals for the global simplex\ntraining. Our experiments show that \\textsc{Floco} accelerates the global\ntraining process, and significantly improves the local accuracy with minimal\ncomputational overhead in cross-silo federated learning settings.\n","authors":["Dennis Grinwald","Philipp Wiesner","Shinichi Nakajima"],"pdf_url":"https://arxiv.org/pdf/2403.03333v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22637v2","updated":"2024-10-31T14:35:31Z","published":"2024-10-30T02:04:23Z","title":"Consistency Diffusion Bridge Models","summary":"  Diffusion models (DMs) have become the dominant paradigm of generative\nmodeling in a variety of domains by learning stochastic processes from noise to\ndata. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\ngenerative modeling that builds stochastic processes between fixed data\nendpoints based on a reference diffusion process, have achieved empirical\nsuccess across tasks with coupled data distribution, such as image-to-image\ntranslation. However, DDBM's sampling process typically requires hundreds of\nnetwork evaluations to achieve decent performance, which may impede their\npractical deployment due to high computational demands. In this work, inspired\nby the recent advance of consistency models in DMs, we tackle this problem by\nlearning the consistency function of the probability-flow ordinary differential\nequation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\nstep given any point on the ODE trajectory. Based on a dedicated general-form\nODE solver, we propose two paradigms: consistency bridge distillation and\nconsistency bridge training, which is flexible to apply on DDBMs with broad\ndesign choices. Experimental results show that our proposed method could sample\n$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\nquality given the same step in various tasks with pixel resolution ranging from\n$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\nas semantic interpolation in the data space.\n","authors":["Guande He","Kaiwen Zheng","Jianfei Chen","Fan Bao","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.22637v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2310.17638v3","updated":"2024-10-31T14:35:05Z","published":"2023-10-26T17:53:24Z","title":"Generative Fractional Diffusion Models","summary":"  We introduce the first continuous-time score-based generative model that\nleverages fractional diffusion processes for its underlying dynamics. Although\ndiffusion models have excelled at capturing data distributions, they still\nsuffer from various limitations such as slow convergence, mode-collapse on\nimbalanced data, and lack of diversity. These issues are partially linked to\nthe use of light-tailed Brownian motion (BM) with independent increments. In\nthis paper, we replace BM with an approximation of its non-Markovian\ncounterpart, fractional Brownian motion (fBM), characterized by correlated\nincrements and Hurst index $H \\in (0,1)$, where $H=0.5$ recovers the classical\nBM. To ensure tractable inference and learning, we employ a recently\npopularized Markov approximation of fBM (MA-fBM) and derive its reverse-time\nmodel, resulting in generative fractional diffusion models (GFDM). We\ncharacterize the forward dynamics using a continuous reparameterization trick\nand propose augmented score matching to efficiently learn the score function,\nwhich is partly known in closed form, at minimal added cost. The ability to\ndrive our diffusion model via MA-fBM offers flexibility and control. $H \\leq\n0.5$ enters the regime of rough paths whereas $H>0.5$ regularizes diffusion\npaths and invokes long-term memory. The Markov approximation allows added\ncontrol by varying the number of Markov processes linearly combined to\napproximate fBM. Our evaluations on real image datasets demonstrate that GFDM\nachieves greater pixel-wise diversity and enhanced image quality, as indicated\nby a lower FID, offering a promising alternative to traditional diffusion\nmodels\n","authors":["Gabriel Nobis","Maximilian Springenberg","Marco Aversa","Michael Detzel","Rembert Daems","Roderick Murray-Smith","Shinichi Nakajima","Sebastian Lapuschkin","Stefano Ermon","Tolga Birdal","Manfred Opper","Christoph Knochenhauer","Luis Oala","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2310.17638v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19534v4","updated":"2024-10-31T14:32:28Z","published":"2024-05-29T21:29:44Z","title":"Preference Learning Algorithms Do Not Learn Preference Rankings","summary":"  Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms.\n","authors":["Angelica Chen","Sadhika Malladi","Lily H. Zhang","Xinyi Chen","Qiuyi Zhang","Rajesh Ranganath","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2405.19534v4.pdf","comment":"NeurIPS 2024 camera-ready"},{"id":"http://arxiv.org/abs/2407.00114v2","updated":"2024-10-31T14:27:50Z","published":"2024-06-27T13:46:11Z","title":"OmniJARVIS: Unified Vision-Language-Action Tokenization Enables\n  Open-World Instruction Following Agents","summary":"  This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps://craftjarvis.org/OmniJARVIS.\n","authors":["Zihao Wang","Shaofei Cai","Zhancun Mu","Haowei Lin","Ceyao Zhang","Xuejie Liu","Qing Li","Anji Liu","Xiaojian Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2407.00114v2.pdf","comment":"accepted on NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23970v1","updated":"2024-10-31T14:25:55Z","published":"2024-10-31T14:25:55Z","title":"TrAct: Making First-layer Pre-Activations Trainable","summary":"  We consider the training of the first layer of vision models and notice the\nclear relationship between pixel values and gradient update magnitudes: the\ngradients arriving at the weights of a first layer are by definition directly\nproportional to (normalized) input pixel values. Thus, an image with low\ncontrast has a smaller impact on learning than an image with higher contrast,\nand a very bright or very dark image has a stronger impact on the weights than\nan image with moderate brightness. In this work, we propose performing gradient\ndescent on the embeddings produced by the first layer of the model. However,\nswitching to discrete inputs with an embedding layer is not a reasonable option\nfor vision models. Thus, we propose the conceptual procedure of (i) a gradient\ndescent step on first layer activations to construct an activation proposal,\nand (ii) finding the optimal weights of the first layer, i.e., those weights\nwhich minimize the squared distance to the activation proposal. We provide a\nclosed form solution of the procedure and adjust it for robust stochastic\ntraining while computing everything efficiently. Empirically, we find that\nTrAct (Training Activations) speeds up training by factors between 1.25x and 4x\nwhile requiring only a small computational overhead. We demonstrate the utility\nof TrAct with different optimizers for a range of different vision models\nincluding convolutional and transformer architectures.\n","authors":["Felix Petersen","Christian Borgelt","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.23970v1.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.06246v2","updated":"2024-10-31T14:24:45Z","published":"2024-06-10T13:23:00Z","title":"Data-Efficient Learning with Neural Programs","summary":"  Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymbolic learning literature. Our evaluation shows\nthat for the latter benchmarks, ISED has comparable performance to\nstate-of-the-art neurosymbolic frameworks. For the former, we use adaptations\nof prior work on gradient approximations of black-box components as a baseline,\nand show that ISED achieves comparable accuracy but in a more data- and\nsample-efficient manner.\n","authors":["Alaia Solko-Breslin","Seewon Choi","Ziyang Li","Neelay Velingker","Rajeev Alur","Mayur Naik","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2406.06246v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23969v1","updated":"2024-10-31T14:22:52Z","published":"2024-10-31T14:22:52Z","title":"Interactive proofs for verifying (quantum) learning and testing","summary":"  We consider the problem of testing and learning from data in the presence of\nresource constraints, such as limited memory or weak data access, which place\nlimitations on the efficiency and feasibility of testing or learning. In\nparticular, we ask the following question: Could a resource-constrained\nlearner/tester use interaction with a resource-unconstrained but untrusted\nparty to solve a learning or testing problem more efficiently than they could\nwithout such an interaction? In this work, we answer this question both\nabstractly and for concrete problems, in two complementary ways: For a wide\nvariety of scenarios, we prove that a resource-constrained learner cannot gain\nany advantage through classical interaction with an untrusted prover. As a\nspecial case, we show that for the vast majority of testing and learning\nproblems in which quantum memory is a meaningful resource, a memory-constrained\nquantum algorithm cannot overcome its limitations via classical communication\nwith a memory-unconstrained quantum prover. In contrast, when quantum\ncommunication is allowed, we construct a variety of interactive proof\nprotocols, for specific learning and testing problems, which allow\nmemory-constrained quantum verifiers to gain significant advantages through\ndelegation to untrusted provers. These results highlight both the limitations\nand potential of delegating learning and testing problems to resource-rich but\nuntrusted third parties.\n","authors":["Matthias C. Caro","Jens Eisert","Marcel Hinsche","Marios Ioannou","Alexander Nietner","Ryan Sweke"],"pdf_url":"https://arxiv.org/pdf/2410.23969v1.pdf","comment":"12 + 33 + 13 pages; 1 table; 2 figures"},{"id":"http://arxiv.org/abs/2405.14808v2","updated":"2024-10-31T14:19:49Z","published":"2024-05-23T17:18:46Z","title":"Implicit Personalization in Language Models: A Systematic Study","summary":"  Implicit Personalization (IP) is a phenomenon of language models inferring a\nuser's background from the implicit cues in the input prompts and tailoring the\nresponse based on this inference. While previous work has touched upon various\ninstances of this problem, there lacks a unified framework to study this\nbehavior. This work systematically studies IP through a rigorous mathematical\nformulation, a multi-perspective moral reasoning framework, and a set of case\nstudies. Our theoretical foundation for IP relies on a structural causal model\nand introduces a novel method, indirect intervention, to estimate the causal\neffect of a mediator variable that cannot be directly intervened upon. Beyond\nthe technical approach, we also introduce a set of moral reasoning principles\nbased on three schools of moral philosophy to study when IP may or may not be\nethically appropriate. Equipped with both mathematical and ethical insights, we\npresent three diverse case studies illustrating the varied nature of the IP\nproblem and offer recommendations for future research. Our code is at\nhttps://github.com/jiarui-liu/IP, and our data is at\nhttps://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData.\n","authors":["Zhijing Jin","Nils Heil","Jiarui Liu","Shehzaad Dhuliawala","Yahang Qi","Bernhard Schölkopf","Rada Mihalcea","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.14808v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.21076v2","updated":"2024-10-31T14:16:36Z","published":"2024-10-28T14:40:01Z","title":"Accelerated Bayesian parameter estimation and model selection for\n  gravitational waves with normalizing flows","summary":"  We present an accelerated pipeline, based on high-performance computing\ntechniques and normalizing flows, for joint Bayesian parameter estimation and\nmodel selection and demonstrate its efficiency in gravitational wave\nastrophysics. We integrate the Jim inference toolkit, a normalizing\nflow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned\nharmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are\nconsistent with traditional nested sampling techniques run on $16$ CPU cores,\nwhile reducing the computation time by factors of $5\\times$ and $15\\times$ for\n$4$-dimensional and $11$-dimensional gravitational wave inference problems,\nrespectively. Our code is available in well-tested and thoroughly documented\nopen-source packages, ensuring accessibility and reproducibility for the wider\nresearch community.\n","authors":["Alicja Polanska","Thibeau Wouters","Peter T. H. Pang","Kaze K. W. Wong","Jason D. McEwen"],"pdf_url":"https://arxiv.org/pdf/2410.21076v2.pdf","comment":"accepted to NeurIPS 2024 workshop on Machine Learning and the\n  Physical Sciences"},{"id":"http://arxiv.org/abs/2410.23955v1","updated":"2024-10-31T14:09:05Z","published":"2024-10-31T14:09:05Z","title":"An Empirical Analysis of Speech Self-Supervised Learning at Multiple\n  Resolutions","summary":"  Self-supervised learning (SSL) models have become crucial in speech\nprocessing, with recent advancements concentrating on developing architectures\nthat capture representations across multiple timescales. The primary goal of\nthese multi-scale architectures is to exploit the hierarchical nature of\nspeech, where lower-resolution components aim to capture representations that\nalign with increasingly abstract concepts (e.g., from phones to words to\nsentences). Although multi-scale approaches have demonstrated some improvements\nover single-scale models, the precise reasons for these enhancements have poor\nempirical support. In this study, we present an initial analysis of layer-wise\nrepresentations in multi-scale architectures, with a focus on Canonical\nCorrelation Analysis (CCA) and Mutual Information (MI). We apply this analysis\nto Multi-Resolution HuBERT (MR-HuBERT) and find that (1) the improved\nperformance on SUPERB tasks is primarily due to the auxiliary low-resolution\nloss rather than the downsampling itself, and (2) downsampling to lower\nresolutions neither improves downstream performance nor correlates with\nhigher-level information (e.g., words), though it does improve computational\nefficiency. These findings challenge assumptions about the multi-scale nature\nof MR-HuBERT and motivate the importance of disentangling computational\nefficiency from learning better representations.\n","authors":["Theo Clark","Benedetta Cevoli","Eloy de Jong","Timofey Abramski","Jamie Dougherty"],"pdf_url":"https://arxiv.org/pdf/2410.23955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23953v1","updated":"2024-10-31T14:07:26Z","published":"2024-10-31T14:07:26Z","title":"Representative Social Choice: From Learning Theory to AI Alignment","summary":"  Social choice theory is the study of preference aggregation across a\npopulation, used both in mechanism design for human agents and in the\ndemocratic alignment of language models. In this study, we propose the\nrepresentative social choice framework for the modeling of democratic\nrepresentation in collective decisions, where the number of issues and\nindividuals are too large for mechanisms to consider all preferences directly.\nThese scenarios are widespread in real-world decision-making processes, such as\njury trials, indirect elections, legislation processes, corporate governance,\nand, more recently, language model alignment. In representative social choice,\nthe population is represented by a finite sample of individual-issue pairs\nbased on which social choice decisions are made. We show that many of the\ndeepest questions in representative social choice can be naturally formulated\nas statistical learning problems, and prove the generalization properties of\nsocial choice mechanisms using the theory of machine learning. We further\nformulate axioms for representative social choice, and prove Arrow-like\nimpossibility theorems with new combinatorial tools of analysis. Our framework\nintroduces the representative approach to social choice, opening up research\ndirections at the intersection of social choice, learning theory, and AI\nalignment.\n","authors":["Tianyi Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23953v1.pdf","comment":"Full version (20 pages). Under review. An excerpt was previously\n  accepted to NeurIPS 2024 Pluralistic Alignment Workshop"},{"id":"http://arxiv.org/abs/2410.23952v1","updated":"2024-10-31T14:06:43Z","published":"2024-10-31T14:06:43Z","title":"Scalable Kernel Inverse Optimization","summary":"  Inverse Optimization (IO) is a framework for learning the unknown objective\nfunction of an expert decision-maker from a past dataset. In this paper, we\nextend the hypothesis class of IO objective functions to a reproducing kernel\nHilbert space (RKHS), thereby enhancing feature representation to an\ninfinite-dimensional space. We demonstrate that a variant of the representer\ntheorem holds for a specific training loss, allowing the reformulation of the\nproblem as a finite-dimensional convex optimization program. To address\nscalability issues commonly associated with kernel methods, we propose the\nSequential Selection Optimization (SSO) algorithm to efficiently train the\nproposed Kernel Inverse Optimization (KIO) model. Finally, we validate the\ngeneralization capabilities of the proposed KIO model and the effectiveness of\nthe SSO algorithm through learning-from-demonstration tasks on the MuJoCo\nbenchmark.\n","authors":["Youyuan Long","Tolga Ok","Pedro Zattoni Scroccaro","Peyman Mohajerin Esfahani"],"pdf_url":"https://arxiv.org/pdf/2410.23952v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23949v1","updated":"2024-10-31T14:04:33Z","published":"2024-10-31T14:04:33Z","title":"Deep Learning Frameworks for Cognitive Radio Networks: Review and Open\n  Research Challenges","summary":"  Deep learning has been proven to be a powerful tool for addressing the most\nsignificant issues in cognitive radio networks, such as spectrum sensing,\nspectrum sharing, resource allocation, and security attacks. The utilization of\ndeep learning techniques in cognitive radio networks can significantly enhance\nthe network's capability to adapt to changing environments and improve the\noverall system's efficiency and reliability. As the demand for higher data\nrates and connectivity increases, B5G/6G wireless networks are expected to\nenable new services and applications significantly. Therefore, the significance\nof deep learning in addressing cognitive radio network challenges cannot be\noverstated. This review article provides valuable insights into potential\nsolutions that can serve as a foundation for the development of future B5G/6G\nservices. By leveraging the power of deep learning, cognitive radio networks\ncan pave the way for the next generation of wireless networks capable of\nmeeting the ever-increasing demands for higher data rates, improved\nreliability, and security.\n","authors":["Senthil Kumar Jagatheesaperumal","Ijaz Ahmad","Marko Höyhtyä","Suleman Khan","Andrei Gurtov"],"pdf_url":"https://arxiv.org/pdf/2410.23949v1.pdf","comment":"The article has been accepted for publication in \"Journal of Network\n  and Computer Applications\" during October 2024"},{"id":"http://arxiv.org/abs/2410.23948v1","updated":"2024-10-31T14:03:37Z","published":"2024-10-31T14:03:37Z","title":"Transformers to Predict the Applicability of Symbolic Integration\n  Routines","summary":"  Symbolic integration is a fundamental problem in mathematics: we consider how\nmachine learning may be used to optimise this task in a Computer Algebra System\n(CAS). We train transformers that predict whether a particular integration\nmethod will be successful, and compare against the existing human-made\nheuristics (called guards) that perform this task in a leading CAS. We find the\ntransformer can outperform these guards, gaining up to 30% accuracy and 70%\nprecision. We further show that the inference time of the transformer is\ninconsequential which shows that it is well-suited to include as a guard in a\nCAS. Furthermore, we use Layer Integrated Gradients to interpret the decisions\nthat the transformer is making. If guided by a subject-matter expert, the\ntechnique can explain some of the predictions based on the input tokens, which\ncan lead to further optimisations.\n","authors":["Rashid Barket","Uzma Shafiq","Matthew England","Juergen Gerhard"],"pdf_url":"https://arxiv.org/pdf/2410.23948v1.pdf","comment":"10 pages, 5 figures, to be published in NeurIPS 2024 MATH-AI Workshop"},{"id":"http://arxiv.org/abs/2410.23940v1","updated":"2024-10-31T13:54:37Z","published":"2024-10-31T13:54:37Z","title":"Quantum Deep Equilibrium Models","summary":"  The feasibility of variational quantum algorithms, the most popular\ncorrespondent of neural networks on noisy, near-term quantum hardware, is\nhighly impacted by the circuit depth of the involved parametrized quantum\ncircuits (PQCs). Higher depth increases expressivity, but also results in a\ndetrimental accumulation of errors. Furthermore, the number of parameters\ninvolved in the PQC significantly influences the performance through the\nnecessary number of measurements to evaluate gradients, which scales linearly\nwith the number of parameters.\n  Motivated by this, we look at deep equilibrium models (DEQs), which mimic an\ninfinite-depth, weight-tied network using a fraction of the memory by employing\na root solver to find the fixed points of the network. In this work, we present\nQuantum Deep Equilibrium Models (QDEQs): a training paradigm that learns\nparameters of a quantum machine learning model given by a PQC using DEQs. To\nour knowledge, no work has yet explored the application of DEQs to QML models.\nWe apply QDEQs to find the parameters of a quantum circuit in two settings: the\nfirst involves classifying MNIST-4 digits with 4 qubits; the second extends it\nto 10 classes of MNIST, FashionMNIST and CIFAR. We find that QDEQ is not only\ncompetitive with comparable existing baseline models, but also achieves higher\nperformance than a network with 5 times more layers. This demonstrates that the\nQDEQ paradigm can be used to develop significantly more shallow quantum\ncircuits for a given task, something which is essential for the utility of\nnear-term quantum computers.\n  Our code is available at https://github.com/martaskrt/qdeq.\n","authors":["Philipp Schleich","Marta Skreta","Lasse B. Kristensen","Rodrigo A. Vargas-Hernández","Alán Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2410.23940v1.pdf","comment":"To be published in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23938v1","updated":"2024-10-31T13:52:59Z","published":"2024-10-31T13:52:59Z","title":"Learning Macroscopic Dynamics from Partial Microscopic Observations","summary":"  Macroscopic observables of a system are of keen interest in real applications\nsuch as the design of novel materials. Current methods rely on microscopic\ntrajectory simulations, where the forces on all microscopic coordinates need to\nbe computed or measured. However, this can be computationally prohibitive for\nrealistic systems. In this paper, we propose a method to learn macroscopic\ndynamics requiring only force computations on a subset of the microscopic\ncoordinates. Our method relies on a sparsity assumption: the force on each\nmicroscopic coordinate relies only on a small number of other coordinates. The\nmain idea of our approach is to map the training procedure on the macroscopic\ncoordinates back to the microscopic coordinates, on which partial force\ncomputations can be used as stochastic estimation to update model parameters.\nWe provide a theoretical justification of this under suitable conditions. We\ndemonstrate the accuracy, force computation efficiency, and robustness of our\nmethod on learning macroscopic closure models from a variety of microscopic\nsystems, including those modeled by partial differential equations or molecular\ndynamics simulations.\n","authors":["Mengyi Chen","Qianxiao Li"],"pdf_url":"https://arxiv.org/pdf/2410.23938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23937v1","updated":"2024-10-31T13:51:59Z","published":"2024-10-31T13:51:59Z","title":"Robust Sparse Regression with Non-Isotropic Designs","summary":"  We develop a technique to design efficiently computable estimators for sparse\nlinear regression in the simultaneous presence of two adversaries: oblivious\nand adaptive. We design several robust algorithms that outperform the state of\nthe art even in the special case when oblivious adversary simply adds Gaussian\nnoise. In particular, we provide a polynomial-time algorithm that with high\nprobability recovers the signal up to error $O(\\sqrt{\\varepsilon})$ as long as\nthe number of samples $n \\ge \\tilde{O}(k^2/\\varepsilon)$, only assuming some\nbounds on the third and the fourth moments of the distribution ${D}$ of the\ndesign.\n  In addition, prior to this work, even in the special case of Gaussian design\nand noise, no polynomial time algorithm was known to achieve error\n$o(\\sqrt{\\varepsilon})$ in the sparse setting $n < d^2$. We show that under\nsome assumptions on the fourth and the eighth moments of ${D}$, there is a\npolynomial-time algorithm that achieves error $o(\\sqrt{\\varepsilon})$ as long\nas $n \\ge \\tilde{O}(k^4 / \\varepsilon^3)$. For Gaussian distribution, this\nalgorithm achieves error $O(\\varepsilon^{3/4})$. Moreover, our algorithm\nachieves error $o(\\sqrt{\\varepsilon})$ for all log-concave distributions if\n$\\varepsilon \\le 1/\\text{polylog(d)}$.\n  Our algorithms are based on the filtering of the covariates that uses\nsum-of-squares relaxations, and weighted Huber loss minimization with $\\ell_1$\nregularizer. We provide a novel analysis of weighted penalized Huber loss that\nis suitable for heavy-tailed designs in the presence of two adversaries.\nFurthermore, we complement our algorithmic results with Statistical Query lower\nbounds, providing evidence that our estimators are likely to have nearly\noptimal sample complexity.\n","authors":["Chih-Hung Liu","Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2410.23937v1.pdf","comment":"NeurIPS 2024; Authors have equal contribution"},{"id":"http://arxiv.org/abs/2410.22086v2","updated":"2024-10-31T13:50:02Z","published":"2024-10-29T14:41:44Z","title":"Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate","summary":"  Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.\n","authors":["Zhiqi Bu","Xiaomeng Jin","Bhanukiran Vinzamuri","Anil Ramakrishna","Kai-Wei Chang","Volkan Cevher","Mingyi Hong"],"pdf_url":"https://arxiv.org/pdf/2410.22086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22938v2","updated":"2024-10-31T13:39:20Z","published":"2024-10-30T11:47:40Z","title":"DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic\n  Signal Control with Missing Data","summary":"  The application of reinforcement learning in traffic signal control (TSC) has\nbeen extensively researched and yielded notable achievements. However, most\nexisting works for TSC assume that traffic data from all surrounding\nintersections is fully and continuously available through sensors. In\nreal-world applications, this assumption often fails due to sensor malfunctions\nor data loss, making TSC with missing data a critical challenge. To meet the\nneeds of practical applications, we introduce DiffLight, a novel conditional\ndiffusion model for TSC under data-missing scenarios in the offline setting.\nSpecifically, we integrate two essential sub-tasks, i.e., traffic data\nimputation and decision-making, by leveraging a Partial Rewards Conditioned\nDiffusion (PRCD) model to prevent missing rewards from interfering with the\nlearning process. Meanwhile, to effectively capture the spatial-temporal\ndependencies among intersections, we design a Spatial-Temporal transFormer\n(STFormer) architecture. In addition, we propose a Diffusion Communication\nMechanism (DCM) to promote better communication and control performance under\ndata-missing scenarios. Extensive experiments on five datasets with various\ndata-missing scenarios demonstrate that DiffLight is an effective controller to\naddress TSC with missing data. The code of DiffLight is released at\nhttps://github.com/lokol5579/DiffLight-release.\n","authors":["Hanyang Chen","Yang Jiang","Shengnan Guo","Xiaowei Mao","Youfang Lin","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2410.22938v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.22193v2","updated":"2024-10-31T13:37:43Z","published":"2024-10-29T16:31:40Z","title":"GoRINNs: Godunov-Riemann Informed Neural Networks for Learning\n  Hyperbolic Conservation Laws","summary":"  We present GoRINNs: numerical analysis-informed neural networks for the\nsolution of inverse problems of non-linear systems of conservation laws.\nGoRINNs are based on high-resolution Godunov schemes for the solution of the\nRiemann problem in hyperbolic Partial Differential Equations (PDEs). In\ncontrast to other existing machine learning methods that learn the numerical\nfluxes of conservative Finite Volume methods, GoRINNs learn the physical flux\nfunction per se. Due to their structure, GoRINNs provide interpretable,\nconservative schemes, that learn the solution operator on the basis of\napproximate Riemann solvers that satisfy the Rankine-Hugoniot condition. The\nperformance of GoRINNs is assessed via four benchmark problems, namely the\nBurgers', the Shallow Water, the Lighthill-Whitham-Richards and the\nPayne-Whitham traffic flow models. The solution profiles of these PDEs exhibit\nshock waves, rarefactions and/or contact discontinuities at finite times. We\ndemonstrate that GoRINNs provide a very high accuracy both in the smooth and\ndiscontinuous regions.\n","authors":["Dimitrios G. Patsatzis","Mario di Bernardo","Lucia Russo","Constantinos Siettos"],"pdf_url":"https://arxiv.org/pdf/2410.22193v2.pdf","comment":"29 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.23922v1","updated":"2024-10-31T13:32:39Z","published":"2024-10-31T13:32:39Z","title":"Analyzing & Reducing the Need for Learning Rate Warmup in GPT Training","summary":"  Learning Rate Warmup is a popular heuristic for training neural networks,\nespecially at larger batch sizes, despite limited understanding of its\nbenefits. Warmup decreases the update size $\\Delta \\mathbf{w}_t = \\eta_t\n\\mathbf{u}_t$ early in training by using lower values for the learning rate\n$\\eta_t$. In this work we argue that warmup benefits training by keeping the\noverall size of $\\Delta \\mathbf{w}_t$ limited, counteracting large initial\nvalues of $\\mathbf{u}_t$. Focusing on small-scale GPT training with AdamW/Lion,\nwe explore the following question: Why and by which criteria are early updates\n$\\mathbf{u}_t$ too large? We analyze different metrics for the update size\nincluding the $\\ell_2$-norm, resulting directional change, and impact on the\nrepresentations of the network, providing a new perspective on warmup. In\nparticular, we find that warmup helps counteract large angular updates as well\nas a limited critical batch size early in training. Finally, we show that the\nneed for warmup can be significantly reduced or eliminated by modifying the\noptimizer to explicitly normalize $\\mathbf{u}_t$ based on the aforementioned\nmetrics.\n","authors":["Atli Kosson","Bettina Messmer","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2410.23922v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23918v1","updated":"2024-10-31T13:26:11Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09160v3","updated":"2024-10-31T13:20:42Z","published":"2023-02-17T22:15:20Z","title":"Identifying Equivalent Training Dynamics","summary":"  Study of the nonlinear evolution deep neural network (DNN) parameters undergo\nduring training has uncovered regimes of distinct dynamical behavior. While a\ndetailed understanding of these phenomena has the potential to advance\nimprovements in training efficiency and robustness, the lack of methods for\nidentifying when DNN models have equivalent dynamics limits the insight that\ncan be gained from prior work. Topological conjugacy, a notion from dynamical\nsystems theory, provides a precise definition of dynamical equivalence,\noffering a possible route to address this need. However, topological\nconjugacies have historically been challenging to compute. By leveraging\nadvances in Koopman operator theory, we develop a framework for identifying\nconjugate and non-conjugate training dynamics. To validate our approach, we\ndemonstrate that comparing Koopman eigenvalues can correctly identify a known\nequivalence between online mirror descent and online gradient descent. We then\nutilize our approach to: (a) identify non-conjugate training dynamics between\nshallow and wide fully connected neural networks; (b) characterize the early\nphase of training dynamics in convolutional neural networks; (c) uncover\nnon-conjugate training dynamics in Transformers that do and do not undergo\ngrokking. Our results, across a range of DNN architectures, illustrate the\nflexibility of our framework and highlight its potential for shedding new light\non training dynamics.\n","authors":["William T. Redman","Juan M. Bello-Rivas","Maria Fonoberova","Ryan Mohr","Ioannis G. Kevrekidis","Igor Mezić"],"pdf_url":"https://arxiv.org/pdf/2302.09160v3.pdf","comment":"23 pages, 5 figures, 6 supplemental figures"},{"id":"http://arxiv.org/abs/2410.23912v1","updated":"2024-10-31T13:17:53Z","published":"2024-10-31T13:17:53Z","title":"RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for\n  Self-Taught Reasoner","summary":"  The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks in a\nstepwise manner. However, training CoT capabilities requires detailed reasoning\ndata, which is often scarce. The self-taught reasoner (STaR) framework\naddresses this by using reinforcement learning to automatically generate\nreasoning steps, reducing reliance on human-labeled data. Although STaR and its\nvariants have demonstrated empirical success, a theoretical foundation\nexplaining these improvements is lacking. This work provides a theoretical\nframework for understanding the effectiveness of reinforcement learning on CoT\nreasoning and STaR. Our contributions are: (1) an analysis of policy\nimprovement, showing why LLM reasoning improves iteratively with STaR; (2)\nconditions for convergence to an optimal reasoning policy; (3) an examination\nof STaR's robustness, explaining how it can improve reasoning even when\nincorporating occasional incorrect steps; and (4) criteria for the quality of\npre-trained models necessary to initiate effective reasoning improvement. This\nframework aims to bridge empirical findings with theoretical insights,\nadvancing reinforcement learning approaches for reasoning in LLMs.\n","authors":["Fu-Chieh Chang","Yu-Ting Lee","Hui-Ying Shih","Pei-Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.23912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20087v2","updated":"2024-10-31T13:10:12Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v2.pdf","comment":"NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)"},{"id":"http://arxiv.org/abs/2303.15124v2","updated":"2024-10-31T13:10:11Z","published":"2023-03-27T11:56:20Z","title":"Blind Inpainting with Object-aware Discrimination for Artificial Marker\n  Removal","summary":"  Medical images often incorporate doctor-added markers that can hinder\nAI-based diagnosis. This issue highlights the need of inpainting techniques to\nrestore the corrupted visual contents. However, existing methods require manual\nmask annotation as input, limiting the application scenarios. In this paper, we\npropose a novel blind inpainting method that automatically reconstructs visual\ncontents within the corrupted regions without mask input as guidance. Our model\nincludes a blind reconstruction network and an object-aware discriminator for\nadversarial training. The reconstruction network contains two branches that\npredict corrupted regions in images and simultaneously restore the missing\nvisual contents. Leveraging the potent recognition capability of a dense object\ndetector, the object-aware discriminator ensures markers undetectable after\ninpainting. Thus, the restored images closely resemble the clean ones. We\nevaluate our method on three datasets of various medical imaging modalities,\nconfirming better performance over other state-of-the-art methods.\n","authors":["Xuechen Guo","Wenhao Hu","Chiming Ni","Wenhao Chai","Shiyan Li","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23903v1","updated":"2024-10-31T13:05:46Z","published":"2024-10-31T13:05:46Z","title":"Neural Network Verification with PyRAT","summary":"  As AI systems are becoming more and more popular and used in various critical\ndomains (health, transport, energy, ...), the need to provide guarantees and\ntrust of their safety is undeniable. To this end, we present PyRAT, a tool\nbased on abstract interpretation to verify the safety and the robustness of\nneural networks. In this paper, we describe the different abstractions used by\nPyRAT to find the reachable states of a neural network starting from its input\nas well as the main features of the tool to provide fast and accurate analysis\nof neural networks. PyRAT has already been used in several collaborations to\nensure safety guarantees, with its second place at the VNN-Comp 2024 showcasing\nits performance.\n","authors":["Augustin Lemesle","Julien Lehmann","Tristan Le Gall"],"pdf_url":"https://arxiv.org/pdf/2410.23903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09451v2","updated":"2024-10-31T13:02:43Z","published":"2024-06-12T15:51:00Z","title":"Enhancing Activity Recognition After Stroke: Generative Adversarial\n  Networks for Kinematic Data Augmentation","summary":"  The generalizability of machine learning (ML) models for wearable monitoring\nin stroke rehabilitation is often constrained by the limited scale and\nheterogeneity of available data. Data augmentation addresses this challenge by\nadding computationally derived data to real data to enrich the variability\nrepresented in the training set. Traditional augmentation methods, such as\nrotation, permutation, and time-warping, have shown some benefits in improving\nclassifier performance, but often fail to produce realistic training examples.\nThis study employs Conditional Generative Adversarial Networks (cGANs) to\ncreate synthetic kinematic data from a publicly available dataset, closely\nmimicking the experimentally measured reaching movements of stroke survivors.\nThis approach not only captures the complex temporal dynamics and common\nmovement patterns after stroke, but also significantly enhances the training\ndataset. By training deep learning models on both synthetic and experimental\ndata, we enhanced task classification accuracy: models incorporating synthetic\ndata attained an overall accuracy of 80.0%, significantly higher than the 66.1%\nseen in models trained solely with real data. These improvements allow for more\nprecise task classification, offering clinicians the potential to monitor\npatient progress more accurately and tailor rehabilitation interventions more\neffectively.\n","authors":["Aaron J. Hadley","Christopher L. Pulliam"],"pdf_url":"https://arxiv.org/pdf/2406.09451v2.pdf","comment":"13 pages, 5 figures, 3 tables; resubmitted to Sensors"},{"id":"http://arxiv.org/abs/2407.04491v2","updated":"2024-10-31T13:02:36Z","published":"2024-07-05T13:29:30Z","title":"Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular\n  Data","summary":"  For classification and regression on tabular data, the dominance of\ngradient-boosted decision trees (GBDTs) has recently been challenged by often\nmuch slower deep learning methods with extensive hyperparameter tuning. We\naddress this discrepancy by introducing (a) RealMLP, an improved multilayer\nperceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and\nRealMLP. We tune RealMLP and the default parameters on a meta-train benchmark\nwith 118 datasets and compare them to hyperparameter-optimized versions on a\ndisjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly\nbenchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large\ntabular datasets (1K--500K samples) show that RealMLP offers a favorable\ntime-accuracy tradeoff compared to other neural baselines and is competitive\nwith GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and\nGBDTs with improved default parameters can achieve excellent results without\nhyperparameter tuning. Finally, we demonstrate that some of RealMLP's\nimprovements can also considerably improve the performance of TabR with default\nparameters.\n","authors":["David Holzmüller","Léo Grinsztajn","Ingo Steinwart"],"pdf_url":"https://arxiv.org/pdf/2407.04491v2.pdf","comment":"NeurIPS 2024. Changes in v2: more baselines, more ablations,\n  introduced RealTabR-D, updated Grinstzajn benchmark protocol, updated\n  text/figures. Code is available at github.com/dholzmueller/pytabkit"},{"id":"http://arxiv.org/abs/2407.15580v2","updated":"2024-10-31T12:59:49Z","published":"2024-07-22T12:16:56Z","title":"Annealed Multiple Choice Learning: Overcoming limitations of\n  Winner-takes-all with annealing","summary":"  We introduce Annealed Multiple Choice Learning (aMCL) which combines\nsimulated annealing with MCL. MCL is a learning framework handling ambiguous\ntasks by predicting a small set of plausible hypotheses. These hypotheses are\ntrained using the Winner-takes-all (WTA) scheme, which promotes the diversity\nof the predictions. However, this scheme may converge toward an arbitrarily\nsuboptimal local minimum, due to the greedy nature of WTA. We overcome this\nlimitation using annealing, which enhances the exploration of the hypothesis\nspace during training. We leverage insights from statistical physics and\ninformation theory to provide a detailed description of the model training\ntrajectory. Additionally, we validate our algorithm by extensive experiments on\nsynthetic datasets, on the standard UCI benchmark, and on speech separation.\n","authors":["David Perera","Victor Letzelter","Théo Mariotte","Adrien Cortés","Mickael Chen","Slim Essid","Gaël Richard"],"pdf_url":"https://arxiv.org/pdf/2407.15580v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.20745v2","updated":"2024-10-31T12:54:46Z","published":"2024-10-28T05:25:47Z","title":"Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large\n  Language Models","summary":"  Online shopping is a complex multi-task, few-shot learning problem with a\nwide and evolving range of entities, relations, and tasks. However, existing\nmodels and benchmarks are commonly tailored to specific tasks, falling short of\ncapturing the full complexity of online shopping. Large Language Models (LLMs),\nwith their multi-task and few-shot learning abilities, have the potential to\nprofoundly transform online shopping by alleviating task-specific engineering\nefforts and by providing users with interactive conversations. Despite the\npotential, LLMs face unique challenges in online shopping, such as\ndomain-specific concepts, implicit knowledge, and heterogeneous user behaviors.\nMotivated by the potential and challenges, we propose Shopping MMLU, a diverse\nmulti-task online shopping benchmark derived from real-world Amazon data.\nShopping MMLU consists of 57 tasks covering 4 major shopping skills: concept\nunderstanding, knowledge reasoning, user behavior alignment, and\nmulti-linguality, and can thus comprehensively evaluate the abilities of LLMs\nas general shop assistants. With Shopping MMLU, we benchmark over 20 existing\nLLMs and uncover valuable insights about practices and prospects of building\nversatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at\nhttps://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we\nhost a competition in KDD Cup 2024 with over 500 participating teams. The\nwinning solutions and the associated workshop can be accessed at our website\nhttps://amazon-kddcup24.github.io/.\n","authors":["Yilun Jin","Zheng Li","Chenwei Zhang","Tianyu Cao","Yifan Gao","Pratik Jayarao","Mao Li","Xin Liu","Ritesh Sarkhel","Xianfeng Tang","Haodong Wang","Zhengyang Wang","Wenju Xu","Jingfeng Yang","Qingyu Yin","Xian Li","Priyanka Nigam","Yi Xu","Kai Chen","Qiang Yang","Meng Jiang","Bing Yin"],"pdf_url":"https://arxiv.org/pdf/2410.20745v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track Accepted. Modified typos\n  in Figure 9"},{"id":"http://arxiv.org/abs/2410.23894v1","updated":"2024-10-31T12:53:56Z","published":"2024-10-31T12:53:56Z","title":"Metamorphic Malware Evolution: The Potential and Peril of Large Language\n  Models","summary":"  Code metamorphism refers to a computer programming exercise wherein the\nprogram modifies its own code (partial or entire) consistently and\nautomatically while retaining its core functionality. This technique is often\nused for online performance optimization and automated crash recovery in\ncertain mission-critical applications. However, the technique has been\nmisappropriated by malware creators to bypass signature-based detection\nmeasures instituted by anti-malware engines. However, current code mutation\nengines used by threat actors offer only a limited degree of mutation, which is\nfrequently detectable via static code analysis. The advent of large language\nmodels (LLMs), such as ChatGPT 4.0 and Google Bard may lead to a significant\nevolution in this landscape. These models have demonstrated a level of\nalgorithm comprehension and code synthesis capability that closely resembles\nhuman abilities. This advancement has sparked concerns among experts that such\nmodels could be exploited by threat actors to generate sophisticated\nmetamorphic malware. This paper explores the potential of several prominent\nLLMs for software code mutation that may be used to reconstruct (with mutation)\nexisting malware code bases or create new forms of embedded mutation engines\nfor next-gen metamorphic malwares. In this work, we introduce a framework for\ncreating self-testing program mutation engines based on LLM/Transformer-based\nmodels. The proposed framework serves as an essential tool in testing next-gen\nmetamorphic malware detection engines.\n","authors":["Pooria Madani"],"pdf_url":"https://arxiv.org/pdf/2410.23894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23893v1","updated":"2024-10-31T12:53:53Z","published":"2024-10-31T12:53:53Z","title":"DiffBatt: A Diffusion Model for Battery Degradation Prediction and\n  Synthesis","summary":"  Battery degradation remains a critical challenge in the pursuit of green\ntechnologies and sustainable energy solutions. Despite significant research\nefforts, predicting battery capacity loss accurately remains a formidable task\ndue to its complex nature, influenced by both aging and cycling behaviors. To\naddress this challenge, we introduce a novel general-purpose model for battery\ndegradation prediction and synthesis, DiffBatt. Leveraging an innovative\ncombination of conditional and unconditional diffusion models with\nclassifier-free guidance and transformer architecture, DiffBatt achieves high\nexpressivity and scalability. DiffBatt operates as a probabilistic model to\ncapture uncertainty in aging behaviors and a generative model to simulate\nbattery degradation. The performance of the model excels in prediction tasks\nwhile also enabling the generation of synthetic degradation curves,\nfacilitating enhanced model training by data augmentation. In the remaining\nuseful life prediction task, DiffBatt provides accurate results with a mean\nRMSE of 196 cycles across all datasets, outperforming all other models and\ndemonstrating superior generalizability. This work represents an important step\ntowards developing foundational models for battery degradation.\n","authors":["Hamidreza Eivazi","André Hebenbrock","Raphael Ginster","Steffen Blömeke","Stefan Wittek","Christoph Hermann","Thomas S. Spengler","Thomas Turek","Andreas Rausch"],"pdf_url":"https://arxiv.org/pdf/2410.23893v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2302.02910v2","updated":"2024-10-31T12:52:55Z","published":"2023-02-06T16:29:50Z","title":"An Empirical Analysis of Fairness Notions under Differential Privacy","summary":"  Recent works have shown that selecting an optimal model architecture suited\nto the differential privacy setting is necessary to achieve the best possible\nutility for a given privacy budget using differentially private stochastic\ngradient descent (DP-SGD)(Tramer and Boneh 2020; Cheng et al. 2022). In light\nof these findings, we empirically analyse how different fairness notions,\nbelonging to distinct classes of statistical fairness criteria (independence,\nseparation and sufficiency), are impacted when one selects a model architecture\nsuitable for DP-SGD, optimized for utility. Using standard datasets from ML\nfairness literature, we show using a rigorous experimental protocol, that by\nselecting the optimal model architecture for DP-SGD, the differences across\ngroups concerning the relevant fairness metrics (demographic parity, equalized\nodds and predictive parity) more often decrease or are negligibly impacted,\ncompared to the non-private baseline, for which optimal model architecture has\nalso been selected to maximize utility. These findings challenge the\nunderstanding that differential privacy will necessarily exacerbate unfairness\nin deep learning models trained on biased datasets.\n","authors":["Anderson Santana de Oliveira","Caelin Kaplan","Khawla Mallat","Tanmay Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2302.02910v2.pdf","comment":"Accepted for oral presentation at the The Fourth AAAI Workshop on\n  Privacy-Preserving Artificial Intelligence (PPAI-23)\n  https://aaai-ppai23.github.io/#accepted_papers"},{"id":"http://arxiv.org/abs/2410.23889v1","updated":"2024-10-31T12:51:40Z","published":"2024-10-31T12:51:40Z","title":"GEPS: Boosting Generalization in Parametric PDE Neural Solvers through\n  Adaptive Conditioning","summary":"  Solving parametric partial differential equations (PDEs) presents significant\nchallenges for data-driven methods due to the sensitivity of spatio-temporal\ndynamics to variations in PDE parameters. Machine learning approaches often\nstruggle to capture this variability. To address this, data-driven approaches\nlearn parametric PDEs by sampling a very large variety of trajectories with\nvarying PDE parameters. We first show that incorporating conditioning\nmechanisms for learning parametric PDEs is essential and that among them,\n$\\textit{adaptive conditioning}$, allows stronger generalization. As existing\nadaptive conditioning methods do not scale well with respect to the number of\nparameters to adapt in the neural solver, we propose GEPS, a simple adaptation\nmechanism to boost GEneralization in Pde Solvers via a first-order optimization\nand low-rank rapid adaptation of a small set of context parameters. We\ndemonstrate the versatility of our approach for both fully data-driven and for\nphysics-aware neural solvers. Validation performed on a whole range of\nspatio-temporal forecasting problems demonstrates excellent performance for\ngeneralizing to unseen conditions including initial conditions, PDE\ncoefficients, forcing terms and solution domain. $\\textit{Project page}$:\nhttps://geps-project.github.io\n","authors":["Armand Kassaï Koupaï","Jorge Misfut Benet","Yuan Yin","Jean-Noël Vittaut","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2410.23889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23884v1","updated":"2024-10-31T12:48:58Z","published":"2024-10-31T12:48:58Z","title":"Failure Modes of LLMs for Causal Reasoning on Narratives","summary":"  In this work, we investigate the causal reasoning abilities of large language\nmodels (LLMs) through the representative problem of inferring causal\nrelationships from narratives. We find that even state-of-the-art language\nmodels rely on unreliable shortcuts, both in terms of the narrative\npresentation and their parametric knowledge. For example, LLMs tend to\ndetermine causal relationships based on the topological ordering of events\n(i.e., earlier events cause later ones), resulting in lower performance\nwhenever events are not narrated in their exact causal order. Similarly, we\ndemonstrate that LLMs struggle with long-term causal reasoning and often fail\nwhen the narratives are long and contain many events. Additionally, we show\nLLMs appear to rely heavily on their parametric knowledge at the expense of\nreasoning over the provided narrative. This degrades their abilities whenever\nthe narrative opposes parametric knowledge. We extensively validate these\nfailure modes through carefully controlled synthetic experiments, as well as\nevaluations on real-world narratives. Finally, we observe that explicitly\ngenerating a causal graph generally improves performance while naive\nchain-of-thought is ineffective. Collectively, our results distill precise\nfailure modes of current state-of-the-art models and can pave the way for\nfuture techniques to enhance causal reasoning in LLMs.\n","authors":["Khurram Yamin","Shantanu Gupta","Gaurav R. Ghosal","Zachary C. Lipton","Bryan Wilder"],"pdf_url":"https://arxiv.org/pdf/2410.23884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14762v3","updated":"2024-10-31T12:46:43Z","published":"2024-05-23T16:30:51Z","title":"Neural Pfaffians: Solving Many Many-Electron Schrödinger Equations","summary":"  Neural wave functions accomplished unprecedented accuracies in approximating\nthe ground state of many-electron systems, though at a high computational cost.\nRecent works proposed amortizing the cost by learning generalized wave\nfunctions across different structures and compounds instead of solving each\nproblem independently. Enforcing the permutation antisymmetry of electrons in\nsuch generalized neural wave functions remained challenging as existing methods\nrequire discrete orbital selection via non-learnable hand-crafted algorithms.\nThis work tackles the problem by defining overparametrized, fully learnable\nneural wave functions suitable for generalization across molecules. We achieve\nthis by relying on Pfaffians rather than Slater determinants. The Pfaffian\nallows us to enforce the antisymmetry on arbitrary electronic systems without\nany constraint on electronic spin configurations or molecular structure. Our\nempirical evaluation finds that a single neural Pfaffian calculates the ground\nstate and ionization energies with chemical accuracy across various systems. On\nthe TinyMol dataset, we outperform the `gold-standard' CCSD(T) CBS reference\nenergies by 1.9m$E_h$ and reduce energy errors compared to previous generalized\nneural wave functions by up to an order of magnitude.\n","authors":["Nicholas Gao","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2405.14762v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23883v1","updated":"2024-10-31T12:45:54Z","published":"2024-10-31T12:45:54Z","title":"'No' Matters: Out-of-Distribution Detection in Multimodality Long\n  Dialogue","summary":"  Out-of-distribution (OOD) detection in multimodal contexts is essential for\nidentifying deviations in combined inputs from different modalities,\nparticularly in applications like open-domain dialogue systems or real-life\ndialogue interactions. This paper aims to improve the user experience that\ninvolves multi-round long dialogues by efficiently detecting OOD dialogues and\nimages. We introduce a novel scoring framework named Dialogue Image Aligning\nand Enhancing Framework (DIAEF) that integrates the visual language models with\nthe novel proposed scores that detect OOD in two key scenarios (1) mismatches\nbetween the dialogue and image input pair and (2) input pairs with previously\nunseen labels. Our experimental results, derived from various benchmarks,\ndemonstrate that integrating image and multi-round dialogue OOD detection is\nmore effective with previously unseen labels than using either modality\nindependently. In the presence of mismatched pairs, our proposed score\neffectively identifies these mismatches and demonstrates strong robustness in\nlong dialogues. This approach enhances domain-aware, adaptive conversational\nagents and establishes baselines for future studies.\n","authors":["Rena Gao","Xuetong Wu","Siwen Luo","Caren Han","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23883v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.15199v3","updated":"2024-10-31T12:44:50Z","published":"2024-04-23T16:35:14Z","title":"Reinforcement Learning with Adaptive Regularization for Safe Control of\n  Critical Systems","summary":"  Reinforcement Learning (RL) is a powerful method for controlling dynamic\nsystems, but its learning mechanism can lead to unpredictable actions that\nundermine the safety of critical systems. Here, we propose RL with Adaptive\nRegularization (RL-AR), an algorithm that enables safe RL exploration by\ncombining the RL policy with a policy regularizer that hard-codes the safety\nconstraints. RL-AR performs policy combination via a \"focus module,\" which\ndetermines the appropriate combination depending on the state--relying more on\nthe safe policy regularizer for less-exploited states while allowing unbiased\nconvergence for well-exploited states. In a series of critical control\napplications, we demonstrate that RL-AR not only ensures safety during training\nbut also achieves a return competitive with the standards of model-free RL that\ndisregards safety.\n","authors":["Haozhe Tian","Homayoun Hamedmoghadam","Robert Shorten","Pietro Ferraro"],"pdf_url":"https://arxiv.org/pdf/2404.15199v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23881v1","updated":"2024-10-31T12:44:07Z","published":"2024-10-31T12:44:07Z","title":"DynaSplit: A Hardware-Software Co-Design Framework for Energy-Aware\n  Inference on Edge","summary":"  The deployment of ML models on edge devices is challenged by limited\ncomputational resources and energy availability. While split computing enables\nthe decomposition of large neural networks (NNs) and allows partial computation\non both edge and cloud devices, identifying the most suitable split layer and\nhardware configurations is a non-trivial task. This process is in fact hindered\nby the large configuration space, the non-linear dependencies between software\nand hardware parameters, the heterogeneous hardware and energy characteristics,\nand the dynamic workload conditions. To overcome this challenge, we propose\nDynaSplit, a two-phase framework that dynamically configures parameters across\nboth software (i.e., split layer) and hardware (e.g., accelerator usage, CPU\nfrequency). During the Offline Phase, we solve a multi-objective optimization\nproblem with a meta-heuristic approach to discover optimal settings. During the\nOnline Phase, a scheduling algorithm identifies the most suitable settings for\nan incoming inference request and configures the system accordingly. We\nevaluate DynaSplit using popular pre-trained NNs on a real-world testbed.\nExperimental results show a reduction in energy consumption up to 72% compared\nto cloud-only computation, while meeting ~90% of user request's latency\nthreshold compared to baselines.\n","authors":["Daniel May","Alessandro Tundo","Shashikant Ilager","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2410.23881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23880v1","updated":"2024-10-31T12:40:38Z","published":"2024-10-31T12:40:38Z","title":"Directly Optimizing Explanations for Desired Properties","summary":"  When explaining black-box machine learning models, it's often important for\nexplanations to have certain desirable properties. Most existing methods\n`encourage' desirable properties in their construction of explanations. In this\nwork, we demonstrate that these forms of encouragement do not consistently\ncreate explanations with the properties that are supposedly being targeted.\nMoreover, they do not allow for any control over which properties are\nprioritized when different properties are at odds with each other. We propose\nto directly optimize explanations for desired properties. Our direct approach\nnot only produces explanations with optimal properties more consistently but\nalso empowers users to control trade-offs between different properties,\nallowing them to create explanations with exactly what is needed for a\nparticular task.\n","authors":["Hiwot Belay Tadesse","Alihan Hüyük","Weiwei Pan","Finale Doshi-Velez"],"pdf_url":"https://arxiv.org/pdf/2410.23880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03941v2","updated":"2024-10-31T12:27:30Z","published":"2024-02-06T12:18:54Z","title":"Discovery of the Hidden World with Large Language Models","summary":"  Revealing the underlying causal mechanisms in the real world is the key to\nthe development of science. Despite the progress in the past decades,\ntraditional causal discovery approaches (CDs) mainly rely on high-quality\nmeasured variables, usually given by human experts, to find causal relations.\nThe lack of well-defined high-level variables in many real-world applications\nhas already been a longstanding roadblock to a broader application of CDs. To\nthis end, this paper presents Causal representatiOn AssistanT (COAT) that\nintroduces large language models (LLMs) to bridge the gap. LLMs are trained on\nmassive observations of the world and have demonstrated great capability in\nextracting key information from unstructured data. Therefore, it is natural to\nemploy LLMs to assist with proposing useful high-level factors and crafting\ntheir measurements. Meanwhile, COAT also adopts CDs to find causal relations\namong the identified variables as well as to provide feedback to LLMs to\niteratively refine the proposed factors. We show that LLMs and CDs are mutually\nbeneficial and the constructed feedback provably also helps with the factor\nproposal. We construct and curate several synthetic and real-world benchmarks\nincluding analysis of human reviews and diagnosis of neuropathic and brain\ntumors, to comprehensively evaluate COAT. Extensive empirical results confirm\nthe effectiveness and reliability of COAT with significant improvements.\n","authors":["Chenxi Liu","Yongqiang Chen","Tongliang Liu","Mingming Gong","James Cheng","Bo Han","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.03941v2.pdf","comment":"NeurIPS 2024; Chenxi and Yongqiang contributed equally; 59 pages, 72\n  figures; Project page: https://causalcoat.github.io/"},{"id":"http://arxiv.org/abs/2404.12404v3","updated":"2024-10-31T12:27:14Z","published":"2024-04-15T17:49:16Z","title":"EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/\n","authors":["Jinhee Kim","Taesung Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2404.12404v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.06255v4","updated":"2024-10-31T12:24:14Z","published":"2024-02-09T09:09:39Z","title":"Fight Back Against Jailbreaking via Prompt Adversarial Tuning","summary":"  While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT.\n","authors":["Yichuan Mo","Yuji Wang","Zeming Wei","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.06255v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09695v2","updated":"2024-10-31T12:22:50Z","published":"2024-10-13T02:10:26Z","title":"Can In-context Learning Really Generalize to Out-of-distribution Tasks?","summary":"  In this work, we explore the mechanism of in-context learning (ICL) on\nout-of-distribution (OOD) tasks that were not encountered during training. To\nachieve this, we conduct synthetic experiments where the objective is to learn\nOOD mathematical functions through ICL using a GPT-2 model. We reveal that\nTransformers may struggle to learn OOD task functions through ICL.\nSpecifically, ICL performance resembles implementing a function within the\npretraining hypothesis space and optimizing it with gradient descent based on\nthe in-context examples. Additionally, we investigate ICL's well-documented\nability to learn unseen abstract labels in context. We demonstrate that such\nability only manifests in the scenarios without distributional shifts and,\ntherefore, may not serve as evidence of new-task-learning ability. Furthermore,\nwe assess ICL's performance on OOD tasks when the model is pretrained on\nmultiple tasks. Both empirical and theoretical analyses demonstrate the\nexistence of the \\textbf{low-test-error preference} of ICL, where it tends to\nimplement the pretraining function that yields low test error in the testing\ncontext. We validate this through numerical experiments. This new theoretical\nresult, combined with our empirical findings, elucidates the mechanism of ICL\nin addressing OOD tasks.\n","authors":["Qixun Wang","Yifei Wang","Yisen Wang","Xianghua Ying"],"pdf_url":"https://arxiv.org/pdf/2410.09695v2.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2410.23870v1","updated":"2024-10-31T12:22:19Z","published":"2024-10-31T12:22:19Z","title":"Noise as a Double-Edged Sword: Reinforcement Learning Exploits\n  Randomized Defenses in Neural Networks","summary":"  This study investigates a counterintuitive phenomenon in adversarial machine\nlearning: the potential for noise-based defenses to inadvertently aid evasion\nattacks in certain scenarios. While randomness is often employed as a defensive\nstrategy against adversarial examples, our research reveals that this approach\ncan sometimes backfire, particularly when facing adaptive attackers using\nreinforcement learning (RL). Our findings show that in specific cases,\nespecially with visually noisy classes, the introduction of noise in the\nclassifier's confidence values can be exploited by the RL attacker, leading to\na significant increase in evasion success rates. In some instances, the\nnoise-based defense scenario outperformed other strategies by up to 20\\% on a\nsubset of classes. However, this effect was not consistent across all\nclassifiers tested, highlighting the complexity of the interaction between\nnoise-based defenses and different models. These results suggest that in some\ncases, noise-based defenses can inadvertently create an adversarial training\nloop beneficial to the RL attacker. Our study emphasizes the need for a more\nnuanced approach to defensive strategies in adversarial machine learning,\nparticularly in safety-critical applications. It challenges the assumption that\nrandomness universally enhances defense against evasion attacks and highlights\nthe importance of considering adaptive, RL-based attackers when designing\nrobust defense mechanisms.\n","authors":["Steve Bakos","Pooria Madani","Heidar Davoudi"],"pdf_url":"https://arxiv.org/pdf/2410.23870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16965v4","updated":"2024-10-31T12:21:32Z","published":"2023-09-29T04:23:58Z","title":"Controlling Continuous Relaxation for Combinatorial Optimization","summary":"  Unsupervised learning (UL)-based solvers for combinatorial optimization (CO)\ntrain a neural network that generates a soft solution by directly optimizing\nthe CO objective using a continuous relaxation strategy. These solvers offer\nseveral advantages over traditional methods and other learning-based methods,\nparticularly for large-scale CO problems. However, UL-based solvers face two\npractical issues: (I) an optimization issue, where UL-based solvers are easily\ntrapped at local optima, and (II) a rounding issue, where UL-based solvers\nrequire artificial post-learning rounding from the continuous space back to the\noriginal discrete space, undermining the robustness of the results. This study\nproposes a Continuous Relaxation Annealing (CRA) strategy, an effective\nrounding-free learning method for UL-based solvers. CRA introduces a penalty\nterm that dynamically shifts from prioritizing continuous solutions,\neffectively smoothing the non-convexity of the objective function, to enforcing\ndiscreteness, eliminating artificial rounding. Experimental results demonstrate\nthat CRA significantly enhances the performance of UL-based solvers,\noutperforming existing UL-based solvers and greedy algorithms in complex CO\nproblems. Additionally, CRA effectively eliminates artificial rounding and\naccelerates the learning process.\n","authors":["Yuma Ichikawa"],"pdf_url":"https://arxiv.org/pdf/2309.16965v4.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.23867v1","updated":"2024-10-31T12:20:36Z","published":"2024-10-31T12:20:36Z","title":"QuACK: A Multipurpose Queuing Algorithm for Cooperative $k$-Armed\n  Bandits","summary":"  We study the cooperative stochastic $k$-armed bandit problem, where a network\nof $m$ agents collaborate to find the optimal action. In contrast to most prior\nwork on this problem, which focuses on extending a specific algorithm to the\nmulti-agent setting, we provide a black-box reduction that allows us to extend\nany single-agent bandit algorithm to the multi-agent setting. Under mild\nassumptions on the bandit environment, we prove that our reduction transfers\nthe regret guarantees of the single-agent algorithm to the multi-agent setting.\nThese guarantees are tight in subgaussian environments, in that using a near\nminimax optimal single-player algorithm is near minimax optimal in the\nmulti-player setting up to an additive graph-dependent quantity. Our reduction\nand theoretical results are also general, and apply to many different bandit\nsettings. By plugging in appropriate single-player algorithms, we can easily\ndevelop provably efficient algorithms for many multi-player settings such as\nheavy-tailed bandits, duelling bandits and bandits with local differential\nprivacy, among others. Experimentally, our approach is competitive with or\noutperforms specialised multi-agent algorithms.\n","authors":["Benjamin Howson","Sarah Filippi","Ciara Pike-Burke"],"pdf_url":"https://arxiv.org/pdf/2410.23867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16173v2","updated":"2024-10-31T12:20:16Z","published":"2024-05-25T10:45:46Z","title":"Diffusion-based Reinforcement Learning via Q-weighted Variational Policy\n  Optimization","summary":"  Diffusion models have garnered widespread attention in Reinforcement Learning\n(RL) for their powerful expressiveness and multimodality. It has been verified\nthat utilizing diffusion policies can significantly improve the performance of\nRL algorithms in continuous control tasks by overcoming the limitations of\nunimodal policies, such as Gaussian policies, and providing the agent with\nenhanced exploration capabilities. However, existing works mainly focus on the\napplication of diffusion policies in offline RL, while their incorporation into\nonline RL is less investigated. The training objective of the diffusion model,\nknown as the variational lower bound, cannot be optimized directly in online RL\ndue to the unavailability of 'good' actions. This leads to difficulties in\nconducting diffusion policy improvement. To overcome this, we propose a novel\nmodel-free diffusion-based online RL algorithm, Q-weighted Variational Policy\nOptimization (QVPO). Specifically, we introduce the Q-weighted variational\nloss, which can be proved to be a tight lower bound of the policy objective in\nonline RL under certain conditions. To fulfill these conditions, the Q-weight\ntransformation functions are introduced for general scenarios. Additionally, to\nfurther enhance the exploration capability of the diffusion policy, we design a\nspecial entropy regularization term. We also develop an efficient behavior\npolicy to enhance sample efficiency by reducing the variance of the diffusion\npolicy during online interactions. Consequently, the QVPO algorithm leverages\nthe exploration capabilities and multimodality of diffusion policies,\npreventing the RL agent from converging to a sub-optimal policy. To verify the\neffectiveness of QVPO, we conduct comprehensive experiments on MuJoCo\nbenchmarks. The final results demonstrate that QVPO achieves state-of-the-art\nperformance on both cumulative reward and sample efficiency.\n","authors":["Shutong Ding","Ke Hu","Zhenhao Zhang","Kan Ren","Weinan Zhang","Jingyi Yu","Jingya Wang","Ye Shi"],"pdf_url":"https://arxiv.org/pdf/2405.16173v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2312.10112v3","updated":"2024-10-31T12:19:37Z","published":"2023-12-15T09:09:25Z","title":"NM-FlowGAN: Modeling sRGB Noise without Paired Images using a Hybrid\n  Approach of Normalizing Flows and GAN","summary":"  Modeling and synthesizing real sRGB noise is crucial for various low-level\nvision tasks, such as building datasets for training image denoising systems.\nThe distribution of real sRGB noise is highly complex and affected by a\nmultitude of factors, making its accurate modeling extremely challenging.\nTherefore, recent studies have proposed methods that employ data-driven\ngenerative models, such as Generative Adversarial Networks (GAN) and\nNormalizing Flows. These studies achieve more accurate modeling of sRGB noise\ncompared to traditional noise modeling methods. However, there are performance\nlimitations due to the inherent characteristics of each generative model. To\naddress this issue, we propose NM-FlowGAN, a hybrid approach that exploits the\nstrengths of both GAN and Normalizing Flows. We combine pixel-wise noise\nmodeling networks based on Normalizing Flows and spatial correlation modeling\nnetworks based on GAN. Specifically, the pixel-wise noise modeling network\nleverages the high training stability of Normalizing Flows to capture noise\ncharacteristics that are affected by a multitude of factors, and the spatial\ncorrelation networks efficiently model pixel-to-pixel relationships. In\nparticular, unlike recent methods that rely on paired noisy images, our method\nsynthesizes noise using clean images and factors that affect noise\ncharacteristics, such as easily obtainable parameters like camera type and ISO\nsettings, making it applicable to various fields where obtaining noisy-clean\nimage pairs is not feasible. In our experiments, our NM-FlowGAN outperforms\nother baselines in the sRGB noise synthesis task. Moreover, the denoising\nneural network trained with synthesized image pairs from our model shows\nsuperior performance compared to other baselines. Our code is available at:\n\\url{https://github.com/YoungJooHan/NM-FlowGAN}.\n","authors":["Young Joo Han","Ha-Jin Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10112v3.pdf","comment":"13 pages, 10 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.23862v1","updated":"2024-10-31T12:13:11Z","published":"2024-10-31T12:13:11Z","title":"$ψ$DAG: Projected Stochastic Approximation Iteration for DAG\n  Structure Learning","summary":"  Learning the structure of Directed Acyclic Graphs (DAGs) presents a\nsignificant challenge due to the vast combinatorial search space of possible\ngraphs, which scales exponentially with the number of nodes. Recent\nadvancements have redefined this problem as a continuous optimization task by\nincorporating differentiable acyclicity constraints. These methods commonly\nrely on algebraic characterizations of DAGs, such as matrix exponentials, to\nenable the use of gradient-based optimization techniques. Despite these\ninnovations, existing methods often face optimization difficulties due to the\nhighly non-convex nature of DAG constraints and the per-iteration computational\ncomplexity. In this work, we present a novel framework for learning DAGs,\nemploying a Stochastic Approximation approach integrated with Stochastic\nGradient Descent (SGD)-based optimization techniques. Our framework introduces\nnew projection methods tailored to efficiently enforce DAG constraints,\nensuring that the algorithm converges to a feasible local minimum. With its low\niteration complexity, the proposed method is well-suited for handling\nlarge-scale problems with improved computational efficiency. We demonstrate the\neffectiveness and scalability of our framework through comprehensive\nexperimental evaluations, which confirm its superior performance across various\nsettings.\n","authors":["Klea Ziu","Slavomír Hanzely","Loka Li","Kun Zhang","Martin Takáč","Dmitry Kamzolov"],"pdf_url":"https://arxiv.org/pdf/2410.23862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11512v3","updated":"2024-10-31T12:10:30Z","published":"2023-05-19T08:22:23Z","title":"Enriching Disentanglement: From Logical Definitions to Quantitative\n  Metrics","summary":"  Disentangling the explanatory factors in complex data is a promising approach\nfor generalizable and data-efficient representation learning. While a variety\nof quantitative metrics for learning and evaluating disentangled\nrepresentations have been proposed, it remains unclear what properties these\nmetrics truly quantify. In this work, we establish algebraic relationships\nbetween logical definitions and quantitative metrics to derive theoretically\ngrounded disentanglement metrics. Concretely, we introduce a compositional\napproach for converting a higher-order predicate into a real-valued quantity by\nreplacing (i) equality with a strict premetric, (ii) the Heyting algebra of\nbinary truth values with a quantale of continuous values, and (iii) quantifiers\nwith aggregators. The metrics induced by logical definitions have strong\ntheoretical guarantees, and some of them are easily differentiable and can be\nused as learning objectives directly. Finally, we empirically demonstrate the\neffectiveness of the proposed metrics by isolating different aspects of\ndisentangled representations.\n","authors":["Yivan Zhang","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2305.11512v3.pdf","comment":"Neural Information Processing Systems 2024"},{"id":"http://arxiv.org/abs/2410.23858v1","updated":"2024-10-31T12:07:52Z","published":"2024-10-31T12:07:52Z","title":"Neural Network Matrix Product Operator: A Multi-Dimensionally Integrable\n  Machine Learning Potential","summary":"  A neural network-based machine learning potential energy surface (PES)\nexpressed in a matrix product operator (NN-MPO) is proposed. The MPO form\nenables efficient evaluation of high-dimensional integrals that arise in\nsolving the time-dependent and time-independent Schr\\\"odinger equation and\neffectively overcomes the so-called curse of dimensionality. This starkly\ncontrasts with other neural network-based machine learning PES methods, such as\nmulti-layer perceptrons (MLPs), where evaluating high-dimensional integrals is\nnot straightforward due to the fully connected topology in their backbone\narchitecture. Nevertheless, the NN-MPO retains the high representational\ncapacity of neural networks. NN-MPO can achieve spectroscopic accuracy with a\ntest mean absolute error (MAE) of 3.03 cm$^{-1}$ for a fully coupled\nsix-dimensional ab initio PES, using only 625 training points distributed\nacross a 0 to 17,000 cm$^{-1}$ energy range. Our Python implementation is\navailable at https://github.com/KenHino/Pompon.\n","authors":["Kentaro Hino","Yuki Kurashige"],"pdf_url":"https://arxiv.org/pdf/2410.23858v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.23856v1","updated":"2024-10-31T12:07:44Z","published":"2024-10-31T12:07:44Z","title":"Can Language Models Perform Robust Reasoning in Chain-of-thought\n  Prompting with Noisy Rationales?","summary":"  This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales.\n","authors":["Zhanke Zhou","Rong Tao","Jianing Zhu","Yiwen Luo","Zengmao Wang","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2410.23856v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23855v1","updated":"2024-10-31T12:05:21Z","published":"2024-10-31T12:05:21Z","title":"RAGraph: A General Retrieval-Augmented Graph Learning Framework","summary":"  Graph Neural Networks (GNNs) have become essential in interpreting relational\ndata across various domains, yet, they often struggle to generalize to unseen\ngraph data that differs markedly from training instances. In this paper, we\nintroduce a novel framework called General Retrieval-Augmented Graph Learning\n(RAGraph), which brings external graph data into the general graph foundation\nmodel to improve model generalization on unseen scenarios. On the top of our\nframework is a toy graph vector library that we established, which captures key\nattributes, such as features and task-specific label information. During\ninference, the RAGraph adeptly retrieves similar toy graphs based on key\nsimilarities in downstream tasks, integrating the retrieved data to enrich the\nlearning context via the message-passing prompting mechanism. Our extensive\nexperimental evaluations demonstrate that RAGraph significantly outperforms\nstate-of-the-art graph learning methods in multiple tasks such as node\nclassification, link prediction, and graph classification across both dynamic\nand static datasets. Furthermore, extensive testing confirms that RAGraph\nconsistently maintains high performance without the need for task-specific\nfine-tuning, highlighting its adaptability, robustness, and broad\napplicability.\n","authors":["Xinke Jiang","Rihong Qiu","Yongxin Xu","Wentao Zhang","Yichen Zhu","Ruizhe Zhang","Yuchen Fang","Xu Chu","Junfeng Zhao","Yasha Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23855v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23854v1","updated":"2024-10-31T12:04:30Z","published":"2024-10-31T12:04:30Z","title":"Airway Labeling Meets Clinical Applications: Reflecting Topology\n  Consistency and Outliers via Learnable Attentions","summary":"  Accurate airway anatomical labeling is crucial for clinicians to identify and\nnavigate complex bronchial structures during bronchoscopy. Automatic airway\nanatomical labeling is challenging due to significant individual variability\nand anatomical variations. Previous methods are prone to generate inconsistent\npredictions, which is harmful for preoperative planning and intraoperative\nnavigation. This paper aims to address these challenges by proposing a novel\nmethod that enhances topological consistency and improves the detection of\nabnormal airway branches.\n  We propose a novel approach incorporating two modules: the Soft Subtree\nConsistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module\nconstructs a soft subtree to capture clinically relevant topological\nrelationships, allowing for flexible feature aggregation within and across\nsubtrees. The ABS module facilitates the interaction between node features and\nprototypes to distinguish abnormal branches, preventing the erroneous\naggregation of features between normal and abnormal nodes.\n  Evaluated on a challenging dataset characterized by severe airway distortion\nand atrophy, our method achieves superior performance compared to\nstate-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the\nsegmental level and an 83.7% accuracy at the subsegmental level, representing a\n1.4% increase in subsegmental accuracy and a 3.1% increase in topological\nconsistency. Notably, the method demonstrates reliable performance in cases\nwith disease-induced airway deformities, ensuring consistent and accurate\nlabeling.\n","authors":["Chenyu Li","Minghui Zhang","Chuyan Zhang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2410.23854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05368v3","updated":"2024-10-31T12:04:12Z","published":"2023-05-09T12:03:42Z","title":"Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptive\n  Residual Module","summary":"  Graph Neural Networks (GNNs), a type of neural network that can learn from\ngraph-structured data through neighborhood information aggregation, have shown\nsuperior performance in various downstream tasks. However, as the number of\nlayers increases, node representations become indistinguishable, which is known\nas over-smoothing. To address this issue, many residual methods have emerged.\nIn this paper, we focus on the over-smoothing issue and related residual\nmethods. Firstly, we revisit over-smoothing from the perspective of overlapping\nneighborhood subgraphs, and based on this, we explain how residual methods can\nalleviate over-smoothing by integrating multiple orders neighborhood subgraphs\nto avoid the indistinguishability of the single high-order neighborhood\nsubgraphs. Additionally, we reveal the drawbacks of previous residual methods,\nsuch as the lack of node adaptability and severe loss of high-order\nneighborhood subgraph information, and propose a\n\\textbf{Posterior-Sampling-based, Node-Adaptive Residual module (PSNR)}. We\ntheoretically demonstrate that PSNR can alleviate the drawbacks of previous\nresidual methods. Furthermore, extensive experiments verify the superiority of\nthe PSNR module in fully observed node classification and missing feature\nscenarios. Our code is available at https://github.com/jingbo02/PSNR-GNN.\n","authors":["Jingbo Zhou","Yixuan Du","Ruqiong Zhang","Jun Xia","Zhizhi Yu","Zelin Zang","Di Jin","Carl Yang","Rui Zhang","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2305.05368v3.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.23846v1","updated":"2024-10-31T11:55:30Z","published":"2024-10-31T11:55:30Z","title":"Case ID detection based on time series data -- the mining use case","summary":"  Process mining gains increasing popularity in business process analysis, also\nin heavy industry. It requires a specific data format called an event log, with\nthe basic structure including a case identifier (case ID), activity (event)\nname, and timestamp. In the case of industrial processes, data is very often\nprovided by a monitoring system as time series of low level sensor readings.\nThis data cannot be directly used for process mining since there is no explicit\nmarking of activities in the event log, and sometimes, case ID is not provided.\nWe propose a novel rule-based algorithm for identification patterns, based on\nthe identification of significant changes in short-term mean values of selected\nvariable to detect case ID. We present our solution on the mining use case. We\ncompare computed results (identified patterns) with expert labels of the same\ndataset. Experiments show that the developed algorithm in the most of the cases\ncorrectly detects IDs in datasets with and without outliers reaching F1 score\nvalues: 96.8% and 97% respectively. We also evaluate our algorithm on dataset\nfrom manufacturing domain reaching value 92.6% for F1 score.\n","authors":["Edyta Brzychczy","Tomasz Pełech-Pilichowski","Ziemowit Dworakowski"],"pdf_url":"https://arxiv.org/pdf/2410.23846v1.pdf","comment":"Presented at EdbA'24 - Fifth International Workshop on Event Data and\n  Behavioral Analytics, ICPM 2024, Kopenhagen, Denmark"},{"id":"http://arxiv.org/abs/2402.08406v3","updated":"2024-10-31T11:47:25Z","published":"2024-02-13T12:11:40Z","title":"Transition Constrained Bayesian Optimization via Markov Decision\n  Processes","summary":"  Bayesian optimization is a methodology to optimize black-box functions.\nTraditionally, it focuses on the setting where you can arbitrarily query the\nsearch space. However, many real-life problems do not offer this flexibility;\nin particular, the search space of the next query may depend on previous ones.\nExample challenges arise in the physical sciences in the form of local movement\nconstraints, required monotonicity in certain variables, and transitions\ninfluencing the accuracy of measurements. Altogether, such transition\nconstraints necessitate a form of planning. This work extends classical\nBayesian optimization via the framework of Markov Decision Processes. We\niteratively solve a tractable linearization of our utility function using\nreinforcement learning to obtain a policy that plans ahead for the entire\nhorizon. This is a parallel to the optimization of an acquisition function in\npolicy space. The resulting policy is potentially history-dependent and\nnon-Markovian. We showcase applications in chemical reactor optimization,\ninformative path planning, machine calibration, and other synthetic examples.\n","authors":["Jose Pablo Folch","Calvin Tsay","Robert M Lee","Behrang Shafei","Weronika Ormaniec","Andreas Krause","Mark van der Wilk","Ruth Misener","Mojmír Mutný"],"pdf_url":"https://arxiv.org/pdf/2402.08406v3.pdf","comment":"10 pages main, 34 pages total, 17 figures, 2 tables. Accepted to\n  NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23840v1","updated":"2024-10-31T11:46:48Z","published":"2024-10-31T11:46:48Z","title":"Deterministic Exploration via Stationary Bellman Error Maximization","summary":"  Exploration is a crucial and distinctive aspect of reinforcement learning\n(RL) that remains a fundamental open problem. Several methods have been\nproposed to tackle this challenge. Commonly used methods inject random noise\ndirectly into the actions, indirectly via entropy maximization, or add\nintrinsic rewards that encourage the agent to steer to novel regions of the\nstate space. Another previously seen idea is to use the Bellman error as a\nseparate optimization objective for exploration. In this paper, we introduce\nthree modifications to stabilize the latter and arrive at a deterministic\nexploration policy. Our separate exploration agent is informed about the state\nof the exploitation, thus enabling it to account for previous experiences.\nFurther components are introduced to make the exploration objective agnostic\ntoward the episode length and to mitigate instability introduced by\nfar-off-policy learning. Our experimental results show that our approach can\noutperform $\\varepsilon$-greedy in dense and sparse reward settings.\n","authors":["Sebastian Griesbach","Carlo D'Eramo"],"pdf_url":"https://arxiv.org/pdf/2410.23840v1.pdf","comment":"Published at the 17th European Workshop for Reinforcement Learning"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.23883v1","updated":"2024-10-31T12:45:54Z","published":"2024-10-31T12:45:54Z","title":"'No' Matters: Out-of-Distribution Detection in Multimodality Long\n  Dialogue","summary":"  Out-of-distribution (OOD) detection in multimodal contexts is essential for\nidentifying deviations in combined inputs from different modalities,\nparticularly in applications like open-domain dialogue systems or real-life\ndialogue interactions. This paper aims to improve the user experience that\ninvolves multi-round long dialogues by efficiently detecting OOD dialogues and\nimages. We introduce a novel scoring framework named Dialogue Image Aligning\nand Enhancing Framework (DIAEF) that integrates the visual language models with\nthe novel proposed scores that detect OOD in two key scenarios (1) mismatches\nbetween the dialogue and image input pair and (2) input pairs with previously\nunseen labels. Our experimental results, derived from various benchmarks,\ndemonstrate that integrating image and multi-round dialogue OOD detection is\nmore effective with previously unseen labels than using either modality\nindependently. In the presence of mismatched pairs, our proposed score\neffectively identifies these mismatches and demonstrates strong robustness in\nlong dialogues. This approach enhances domain-aware, adaptive conversational\nagents and establishes baselines for future studies.\n","authors":["Rena Gao","Xuetong Wu","Siwen Luo","Caren Han","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23883v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.23861v1","updated":"2024-10-31T12:11:17Z","published":"2024-10-31T12:11:17Z","title":"Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models","summary":"  Large Multimodal Models (LMMs) have demonstrated the ability to interact with\nhumans under real-world conditions by combining Large Language Models (LLMs)\nand modality encoders to align multimodal information (visual and auditory)\nwith text. However, such models raise new safety challenges of whether models\nthat are safety-aligned on text also exhibit consistent safeguards for\nmultimodal inputs. Despite recent safety-alignment research on vision LMMs, the\nsafety of audio LMMs remains under-explored. In this work, we comprehensively\nred team the safety of five advanced audio LMMs under three settings: (i)\nharmful questions in both audio and text formats, (ii) harmful questions in\ntext format accompanied by distracting non-speech audio, and (iii)\nspeech-specific jailbreaks. Our results under these settings demonstrate that\nopen-source audio LMMs suffer an average attack success rate of 69.14% on\nharmful audio questions, and exhibit safety vulnerabilities when distracted\nwith non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro\nachieve an attack success rate of 70.67% on the harmful query benchmark. We\nprovide insights on what could cause these reported safety-misalignments.\nWarning: this paper contains offensive examples.\n","authors":["Hao Yang","Lizhen Qu","Ehsan Shareghi","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2410.23861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23663v1","updated":"2024-10-31T06:26:00Z","published":"2024-10-31T06:26:00Z","title":"DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake\n  Detection","summary":"  With the advancement of deepfake generation techniques, the importance of\ndeepfake detection in protecting multimedia content integrity has become\nincreasingly obvious. Recently, temporal inconsistency clues have been explored\nto improve the generalizability of deepfake video detection. According to our\nobservation, the temporal artifacts of forged videos in terms of motion\ninformation usually exhibits quite distinct inconsistency patterns along\nhorizontal and vertical directions, which could be leveraged to improve the\ngeneralizability of detectors. In this paper, a transformer-based framework for\nDiffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits\ndirectional inconsistencies for deepfake video detection. Specifically, DIP\nbegins with a spatiotemporal encoder to represent spatiotemporal information. A\ndirectional inconsistency decoder is adopted accordingly, where direction-aware\nattention and inconsistency diffusion are incorporated to explore potential\ninconsistency patterns and jointly learn the inherent relationships. In\naddition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to\ncontrast spatiotemporally augmented sample pairs and prevent the model from\noverfitting nonessential forgery artifacts. Extensive experiments on several\npublic datasets demonstrate that our method could effectively identify\ndirectional forgery clues and achieve state-of-the-art performance.\n","authors":["Fan Nie","Jiangqun Ni","Jian Zhang","Bin Zhang","Weizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.23663v1.pdf","comment":"13 pages, accepted with IEEE Trans. on Multimedia"},{"id":"http://arxiv.org/abs/2410.23230v2","updated":"2024-10-31T04:20:22Z","published":"2024-10-30T17:18:53Z","title":"Aligning Audio-Visual Joint Representations with an Agentic Workflow","summary":"  Visual content and accompanied audio signals naturally formulate a joint\nrepresentation to improve audio-visual (AV) related applications. While studies\ndevelop various AV representation learning frameworks, the importance of AV\ndata alignment is usually undermined for achieving high-quality representation.\nWe observe that an audio signal may contain background noise interference.\nAlso, non-synchronization may appear between audio and video streams. These\nnon-strict data alignment limits representation quality and downgrade\napplication performance. In this paper, we propose to improve AV joint\nrepresentations from a data-centric perspective by aligning audio signals to\nvisual data. Our alignment is conducted in an agentic workflow controlled by an\nLLM-based assistant named AVAgent. For each input AV data pair, our AVAgent\nuses a multi-modal LLM to convert audio and visual data into language\ndescriptions separately (i.e., tool use). Then, AVAgent reasons whether this\npaired data is aligned well and plans to edit the audio signal if needed (i.e.,\nplanning). The audio editing is executed by predefined actions that filter\nnoise or augment data. Moreover, we use a VLM to evaluate how modified audio\nsignals match the visual content and provide feedback to AVAgent (i.e.,\nreflection). The tool use, planning, and reflection steps operate cyclically to\nbecome an agentic workflow where audio signals are gradually aligned to visual\ncontent. To this end, existing methods can directly leverage the aligned AV\ndata via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art\nperformance of the proposed approach against previous baselines in diverse\ndownstream tasks.\n","authors":["Shentong Mo","Yibing Song"],"pdf_url":"https://arxiv.org/pdf/2410.23230v2.pdf","comment":null}]},"2024-10-30T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.23514v1","updated":"2024-10-30T23:51:01Z","published":"2024-10-30T23:51:01Z","title":"Neural spell-checker: Beyond words with synthetic data generation","summary":"  Spell-checkers are valuable tools that enhance communication by identifying\nmisspelled words in written texts. Recent improvements in deep learning, and in\nparticular in large language models, have opened new opportunities to improve\ntraditional spell-checkers with new functionalities that not only assess\nspelling correctness but also the suitability of a word for a given context. In\nour work, we present and compare two new spell-checkers and evaluate them on\nsynthetic, learner, and more general-domain Slovene datasets. The first\nspell-checker is a traditional, fast, word-based approach, based on a\nmorphological lexicon with a significantly larger word list compared to\nexisting spell-checkers. The second approach uses a language model trained on a\nlarge corpus with synthetically inserted errors. We present the training data\nconstruction strategies, which turn out to be a crucial component of neural\nspell-checkers. Further, the proposed neural model significantly outperforms\nall existing spell-checkers for Slovene in both precision and recall.\n","authors":["Matej Klemen","Martin Božič","Špela Arhar Holdt","Marko Robnik-Šikonja"],"pdf_url":"https://arxiv.org/pdf/2410.23514v1.pdf","comment":"Camera-ready version. Accepted to TSD 2024"},{"id":"http://arxiv.org/abs/2407.05952v2","updated":"2024-10-30T23:44:31Z","published":"2024-06-29T21:24:19Z","title":"H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables","summary":"  Tabular reasoning involves interpreting natural language queries about\ntabular data, which presents a unique challenge of combining language\nunderstanding with structured data analysis. Existing methods employ either\ntextual reasoning, which excels in semantic interpretation but struggles with\nmathematical operations, or symbolic reasoning, which handles computations well\nbut lacks semantic understanding. This paper introduces a novel algorithm\nH-STAR that integrates both symbolic and semantic (textual) approaches in a\ntwo-stage process to address these limitations. H-STAR employs: (1) step-wise\ntable extraction using `multi-view' column retrieval followed by row\nextraction, and (2) adaptive reasoning that adapts reasoning strategies based\non question types, utilizing semantic reasoning for direct lookup and complex\nlexical queries while augmenting textual reasoning with symbolic reasoning\nsupport for quantitative and logical tasks. Our extensive experiments\ndemonstrate that H-STAR significantly outperforms state-of-the-art methods\nacross three tabular question-answering (QA) and fact-verification datasets,\nunderscoring its effectiveness and efficiency.\n","authors":["Nikhil Abhyankar","Vivek Gupta","Dan Roth","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2407.05952v2.pdf","comment":"22 pages, 16 tables, 21 figures"},{"id":"http://arxiv.org/abs/2410.23511v1","updated":"2024-10-30T23:35:21Z","published":"2024-10-30T23:35:21Z","title":"Dynamic Strategy Planning for Efficient Question Answering with Large\n  Language Models","summary":"  Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought),\nplanning (e.g., SelfAsk), and retrieval augmented generation strategies to\nimprove the performance of Large Language Models (LLMs) on various tasks, such\nas question answering. However, using a single fixed strategy to answer\ndifferent kinds of questions is suboptimal in performance and inefficient in\nterms of generated output tokens and performed retrievals. In our work, we\npropose a novel technique DyPlan, to induce a dynamic strategy selection\nprocess in LLMs, to improve performance and reduce costs in question-answering.\nDyPlan incorporates an initial decision step to select the most suitable\nstrategy conditioned on the input question and guides the LLM's response\ngeneration accordingly. We extend DyPlan to DyPlan-verify, adding an internal\nverification and correction process to further enrich the generated answer.\nExperiments on three prominent multi-hop question answering (MHQA) datasets\nreveal how DyPlan can improve model performance by 7-13% while reducing the\ncost by 11-32% relative to the best baseline model.\n","authors":["Tanmay Parekh","Pradyot Prakash","Alexander Radovic","Akshay Shekher","Denis Savenkov"],"pdf_url":"https://arxiv.org/pdf/2410.23511v1.pdf","comment":"Under review at ACL Rolling Review"},{"id":"http://arxiv.org/abs/2410.23510v1","updated":"2024-10-30T23:34:45Z","published":"2024-10-30T23:34:45Z","title":"Tiny Transformers Excel at Sentence Compression","summary":"  It is staggering that words of the English language, which are on average\nrepresented by 5--6 bytes of ASCII, require as much as 24 kilobytes when served\nto large language models. We show that there is room for more information in\nevery token embedding. We demonstrate that 1--3-layer transformers are capable\nof encoding and subsequently decoding standard English sentences into as little\nas a single 3-kilobyte token. Our work implies that even small networks can\nlearn to construct valid English sentences and suggests the possibility of\noptimising large language models by moving from sub-word token embeddings\ntowards larger fragments of text.\n","authors":["Peter Belcak","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2410.23510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23507v1","updated":"2024-10-30T23:27:54Z","published":"2024-10-30T23:27:54Z","title":"Efficient and Interpretable Grammatical Error Correction with Mixture of\n  Experts","summary":"  Error type information has been widely used to improve the performance of\ngrammatical error correction (GEC) models, whether for generating corrections,\nre-ranking them, or combining GEC models. Combining GEC models that have\ncomplementary strengths in correcting different error types is very effective\nin producing better corrections. However, system combination incurs a high\ncomputational cost due to the need to run inference on the base systems before\nrunning the combination method itself. Therefore, it would be more efficient to\nhave a single model with multiple sub-networks that specialize in correcting\ndifferent error types. In this paper, we propose a mixture-of-experts model,\nMoECE, for grammatical error correction. Our model successfully achieves the\nperformance of T5-XL with three times fewer effective parameters. Additionally,\nour model produces interpretable corrections by also identifying the error type\nduring inference.\n","authors":["Muhammad Reza Qorib","Alham Fikri Aji","Hwee Tou Ng"],"pdf_url":"https://arxiv.org/pdf/2410.23507v1.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.23506v1","updated":"2024-10-30T23:26:06Z","published":"2024-10-30T23:26:06Z","title":"Learning to Achieve Goals with Belief State Transformers","summary":"  We introduce the \"Belief State Transformer\", a next-token predictor that\ntakes both a prefix and suffix as inputs, with a novel objective of predicting\nboth the next token for the prefix and the previous token for the suffix. The\nBelief State Transformer effectively learns to solve challenging problems that\nconventional forward-only transformers struggle with, in a domain-independent\nfashion. Key to this success is learning a compact belief state that captures\nall relevant information necessary for accurate predictions. Empirical\nablations show that each component of the model is essential in difficult\nscenarios where standard Transformers fall short. For the task of story writing\nwith known prefixes and suffixes, our approach outperforms the\nFill-in-the-Middle method for reaching known goals and demonstrates improved\nperformance even when the goals are unknown. Altogether, the Belief State\nTransformer enables more efficient goal-conditioned decoding, better test-time\ninference, and high-quality text representations on small scale problems.\n","authors":["Edward S. Hu","Kwangjun Ahn","Qinghua Liu","Haoran Xu","Manan Tomar","Ada Langford","Dinesh Jayaraman","Alex Lamb","John Langford"],"pdf_url":"https://arxiv.org/pdf/2410.23506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23501v1","updated":"2024-10-30T23:19:29Z","published":"2024-10-30T23:19:29Z","title":"All or None: Identifiable Linear Properties of Next-token Predictors in\n  Language Modeling","summary":"  We analyze identifiability as a possible explanation for the ubiquity of\nlinear properties across language models, such as the vector difference between\nthe representations of \"easy\" and \"easiest\" being parallel to that between\n\"lucky\" and \"luckiest\". For this, we ask whether finding a linear property in\none model implies that any model that induces the same distribution has that\nproperty, too. To answer that, we first prove an identifiability result to\ncharacterize distribution-equivalent next-token predictors, lifting a diversity\nrequirement of previous results. Second, based on a refinement of relational\nlinearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how\nmany notions of linearity are amenable to our analysis. Finally, we show that\nunder suitable conditions, these linear properties either hold in all or none\ndistribution-equivalent next-token predictors.\n","authors":["Emanuele Marconato","Sébastien Lachapelle","Sebastian Weichwald","Luigi Gresele"],"pdf_url":"https://arxiv.org/pdf/2410.23501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23496v1","updated":"2024-10-30T22:58:57Z","published":"2024-10-30T22:58:57Z","title":"Smaller Large Language Models Can Do Moral Self-Correction","summary":"  Self-correction is one of the most amazing emerging capabilities of Large\nLanguage Models (LLMs), enabling LLMs to self-modify an inappropriate output\ngiven a natural language feedback which describes the problems of that output.\nMoral self-correction is a post-hoc approach correcting unethical generations\nwithout requiring a gradient update, making it both computationally lightweight\nand capable of preserving the language modeling ability. Previous works have\nshown that LLMs can self-debias, and it has been reported that small models,\ni.e., those with less than 22B parameters, are not capable of moral\nself-correction. However, there is no direct proof as to why such smaller\nmodels fall short of moral self-correction, though previous research\nhypothesizes that larger models are skilled in following instructions and\nunderstanding abstract social norms. In this paper, we empirically validate\nthis hypothesis in the context of social stereotyping, through meticulous\nprompting. Our experimental results indicate that (i) surprisingly, 3.8B LLMs\nwith proper safety alignment fine-tuning can achieve very good moral\nself-correction performance, highlighting the significant effects of safety\nalignment; and (ii) small LLMs are indeed weaker than larger-scale models in\nterms of comprehending social norms and self-explanation through CoT, but all\nscales of LLMs show bad self-correction performance given unethical\ninstructions.\n","authors":["Guangliang Liu","Zhiyu Xue","Rongrong Wang","Kristen Marie Johnson"],"pdf_url":"https://arxiv.org/pdf/2410.23496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14577v4","updated":"2024-10-30T22:58:40Z","published":"2024-05-23T13:51:55Z","title":"Representation Noising: A Defence Mechanism Against Harmful Finetuning","summary":"  Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that\noperates even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the efficacy of our defence lies in its ``depth'': the degree to\nwhich information about harmful representations is removed across all layers of\nthe LLM. We also find areas where RepNoise still remains ineffective and\nhighlight how those limitations can inform future research.\n","authors":["Domenic Rosati","Jan Wehner","Kai Williams","Łukasz Bartoszcze","David Atanasov","Robie Gonzales","Subhabrata Majumdar","Carsten Maple","Hassan Sajjad","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2405.14577v4.pdf","comment":"Published in NeurIPs 2024"},{"id":"http://arxiv.org/abs/2410.16208v2","updated":"2024-10-30T22:31:54Z","published":"2024-10-21T17:11:21Z","title":"Compute-Constrained Data Selection","summary":"  Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. These experiments show the\nvalidity of this model in real-world experiments. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective.\n","authors":["Junjie Oscar Yin","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.16208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23478v1","updated":"2024-10-30T22:00:34Z","published":"2024-10-30T22:00:34Z","title":"Collage: Decomposable Rapid Prototyping for Information Extraction on\n  Scientific PDFs","summary":"  Recent years in NLP have seen the continued development of domain-specific\ninformation extraction tools for scientific documents, alongside the release of\nincreasingly multimodal pretrained transformer models. While the opportunity\nfor scientists outside of NLP to evaluate and apply such systems to their own\ndomains has never been clearer, these models are difficult to compare: they\naccept different input formats, are often black-box and give little insight\ninto processing failures, and rarely handle PDF documents, the most common\nformat of scientific publication. In this work, we present Collage, a tool\ndesigned for rapid prototyping, visualization, and evaluation of different\ninformation extraction models on scientific PDFs. Collage allows the use and\nevaluation of any HuggingFace token classifier, several LLMs, and multiple\nother task-specific models out of the box, and provides extensible software\ninterfaces to accelerate experimentation with new models. Further, we enable\nboth developers and users of NLP-based tools to inspect, debug, and better\nunderstand modeling pipelines by providing granular views of intermediate\nstates of processing. We demonstrate our system in the context of information\nextraction to assist with literature review in materials science.\n","authors":["Sireesh Gururaja","Yueheng Zhang","Guannan Tang","Tianhao Zhang","Kevin Murphy","Yu-Tsen Yi","Junwon Seo","Anthony Rollett","Emma Strubell"],"pdf_url":"https://arxiv.org/pdf/2410.23478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18400v6","updated":"2024-10-30T21:22:54Z","published":"2024-05-28T17:40:48Z","title":"Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass","summary":"  Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.\n","authors":["Ethan Shen","Alan Fan","Sarah M. Pratt","Jae Sung Park","Matthew Wallingford","Sham M. Kakade","Ari Holtzman","Ranjay Krishna","Ali Farhadi","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2405.18400v6.pdf","comment":"23 pages, 16 figures, accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23463v1","updated":"2024-10-30T21:08:07Z","published":"2024-10-30T21:08:07Z","title":"MDCure: A Scalable Pipeline for Multi-Document Instruction-Following","summary":"  Multi-document (MD) processing is crucial for LLMs to handle real-world tasks\nsuch as summarization and question-answering across large sets of documents.\nWhile LLMs have improved at processing long inputs, MD contexts still present\nchallenges, such as managing inter-document dependencies, redundancy, and\nincoherent structures. We introduce MDCure, a scalable and effective\nfine-tuning pipeline to enhance the MD capabilities of LLMs without the\ncomputational cost of pre-training or reliance on human annotated data. MDCure\nis based on generation of high-quality synthetic MD instruction data from sets\nof related articles via targeted prompts. We further introduce MDCureRM, a\nmulti-objective reward model which filters generated data based on their\ntraining utility for MD settings. With MDCure, we fine-tune a variety of LLMs,\nfrom the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in\nsize. Extensive evaluations on a wide range of MD and long-context benchmarks\nspanning various tasks show MDCure consistently improves performance over\npre-trained baselines and over corresponding base models by up to 75.5%. Our\ncode, datasets, and models are available at https://github.com/yale-nlp/MDCure.\n","authors":["Gabrielle Kaili-May Liu","Bowen Shi","Avi Caciularu","Idan Szpektor","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2410.23463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23452v1","updated":"2024-10-30T20:48:34Z","published":"2024-10-30T20:48:34Z","title":"Graph-Augmented Relation Extraction Model with LLMs-Generated Support\n  Document","summary":"  This study introduces a novel approach to sentence-level relation extraction\n(RE) that integrates Graph Neural Networks (GNNs) with Large Language Models\n(LLMs) to generate contextually enriched support documents. By harnessing the\npower of LLMs to generate auxiliary information, our approach crafts an\nintricate graph representation of textual data. This graph is subsequently\nprocessed through a Graph Neural Network (GNN) to refine and enrich the\nembeddings associated with each entity ensuring a more nuanced and\ninterconnected understanding of the data. This methodology addresses the\nlimitations of traditional sentence-level RE models by incorporating broader\ncontexts and leveraging inter-entity interactions, thereby improving the\nmodel's ability to capture complex relationships across sentences. Our\nexperiments, conducted on the CrossRE dataset, demonstrate the effectiveness of\nour approach, with notable improvements in performance across various domains.\nThe results underscore the potential of combining GNNs with LLM-generated\ncontext to advance the field of relation extraction.\n","authors":["Vicky Dong","Hao Yu","Yao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.23452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13762v2","updated":"2024-10-30T20:40:04Z","published":"2024-06-19T18:22:32Z","title":"Unveiling the Hidden Structure of Self-Attention via Kernel Principal\n  Component Analysis","summary":"  The remarkable success of transformers in sequence modeling tasks, spanning\nvarious applications in natural language processing and computer vision, is\nattributed to the critical role of self-attention. Similar to the development\nof most deep learning models, the construction of these attention mechanisms\nrelies on heuristics and experience. In our work, we derive self-attention from\nkernel principal component analysis (kernel PCA) and show that self-attention\nprojects its query vectors onto the principal component axes of its key matrix\nin a feature space. We then formulate the exact formula for the value matrix in\nself-attention, theoretically and empirically demonstrating that this value\nmatrix captures the eigenvectors of the Gram matrix of the key vectors in\nself-attention. Leveraging our kernel PCA framework, we propose Attention with\nRobust Principal Components (RPC-Attention), a novel class of robust attention\nthat is resilient to data contamination. We empirically demonstrate the\nadvantages of RPC-Attention over softmax attention on the ImageNet-1K object\nclassification, WikiText-103 language modeling, and ADE20K image segmentation\ntask.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.13762v2.pdf","comment":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/KPCA_code"},{"id":"http://arxiv.org/abs/2410.23438v1","updated":"2024-10-30T20:29:10Z","published":"2024-10-30T20:29:10Z","title":"Learning and Transferring Sparse Contextual Bigrams with Linear\n  Transformers","summary":"  Transformers have excelled in natural language modeling and one reason behind\nthis success is their exceptional ability to combine contextual informal and\nglobal knowledge. However, the theoretical basis remains unclear. In this\npaper, first we introduce the Sparse Contextual Bigram (SCB), a natural\nextension of the classical bigram model, where the next token's generation\ndepends on a sparse set of earlier positions determined by the last token. We\nthen analyze the training dynamics and sample complexity of learning SCB using\na one-layer linear transformer with a gradient-based algorithm. We show that\nwhen trained from scratch, the training process can be split into an initial\nsample-intensive stage where the correlation is boosted from zero to a\nnontrivial value, followed by a more sample-efficient stage of further\nimprovement. Additionally, we prove that, provided a nontrivial correlation\nbetween the downstream and pretraining tasks, finetuning from a pretrained\nmodel allows us to bypass the initial sample-intensive stage. We also\nempirically demonstrate that our algorithm can outperform SGD in this setting\nand discuss its relationship with the usual softmax-based transformers.\n","authors":["Yunwei Ren","Zixuan Wang","Jason D. Lee"],"pdf_url":"https://arxiv.org/pdf/2410.23438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23437v1","updated":"2024-10-30T20:28:10Z","published":"2024-10-30T20:28:10Z","title":"Mind the Gap: A Generalized Approach for Cross-Modal Embedding Alignment","summary":"  Retrieval-Augmented Generation (RAG) systems enhance text generation by\nincorporating external knowledge but often struggle when retrieving context\nacross different text modalities due to semantic gaps. We introduce a\ngeneralized projection-based method, inspired by adapter modules in transfer\nlearning, that efficiently bridges these gaps between various text types, such\nas programming code and pseudocode, or English and French sentences. Our\napproach emphasizes speed, accuracy, and data efficiency, requiring minimal\nresources for training and inference. By aligning embeddings from heterogeneous\ntext modalities into a unified space through a lightweight projection network,\nour model significantly outperforms traditional retrieval methods like the\nOkapi BM25 algorithm and models like Dense Passage Retrieval (DPR), while\napproaching the accuracy of Sentence Transformers. Extensive evaluations\ndemonstrate the effectiveness and generalizability of our method across\ndifferent tasks, highlighting its potential for real-time, resource-constrained\napplications.\n","authors":["Arihan Yadav","Alan McMillan"],"pdf_url":"https://arxiv.org/pdf/2410.23437v1.pdf","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.23426v1","updated":"2024-10-30T20:09:37Z","published":"2024-10-30T20:09:37Z","title":"Social Science Meets LLMs: How Reliable Are Large Language Models in\n  Social Simulations?","summary":"  Large Language Models (LLMs) are increasingly employed for simulations,\nenabling applications in role-playing agents and Computational Social Science\n(CSS). However, the reliability of these simulations is under-explored, which\nraises concerns about the trustworthiness of LLMs in these applications. In\nthis paper, we aim to answer ``How reliable is LLM-based simulation?'' To\naddress this, we introduce TrustSim, an evaluation dataset covering 10\nCSS-related topics, to systematically investigate the reliability of the LLM\nsimulation. We conducted experiments on 14 LLMs and found that inconsistencies\npersist in the LLM-based simulated roles. In addition, the consistency level of\nLLMs does not strongly correlate with their general performance. To enhance the\nreliability of LLMs in simulation, we proposed Adaptive Learning Rate Based\nORPO (AdaORPO), a reinforcement learning-based algorithm to improve the\nreliability in simulation across 7 LLMs. Our research provides a foundation for\nfuture studies to explore more robust and trustworthy LLM-based simulations.\n","authors":["Yue Huang","Zhengqing Yuan","Yujun Zhou","Kehan Guo","Xiangqi Wang","Haomin Zhuang","Weixiang Sun","Lichao Sun","Jindong Wang","Yanfang Ye","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.23426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10725v3","updated":"2024-10-30T19:42:57Z","published":"2024-05-17T12:15:07Z","title":"INDUS: Effective and Efficient Language Models for Scientific\n  Applications","summary":"  Large language models (LLMs) trained on general domain corpora showed\nremarkable results on natural language processing (NLP) tasks. However,\nprevious research demonstrated LLMs trained using domain-focused corpora\nperform better on specialized tasks. Inspired by this insight, we developed\nINDUS, a comprehensive suite of LLMs tailored for the closely-related domains\nof Earth science, biology, physics, heliophysics, planetary sciences and\nastrophysics, and trained using curated scientific corpora drawn from diverse\ndata sources. The suite of models include: (1) an encoder model trained using\ndomain-specific vocabulary and corpora to address NLP tasks, (2) a\ncontrastive-learning based text embedding model trained using a diverse set of\ndatasets to address information retrieval tasks and (3) smaller versions of\nthese models created using knowledge distillation for applications which have\nlatency or resource constraints. We also created three new scientific benchmark\ndatasets, CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA) and\nNASA-IR (IR) to accelerate research in these multi-disciplinary fields. We show\nthat our models outperform both general-purpose (RoBERTa) and domain-specific\n(SCIBERT) encoders on these new tasks as well as existing tasks in the domains\nof interest. Furthermore, we demonstrate the use of these models in two\nindustrial settings -- as a retrieval model for large-scale vector search\napplications and in automatic content tagging systems.\n","authors":["Bishwaranjan Bhattacharjee","Aashka Trivedi","Masayasu Muraoka","Muthukumaran Ramasubramanian","Takuma Udagawa","Iksha Gurung","Nishan Pantha","Rong Zhang","Bharath Dandala","Rahul Ramachandran","Manil Maskey","Kaylin Bugbee","Mike Little","Elizabeth Fancher","Irina Gerasimov","Armin Mehrabian","Lauren Sanders","Sylvain Costes","Sergi Blanco-Cuaresma","Kelly Lockhart","Thomas Allen","Felix Grezes","Megan Ansdell","Alberto Accomazzi","Yousef El-Kurdi","Davis Wertheimer","Birgit Pfitzmann","Cesar Berrospi Ramis","Michele Dolfi","Rafael Teixeira de Lima","Panagiotis Vagenas","S. Karthik Mukkavilli","Peter Staar","Sanaz Vahidinia","Ryan McGranaghan","Tsendgar Lee"],"pdf_url":"https://arxiv.org/pdf/2405.10725v3.pdf","comment":"EMNLP 2024 (Industry Track)"},{"id":"http://arxiv.org/abs/2406.11717v3","updated":"2024-10-30T18:57:07Z","published":"2024-06-17T16:36:12Z","title":"Refusal in Language Models Is Mediated by a Single Direction","summary":"  Conversational large language models are fine-tuned for both\ninstruction-following and safety, resulting in models that obey benign requests\nbut refuse harmful ones. While this refusal behavior is widespread across chat\nmodels, its underlying mechanisms remain poorly understood. In this work, we\nshow that refusal is mediated by a one-dimensional subspace, across 13 popular\nopen-source chat models up to 72B parameters in size. Specifically, for each\nmodel, we find a single direction such that erasing this direction from the\nmodel's residual stream activations prevents it from refusing harmful\ninstructions, while adding this direction elicits refusal on even harmless\ninstructions. Leveraging this insight, we propose a novel white-box jailbreak\nmethod that surgically disables refusal with minimal effect on other\ncapabilities. Finally, we mechanistically analyze how adversarial suffixes\nsuppress propagation of the refusal-mediating direction. Our findings\nunderscore the brittleness of current safety fine-tuning methods. More broadly,\nour work showcases how an understanding of model internals can be leveraged to\ndevelop practical methods for controlling model behavior.\n","authors":["Andy Arditi","Oscar Obeso","Aaquib Syed","Daniel Paleka","Nina Panickssery","Wes Gurnee","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2406.11717v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15071v3","updated":"2024-10-30T18:47:53Z","published":"2024-05-23T21:42:19Z","title":"Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to\n  the Edge of Generalization","summary":"  We study whether transformers can learn to implicitly reason over parametric\nknowledge, a skill that even the most capable language models struggle with.\nFocusing on two representative reasoning types, composition and comparison, we\nconsistently find that transformers can learn implicit reasoning, but only\nthrough grokking, i.e., extended training far beyond overfitting. The levels of\ngeneralization also vary across reasoning types: when faced with\nout-of-distribution examples, transformers fail to systematically generalize\nfor composition but succeed for comparison. We delve into the model's internals\nthroughout training, conducting analytical experiments that reveal: 1) the\nmechanism behind grokking, such as the formation of the generalizing circuit\nand its relation to the relative efficiency of generalizing and memorizing\ncircuits, and 2) the connection between systematicity and the configuration of\nthe generalizing circuit. Our findings guide data and training setup to better\ninduce implicit reasoning and suggest potential improvements to the transformer\narchitecture, such as encouraging cross-layer knowledge sharing. Furthermore,\nwe demonstrate that for a challenging reasoning task with a large search space,\nGPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly\nregardless of prompting styles or retrieval augmentation, while a fully grokked\ntransformer can achieve near-perfect accuracy, showcasing the power of\nparametric memory for complex reasoning.\n","authors":["Boshi Wang","Xiang Yue","Yu Su","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2405.15071v3.pdf","comment":"NeurIPS 2024. Code and data:\n  https://github.com/OSU-NLP-Group/GrokkedTransformer"},{"id":"http://arxiv.org/abs/2408.05241v4","updated":"2024-10-30T18:37:57Z","published":"2024-08-05T20:49:48Z","title":"Large Model Strategic Thinking, Small Model Efficiency: Transferring\n  Theory of Mind in Large Language Models","summary":"  As the performance of larger, newer Large Language Models continues to\nimprove for strategic Theory of Mind (ToM) tasks, the demand for these\nstate-of-the-art models increases commensurately. However, their deployment is\ncostly both in terms of processing power and time. In this paper, we\ninvestigate the feasibility of creating smaller, highly-performing specialized\nalgorithms by way of fine-tuning. To do this, we first present a large\npre-trained model with 20 unique scenarios that combine different social\ncontexts with games of varying social dilemmas, record its answers, and use\nthem for Q&A fine-tuning on a smaller model of the same family. Our focus is on\nin-context game-theoretic decision-making, the same domain within which human\ninteraction occurs and that requires both a theory of mind (or a semblance\nthereof) and an understanding of social dynamics. The smaller model is\ntherefore trained not just on the answers provided, but also on the motivations\nprovided by the larger model, which should contain advice and guidelines to\nnavigate both strategic dilemmas and social cues. We find that the fine-tuned\nsmaller language model consistently bridged the gap in performance between the\nsmaller pre-trained version of the model and its larger relative and that its\nimprovements extended in areas and contexts beyond the ones provided in the\ntraining examples, including on out-of-sample scenarios that include completely\ndifferent game structures. On average for all games, through fine-tuning, the\nsmaller model showed a 46% improvement measured as alignment towards the\nbehavior of the larger model, with 100% representing indistinguishable\nbehavior. When presented with out-of-sample social contexts and games, the\nfine-tuned model still displays remarkable levels of alignment, reaching an\nimprovement of 18% and 28% respectively.\n","authors":["Nunzio Lore","Sepehr Ilami","Babak Heydari"],"pdf_url":"https://arxiv.org/pdf/2408.05241v4.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.15299v2","updated":"2024-10-30T18:29:51Z","published":"2024-10-20T06:01:34Z","title":"Does ChatGPT Have a Poetic Style?","summary":"  Generating poetry has become a popular application of LLMs, perhaps\nespecially of OpenAI's widely-used chatbot ChatGPT. What kind of poet is\nChatGPT? Does ChatGPT have its own poetic style? Can it successfully produce\npoems in different styles? To answer these questions, we prompt the GPT-3.5 and\nGPT-4 models to generate English-language poems in 24 different poetic forms\nand styles, about 40 different subjects, and in response to 3 different writing\nprompt templates. We then analyze the resulting 5.7k poems, comparing them to a\nsample of 3.7k poems from the Poetry Foundation and the Academy of American\nPoets. We find that the GPT models, especially GPT-4, can successfully produce\npoems in a range of both common and uncommon English-language forms in\nsuperficial yet noteworthy ways, such as by producing poems of appropriate\nlengths for sonnets (14 lines), villanelles (19 lines), and sestinas (39\nlines). But the GPT models also exhibit their own distinct stylistic\ntendencies, both within and outside of these specific forms. Our results show\nthat GPT poetry is much more constrained and uniform than human poetry, showing\na strong penchant for rhyme, quatrains (4-line stanzas), iambic meter,\nfirst-person plural perspectives (we, us, our), and specific vocabulary like\n\"heart,\" \"embrace,\" \"echo,\" and \"whisper.\"\n","authors":["Melanie Walsh","Anna Preus","Elizabeth Gronski"],"pdf_url":"https://arxiv.org/pdf/2410.15299v2.pdf","comment":"CHR 2024: Computational Humanities Research Conference"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.16408v2","updated":"2024-10-30T22:35:58Z","published":"2024-09-24T19:17:15Z","title":"Modern Hopfield Networks meet Encoded Neural Representations --\n  Addressing Practical Considerations","summary":"  Content-addressable memories such as Modern Hopfield Networks (MHN) have been\nstudied as mathematical models of auto-association and storage/retrieval in the\nhuman declarative memory, yet their practical use for large-scale content\nstorage faces challenges. Chief among them is the occurrence of meta-stable\nstates, particularly when handling large amounts of high dimensional content.\nThis paper introduces Hopfield Encoding Networks (HEN), a framework that\nintegrates encoded neural representations into MHNs to improve pattern\nseparability and reduce meta-stable states. We show that HEN can also be used\nfor retrieval in the context of hetero association of images with natural\nlanguage queries, thus removing the limitation of requiring access to partial\ncontent in the same domain. Experimental results demonstrate substantial\nreduction in meta-stable states and increased storage capacity while still\nenabling perfect recall of a significantly larger number of inputs advancing\nthe practical utility of associative memory networks for real-world tasks.\n","authors":["Satyananda Kashyap","Niharika S. D'Souza","Luyao Shi","Ken C. L. Wong","Hongzhi Wang","Tanveer Syeda-Mahmood"],"pdf_url":"https://arxiv.org/pdf/2409.16408v2.pdf","comment":"17 pages, 8 figures, accepted as a workshop paper at UniReps @\n  Neurips 2024"},{"id":"http://arxiv.org/abs/2410.23437v1","updated":"2024-10-30T20:28:10Z","published":"2024-10-30T20:28:10Z","title":"Mind the Gap: A Generalized Approach for Cross-Modal Embedding Alignment","summary":"  Retrieval-Augmented Generation (RAG) systems enhance text generation by\nincorporating external knowledge but often struggle when retrieving context\nacross different text modalities due to semantic gaps. We introduce a\ngeneralized projection-based method, inspired by adapter modules in transfer\nlearning, that efficiently bridges these gaps between various text types, such\nas programming code and pseudocode, or English and French sentences. Our\napproach emphasizes speed, accuracy, and data efficiency, requiring minimal\nresources for training and inference. By aligning embeddings from heterogeneous\ntext modalities into a unified space through a lightweight projection network,\nour model significantly outperforms traditional retrieval methods like the\nOkapi BM25 algorithm and models like Dense Passage Retrieval (DPR), while\napproaching the accuracy of Sentence Transformers. Extensive evaluations\ndemonstrate the effectiveness and generalizability of our method across\ndifferent tasks, highlighting its potential for real-time, resource-constrained\napplications.\n","authors":["Arihan Yadav","Alan McMillan"],"pdf_url":"https://arxiv.org/pdf/2410.23437v1.pdf","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.10725v3","updated":"2024-10-30T19:42:57Z","published":"2024-05-17T12:15:07Z","title":"INDUS: Effective and Efficient Language Models for Scientific\n  Applications","summary":"  Large language models (LLMs) trained on general domain corpora showed\nremarkable results on natural language processing (NLP) tasks. However,\nprevious research demonstrated LLMs trained using domain-focused corpora\nperform better on specialized tasks. Inspired by this insight, we developed\nINDUS, a comprehensive suite of LLMs tailored for the closely-related domains\nof Earth science, biology, physics, heliophysics, planetary sciences and\nastrophysics, and trained using curated scientific corpora drawn from diverse\ndata sources. The suite of models include: (1) an encoder model trained using\ndomain-specific vocabulary and corpora to address NLP tasks, (2) a\ncontrastive-learning based text embedding model trained using a diverse set of\ndatasets to address information retrieval tasks and (3) smaller versions of\nthese models created using knowledge distillation for applications which have\nlatency or resource constraints. We also created three new scientific benchmark\ndatasets, CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA) and\nNASA-IR (IR) to accelerate research in these multi-disciplinary fields. We show\nthat our models outperform both general-purpose (RoBERTa) and domain-specific\n(SCIBERT) encoders on these new tasks as well as existing tasks in the domains\nof interest. Furthermore, we demonstrate the use of these models in two\nindustrial settings -- as a retrieval model for large-scale vector search\napplications and in automatic content tagging systems.\n","authors":["Bishwaranjan Bhattacharjee","Aashka Trivedi","Masayasu Muraoka","Muthukumaran Ramasubramanian","Takuma Udagawa","Iksha Gurung","Nishan Pantha","Rong Zhang","Bharath Dandala","Rahul Ramachandran","Manil Maskey","Kaylin Bugbee","Mike Little","Elizabeth Fancher","Irina Gerasimov","Armin Mehrabian","Lauren Sanders","Sylvain Costes","Sergi Blanco-Cuaresma","Kelly Lockhart","Thomas Allen","Felix Grezes","Megan Ansdell","Alberto Accomazzi","Yousef El-Kurdi","Davis Wertheimer","Birgit Pfitzmann","Cesar Berrospi Ramis","Michele Dolfi","Rafael Teixeira de Lima","Panagiotis Vagenas","S. Karthik Mukkavilli","Peter Staar","Sanaz Vahidinia","Ryan McGranaghan","Tsendgar Lee"],"pdf_url":"https://arxiv.org/pdf/2405.10725v3.pdf","comment":"EMNLP 2024 (Industry Track)"},{"id":"http://arxiv.org/abs/2402.13959v3","updated":"2024-10-30T17:16:49Z","published":"2024-02-21T17:41:17Z","title":"Retention Induced Biases in a Recommendation System with Heterogeneous\n  Users","summary":"  I examine a conceptual model of a recommendation system (RS) with user inflow\nand churn dynamics. When inflow and churn balance out, the user distribution\nreaches a steady state. Changing the recommendation algorithm alters the steady\nstate and creates a transition period. During this period, the RS behaves\ndifferently from its new steady state. In particular, A/B experiment metrics\nobtained in transition periods are biased indicators of the RS's long-term\nperformance. Scholars and practitioners, however, often conduct A/B tests\nshortly after introducing new algorithms to validate their effectiveness. This\nA/B experiment paradigm, widely regarded as the gold standard for assessing RS\nimprovements, may consequently yield false conclusions. I also briefly touch on\nthe data bias caused by the user retention dynamics.\n","authors":["Shichao Ma"],"pdf_url":"https://arxiv.org/pdf/2402.13959v3.pdf","comment":"This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in advances in Bias and Fairness in Information\n  Retrieval. BIAS 2024. Communications in Computer and Information Science, vol\n  2227. Springer, and is available online at\n  https://doi.org/10.1007/978-3-031-71975-2_2"},{"id":"http://arxiv.org/abs/2410.23180v1","updated":"2024-10-30T16:37:04Z","published":"2024-10-30T16:37:04Z","title":"ReasoningRec: Bridging Personalized Recommendations and\n  Human-Interpretable Explanations through LLM Reasoning","summary":"  This paper presents ReasoningRec, a reasoning-based recommendation framework\nthat leverages Large Language Models (LLMs) to bridge the gap between\nrecommendations and human-interpretable explanations. In contrast to\nconventional recommendation systems that rely on implicit user-item\ninteractions, ReasoningRec employs LLMs to model users and items, focusing on\npreferences, aversions, and explanatory reasoning. The framework utilizes a\nlarger LLM to generate synthetic explanations for user preferences,\nsubsequently used to fine-tune a smaller LLM for enhanced recommendation\naccuracy and human-interpretable explanation. Our experimental study\ninvestigates the impact of reasoning and contextual information on personalized\nrecommendations, revealing that the quality of contextual and personalized data\nsignificantly influences the LLM's capacity to generate plausible explanations.\nEmpirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art\nmethods by up to 12.5\\% in recommendation prediction while concurrently\nproviding human-intelligible explanations. The code is available here:\nhttps://github.com/millenniumbismay/reasoningrec.\n","authors":["Millennium Bismay","Xiangjue Dong","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2410.23180v1.pdf","comment":"Large Language Model, Recommendation, Human-Interpretable Reasoning,\n  Personalization"},{"id":"http://arxiv.org/abs/2410.23166v1","updated":"2024-10-30T16:18:22Z","published":"2024-10-30T16:18:22Z","title":"SciPIP: An LLM-based Scientific Paper Idea Proposer","summary":"  The exponential growth of knowledge and the increasing complexity of\ninterdisciplinary research pose significant challenges for researchers,\nincluding information overload and difficulties in exploring novel ideas. The\nadvancements in large language models (LLMs), such as GPT-4, have shown great\npotential in enhancing idea proposals, but how to effectively utilize large\nmodels for reasonable idea proposal has not been thoroughly explored. This\npaper proposes a scientific paper idea proposer (SciPIP). Based on a\nuser-provided research background, SciPIP retrieves helpful papers from a\nliterature database while leveraging the capabilities of LLMs to generate more\nnovel and feasible ideas. To this end, 1) we construct a literature retrieval\ndatabase, extracting lots of papers' multi-dimension information for fast\naccess. Then, a literature retrieval method based on semantics, entity, and\ncitation co-occurrences is proposed to search relevant literature from multiple\naspects based on the user-provided background. 2) After literature retrieval,\nwe introduce dual-path idea proposal strategies, where one path infers\nsolutions from the retrieved literature and the other path generates original\nideas through model brainstorming. We then combine the two to achieve a good\nbalance between feasibility and originality. Through extensive experiments on\nthe natural language processing (NLP) field, we demonstrate that SciPIP can\nretrieve citations similar to those of existing top conference papers and\ngenerate many ideas consistent with them. Additionally, we evaluate the\noriginality of other ideas generated by SciPIP using large language models,\nfurther validating the effectiveness of our proposed method. The code and the\ndatabase are released at https://github.com/cheerss/SciPIP.\n","authors":["Wenxiao Wang","Lihui Gu","Liye Zhang","Yunxiang Luo","Yi Dai","Chen Shen","Liang Xie","Binbin Lin","Xiaofei He","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2410.23166v1.pdf","comment":"25 pages, 5 figures, 19 tables"},{"id":"http://arxiv.org/abs/2410.23136v1","updated":"2024-10-30T15:48:36Z","published":"2024-10-30T15:48:36Z","title":"Real-Time Personalization for LLM-based Recommendation with Customized\n  In-Context Learning","summary":"  Frequently updating Large Language Model (LLM)-based recommender systems to\nadapt to new user interests -- as done for traditional ones -- is impractical\ndue to high training costs, even with acceleration methods. This work explores\nadapting to dynamic user interests without any model updates by leveraging\nIn-Context Learning (ICL), which allows LLMs to learn new tasks from few-shot\nexamples provided in the input. Using new-interest examples as the ICL few-shot\nexamples, LLMs may learn real-time interest directly, avoiding the need for\nmodel updates. However, existing LLM-based recommenders often lose the\nin-context learning ability during recommendation tuning, while the original\nLLM's in-context learning lacks recommendation-specific focus. To address this,\nwe propose RecICL, which customizes recommendation-specific in-context learning\nfor real-time recommendations. RecICL organizes training examples in an\nin-context learning format, ensuring that in-context learning ability is\npreserved and aligned with the recommendation task during tuning.\n  Extensive experiments demonstrate RecICL's effectiveness in delivering\nreal-time recommendations without requiring model updates. Our code is\navailable at https://github.com/ym689/rec_icl.\n","authors":["Keqin Bao","Ming Yan","Yang Zhang","Jizhi Zhang","Wenjie Wang","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2410.23136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23090v1","updated":"2024-10-30T15:06:32Z","published":"2024-10-30T15:06:32Z","title":"CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) has become a powerful paradigm for\nenhancing large language models (LLMs) through external knowledge retrieval.\nDespite its widespread attention, existing academic research predominantly\nfocuses on single-turn RAG, leaving a significant gap in addressing the\ncomplexities of multi-turn conversations found in real-world applications. To\nbridge this gap, we introduce CORAL, a large-scale benchmark designed to assess\nRAG systems in realistic multi-turn conversational settings. CORAL includes\ndiverse information-seeking conversations automatically derived from Wikipedia\nand tackles key challenges such as open-domain coverage, knowledge intensity,\nfree-form responses, and topic shifts. It supports three core tasks of\nconversational RAG: passage retrieval, response generation, and citation\nlabeling. We propose a unified framework to standardize various conversational\nRAG methods and conduct a comprehensive evaluation of these methods on CORAL,\ndemonstrating substantial opportunities for improving existing approaches.\n","authors":["Yiruo Cheng","Kelong Mao","Ziliang Zhao","Guanting Dong","Hongjin Qian","Yongkang Wu","Tetsuya Sakai","Ji-Rong Wen","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2410.23090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08615v3","updated":"2024-10-30T14:56:09Z","published":"2022-03-16T13:31:48Z","title":"Scientific and Technological Information Oriented Semantics-adversarial\n  and Media-adversarial Cross-media Retrieval","summary":"  Cross-media retrieval of scientific and technological information is one of\nthe important tasks in the cross-media study. Cross-media scientific and\ntechnological information retrieval obtain target information from massive\nmulti-source and heterogeneous scientific and technological resources, which\nhelps to design applications that meet users' needs, including scientific and\ntechnological information recommendation, personalized scientific and\ntechnological information retrieval, etc. The core of cross-media retrieval is\nto learn a common subspace, so that data from different media can be directly\ncompared with each other after being mapped into this subspace. In subspace\nlearning, existing methods often focus on modeling the discrimination of\nintra-media data and the invariance of inter-media data after mapping; however,\nthey ignore the semantic consistency of inter-media data before and after\nmapping and media discrimination of intra-semantics data, which limit the\nresult of cross-media retrieval. In light of this, we propose a scientific and\ntechnological information oriented Semantics-adversarial and Media-adversarial\nCross-media Retrieval method (SMCR) to find an effective common subspace.\nSpecifically, SMCR minimizes the loss of inter-media semantic consistency in\naddition to modeling intra-media semantic discrimination, to preserve semantic\nsimilarity before and after mapping. Furthermore, SMCR constructs a basic\nfeature mapping network and a refined feature mapping network to jointly\nminimize the media discriminative loss within semantics, so as to enhance the\nfeature mapping network's ability to confuse the media discriminant network.\nExperimental results on two datasets demonstrate that the proposed SMCR\noutperforms state-of-the-art methods in cross-media retrieval.\n","authors":["Ang Li","Junping Du","Feifei Kou","Zhe Xue","Xin Xu","Mingying Xu","Yang Jiang"],"pdf_url":"https://arxiv.org/pdf/2203.08615v3.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.23023v1","updated":"2024-10-30T13:53:46Z","published":"2024-10-30T13:53:46Z","title":"A Universal Sets-level Optimization Framework for Next Set\n  Recommendation","summary":"  Next Set Recommendation (NSRec), encompassing related tasks such as next\nbasket recommendation and temporal sets prediction, stands as a trending\nresearch topic. Although numerous attempts have been made on this topic, there\nare certain drawbacks: (i) Existing studies are still confined to utilizing\nobjective functions commonly found in Next Item Recommendation (NIRec), such as\nbinary cross entropy and BPR, which are calculated based on individual item\ncomparisons; (ii) They place emphasis on building sophisticated learning models\nto capture intricate dependency relationships across sequential sets, but\nfrequently overlook pivotal dependency in their objective functions; (iii)\nDiversity factor within sequential sets is frequently overlooked. In this\nresearch, we endeavor to unveil a universal and S ets-level optimization\nframework for N ext Set Recommendation (SNSRec), offering a holistic fusion of\ndiversity distribution and intricate dependency relationships within temporal\nsets. To realize this, the following contributions are made: (i) We directly\nmodel the temporal set in a sequence as a cohesive entity, leveraging the\nStructured Determinantal Point Process (SDPP), wherein the probabilistic DPP\ndistribution prioritizes collections of structures (sequential sets) instead of\nindividual items; (ii) We introduce a co-occurrence representation to discern\nand acknowledge the importance of different sets; (iii) We propose a sets-level\noptimization criterion, which integrates the diversity distribution and\ndependency relations across the entire sequence of sets, guiding the model to\nrecommend relevant and diversified set. Extensive experiments on real-world\ndatasets show that our approach consistently outperforms previous methods on\nboth relevance and diversity.\n","authors":["Yuli Liu","Min Liu","Christian Walder","Lexing Xie"],"pdf_url":"https://arxiv.org/pdf/2410.23023v1.pdf","comment":"Accepter at CIKM2024"},{"id":"http://arxiv.org/abs/2410.22972v1","updated":"2024-10-30T12:39:39Z","published":"2024-10-30T12:39:39Z","title":"DataRec: A Framework for Standardizing Recommendation Data Processing\n  and Analysis","summary":"  Thanks to the great interest posed by researchers and companies,\nrecommendation systems became a cornerstone of machine learning applications.\nHowever, concerns have arisen recently about the need for reproducibility,\nmaking it challenging to identify suitable pipelines. Several frameworks have\nbeen proposed to improve reproducibility, covering the entire process from data\nreading to performance evaluation. Despite this effort, these solutions often\noverlook the role of data management, do not promote interoperability, and\nneglect data analysis despite its well-known impact on recommender performance.\nTo address these gaps, we propose DataRec, which facilitates using and\nmanipulating recommendation datasets. DataRec supports reading and writing in\nvarious formats, offers filtering and splitting techniques, and enables data\ndistribution analysis using well-known metrics. It encourages a unified\napproach to data manipulation by allowing data export in formats compatible\nwith several recommendation frameworks.\n","authors":["Alberto Carlo Maria Mancino","Salvatore Bufi","Angela Di Fazio","Daniele Malitesta","Claudio Pomo","Antonio Ferrara","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2410.22972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22844v1","updated":"2024-10-30T09:23:14Z","published":"2024-10-30T09:23:14Z","title":"Understanding and Improving Adversarial Collaborative Filtering for\n  Robust Recommendation","summary":"  Adversarial Collaborative Filtering (ACF), which typically applies\nadversarial perturbations at user and item embeddings through adversarial\ntraining, is widely recognized as an effective strategy for enhancing the\nrobustness of Collaborative Filtering (CF) recommender systems against\npoisoning attacks. Besides, numerous studies have empirically shown that ACF\ncan also improve recommendation performance compared to traditional CF. Despite\nthese empirical successes, the theoretical understanding of ACF's effectiveness\nin terms of both performance and robustness remains unclear. To bridge this\ngap, in this paper, we first theoretically show that ACF can achieve a lower\nrecommendation error compared to traditional CF with the same training epochs\nin both clean and poisoned data contexts. Furthermore, by establishing bounds\nfor reductions in recommendation error during ACF's optimization process, we\nfind that applying personalized magnitudes of perturbation for different users\nbased on their embedding scales can further improve ACF's effectiveness.\nBuilding on these theoretical understandings, we propose Personalized Magnitude\nAdversarial Collaborative Filtering (PamaCF). Extensive experiments demonstrate\nthat PamaCF effectively defends against various types of poisoning attacks\nwhile significantly enhancing recommendation performance.\n","authors":["Kaike Zhang","Qi Cao","Yunfan Wu","Fei Sun","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.22844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22832v1","updated":"2024-10-30T09:15:51Z","published":"2024-10-30T09:15:51Z","title":"HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language\n  Models","summary":"  Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge, making them adaptable and\ncost-effective for various applications. However, the growing reliance on these\nsystems also introduces potential security risks. In this work, we reveal a\nnovel vulnerability, the retrieval prompt hijack attack (HijackRAG), which\nenables attackers to manipulate the retrieval mechanisms of RAG systems by\ninjecting malicious texts into the knowledge database. When the RAG system\nencounters target questions, it generates the attacker's pre-determined answers\ninstead of the correct ones, undermining the integrity and trustworthiness of\nthe system. We formalize HijackRAG as an optimization problem and propose both\nblack-box and white-box attack strategies tailored to different levels of the\nattacker's knowledge. Extensive experiments on multiple benchmark datasets show\nthat HijackRAG consistently achieves high attack success rates, outperforming\nexisting baseline attacks. Furthermore, we demonstrate that the attack is\ntransferable across different retriever models, underscoring the widespread\nrisk it poses to RAG systems. Lastly, our exploration of various defense\nmechanisms reveals that they are insufficient to counter HijackRAG, emphasizing\nthe urgent need for more robust security measures to protect RAG systems in\nreal-world deployments.\n","authors":["Yucheng Zhang","Qinfeng Li","Tianyu Du","Xuhong Zhang","Xinkui Zhao","Zhengwen Feng","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2410.22832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22809v1","updated":"2024-10-30T08:41:13Z","published":"2024-10-30T08:41:13Z","title":"Causality-Enhanced Behavior Sequence Modeling in LLMs for Personalized\n  Recommendation","summary":"  Recent advancements in recommender systems have focused on leveraging Large\nLanguage Models (LLMs) to improve user preference modeling, yielding promising\noutcomes. However, current LLM-based approaches struggle to fully leverage user\nbehavior sequences, resulting in suboptimal preference modeling for\npersonalized recommendations. In this study, we propose a novel Counterfactual\nFine-Tuning (CFT) method to address this issue by explicitly emphasizing the\nrole of behavior sequences when generating recommendations. Specifically, we\nemploy counterfactual reasoning to identify the causal effects of behavior\nsequences on model output and introduce a task that directly fits the\nground-truth labels based on these effects, achieving the goal of explicit\nemphasis. Additionally, we develop a token-level weighting mechanism to adjust\nthe emphasis strength for different item tokens, reflecting the diminishing\ninfluence of behavior sequences from earlier to later tokens during predicting\nan item. Extensive experiments on real-world datasets demonstrate that CFT\neffectively improves behavior sequence modeling. Our codes are available at\nhttps://github.com/itsmeyjt/CFT.\n","authors":["Yang Zhang","Juntao You","Yimeng Bai","Jizhi Zhang","Keqin Bao","Wenjie Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.22809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22790v1","updated":"2024-10-30T08:09:33Z","published":"2024-10-30T08:09:33Z","title":"Dual Contrastive Transformer for Hierarchical Preference Modeling in\n  Sequential Recommendation","summary":"  Sequential recommender systems (SRSs) aim to predict the subsequent items\nwhich may interest users via comprehensively modeling users' complex preference\nembedded in the sequence of user-item interactions. However, most of existing\nSRSs often model users' single low-level preference based on item ID\ninformation while ignoring the high-level preference revealed by item attribute\ninformation, such as item category. Furthermore, they often utilize limited\nsequence context information to predict the next item while overlooking richer\ninter-item semantic relations. To this end, in this paper, we proposed a novel\nhierarchical preference modeling framework to substantially model the complex\nlow- and high-level preference dynamics for accurate sequential recommendation.\nSpecifically, in the framework, a novel dual-transformer module and a novel\ndual contrastive learning scheme have been designed to discriminatively learn\nusers' low- and high-level preference and to effectively enhance both low- and\nhigh-level preference learning respectively. In addition, a novel\nsemantics-enhanced context embedding module has been devised to generate more\ninformative context embedding for further improving the recommendation\nperformance. Extensive experiments on six real-world datasets have demonstrated\nboth the superiority of our proposed method over the state-of-the-art ones and\nthe rationality of our design.\n","authors":["Chengkai Huang","Shoujin Wang","Xianzhi Wang","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2410.22790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19453v4","updated":"2024-10-30T07:04:25Z","published":"2023-10-30T11:25:03Z","title":"FLIP: Fine-grained Alignment between ID-based Models and Pretrained\n  Language Models for CTR Prediction","summary":"  Click-through rate (CTR) prediction plays as a core function module in\nvarious personalized online services. The traditional ID-based models for CTR\nprediction take as inputs the one-hot encoded ID features of tabular modality,\nwhich capture the collaborative signals via feature interaction modeling. But\nthe one-hot encoding discards the semantic information included in the textual\nfeatures. Recently, the emergence of Pretrained Language Models(PLMs) has given\nrise to another paradigm, which takes as inputs the sentences of textual\nmodality obtained by hard prompt templates and adopts PLMs to extract the\nsemantic knowledge. However, PLMs often face challenges in capturing field-wise\ncollaborative signals and distinguishing features with subtle textual\ndifferences. In this paper, to leverage the benefits of both paradigms and\nmeanwhile overcome their limitations, we propose to conduct Fine-grained\nfeature-level ALignment between ID-based Models and Pretrained Language\nModels(FLIP) for CTR prediction. Unlike most methods that solely rely on global\nviews through instance-level contrastive learning, we design a novel jointly\nmasked tabular/language modeling task to learn fine-grained alignment between\ntabular IDs and word tokens. Specifically, the masked data of one modality (IDs\nand tokens) has to be recovered with the help of the other modality, which\nestablishes the feature-level interaction and alignment via sufficient mutual\ninformation extraction between dual modalities. Moreover, we propose to jointly\nfinetune the ID-based model and PLM by adaptively combining the output of both\nmodels, thus achieving superior performance in downstream CTR prediction tasks.\nExtensive experiments on three real-world datasets demonstrate that FLIP\noutperforms SOTA baselines, and is highly compatible with various ID-based\nmodels and PLMs. The code is at \\url{https://github.com/justarter/FLIP}.\n","authors":["Hangyu Wang","Jianghao Lin","Xiangyang Li","Bo Chen","Chenxu Zhu","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2310.19453v4.pdf","comment":"Accepted by RecSys 2024"},{"id":"http://arxiv.org/abs/2406.13249v2","updated":"2024-10-30T06:41:45Z","published":"2024-06-19T06:19:48Z","title":"R^2AG: Incorporating Retrieval Information into Retrieval Augmented\n  Generation","summary":"  Retrieval augmented generation (RAG) has been applied in many scenarios to\naugment large language models (LLMs) with external documents provided by\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\ndifferences in their training objectives and architectures. This misalignment\nforces LLMs to passively accept the documents provided by the retrievers,\nleading to incomprehension in the generation process, where the LLMs are\nburdened with the task of distinguishing these documents using their inherent\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\nthis gap by incorporating Retrieval information into Retrieval Augmented\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\nretrieval-aware prompting strategy is designed to integrate retrieval\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\nwhere LLMs and retrievers are frozen. Extensive experiments across five\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\nthe generation process, thereby filling the semantic gap.\n","authors":["Fuda Ye","Shuangyin Li","Yongqi Zhang","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2406.13249v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2403.00880v2","updated":"2024-10-30T05:18:03Z","published":"2024-03-01T08:50:27Z","title":"CIDGMed: Causal Inference-Driven Medication Recommendation with Enhanced\n  Dual-Granularity Learning","summary":"  Medication recommendation aims to integrate patients' long-term health\nrecords to provide accurate and safe medication combinations for specific\nhealth states. Existing methods often fail to deeply explore the true causal\nrelationships between diseases/procedures and medications, resulting in biased\nrecommendations. Additionally, in medication representation learning, the\nrelationships between information at different granularities of medications,\ncoarse-grained (medication itself) and fine-grained (molecular level), are not\neffectively integrated, leading to biases in representation learning. To\naddress these limitations, we propose the Causal Inference-driven\nDual-Granularity Medication Recommendation method (CIDGMed). Our approach\nleverages causal inference to uncover the relationships between\ndiseases/procedures and medications, thereby enhancing the rationality and\ninterpretability of recommendations. By integrating coarse-grained medication\neffects with fine-grained molecular structure information, CIDGMed provides a\ncomprehensive representation of medications. Additionally, we employ a bias\ncorrection model during the prediction phase to further refine recommendations,\nensuring both accuracy and safety. Through extensive experiments, CIDGMed\nsignificantly outperforms current state-of-the-art models across multiple\nmetrics, achieving a 2.54% increase in accuracy, a 3.65% reduction in side\neffects, and a 39.42% improvement in time efficiency. Additionally, we\ndemonstrate the rationale of CIDGMed through a case study.\n","authors":["Shunpan Liang","Xiang Li","Shi Mu","Chen Li","Yu Lei","Yulei Hou","Tengfei Ma"],"pdf_url":"https://arxiv.org/pdf/2403.00880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10225v5","updated":"2024-10-30T02:58:14Z","published":"2024-01-18T18:59:11Z","title":"ChatQA: Surpassing GPT-4 on Conversational QA and RAG","summary":"  In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on\nretrieval-augmented generation (RAG) and conversational question answering\n(QA). To enhance generation, we propose a two-stage instruction tuning method\nthat significantly boosts the performance of RAG. For effective retrieval, we\nintroduce a dense retriever optimized for conversational QA, which yields\nresults comparable to the alternative state-of-the-art query rewriting models,\nwhile substantially reducing deployment costs. We also present the ChatRAG\nBench, which encompasses ten datasets covering comprehensive evaluations on\nRAG, table-related QA, arithmetic calculations, and scenarios involving\nunanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a\nweaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score:\n53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without\nrelying on any synthetic data from OpenAI GPT models. Notably, the\nLlama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09,\nachieving a 4.4% improvement. To advance research in this field, we\nopen-sourced the model weights, instruction tuning data, ChatRAG Bench, and\nretriever for the community: https://chatqa-project.github.io/.\n","authors":["Zihan Liu","Wei Ping","Rajarshi Roy","Peng Xu","Chankyu Lee","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2401.10225v5.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2212.08841v4","updated":"2024-10-30T02:36:38Z","published":"2022-12-17T10:43:25Z","title":"AugTriever: Unsupervised Dense Retrieval and Domain Adaptation by\n  Scalable Data Augmentation","summary":"  Dense retrievers have made significant strides in text retrieval and\nopen-domain question answering. However, most of these achievements have relied\nheavily on extensive human-annotated supervision. In this study, we aim to\ndevelop unsupervised methods for improving dense retrieval models. We propose\ntwo approaches that enable annotation-free and scalable training by creating\npseudo querydocument pairs: query extraction and transferred query generation.\nThe query extraction method involves selecting salient spans from the original\ndocument to generate pseudo queries. On the other hand, the transferred query\ngeneration method utilizes generation models trained for other NLP tasks, such\nas summarization, to produce pseudo queries. Through extensive experimentation,\nwe demonstrate that models trained using these augmentation methods can achieve\ncomparable, if not better, performance than multiple strong dense baselines.\nMoreover, combining these strategies leads to further improvements, resulting\nin superior performance of unsupervised dense retrieval, unsupervised domain\nadaptation and supervised finetuning, benchmarked on both BEIR and ODQA\ndatasets. Code and datasets are publicly available at\nhttps://github.com/salesforce/AugTriever.\n","authors":["Rui Meng","Ye Liu","Semih Yavuz","Divyansh Agarwal","Lifu Tu","Ning Yu","Jianguo Zhang","Meghana Bhat","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.08841v4.pdf","comment":"DCAI24, October 25, 2024, Boise, ID"},{"id":"http://arxiv.org/abs/2311.08593v2","updated":"2024-10-30T01:26:09Z","published":"2023-11-14T23:28:36Z","title":"Summarization-Based Document IDs for Generative Retrieval with Language\n  Models","summary":"  Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a popular\napproach for end-to-end document retrieval that directly generates document\nidentifiers given an input query. We introduce summarization-based document\nIDs, in which each document's ID is composed of an extractive summary or\nabstractive keyphrases generated by a language model, rather than an integer ID\nsequence or bags of n-grams as proposed in past work. We find that abstractive,\ncontent-based IDs (ACID) and an ID based on the first 30 tokens are very\neffective in direct comparisons with previous approaches to ID creation. We\nshow that using ACID improves top-10 and top-20 recall by 15.6% and 14.4%\n(relative) respectively versus the cluster-based integer ID baseline on the\nMSMARCO 100k retrieval task, and 9.8% and 9.9% respectively on the\nWikipedia-based NQ 100k retrieval task. Our results demonstrate the\neffectiveness of human-readable, natural-language IDs created through\nsummarization for generative retrieval. We also observed that extractive IDs\noutperformed abstractive IDs on Wikipedia articles in NQ but not the snippets\nin MSMARCO, which suggests that document characteristics affect generative\nretrieval performance.\n","authors":["Haoxin Li","Daniel Cheng","Phillip Keung","Jungo Kasai","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2311.08593v2.pdf","comment":"To appear at the NLP for Wikipedia Workshop in EMNLP 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.23479v1","updated":"2024-10-30T22:06:14Z","published":"2024-10-30T22:06:14Z","title":"The Trail Making Test in Virtual Reality (TMT-VR): The Effects of\n  Interaction Modes and Gaming Skills on Cognitive Performance of Young Adults","summary":"  Virtual Reality (VR) is increasingly used in neuropsychological assessments\ndue to its ability to simulate real-world environments. This study aimed to\ndevelop and evaluate the Trail Making Test in VR (TMT-VR) and investigate the\neffects of different interaction modes and gaming skills on cognitive\nperformance. A total of 71 young female and male adults (aged 18-35) with high\nand low gaming skills participated in this study. Participants completed the\nTMT-VR using three interaction modes as follows: eye tracking, head movement,\nand controller. Performance metrics included task completion time and accuracy.\nUser experience, usability, and acceptability of TMT-VR were also examined.\nResults showed that both eye tracking and head movement modes significantly\noutperformed the controller in terms of task completion time and accuracy. No\nsignificant differences were found between eye tracking and head movement\nmodes. Gaming skills did not significantly influence task performance using any\ninteraction mode. The TMT-VR demonstrates high usability, acceptability, and\nuser experience among participants. The findings suggest that VR-based\nassessments can effectively measure cognitive performance without being\ninfluenced by prior gaming skills, indicating potential applicability for\ndiverse populations.\n","authors":["Evgenia Giatzoglou","Panagiotis Vorias","Ryan Kemm","Irene Karayianni","Chrysanthi Nega","Panagiotis Kourtesis"],"pdf_url":"https://arxiv.org/pdf/2410.23479v1.pdf","comment":"25 Pages, 7 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2406.14515v3","updated":"2024-10-30T13:38:10Z","published":"2024-06-20T17:26:01Z","title":"MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding","summary":"  The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit.\n","authors":["Xinyu Fang","Kangrui Mao","Haodong Duan","Xiangyu Zhao","Yining Li","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14515v3.pdf","comment":"Accepted in NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.23325v1","updated":"2024-10-30T13:17:13Z","published":"2024-10-30T13:17:13Z","title":"Transfer Learning in Vocal Education: Technical Evaluation of Limited\n  Samples Describing Mezzo-soprano","summary":"  Vocal education in the music field is difficult to quantify due to the\nindividual differences in singers' voices and the different quantitative\ncriteria of singing techniques. Deep learning has great potential to be applied\nin music education due to its efficiency to handle complex data and perform\nquantitative analysis. However, accurate evaluations with limited samples over\nrare vocal types, such as Mezzo-soprano, requires extensive well-annotated data\nsupport using deep learning models. In order to attain the objective, we\nperform transfer learning by employing deep learning models pre-trained on the\nImageNet and Urbansound8k datasets for the improvement on the precision of\nvocal technique evaluation. Furthermore, we tackle the problem of the lack of\nsamples by constructing a dedicated dataset, the Mezzo-soprano Vocal Set (MVS),\nfor vocal technique assessment. Our experimental results indicate that transfer\nlearning increases the overall accuracy (OAcc) of all models by an average of\n8.3%, with the highest accuracy at 94.2%. We not only provide a novel approach\nto evaluating Mezzo-soprano vocal techniques but also introduce a new\nquantitative assessment method for music education.\n","authors":["Zhenyi Hou","Xu Zhao","Kejie Ye","Xinyu Sheng","Shanggerile Jiang","Jiajing Xia","Yitao Zhang","Chenxi Ban","Daijun Luo","Jiaxing Chen","Yan Zou","Yuchao Feng","Guangyu Fan","Xin Yuan"],"pdf_url":"https://arxiv.org/pdf/2410.23325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22803v1","updated":"2024-10-30T08:31:58Z","published":"2024-10-30T08:31:58Z","title":"DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event\n  Localization and Detection","summary":"  This paper describes sound event localization and detection (SELD) for\nspatial audio recordings captured by firstorder ambisonics (FOA) microphones.\nIn this task, one may train a deep neural network (DNN) using FOA data\nannotated with the classes and directions of arrival (DOAs) of sound events.\nHowever, the performance of this approach is severely bounded by the amount of\nannotated data. To overcome this limitation, we propose a novel method of\npretraining the feature extraction part of the DNN in a self-supervised manner.\nWe use spatial audio-visual recordings abundantly available as virtual reality\ncontents. Assuming that sound objects are concurrently observed by the FOA\nmicrophones and the omni-directional camera, we jointly train audio and visual\nencoders with contrastive learning such that the audio and visual embeddings of\nthe same recording and DOA are made close. A key feature of our method is that\nthe DOA-wise audio embeddings are jointly extracted from the raw audio data,\nwhile the DOA-wise visual embeddings are separately extracted from the local\nvisual crops centered on the corresponding DOA. This encourages the latent\nfeatures of the audio encoder to represent both the classes and DOAs of sound\nevents. The experiment using the DCASE2022 Task 3 dataset of 20 hours shows\nnon-annotated audio-visual recordings of 100 hours reduced the error score of\nSELD from 36.4 pts to 34.9 pts.\n","authors":["Yoto Fujita","Yoshiaki Bando","Keisuke Imoto","Masaki Onishi","Kazuyoshi Yoshii"],"pdf_url":"https://arxiv.org/pdf/2410.22803v1.pdf","comment":"Accepted to APSIPA2023"},{"id":"http://arxiv.org/abs/2410.22023v2","updated":"2024-10-30T04:29:42Z","published":"2024-10-29T13:13:30Z","title":"Feature distribution Adaptation Network for Speech Emotion Recognition","summary":"  In this paper, we propose a novel deep inductive transfer learning framework,\nnamed feature distribution adaptation network, to tackle the challenging\nmulti-modal speech emotion recognition problem. Our method aims to use deep\ntransfer learning strategies to align visual and audio feature distributions to\nobtain consistent representation of emotion, thereby improving the performance\nof speech emotion recognition. In our model, the pre-trained ResNet-34 is\nutilized for feature extraction for facial expression images and acoustic Mel\nspectrograms, respectively. Then, the cross-attention mechanism is introduced\nto model the intrinsic similarity relationships of multi-modal features.\nFinally, the multi-modal feature distribution adaptation is performed\nefficiently with feed-forward network, which is extended using the local\nmaximum mean discrepancy loss. Experiments are carried out on two benchmark\ndatasets, and the results demonstrate that our model can achieve excellent\nperformance compared with existing ones.\n","authors":["Shaokai Li","Yixuan Ji","Peng Song","Haoqin Sun","Wenming Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.22023v2.pdf","comment":null}]},"2024-10-29T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2309.01610v4","updated":"2024-10-29T23:21:49Z","published":"2023-09-04T13:49:48Z","title":"Fairness in Ranking under Disparate Uncertainty","summary":"  Ranking is a ubiquitous method for focusing the attention of human evaluators\non a manageable subset of options. Its use as part of human decision-making\nprocesses ranges from surfacing potentially relevant products on an e-commerce\nsite to prioritizing college applications for human review. While ranking can\nmake human evaluation more effective by focusing attention on the most\npromising options, we argue that it can introduce unfairness if the uncertainty\nof the underlying relevance model differs between groups of options.\nUnfortunately, such disparity in uncertainty appears widespread, often to the\ndetriment of minority groups for which relevance estimates can have higher\nuncertainty due to a lack of data or appropriate features. To address this\nfairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness\ncriterion for ranking and show that it corresponds to a group-wise fair lottery\namong the relevant options even in the presence of disparate uncertainty. EOR\noptimizes for an even cost burden on all groups, unlike the conventional\nProbability Ranking Principle, and is fundamentally different from existing\nnotions of fairness in rankings, such as demographic parity and proportional\nRooney rule constraints that are motivated by proportional representation\nrelative to group size. To make EOR ranking practical, we present an efficient\nalgorithm for computing it in time $O(n \\log(n))$ and prove its close\napproximation guarantee to the globally optimal solution. In a comprehensive\nempirical evaluation on synthetic data, a US Census dataset, and a real-world\naudit of Amazon search queries, we find that the algorithm reliably guarantees\nEOR fairness while providing effective rankings.\n","authors":["Richa Rastogi","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2309.01610v4.pdf","comment":"Camera ready version at EAAMO'24"},{"id":"http://arxiv.org/abs/2409.14192v2","updated":"2024-10-29T21:10:59Z","published":"2024-09-21T16:46:15Z","title":"Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic\n  Extraction","summary":"  Integrating structured knowledge from tabular formats poses significant\nchallenges within natural language processing (NLP), mainly when dealing with\ncomplex, semi-structured tables like those found in the FeTaQA dataset. These\ntables require advanced methods to interpret and generate meaningful responses\naccurately. Traditional approaches, such as SQL and SPARQL, often fail to fully\ncapture the semantics of such data, especially in the presence of irregular\ntable structures like web tables. This paper addresses these challenges by\nproposing a novel approach that extracts triples straightforward from tabular\ndata and integrates it with a retrieval-augmented generation (RAG) model to\nenhance the accuracy, coherence, and contextual richness of responses generated\nby a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly\noutperforms existing baselines on the FeTaQA dataset, particularly excelling in\nSacre-BLEU and ROUGE metrics. It effectively generates contextually accurate\nand detailed long-form answers from tables, showcasing its strength in complex\ndata interpretation.\n","authors":["Hossein Sholehrasa","Sanaz Saki Norouzi","Pascal Hitzler","Majid Jaberi-Douraki"],"pdf_url":"https://arxiv.org/pdf/2409.14192v2.pdf","comment":"We are withdrawing this paper to address foundational aspects that\n  are critical for ensuring its accuracy and integrity before any potential\n  resubmission"},{"id":"http://arxiv.org/abs/2410.22476v1","updated":"2024-10-29T19:10:12Z","published":"2024-10-29T19:10:12Z","title":"A Pointer Network-based Approach for Joint Extraction and Detection of\n  Multi-Label Multi-Class Intents","summary":"  In task-oriented dialogue systems, intent detection is crucial for\ninterpreting user queries and providing appropriate responses. Existing\nresearch primarily addresses simple queries with a single intent, lacking\neffective systems for handling complex queries with multiple intents and\nextracting different intent spans. Additionally, there is a notable absence of\nmultilingual, multi-intent datasets. This study addresses three critical tasks:\nextracting multiple intent spans from queries, detecting multiple intents, and\ndeveloping a multi-lingual multi-label intent dataset. We introduce a novel\nmulti-label multi-class intent detection dataset (MLMCID-dataset) curated from\nexisting benchmark datasets. We also propose a pointer network-based\narchitecture (MLMCID) to extract intent spans and detect multiple intents with\ncoarse and fine-grained labels in the form of sextuplets. Comprehensive\nanalysis demonstrates the superiority of our pointer network-based system over\nbaseline approaches in terms of accuracy and F1-score across various datasets.\n","authors":["Ankan Mullick","Sombit Bose","Abhilash Nandy","Gajula Sai Chaitanya","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2410.22476v1.pdf","comment":"Accepted at EMNLP 2024 Findings (Long Paper)"},{"id":"http://arxiv.org/abs/2407.09252v3","updated":"2024-10-29T17:34:54Z","published":"2024-07-12T13:30:44Z","title":"Context Embeddings for Efficient Answer Generation in RAG","summary":"  Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods.\n","authors":["David Rau","Shuai Wang","Hervé Déjean","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2407.09252v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.22249v1","updated":"2024-10-29T17:13:54Z","published":"2024-10-29T17:13:54Z","title":"Pushing the Performance Envelope of DNN-based Recommendation Systems\n  Inference on GPUs","summary":"  Personalized recommendation is a ubiquitous application on the internet, with\nmany industries and hyperscalers extensively leveraging Deep Learning\nRecommendation Models (DLRMs) for their personalization needs (like ad serving\nor movie suggestions). With growing model and dataset sizes pushing computation\nand memory requirements, GPUs are being increasingly preferred for executing\nDLRM inference. However, serving newer DLRMs, while meeting acceptable\nlatencies, continues to remain challenging, making traditional deployments\nincreasingly more GPU-hungry, resulting in higher inference serving costs. In\nthis paper, we show that the embedding stage continues to be the primary\nbottleneck in the GPU inference pipeline, leading up to a 3.2x embedding-only\nperformance slowdown.\n  To thoroughly grasp the problem, we conduct a detailed microarchitecture\ncharacterization and highlight the presence of low occupancy in the standard\nembedding kernels. By leveraging direct compiler optimizations, we achieve\noptimal occupancy, pushing the performance by up to 53%. Yet, long memory\nlatency stalls continue to exist. To tackle this challenge, we propose\nspecialized plug-and-play-based software prefetching and L2 pinning techniques,\nwhich help in hiding and decreasing the latencies. Further, we propose\ncombining them, as they complement each other. Experimental evaluations using\nA100 GPUs with large models and datasets show that our proposed techniques\nimprove performance by up to 103% for the embedding stage, and up to 77% for\nthe overall DLRM inference pipeline.\n","authors":["Rishabh Jain","Vivek M. Bhasi","Adwait Jog","Anand Sivasubramaniam","Mahmut T. Kandemir","Chita R. Das"],"pdf_url":"https://arxiv.org/pdf/2410.22249v1.pdf","comment":"This work has been accepted in the 57th MICRO\n  (https://microarch.org/micro57/program/). Please check appendix for details\n  on reproducing our work including codebase and steps"},{"id":"http://arxiv.org/abs/2410.22233v1","updated":"2024-10-29T17:01:05Z","published":"2024-10-29T17:01:05Z","title":"ContextIQ: A Multimodal Expert-Based Video Retrieval System for\n  Contextual Advertising","summary":"  Contextual advertising serves ads that are aligned to the content that the\nuser is viewing. The rapid growth of video content on social platforms and\nstreaming services, along with privacy concerns, has increased the need for\ncontextual advertising. Placing the right ad in the right context creates a\nseamless and pleasant ad viewing experience, resulting in higher audience\nengagement and, ultimately, better ad monetization. From a technology\nstandpoint, effective contextual advertising requires a video retrieval system\ncapable of understanding complex video content at a very granular level.\nCurrent text-to-video retrieval models based on joint multimodal training\ndemand large datasets and computational resources, limiting their practicality\nand lacking the key functionalities required for ad ecosystem integration. We\nintroduce ContextIQ, a multimodal expert-based video retrieval system designed\nspecifically for contextual advertising. ContextIQ utilizes modality-specific\nexperts-video, audio, transcript (captions), and metadata such as objects,\nactions, emotion, etc.-to create semantically rich video representations. We\nshow that our system, without joint training, achieves better or comparable\nresults to state-of-the-art models and commercial solutions on multiple\ntext-to-video retrieval benchmarks. Our ablation studies highlight the benefits\nof leveraging multiple modalities for enhanced video retrieval accuracy instead\nof using a vision-language model alone. Furthermore, we show how video\nretrieval systems such as ContextIQ can be used for contextual advertising in\nan ad ecosystem while also addressing concerns related to brand safety and\nfiltering inappropriate content.\n","authors":["Ashutosh Chaubey","Anoubhav Agarwaal","Sartaki Sinha Roy","Aayush Agarwal","Susmita Ghose"],"pdf_url":"https://arxiv.org/pdf/2410.22233v1.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2410.22182v1","updated":"2024-10-29T16:19:08Z","published":"2024-10-29T16:19:08Z","title":"Synthetic Data Generation with Large Language Models for Personalized\n  Community Question Answering","summary":"  Personalization in Information Retrieval (IR) is a topic studied by the\nresearch community since a long time. However, there is still a lack of\ndatasets to conduct large-scale evaluations of personalized IR; this is mainly\ndue to the fact that collecting and curating high-quality user-related\ninformation requires significant costs and time investment. Furthermore, the\ncreation of datasets for Personalized IR (PIR) tasks is affected by both\nprivacy concerns and the need for accurate user-related data, which are often\nnot publicly available. Recently, researchers have started to explore the use\nof Large Language Models (LLMs) to generate synthetic datasets, which is a\npossible solution to generate data for low-resource tasks. In this paper, we\ninvestigate the potential of Large Language Models (LLMs) for generating\nsynthetic documents to train an IR system for a Personalized Community Question\nAnswering task. To study the effectiveness of IR models fine-tuned on\nLLM-generated data, we introduce a new dataset, named Sy-SE-PQA. We build\nSy-SE-PQA based on an existing dataset, SE-PQA, which consists of questions and\nanswers posted on the popular StackExchange communities. Starting from\nquestions in SE-PQA, we generate synthetic answers using different prompt\ntechniques and LLMs. Our findings suggest that LLMs have high potential in\ngenerating data tailored to users' needs. The synthetic data can replace\nhuman-written training data, even if the generated data may contain incorrect\ninformation.\n","authors":["Marco Braga","Pranav Kasela","Alessandro Raganato","Gabriella Pasi"],"pdf_url":"https://arxiv.org/pdf/2410.22182v1.pdf","comment":"Accepted in WI-IAT '24"},{"id":"http://arxiv.org/abs/2410.22136v1","updated":"2024-10-29T15:32:36Z","published":"2024-10-29T15:32:36Z","title":"SimRec: Mitigating the Cold-Start Problem in Sequential Recommendation\n  by Integrating Item Similarity","summary":"  Sequential recommendation systems often struggle to make predictions or take\naction when dealing with cold-start items that have limited amount of\ninteractions. In this work, we propose SimRec - a new approach to mitigate the\ncold-start problem in sequential recommendation systems. SimRec addresses this\nchallenge by leveraging the inherent similarity among items, incorporating item\nsimilarities into the training process through a customized loss function.\nImportantly, this enhancement is attained with identical model architecture and\nthe same amount of trainable parameters, resulting in the same inference time\nand requiring minimal additional effort. This novel approach results in a\nrobust contextual sequential recommendation model capable of effectively\nhandling rare items, including those that were not explicitly seen during\ntraining, thereby enhancing overall recommendation performance. Rigorous\nevaluations against multiple baselines on diverse datasets showcase SimRec's\nsuperiority, particularly in scenarios involving items occurring less than 10\ntimes in the training data. The experiments reveal an impressive improvement,\nwith SimRec achieving up to 78% higher HR@10 compared to SASRec. Notably,\nSimRec outperforms strong baselines on sparse datasets while delivering on-par\nperformance on dense datasets. Our code is available at\nhttps://github.com/amazon-science/sequential-recommendation-using-similarity.\n","authors":["Shaked Brody","Shoval Lagziel"],"pdf_url":"https://arxiv.org/pdf/2410.22136v1.pdf","comment":"ACM RecSys 2024 Workshop on Context-Aware Recommender Systems"},{"id":"http://arxiv.org/abs/2410.22123v1","updated":"2024-10-29T15:24:27Z","published":"2024-10-29T15:24:27Z","title":"Testing Identity of Distributions under Kolmogorov Distance in\n  Polylogarithmic Space","summary":"  Suppose we have a sample from a distribution $D$ and we want to test whether\n$D = D^*$ for a fixed distribution $D^*$. Specifically, we want to reject with\nconstant probability, if the distance of $D$ from $D^*$ is $\\geq \\varepsilon$\nin a given metric. In the case of continuous distributions, this has been\nstudied thoroughly in the statistics literature. Namely, for the well-studied\nKolmogorov metric a test is known that uses the optimal $O(1/\\varepsilon^2)$\nsamples.\n  However, this test naively uses also space $O(1/\\varepsilon^2)$, and previous\nwork improved this to $O(1/\\varepsilon)$. In this paper, we show that much less\nspace suffices -- we give an algorithm that uses space $O(\\log^4\n\\varepsilon^{-1})$ in the streaming setting while also using an asymptotically\noptimal number of samples. This is in contrast with the standard total\nvariation distance on discrete distributions for which such space reduction is\nknown to be impossible. Finally, we state 9 related open problems that we hope\nwill spark interest in this and related problems.\n","authors":["Christian Janos Lebeda","Jakub Tětek"],"pdf_url":"https://arxiv.org/pdf/2410.22123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09713v2","updated":"2024-10-29T13:19:12Z","published":"2024-10-13T03:45:24Z","title":"Agentic Information Retrieval","summary":"  What will information entry look like in the next generation of digital\nproducts? Since the 1970s, user access to relevant information has relied on\ndomain-specific architectures of information retrieval (IR). Over the past two\ndecades, the advent of modern IR systems, including web search engines and\npersonalized recommender systems, has greatly improved the efficiency of\nretrieving relevant information from vast data corpora. However, the core\nparadigm of these IR systems remains largely unchanged, relying on filtering a\npredefined set of candidate items. Since 2022, breakthroughs in large language\nmodels (LLMs) have begun transforming how information is accessed, establishing\na new technical paradigm. In this position paper, we introduce Agentic\nInformation Retrieval (Agentic IR), a novel IR paradigm shaped by the\ncapabilities of LLM agents. Agentic IR expands the scope of accessible tasks\nand leverages a suite of new techniques to redefine information retrieval. We\ndiscuss three types of cutting-edge applications of agentic IR and the\nchallenges faced. We propose that agentic IR holds promise for generating\ninnovative applications, potentially becoming a central information entry point\nin future digital ecosystems.\n","authors":["Weinan Zhang","Junwei Liao","Ning Li","Kounianhua Du"],"pdf_url":"https://arxiv.org/pdf/2410.09713v2.pdf","comment":"11 pages, position paper"},{"id":"http://arxiv.org/abs/2409.02856v2","updated":"2024-10-29T13:02:50Z","published":"2024-09-04T16:29:25Z","title":"Building a Scalable, Effective, and Steerable Search and Ranking\n  Platform","summary":"  Modern e-commerce platforms offer vast product selections, making it\ndifficult for customers to find items that they like and that are relevant to\ntheir current session intent. This is why it is key for e-commerce platforms to\nhave near real-time scalable and adaptable personalized ranking and search\nsystems. While numerous methods exist in the scientific literature for building\nsuch systems, many are unsuitable for large-scale industrial use due to\ncomplexity and performance limitations. Consequently, industrial ranking\nsystems often resort to computationally efficient yet simplistic retrieval or\ncandidate generation approaches, which overlook near real-time and\nheterogeneous customer signals, which results in a less personalized and\nrelevant experience. Moreover, related customer experiences are served by\ncompletely different systems, which increases complexity, maintenance, and\ninconsistent experiences.\n  In this paper, we present a personalized, adaptable near real-time ranking\nplatform that is reusable across various use cases, such as browsing and\nsearch, and that is able to cater to millions of items and customers under\nheavy load (thousands of requests per second). We employ transformer-based\nmodels through different ranking layers which can learn complex behavior\npatterns directly from customer action sequences while being able to\nincorporate temporal (e.g. in-session) and contextual information. We validate\nour system through a series of comprehensive offline and online real-world\nexperiments at a large online e-commerce platform, and we demonstrate its\nsuperiority when compared to existing systems, both in terms of customer\nexperience as well as in net revenue. Finally, we share the lessons learned\nfrom building a comprehensive, modern ranking platform for use in a large-scale\ne-commerce environment.\n","authors":["Marjan Celikik","Jacek Wasilewski","Ana Peleteiro Ramallo","Alexey Kurennoy","Evgeny Labzin","Danilo Ascione","Tural Gurbanov","Géraud Le Falher","Andrii Dzhoha","Ian Harris"],"pdf_url":"https://arxiv.org/pdf/2409.02856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22013v1","updated":"2024-10-29T13:02:11Z","published":"2024-10-29T13:02:11Z","title":"Modeling Temporal Positive and Negative Excitation for Sequential\n  Recommendation","summary":"  Sequential recommendation aims to predict the next item which interests users\nvia modeling their interest in items over time. Most of the existing works on\nsequential recommendation model users' dynamic interest in specific items while\noverlooking users' static interest revealed by some static attribute\ninformation of items, e.g., category, or brand. Moreover, existing works often\nonly consider the positive excitation of a user's historical interactions on\nhis/her next choice on candidate items while ignoring the commonly existing\nnegative excitation, resulting in insufficient modeling dynamic interest. The\noverlook of static interest and negative excitation will lead to incomplete\ninterest modeling and thus impede the recommendation performance. To this end,\nin this paper, we propose modeling both static interest and negative excitation\nfor dynamic interest to further improve the recommendation performance.\nAccordingly, we design a novel Static-Dynamic Interest Learning (SDIL)\nframework featured with a novel Temporal Positive and Negative Excitation\nModeling (TPNE) module for accurate sequential recommendation. TPNE is\nspecially designed for comprehensively modeling dynamic interest based on\ntemporal positive and negative excitation learning. Extensive experiments on\nthree real-world datasets show that SDIL can effectively capture both static\nand dynamic interest and outperforms state-of-the-art baselines.\n","authors":["Chengkai Huang","Shoujin Wang","Xianzhi Wang","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2410.22013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21967v1","updated":"2024-10-29T11:51:06Z","published":"2024-10-29T11:51:06Z","title":"Dual Conditional Diffusion Models for Sequential Recommendation","summary":"  Recent advancements in diffusion models have shown promising results in\nsequential recommendation (SR). However, current diffusion-based methods still\nexhibit two key limitations. First, they implicitly model the diffusion process\nfor target item embeddings rather than the discrete target item itself, leading\nto inconsistency in the recommendation process. Second, existing methods rely\non either implicit or explicit conditional diffusion models, limiting their\nability to fully capture the context of user behavior and leading to less\nrobust target item embeddings. In this paper, we propose the Dual Conditional\nDiffusion Models for Sequential Recommendation (DCRec), introducing a\ndiscrete-to-continuous sequential recommendation diffusion framework. Our\nframework introduces a complete Markov chain to model the transition from the\nreversed target item representation to the discrete item index, bridging the\ndiscrete and continuous item spaces for diffusion models and ensuring\nconsistency with the diffusion framework. Building on this framework, we\npresent the Dual Conditional Diffusion Transformer (DCDT) that incorporates the\nimplicit conditional and the explicit conditional for diffusion-based SR.\nExtensive experiments on public benchmark datasets demonstrate that DCRec\noutperforms state-of-the-art methods.\n","authors":["Hongtao Huang","Chengkai Huang","Xiaojun Chang","Wen Hu","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2410.21967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21892v1","updated":"2024-10-29T09:36:59Z","published":"2024-10-29T09:36:59Z","title":"Guided Diffusion-based Counterfactual Augmentation for Robust\n  Session-based Recommendation","summary":"  Session-based recommendation (SR) models aim to recommend top-K items to a\nuser, based on the user's behaviour during the current session. Several SR\nmodels are proposed in the literature, however,concerns have been raised about\ntheir susceptibility to inherent biases in the training data (observed data)\nsuch as popularity bias. SR models when trained on the biased training data may\nencounter performance challenges on out-of-distribution data in real-world\nscenarios. One way to mitigate popularity bias is counterfactual data\naugmentation. Compared to prior works that rely on generating data using SR\nmodels, we focus on utilizing the capabilities of state-of-the art diffusion\nmodels for generating counterfactual data. We propose a guided diffusion-based\ncounterfactual augmentation framework for SR. Through a combination of offline\nand online experiments on a real-world and simulated dataset, respectively, we\nshow that our approach performs significantly better than the baseline SR\nmodels and other state-of-the art augmentation frameworks. More importantly,\nour framework shows significant improvement on less popular target items, by\nachieving up to 20% gain in Recall and 13% gain in CTR on real-world and\nsimulated datasets,respectively.\n","authors":["Muskan Gupta","Priyanka Gupta","Lovekesh Vig"],"pdf_url":"https://arxiv.org/pdf/2410.21892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09142v2","updated":"2024-10-29T09:13:49Z","published":"2024-03-14T07:40:54Z","title":"USimAgent: Large Language Models for Simulating Search Users","summary":"  Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators. The code and data are accessible at\nhttps://github.com/Meow-E/USimAgent.\n","authors":["Erhan Zhang","Xingzhu Wang","Peiyuan Gong","Yankai Lin","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2403.09142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21876v1","updated":"2024-10-29T09:11:28Z","published":"2024-10-29T09:11:28Z","title":"Application of Audio Fingerprinting Techniques for Real-Time Scalable\n  Speech Retrieval and Speech Clusterization","summary":"  Audio fingerprinting techniques have seen great advances in recent years,\nenabling accurate and fast audio retrieval even in conditions when the queried\naudio sample has been highly deteriorated or recorded in noisy conditions.\nExpectedly, most of the existing work is centered around music, with popular\nmusic identification services such as Apple's Shazam or Google's Now Playing\ndesigned for individual audio recognition on mobile devices. However, the\nspectral content of speech differs from that of music, necessitating\nmodifications to current audio fingerprinting approaches. This paper offers\nfresh insights into adapting existing techniques to address the specialized\nchallenge of speech retrieval in telecommunications and cloud communications\nplatforms. The focus is on achieving rapid and accurate audio retrieval in\nbatch processing instead of facilitating single requests, typically on a\ncentralized server. Moreover, the paper demonstrates how this approach can be\nutilized to support audio clustering based on speech transcripts without\nundergoing actual speech-to-text conversion. This optimization enables\nsignificantly faster processing without the need for GPU computing, a\nrequirement for real-time operation that is typically associated with\nstate-of-the-art speech-to-text tools.\n","authors":["Kemal Altwlkany","Sead Delalić","Adis Alihodžić","Elmedin Selmanović","Damir Hasić"],"pdf_url":"https://arxiv.org/pdf/2410.21876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21801v1","updated":"2024-10-29T07:13:47Z","published":"2024-10-29T07:13:47Z","title":"PerSRV: Personalized Sticker Retrieval with Vision-Language Model","summary":"  Instant Messaging is a popular means for daily communication, allowing users\nto send text and stickers. As the saying goes, \"a picture is worth a thousand\nwords\", so developing an effective sticker retrieval technique is crucial for\nenhancing user experience. However, existing sticker retrieval methods rely on\nlabeled data to interpret stickers, and general-purpose Vision-Language Models\n(VLMs) often struggle to capture the unique semantics of stickers.\nAdditionally, relevant-based sticker retrieval methods lack personalization,\ncreating a gap between diverse user expectations and retrieval results. To\naddress these, we propose the Personalized Sticker Retrieval with\nVision-Language Model framework, namely PerSRV, structured into offline\ncalculations and online processing modules. The online retrieval part follows\nthe paradigm of relevant recall and personalized ranking, supported by the\noffline pre-calculation parts, which are sticker semantic understanding,\nutility evaluation and personalization modules. Firstly, for sticker-level\nsemantic understanding, we supervised fine-tuned LLaVA-1.5-7B to generate\nhuman-like sticker semantics, complemented by textual content extracted from\nfigures and historical interaction queries. Secondly, we investigate three\ncrowd-sourcing metrics for sticker utility evaluation. Thirdly, we cluster\nstyle centroids based on users' historical interactions to achieve personal\npreference modeling. Finally, we evaluate our proposed PerSRV method on a\npublic sticker retrieval dataset from WeChat, containing 543,098 candidates and\n12,568 interactions. Experimental results show that PerSRV significantly\noutperforms existing methods in multi-modal sticker retrieval. Additionally,\nour fine-tuned VLM delivers notable improvements in sticker semantic\nunderstandings.\n","authors":["Heng Er Metilda Chee","Jiayin Wang","Zhiqiang Guo","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21745v1","updated":"2024-10-29T05:18:34Z","published":"2024-10-29T05:18:34Z","title":"A Dual Adaptive Assignment Approach for Robust Graph-Based Clustering","summary":"  Graph clustering is an essential aspect of network analysis that involves\ngrouping nodes into separate clusters. Recent developments in deep learning\nhave resulted in advanced deep graph clustering techniques, which have proven\neffective in many applications. Nonetheless, these methods often encounter\ndifficulties when dealing with the complexities of real-world graphs,\nparticularly in the presence of noisy edges. Additionally, many denoising graph\nclustering strategies tend to suffer from lower performance compared to their\nnon-denoised counterparts, training instability, and challenges in scaling to\nlarge datasets. To tackle these issues, we introduce a new framework called the\nDual Adaptive Assignment Approach for Robust Graph-Based Clustering (RDSA).\nRDSA consists of three key components: (i) a node embedding module that\neffectively integrates the graph's topological features and node attributes;\n(ii) a structure-based soft assignment module that improves graph modularity by\nutilizing an affinity matrix for node assignments; and (iii) a node-based soft\nassignment module that identifies community landmarks and refines node\nassignments to enhance the model's robustness. We assess RDSA on various\nreal-world datasets, demonstrating its superior performance relative to\nexisting state-of-the-art methods. Our findings indicate that RDSA provides\nrobust clustering across different graph types, excelling in clustering\neffectiveness and robustness, including adaptability to noise, stability, and\nscalability.\n","authors":["Yang Xiang","Li Fan","Tulika Saha","Yushan Pan","Haiyang Zhang","Chengtao Ji"],"pdf_url":"https://arxiv.org/pdf/2410.21745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17276v2","updated":"2024-10-29T04:32:17Z","published":"2024-10-08T00:23:17Z","title":"Evaluating Performance and Bias of Negative Sampling in Large-Scale\n  Sequential Recommendation Models","summary":"  Large-scale industrial recommendation models predict the most relevant items\nfrom catalogs containing millions or billions of options. To train these models\nefficiently, a small set of irrelevant items (negative samples) is selected\nfrom the vast catalog for each relevant item (positive example), helping the\nmodel distinguish between relevant and irrelevant items. Choosing the right\nnegative sampling method is a common challenge. We address this by implementing\nand comparing various negative sampling methods - random, popularity-based,\nin-batch, mixed, adaptive, and adaptive with mixed variants - on modern\nsequential recommendation models. Our experiments, including hyperparameter\noptimization and 20x repeats on three benchmark datasets with varying\npopularity biases, show how the choice of method and dataset characteristics\nimpact key model performance metrics. We also reveal that average performance\nmetrics often hide imbalances across popularity bands (head, mid, tail). We\nfind that commonly used random negative sampling reinforces popularity bias and\nperforms best for head items. Popularity-based methods (in-batch and global\npopularity negative sampling) can offer balanced performance at the cost of\nlower overall model performance results. Our study serves as a practical guide\nto the trade-offs in selecting a negative sampling method for large-scale\nsequential recommendation models. Code, datasets, experimental results and\nhyperparameters are available at:\nhttps://github.com/apple/ml-negative-sampling.\n","authors":["Arushi Prakash","Dimitrios Bermperidis","Srivas Chennu"],"pdf_url":"https://arxiv.org/pdf/2410.17276v2.pdf","comment":"Workshop for Large Recommender Systems (LargeRecSys), 18th ACM\n  Conference on Recommender Systems, 2024, Bari, Italy"},{"id":"http://arxiv.org/abs/2311.10764v2","updated":"2024-10-29T02:56:26Z","published":"2023-11-15T06:36:11Z","title":"Deep Group Interest Modeling of Full Lifelong User Behaviors for CTR\n  Prediction","summary":"  Extracting users' interests from their lifelong behavior sequence is crucial\nfor predicting Click-Through Rate (CTR). Most current methods employ a\ntwo-stage process for efficiency: they first select historical behaviors\nrelated to the candidate item and then deduce the user's interest from this\nnarrowed-down behavior sub-sequence. This two-stage paradigm, though effective,\nleads to information loss. Solely using users' lifelong click behaviors doesn't\nprovide a complete picture of their interests, leading to suboptimal\nperformance. In our research, we introduce the Deep Group Interest Network\n(DGIN), an end-to-end method to model the user's entire behavior history. This\nincludes all post-registration actions, such as clicks, cart additions,\npurchases, and more, providing a nuanced user understanding. We start by\ngrouping the full range of behaviors using a relevant key (like item_id) to\nenhance efficiency. This process reduces the behavior length significantly,\nfrom O(10^4) to O(10^2). To mitigate the potential loss of information due to\ngrouping, we incorporate two categories of group attributes. Within each group,\nwe calculate statistical information on various heterogeneous behaviors (like\nbehavior counts) and employ self-attention mechanisms to highlight unique\nbehavior characteristics (like behavior type). Based on this reorganized\nbehavior data, the user's interests are derived using the Transformer\ntechnique. Additionally, we identify a subset of behaviors that share the same\nitem_id with the candidate item from the lifelong behavior sequence. The\ninsights from this subset reveal the user's decision-making process related to\nthe candidate item, improving prediction accuracy. Our comprehensive\nevaluation, both on industrial and public datasets, validates DGIN's efficacy\nand efficiency.\n","authors":["Qi Liu","Xuyang Hou","Haoran Jin","Xiaolong Chen","Jin Chen","Defu Lian","Zhe Wang","Jia Cheng","Jun Lei"],"pdf_url":"https://arxiv.org/pdf/2311.10764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00041v1","updated":"2024-10-29T14:45:12Z","published":"2024-10-29T14:45:12Z","title":"NeuroSym-BioCAT: Leveraging Neuro-Symbolic Methods for Biomedical\n  Scholarly Document Categorization and Question Answering","summary":"  The growing volume of biomedical scholarly document abstracts presents an\nincreasing challenge in efficiently retrieving accurate and relevant\ninformation. To address this, we introduce a novel approach that integrates an\noptimized topic modelling framework, OVB-LDA, with the BI-POP CMA-ES\noptimization technique for enhanced scholarly document abstract categorization.\nComplementing this, we employ the distilled MiniLM model, fine-tuned on\ndomain-specific data, for high-precision answer extraction. Our approach is\nevaluated across three configurations: scholarly document abstract retrieval,\ngold-standard scholarly documents abstract, and gold-standard snippets,\nconsistently outperforming established methods such as RYGH and bio-answer\nfinder. Notably, we demonstrate that extracting answers from scholarly\ndocuments abstracts alone can yield high accuracy, underscoring the sufficiency\nof abstracts for many biomedical queries. Despite its compact size, MiniLM\nexhibits competitive performance, challenging the prevailing notion that only\nlarge, resource-intensive models can handle such complex tasks. Our results,\nvalidated across various question types and evaluation batches, highlight the\nrobustness and adaptability of our method in real-world biomedical\napplications. While our approach shows promise, we identify challenges in\nhandling complex list-type questions and inconsistencies in evaluation metrics.\nFuture work will focus on refining the topic model with more extensive\ndomain-specific datasets, further optimizing MiniLM and utilizing large\nlanguage models (LLM) to improve both precision and efficiency in biomedical\nquestion answering.\n","authors":["Parvez Zamil","Gollam Rabby","Md. Sadekur Rahman","Sören Auer"],"pdf_url":"https://arxiv.org/pdf/2411.00041v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.21144v2","updated":"2024-10-29T16:25:34Z","published":"2024-10-28T15:44:35Z","title":"Enhancing Learned Image Compression via Cross Window-based Attention","summary":"  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods.\n","authors":["Priyanka Mudgal","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21144v2.pdf","comment":"Paper accepted and presented in ISVC'24. Copyrights stay with ISVC"},{"id":"http://arxiv.org/abs/2410.22112v1","updated":"2024-10-29T15:11:45Z","published":"2024-10-29T15:11:45Z","title":"Multimodal Semantic Communication for Generative Audio-Driven Video\n  Conferencing","summary":"  This paper studies an efficient multimodal data communication scheme for\nvideo conferencing. In our considered system, a speaker gives a talk to the\naudiences, with talking head video and audio being transmitted. Since the\nspeaker does not frequently change posture and high-fidelity transmission of\naudio (speech and music) is required, redundant visual video data exists and\ncan be removed by generating the video from the audio. To this end, we propose\na wave-to-video (Wav2Vid) system, an efficient video transmission framework\nthat reduces transmitted data by generating talking head video from audio. In\nparticular, full-duration audio and short-duration video data are synchronously\ntransmitted through a wireless channel, with neural networks (NNs) extracting\nand encoding audio and video semantics. The receiver then combines the decoded\naudio and video data, as well as uses a generative adversarial network (GAN)\nbased model to generate the lip movement videos of the speaker. Simulation\nresults show that the proposed Wav2Vid system can reduce the amount of\ntransmitted data by up to 83% while maintaining the perceptual quality of the\ngenerated conferencing video.\n","authors":["Haonan Tong","Haopeng Li","Hongyang Du","Zhaohui Yang","Changchuan Yin","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2410.22112v1.pdf","comment":"accepted by IEEE Wireless Communications Letters"},{"id":"http://arxiv.org/abs/2310.16334v2","updated":"2024-10-29T14:53:47Z","published":"2023-10-25T03:30:37Z","title":"Structured Multi-Track Accompaniment Arrangement via Style Prior\n  Modelling","summary":"  In the realm of music AI, arranging rich and structured multi-track\naccompaniments from a simple lead sheet presents significant challenges. Such\nchallenges include maintaining track cohesion, ensuring long-term coherence,\nand optimizing computational efficiency. In this paper, we introduce a novel\nsystem that leverages prior modelling over disentangled style factors to\naddress these challenges. Our method presents a two-stage process: initially, a\npiano arrangement is derived from the lead sheet by retrieving piano texture\nstyles; subsequently, a multi-track orchestration is generated by infusing\norchestral function styles into the piano arrangement. Our key design is the\nuse of vector quantization and a unique multi-stream Transformer to model the\nlong-term flow of the orchestration style, which enables flexible,\ncontrollable, and structured music generation. Experiments show that by\nfactorizing the arrangement task into interpretable sub-stages, our approach\nenhances generative capacity while improving efficiency. Additionally, our\nsystem supports a variety of music genres and provides style control at\ndifferent composition hierarchies. We further show that our system achieves\nsuperior coherence, structure, and overall arrangement quality compared to\nexisting baselines.\n","authors":["Jingwei Zhao","Gus Xia","Ziyu Wang","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16334v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.22046v1","updated":"2024-10-29T13:53:09Z","published":"2024-10-29T13:53:09Z","title":"CHORDONOMICON: A Dataset of 666,000 Songs and their Chord Progressions","summary":"  Chord progressions encapsulate important information about music, pertaining\nto its structure and conveyed emotions. They serve as the backbone of musical\ncomposition, and in many cases, they are the sole information required for a\nmusician to play along and follow the music. Despite their importance, chord\nprogressions as a data domain remain underexplored. There is a lack of\nlarge-scale datasets suitable for deep learning applications, and limited\nresearch exploring chord progressions as an input modality. In this work, we\npresent Chordonomicon, a dataset of over 666,000 songs and their chord\nprogressions, annotated with structural parts, genre, and release date -\ncreated by scraping various sources of user-generated progressions and\nassociated metadata. We demonstrate the practical utility of the Chordonomicon\ndataset for classification and generation tasks, and discuss its potential to\nprovide valuable insights to the research community. Chord progressions are\nunique in their ability to be represented in multiple formats (e.g. text,\ngraph) and the wealth of information chords convey in given contexts, such as\ntheir harmonic function . These characteristics make the Chordonomicon an ideal\ntestbed for exploring advanced machine learning techniques, including\ntransformers, graph machine learning, and hybrid systems that combine knowledge\nrepresentation and machine learning.\n","authors":["Spyridon Kantarelis","Konstantinos Thomas","Vassilis Lyberatos","Edmund Dervakos","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2410.22046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11832v2","updated":"2024-10-29T06:44:36Z","published":"2024-06-17T17:59:44Z","title":"Unveiling Encoder-Free Vision-Language Models","summary":"  Existing vision-language models (VLMs) mostly rely on vision encoders to\nextract visual features followed by large language models (LLMs) for\nvisual-language tasks. However, the vision encoders set a strong inductive bias\nin abstracting visual representation, e.g., resolution, aspect ratio, and\nsemantic priors, which could impede the flexibility and efficiency of the VLMs.\nTraining pure VLMs that accept the seamless vision and language inputs, i.e.,\nwithout vision encoders, remains challenging and rarely explored. Empirical\nobservations reveal that direct training without encoders results in slow\nconvergence and large performance gaps. In this work, we bridge the gap between\nencoder-based and encoder-free models, and present a simple yet effective\ntraining recipe towards pure VLMs. Specifically, we unveil the key aspects of\ntraining encoder-free VLMs efficiently via thorough experiments: (1) Bridging\nvision-language representation inside one unified decoder; (2) Enhancing visual\nrecognition capability via extra supervision. With these strategies, we launch\nEVE, an encoder-free vision-language model that can be trained and forwarded\nefficiently. Notably, solely utilizing 35M publicly accessible data, EVE can\nimpressively rival the encoder-based VLMs of similar capacities across multiple\nvision-language benchmarks. It significantly outperforms the counterpart\nFuyu-8B with mysterious training procedures and undisclosed training data. We\nbelieve that EVE provides a transparent and efficient route for developing a\npure decoder-only architecture across modalities. Our code and models are\npublicly available at: https://github.com/baaivision/EVE.\n","authors":["Haiwen Diao","Yufeng Cui","Xiaotong Li","Yueze Wang","Huchuan Lu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.11832v2.pdf","comment":"17 pages, 8 figures, Accepted by NeurIPS2024 (spotlight)"},{"id":"http://arxiv.org/abs/2410.21169v2","updated":"2024-10-29T06:32:24Z","published":"2024-10-28T16:11:35Z","title":"Document Parsing Unveiled: Techniques, Challenges, and Prospects for\n  Structured Information Extraction","summary":"  Document parsing is essential for converting unstructured and semi-structured\ndocuments-such as contracts, academic papers, and invoices-into structured,\nmachine-readable data. Document parsing extract reliable structured data from\nunstructured inputs, providing huge convenience for numerous applications.\nEspecially with recent achievements in Large Language Models, document parsing\nplays an indispensable role in both knowledge base construction and training\ndata generation. This survey presents a comprehensive review of the current\nstate of document parsing, covering key methodologies, from modular pipeline\nsystems to end-to-end models driven by large vision-language models. Core\ncomponents such as layout detection, content extraction (including text,\ntables, and mathematical expressions), and multi-modal data integration are\nexamined in detail. Additionally, this paper discusses the challenges faced by\nmodular document parsing systems and vision-language models in handling complex\nlayouts, integrating multiple modules, and recognizing high-density text. It\nemphasizes the importance of developing larger and more diverse datasets and\noutlines future research directions.\n","authors":["Qintong Zhang","Victor Shea-Jay Huang","Bin Wang","Junyuan Zhang","Zhengren Wang","Hao Liang","Shawn Wang","Matthieu Lin","Conghui He","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00831v1","updated":"2024-10-29T13:49:23Z","published":"2024-10-29T13:49:23Z","title":"Saliency-Based diversity and fairness Metric and\n  FaceKeepOriginalAugment: A Novel Approach for Enhancing Fairness and\n  Diversity","summary":"  Data augmentation has become a pivotal tool in enhancing the performance of\ncomputer vision tasks, with the KeepOriginalAugment method emerging as a\nstandout technique for its intelligent incorporation of salient regions within\nless prominent areas, enabling augmentation in both regions. Despite its\nsuccess in image classification, its potential in addressing biases remains\nunexplored. In this study, we introduce an extension of the KeepOriginalAugment\nmethod, termed FaceKeepOriginalAugment, which explores various debiasing\naspects-geographical, gender, and stereotypical biases-in computer vision\nmodels. By maintaining a delicate balance between data diversity and\ninformation preservation, our approach empowers models to exploit both diverse\nsalient and non-salient regions, thereby fostering increased diversity and\ndebiasing effects. We investigate multiple strategies for determining the\nplacement of the salient region and swapping perspectives to decide which part\nundergoes augmentation. Leveraging the Image Similarity Score (ISS), we\nquantify dataset diversity across a range of datasets, including Flickr Faces\nHQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse\nDataset. We evaluate the effectiveness of FaceKeepOriginalAugment in mitigating\ngender bias across CEO, Engineer, Nurse, and School Teacher datasets, utilizing\nthe Image-Image Association Score (IIAS) in convolutional neural networks\n(CNNs) and vision transformers (ViTs). Our findings shows the efficacy of\nFaceKeepOriginalAugment in promoting fairness and inclusivity within computer\nvision models, demonstrated by reduced gender bias and enhanced overall\nfairness. Additionally, we introduce a novel metric, Saliency-Based Diversity\nand Fairness Metric, which quantifies both diversity and fairness while\nhandling data imbalance across various datasets.\n","authors":["Teerath Kumar","Alessandra Mileo","Malika Bendechache"],"pdf_url":"https://arxiv.org/pdf/2411.00831v1.pdf","comment":"Paper is underReview in Image and Vision Computing Journal special\n  issue: Advancing Transparency and Privacy: Explainable AI and Synthetic Data\n  in Biometrics and Computer Vision"}]},"2024-10-28T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.08660v2","updated":"2024-10-28T22:29:03Z","published":"2024-03-27T18:53:04Z","title":"How Does Message Passing Improve Collaborative Filtering?","summary":"  Collaborative filtering (CF) has exhibited prominent results for recommender\nsystems and been broadly utilized for real-world applications. A branch of\nresearch enhances CF methods by message passing used in graph neural networks,\ndue to its strong capabilities of extracting knowledge from graph-structured\ndata, like user-item bipartite graphs that naturally exist in CF. They assume\nthat message passing helps CF methods in a manner akin to its benefits for\ngraph-based learning tasks in general. However, even though message passing\nempirically improves CF, whether or not this assumption is correct still needs\nverification. To address this gap, we formally investigate why message passing\nhelps CF from multiple perspectives and show that many assumptions made by\nprevious works are not entirely accurate. With our curated ablation studies and\ntheoretical analyses, we discover that (1) message passing improves the CF\nperformance primarily by additional representations passed from neighbors\nduring the forward pass instead of additional gradient updates to neighbor\nrepresentations during the model back-propagation and (ii) message passing\nusually helps low-degree nodes more than high-degree nodes. Utilizing these\nnovel findings, we present Test-time Aggregation for CF, namely TAG-CF, a\ntest-time augmentation framework that only conducts message passing once at\ninference time. The key novelty of TAG-CF is that it effectively utilizes graph\nknowledge while circumventing most of notorious computational overheads of\nmessage passing. Besides, TAG-CF is extremely versatile can be used as a\nplug-and-play module to enhance representations trained by different CF\nsupervision signals. Evaluated on six datasets, TAG-CF consistently improves\nthe recommendation performance of CF methods without graph by up to 39.2% on\ncold users and 31.7% on all users, with little to no extra computational\noverheads.\n","authors":["Mingxuan Ju","William Shiao","Zhichun Guo","Yanfang Ye","Yozen Liu","Neil Shah","Tong Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.08660v2.pdf","comment":"Accepted to NeurIPS'24. Code available at:\n  https://github.com/snap-research/Test-time-Aggregation-for-CF"},{"id":"http://arxiv.org/abs/2410.21549v1","updated":"2024-10-28T21:25:38Z","published":"2024-10-28T21:25:38Z","title":"Semantic Search Evaluation","summary":"  We propose a novel method for evaluating the performance of a content search\nsystem that measures the semantic match between a query and the results\nreturned by the search system. We introduce a metric called \"on-topic rate\" to\nmeasure the percentage of results that are relevant to the query. To achieve\nthis, we design a pipeline that defines a golden query set, retrieves the top K\nresults for each query, and sends calls to GPT 3.5 with formulated prompts. Our\nsemantic evaluation pipeline helps identify common failure patterns and goals\nagainst the metric for relevance improvements.\n","authors":["Chujie Zheng","Jeffrey Wang","Shuqian Albee Zhang","Anand Kishore","Siddharth Singh"],"pdf_url":"https://arxiv.org/pdf/2410.21549v1.pdf","comment":"Accepted by 3rd International Workshop on Industrial Recommendation\n  Systems (at CIKM 2024)"},{"id":"http://arxiv.org/abs/2410.21529v1","updated":"2024-10-28T20:55:00Z","published":"2024-10-28T20:55:00Z","title":"Can Users Detect Biases or Factual Errors in Generated Responses in\n  Conversational Information-Seeking?","summary":"  Information-seeking dialogues span a wide range of questions, from simple\nfactoid to complex queries that require exploring multiple facets and\nviewpoints. When performing exploratory searches in unfamiliar domains, users\nmay lack background knowledge and struggle to verify the system-provided\ninformation, making them vulnerable to misinformation. We investigate the\nlimitations of response generation in conversational information-seeking\nsystems, highlighting potential inaccuracies, pitfalls, and biases in the\nresponses. The study addresses the problem of query answerability and the\nchallenge of response incompleteness. Our user studies explore how these issues\nimpact user experience, focusing on users' ability to identify biased,\nincorrect, or incomplete responses. We design two crowdsourcing tasks to assess\nuser experience with different system response variants, highlighting critical\nissues to be addressed in future conversational information-seeking research.\nOur analysis reveals that it is easier for users to detect response\nincompleteness than query answerability and user satisfaction is mostly\nassociated with response diversity, not factual correctness.\n","authors":["Weronika Łajewska","Krisztian Balog","Damiano Spina","Johanne Trippas"],"pdf_url":"https://arxiv.org/pdf/2410.21529v1.pdf","comment":"Extended version of the paper that appeared in the Proceedings of the\n  2024 Annual International ACM SIGIR Conference on Research and Development in\n  Information Retrieval in the Asia Pacific Region (SIGIR-AP '24)"},{"id":"http://arxiv.org/abs/2410.21487v1","updated":"2024-10-28T19:52:09Z","published":"2024-10-28T19:52:09Z","title":"Enhancing CTR Prediction in Recommendation Domain with Search Query\n  Representation","summary":"  Many platforms, such as e-commerce websites, offer both search and\nrecommendation services simultaneously to better meet users' diverse needs.\nRecommendation services suggest items based on user preferences, while search\nservices allow users to search for items before providing recommendations.\nSince users and items are often shared between the search and recommendation\ndomains, there is a valuable opportunity to enhance the recommendation domain\nby leveraging user preferences extracted from the search domain. Existing\napproaches either overlook the shift in user intention between these domains or\nfail to capture the significant impact of learning from users' search queries\non understanding their interests.\n  In this paper, we propose a framework that learns from user search query\nembeddings within the context of user preferences in the recommendation domain.\nSpecifically, user search query sequences from the search domain are used to\npredict the items users will click at the next time point in the recommendation\ndomain. Additionally, the relationship between queries and items is explored\nthrough contrastive learning. To address issues of data sparsity, the diffusion\nmodel is incorporated to infer positive items the user will select after\nsearching with certain queries in a denoising manner, which is particularly\neffective in preventing false positives. Effectively extracting this\ninformation, the queries are integrated into click-through rate prediction in\nthe recommendation domain. Experimental analysis demonstrates that our model\noutperforms state-of-the-art models in the recommendation domain.\n","authors":["Yuening Wang","Man Chen","Yaochen Hu","Wei Guo","Yingxue Zhang","Huifeng Guo","Yong Liu","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2410.21487v1.pdf","comment":"Accepted by CIKM 2024 Full Research Track"},{"id":"http://arxiv.org/abs/2410.21484v1","updated":"2024-10-28T19:49:53Z","published":"2024-10-28T19:49:53Z","title":"A Systematic Review of Machine Learning in Sports Betting: Techniques,\n  Challenges, and Future Directions","summary":"  The sports betting industry has experienced rapid growth, driven largely by\ntechnological advancements and the proliferation of online platforms. Machine\nlearning (ML) has played a pivotal role in the transformation of this sector by\nenabling more accurate predictions, dynamic odds-setting, and enhanced risk\nmanagement for both bookmakers and bettors. This systematic review explores\nvarious ML techniques, including support vector machines, random forests, and\nneural networks, as applied in different sports such as soccer, basketball,\ntennis, and cricket. These models utilize historical data, in-game statistics,\nand real-time information to optimize betting strategies and identify value\nbets, ultimately improving profitability. For bookmakers, ML facilitates\ndynamic odds adjustment and effective risk management, while bettors leverage\ndata-driven insights to exploit market inefficiencies. This review also\nunderscores the role of ML in fraud detection, where anomaly detection models\nare used to identify suspicious betting patterns. Despite these advancements,\nchallenges such as data quality, real-time decision-making, and the inherent\nunpredictability of sports outcomes remain. Ethical concerns related to\ntransparency and fairness are also of significant importance. Future research\nshould focus on developing adaptive models that integrate multimodal data and\nmanage risk in a manner akin to financial portfolios. This review provides a\ncomprehensive examination of the current applications of ML in sports betting,\nand highlights both the potential and the limitations of these technologies.\n","authors":["René Manassé Galekwa","Jean Marie Tshimula","Etienne Gael Tajeuna","Kyamakya Kyandoghere"],"pdf_url":"https://arxiv.org/pdf/2410.21484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15239v3","updated":"2024-10-28T17:52:17Z","published":"2024-07-21T18:08:44Z","title":"Assessing Brittleness of Image-Text Retrieval Benchmarks from\n  Vision-Language Models Perspective","summary":"  We examine the brittleness of the image-text retrieval (ITR) evaluation\npipeline with a focus on concept granularity. We start by analyzing two common\nbenchmarks, MS-COCO and Flickr30k, and compare them with augmented,\nfine-grained versions, MS-COCO-FG and Flickr30k-FG, given a specified set of\nlinguistic features capturing concept granularity. Flickr30k-FG and MS COCO-FG\nconsistently give rise to higher scores across all the selected features. To\nfurther our understanding of the impact of granularity we consider a novel\ntaxonomy of query perturbations. We apply these perturbations to the selected\ndatasets. We evaluate four diverse state-of-the-art Vision-Language models on\nboth the standard and fine-grained datasets under zero-shot conditions, with\nand without the applied perturbations. The results demonstrate that although\nperturbations generally degrade model performance, the fine-grained datasets\nexhibit a smaller performance drop than their standard counterparts. The\nrelative performance drop across all setups is consistent across all models and\ndatasets, indicating that the issue lies within the benchmarks themselves. We\nconclude by providing an agenda for improving ITR evaluation pipelines.\n","authors":["Mariya Hendriksen","Shuo Zhang","Ridho Reinanda","Mohamed Yahya","Edgar Meij","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2407.15239v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21242v1","updated":"2024-10-28T17:40:40Z","published":"2024-10-28T17:40:40Z","title":"Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback","summary":"  Building effective dense retrieval systems remains difficult when relevance\nsupervision is not available. Recent work has looked to overcome this challenge\nby using a Large Language Model (LLM) to generate hypothetical documents that\ncan be used to find the closest real document. However, this approach relies\nsolely on the LLM to have domain-specific knowledge relevant to the query,\nwhich may not be practical. Furthermore, generating hypothetical documents can\nbe inefficient as it requires the LLM to generate a large number of tokens for\neach query. To address these challenges, we introduce Real Document Embeddings\nfrom Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF\nproposes to re-frame hypothetical document generation as a relevance estimation\ntask, using an LLM to select which documents should be used for nearest\nneighbor search. Through this re-framing, the LLM no longer needs\ndomain-specific knowledge but only needs to judge what is relevant.\nAdditionally, relevance estimation only requires the LLM to output a single\ntoken, thereby improving search latency. Our experiments show that ReDE-RF\nconsistently surpasses state-of-the-art zero-shot dense retrieval methods\nacross a wide range of low-resource retrieval datasets while also making\nsignificant improvements in latency per-query.\n","authors":["Nour Jedidi","Yung-Sung Chuang","Leslie Shing","James Glass"],"pdf_url":"https://arxiv.org/pdf/2410.21242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21220v1","updated":"2024-10-28T17:04:18Z","published":"2024-10-28T17:04:18Z","title":"Vision Search Assistant: Empower Vision-Language Models as Multimodal\n  Search Engines","summary":"  Search engines enable the retrieval of unknown information with texts.\nHowever, traditional methods fall short when it comes to understanding\nunfamiliar visual content, such as identifying an object that the model has\nnever seen before. This challenge is particularly pronounced for large\nvision-language models (VLMs): if the model has not been exposed to the object\ndepicted in an image, it struggles to generate reliable answers to the user's\nquestion regarding that image. Moreover, as new objects and events continuously\nemerge, frequently updating VLMs is impractical due to heavy computational\nburdens. To address this limitation, we propose Vision Search Assistant, a\nnovel framework that facilitates collaboration between VLMs and web agents.\nThis approach leverages VLMs' visual understanding capabilities and web agents'\nreal-time information access to perform open-world Retrieval-Augmented\nGeneration via the web. By integrating visual and textual representations\nthrough this collaboration, the model can provide informed responses even when\nthe image is novel to the system. Extensive experiments conducted on both\nopen-set and closed-set QA benchmarks demonstrate that the Vision Search\nAssistant significantly outperforms the other models and can be widely applied\nto existing VLMs.\n","authors":["Zhixin Zhang","Yiyuan Zhang","Xiaohan Ding","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2410.21220v1.pdf","comment":"Code is available at https://github.com/cnzzx/VSA"},{"id":"http://arxiv.org/abs/2410.19177v2","updated":"2024-10-28T15:51:46Z","published":"2024-10-24T22:13:48Z","title":"Sentiment-Driven Community Detection in a Network of Perfume Preferences","summary":"  Network analysis is increasingly important across various fields, including\nthe fragrance industry, where perfumes are represented as nodes and shared user\npreferences as edges in perfume networks. Community detection can uncover\nclusters of similar perfumes, providing insights into consumer preferences,\nenhancing recommendation systems, and informing targeted marketing strategies.\n  This study aims to apply community detection techniques to group perfumes\nfavored by users into relevant clusters for better recommendations. We\nconstructed a bipartite network from user reviews on the Persian retail\nplatform \"Atrafshan,\" with nodes representing users and perfumes, and edges\nformed by positive comments. This network was transformed into a Perfume\nCo-Preference Network, connecting perfumes liked by the same users. By applying\ncommunity detection algorithms, we identified clusters based on shared\npreferences, enhancing our understanding of user sentiment in the fragrance\nmarket.\n  To improve sentiment analysis, we integrated emojis and a user voting system\nfor greater accuracy. Emojis, aligned with their Persian counterparts, captured\nthe emotional tone of reviews, while user ratings for scent, longevity, and\nsillage refined sentiment classification. Edge weights were adjusted by\ncombining adjacency values with user ratings in a 60:40 ratio, reflecting both\nconnection strength and user preferences. These enhancements led to improved\nmodularity of detected communities, resulting in more accurate perfume\ngroupings.\n  This research pioneers the use of community detection in perfume networks,\noffering new insights into consumer preferences. Our advancements in sentiment\nanalysis and edge weight refinement provide actionable insights for optimizing\nproduct recommendations and marketing strategies in the fragrance industry.\n","authors":["Kamand Kalashi","Sajjad Saed","Babak Teimourpour"],"pdf_url":"https://arxiv.org/pdf/2410.19177v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13399v2","updated":"2024-10-28T15:48:08Z","published":"2024-08-23T22:51:14Z","title":"Transforming Location Retrieval at Airbnb: A Journey from Heuristics to\n  Reinforcement Learning","summary":"  The Airbnb search system grapples with many unique challenges as it continues\nto evolve. We oversee a marketplace that is nuanced by geography, diversity of\nhomes, and guests with a variety of preferences. Crafting an efficient search\nsystem that can accommodate diverse guest needs, while showcasing relevant\nhomes lies at the heart of Airbnb's success. Airbnb search has many challenges\nthat parallel other recommendation and search systems but it has a unique\ninformation retrieval problem, upstream of ranking, called location retrieval.\nIt requires defining a topological map area that is relevant to the searched\nquery for homes listing retrieval. The purpose of this paper is to demonstrate\nthe methodology, challenges, and impact of building a machine learning based\nlocation retrieval product from the ground up. Despite the lack of suitable,\nprevalent machine learning based approaches, we tackle cold start,\ngeneralization, differentiation and algorithmic bias. We detail the efficacy of\nheuristics, statistics, machine learning, and reinforcement learning approaches\nto solve these challenges, particularly for systems that are often unexplored\nby current literature.\n","authors":["Dillon Davis","Huiji Gao","Thomas Legrand","Weiwei Guo","Malay Haldar","Alex Deng","Han Zhao","Liwei He","Sanjeev Katariya"],"pdf_url":"https://arxiv.org/pdf/2408.13399v2.pdf","comment":"Published at CIKM 2024"},{"id":"http://arxiv.org/abs/2410.21048v1","updated":"2024-10-28T14:03:39Z","published":"2024-10-28T14:03:39Z","title":"Pay Attention to Attention for Sequential Recommendation","summary":"  Transformer-based approaches have demonstrated remarkable success in various\nsequence-based tasks. However, traditional self-attention models may not\nsufficiently capture the intricate dependencies within items in sequential\nrecommendation scenarios. This is due to the lack of explicit emphasis on\nattention weights, which play a critical role in allocating attention and\nunderstanding item-to-item correlations. To better exploit the potential of\nattention weights and improve the capability of sequential recommendation in\nlearning high-order dependencies, we propose a novel sequential recommendation\n(SR) approach called attention weight refinement (AWRSR). AWRSR enhances the\neffectiveness of self-attention by additionally paying attention to attention\nweights, allowing for more refined attention distributions of correlations\namong items. We conduct comprehensive experiments on multiple real-world\ndatasets, demonstrating that our approach consistently outperforms\nstate-of-the-art SR models. Moreover, we provide a thorough analysis of AWRSR's\neffectiveness in capturing higher-level dependencies. These findings suggest\nthat AWRSR offers a promising new direction for enhancing the performance of\nself-attention architecture in SR tasks, with potential applications in other\nsequence-based problems as well.\n","authors":["Yuli Liu","Min Liu","Xiaojing Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21048v1.pdf","comment":"Accepted at RecSys 2024"},{"id":"http://arxiv.org/abs/2410.20965v1","updated":"2024-10-28T12:36:00Z","published":"2024-10-28T12:36:00Z","title":"Simultaneous Unlearning of Multiple Protected User Attributes From\n  Variational Autoencoder Recommenders Using Adversarial Training","summary":"  In widely used neural network-based collaborative filtering models, users'\nhistory logs are encoded into latent embeddings that represent the users'\npreferences. In this setting, the models are capable of mapping users'\nprotected attributes (e.g., gender or ethnicity) from these user embeddings\neven without explicit access to them, resulting in models that may treat\nspecific demographic user groups unfairly and raise privacy issues. While prior\nwork has approached the removal of a single protected attribute of a user at a\ntime, multiple attributes might come into play in real-world scenarios. In the\nwork at hand, we present AdvXMultVAE which aims to unlearn multiple protected\nattributes (exemplified by gender and age) simultaneously to improve fairness\nacross demographic user groups. For this purpose, we couple a variational\nautoencoder (VAE) architecture with adversarial training (AdvMultVAE) to\nsupport simultaneous removal of the users' protected attributes with continuous\nand/or categorical values. Our experiments on two datasets, LFM-2b-100k and\nMl-1m, from the music and movie domains, respectively, show that our approach\ncan yield better results than its singular removal counterparts (based on\nAdvMultVAE) in effectively mitigating demographic biases whilst improving the\nanonymity of latent embeddings.\n","authors":["Gustavo Escobedo","Christian Ganhör","Stefan Brandl","Mirjam Augstein","Markus Schedl"],"pdf_url":"https://arxiv.org/pdf/2410.20965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19014v4","updated":"2024-10-28T11:11:04Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v4.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2410.20909v1","updated":"2024-10-28T10:39:08Z","published":"2024-10-28T10:39:08Z","title":"Challenges in Implementing a Recommender System for Historical Research\n  in the Humanities","summary":"  This extended abstract describes the challenges in implementing recommender\nsystems for digital archives in the humanities, focusing on Monasterium.net, a\nplatform for historical legal documents. We discuss three key aspects: (i) the\nunique characteristics of so-called charters as items for recommendation, (ii)\nthe complex multi-stakeholder environment, and (iii) the distinct\ninformation-seeking behavior of scholars in the humanities. By examining these\nfactors, we aim to contribute to the development of more effective and tailored\nrecommender systems for (digital) humanities research.\n","authors":["Florian Atzenhofer-Baumgartner","Bernhard C. Geiger","Christoph Trattner","Georg Vogeler","Dominik Kowald"],"pdf_url":"https://arxiv.org/pdf/2410.20909v1.pdf","comment":"Presented at AltRecSys 2024: The First Workshop on Alternative,\n  Unexpected, and Critical Ideas in Recommendation, October 18, 2024,\n  co-located with the ACM Conference on Recommender Systems 2024 (RecSys 2024),\n  Bari, Italy"},{"id":"http://arxiv.org/abs/2410.20868v1","updated":"2024-10-28T09:36:03Z","published":"2024-10-28T09:36:03Z","title":"RecFlow: An Industrial Full Flow Recommendation Dataset","summary":"  Industrial recommendation systems (RS) rely on the multi-stage pipeline to\nbalance effectiveness and efficiency when delivering items from a vast corpus\nto users. Existing RS benchmark datasets primarily focus on the exposure space,\nwhere novel RS algorithms are trained and evaluated. However, when these\nalgorithms transition to real world industrial RS, they face a critical\nchallenge of handling unexposed items which are a significantly larger space\nthan the exposed one. This discrepancy profoundly impacts their practical\nperformance. Additionally, these algorithms often overlook the intricate\ninterplay between multiple RS stages, resulting in suboptimal overall system\nperformance. To address this issue, we introduce RecFlow, an industrial full\nflow recommendation dataset designed to bridge the gap between offline RS\nbenchmarks and the real online environment. Unlike existing datasets, RecFlow\nincludes samples not only from the exposure space but also unexposed items\nfiltered at each stage of the RS funnel. Our dataset comprises 38M interactions\nfrom 42K users across nearly 9M items with additional 1.9B stage samples\ncollected from 9.3M online requests over 37 days and spanning 6 stages.\nLeveraging the RecFlow dataset, we conduct courageous exploration experiments,\nshowcasing its potential in designing new algorithms to enhance effectiveness\nby incorporating stage-specific samples. Some of these algorithms have already\nbeen deployed online, consistently yielding significant gains. We propose\nRecFlow as the first comprehensive benchmark dataset for the RS community,\nsupporting research on designing algorithms at any stage, study of selection\nbias, debiased algorithms, multi-stage consistency and optimality, multi-task\nrecommendation, and user behavior modeling. The RecFlow dataset, along with the\ncorresponding source code, is available at\nhttps://github.com/RecFlow-ICLR/RecFlow.\n","authors":["Qi Liu","Kai Zheng","Rui Huang","Wuchao Li","Kuo Cai","Yuan Chai","Yanan Niu","Yiqun Hui","Bing Han","Na Mou","Hongning Wang","Wentian Bao","Yunen Yu","Guorui Zhou","Han Li","Yang Song","Defu Lian","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2410.20868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20859v1","updated":"2024-10-28T09:21:15Z","published":"2024-10-28T09:21:15Z","title":"Leveraging AI and Sentiment Analysis for Forecasting Election Outcomes\n  in Mauritius","summary":"  This study explores the use of AI-driven sentiment analysis as a novel tool\nfor forecasting election outcomes, focusing on Mauritius' 2024 elections. In\nthe absence of reliable polling data, we analyze media sentiment toward two\nmain political parties L'Alliance Lepep and L'Alliance Du Changement by\nclassifying news articles from prominent Mauritian media outlets as positive,\nnegative, or neutral. We employ a multilingual BERT-based model and a custom\nSentiment Scoring Algorithm to quantify sentiment dynamics and apply the\nSentiment Impact Score (SIS) for measuring sentiment influence over time. Our\nforecast model suggests L'Alliance Du Changement is likely to secure a minimum\nof 37 seats, while L'Alliance Lepep is predicted to obtain the remaining 23\nseats out of the 60 available. Findings indicate that positive media sentiment\nstrongly correlates with projected electoral gains, underscoring the role of\nmedia in shaping public perception. This approach not only mitigates media bias\nthrough adjusted scoring but also serves as a reliable alternative to\ntraditional polling. The study offers a scalable methodology for political\nforecasting in regions with limited polling infrastructure and contributes to\nadvancements in the field of political data science.\n","authors":["Missie Chercheur","Malkenzie Bovafiz"],"pdf_url":"https://arxiv.org/pdf/2410.20859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20820v1","updated":"2024-10-28T08:11:17Z","published":"2024-10-28T08:11:17Z","title":"Temporal Streaming Batch Principal Component Analysis for Time Series\n  Classification","summary":"  In multivariate time series classification, although current sequence\nanalysis models have excellent classification capabilities, they show\nsignificant shortcomings when dealing with long sequence multivariate data,\nsuch as prolonged training times and decreased accuracy. This paper focuses on\noptimizing model performance for long-sequence multivariate data by mitigating\nthe impact of extended time series and multiple variables on the model. We\npropose a principal component analysis (PCA)-based temporal streaming\ncompression and dimensionality reduction algorithm for time series data\n(temporal streaming batch PCA, TSBPCA), which continuously updates the compact\nrepresentation of the entire sequence through streaming PCA time estimation\nwith time block updates, enhancing the data representation capability of a\nrange of sequence analysis models. We evaluated this method using various\nmodels on five real datasets, and the experimental results show that our method\nperforms well in terms of classification accuracy and time efficiency. Notably,\nour method demonstrates a trend of increasing effectiveness as sequence length\ngrows; on the two longest sequence datasets, accuracy improved by about 7.2%,\nand execution time decreased by 49.5%.\n","authors":["Enshuo Yan","Huachuan Wang","Weihao Xia"],"pdf_url":"https://arxiv.org/pdf/2410.20820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06941v2","updated":"2024-10-28T07:51:29Z","published":"2024-08-13T14:59:44Z","title":"OpenResearcher: Unleashing AI for Accelerated Scientific Research","summary":"  The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher.\n","authors":["Yuxiang Zheng","Shichao Sun","Lin Qiu","Dongyu Ru","Cheng Jiayang","Xuefeng Li","Jifan Lin","Binjie Wang","Yun Luo","Renjie Pan","Yang Xu","Qingkai Min","Zizhao Zhang","Yiwen Wang","Wenjie Li","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2408.06941v2.pdf","comment":"Accepted to Demo track of EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.04553v2","updated":"2024-10-28T07:38:11Z","published":"2024-06-06T23:44:33Z","title":"Better Late Than Never: Formulating and Benchmarking Recommendation\n  Editing","summary":"  Recommendation systems play a pivotal role in suggesting items to users based\non their preferences. However, in online platforms, these systems inevitably\noffer unsuitable recommendations due to limited model capacity, poor data\nquality, or evolving user interests. Enhancing user experience necessitates\nefficiently rectify such unsuitable recommendation behaviors. This paper\nintroduces a novel and significant task termed recommendation editing, which\nfocuses on modifying known and unsuitable recommendation behaviors.\nSpecifically, this task aims to adjust the recommendation model to eliminate\nknown unsuitable items without accessing training data or retraining the model.\nWe formally define the problem of recommendation editing with three primary\nobjectives: strict rectification, collaborative rectification, and concentrated\nrectification. Three evaluation metrics are developed to quantitatively assess\nthe achievement of each objective. We present a straightforward yet effective\nbenchmark for recommendation editing using novel Editing Bayesian Personalized\nRanking Loss. To demonstrate the effectiveness of the proposed method, we\nestablish a comprehensive benchmark that incorporates various methods from\nrelated fields. Codebase is available at\nhttps://github.com/cycl2018/Recommendation-Editing.\n","authors":["Chengyu Lai","Sheng Zhou","Zhimeng Jiang","Qiaoyu Tan","Yuanchen Bei","Jiawei Chen","Ningyu Zhang","Jiajun Bu"],"pdf_url":"https://arxiv.org/pdf/2406.04553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20778v1","updated":"2024-10-28T06:39:01Z","published":"2024-10-28T06:39:01Z","title":"Beyond Positive History: Re-ranking with List-level Hybrid Feedback","summary":"  As the last stage of recommender systems, re-ranking generates a re-ordered\nlist that aligns with the user's preference. However, previous works generally\nfocus on item-level positive feedback as history (e.g., only clicked items) and\nignore that users provide positive or negative feedback on items in the entire\nlist. This list-level hybrid feedback can reveal users' holistic preferences\nand reflect users' comparison behavior patterns manifesting within a list. Such\npatterns could predict user behaviors on candidate lists, thus aiding better\nre-ranking. Despite appealing benefits, extracting and integrating preferences\nand behavior patterns from list-level hybrid feedback into re-ranking multiple\nitems remains challenging. To this end, we propose Re-ranking with List-level\nHybrid Feedback (dubbed RELIFE). It captures user's preferences and behavior\npatterns with three modules: a Disentangled Interest Miner to disentangle the\nuser's preferences into interests and disinterests, a Sequential Preference\nMixer to learn users' entangled preferences considering the context of\nfeedback, and a Comparison-aware Pattern Extractor to capture user's behavior\npatterns within each list. Moreover, for better integration of patterns,\ncontrastive learning is adopted to align the behavior patterns of candidate and\nhistorical lists. Extensive experiments show that RELIFE significantly\noutperforms SOTA re-ranking baselines.\n","authors":["Muyan Weng","Yunjia Xi","Weiwen Liu","Bo Chen","Jianghao Lin","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.20778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20730v1","updated":"2024-10-28T04:49:05Z","published":"2024-10-28T04:49:05Z","title":"GPRec: Bi-level User Modeling for Deep Recommenders","summary":"  GPRec explicitly categorizes users into groups in a learnable manner and\naligns them with corresponding group embeddings. We design the dual group\nembedding space to offer a diverse perspective on group preferences by\ncontrasting positive and negative patterns. On the individual level, GPRec\nidentifies personal preferences from ID-like features and refines the obtained\nindividual representations to be independent of group ones, thereby providing a\nrobust complement to the group-level modeling. We also present various\nstrategies for the flexible integration of GPRec into various DRS models.\nRigorous testing of GPRec on three public datasets has demonstrated significant\nimprovements in recommendation quality.\n","authors":["Yejing Wang","Dong Xu","Xiangyu Zhao","Zhiren Mao","Peng Xiang","Ling Yan","Yao Hu","Zijian Zhang","Xuetao Wei","Qidong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.20730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.08063v6","updated":"2024-10-28T03:39:32Z","published":"2022-02-16T13:44:00Z","title":"Information Extraction in Low-Resource Scenarios: Survey and Perspective","summary":"  Information Extraction (IE) seeks to derive structured information from\nunstructured texts, often facing challenges in low-resource scenarios due to\ndata scarcity and unseen classes. This paper presents a review of neural\napproaches to low-resource IE from \\emph{traditional} and \\emph{LLM-based}\nperspectives, systematically categorizing them into a fine-grained taxonomy.\nThen we conduct empirical study on LLM-based methods compared with previous\nstate-of-the-art models, and discover that (1) well-tuned LMs are still\npredominant; (2) tuning open-resource LLMs and ICL with GPT family is promising\nin general; (3) the optimal LLM-based technical solution for low-resource IE\ncan be task-dependent. In addition, we discuss low-resource IE with LLMs,\nhighlight promising applications, and outline potential research directions.\nThis survey aims to foster understanding of this field, inspire new ideas, and\nencourage widespread applications in both academia and industry.\n","authors":["Shumin Deng","Yubo Ma","Ningyu Zhang","Yixin Cao","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2202.08063v6.pdf","comment":"Accepted by 15th IEEE International Conference on Knowledge Graphs\n  (ICKG2024). Paper List:\n  \\url{https://github.com/zjunlp/Low-resource-KEPapers}; Data and Code: \\url{\n  https://github.com/mayubo2333/LLM_project}"},{"id":"http://arxiv.org/abs/2410.20643v1","updated":"2024-10-28T00:39:22Z","published":"2024-10-28T00:39:22Z","title":"GenUP: Generative User Profilers as In-Context Learners for Next POI\n  Recommender Systems","summary":"  Traditional POI recommendation systems often lack transparency,\ninterpretability, and scrutability due to their reliance on dense vector-based\nuser embeddings. Furthermore, the cold-start problem -- where systems have\ninsufficient data for new users -- limits their ability to generate accurate\nrecommendations. Existing methods often address this by leveraging similar\ntrajectories from other users, but this approach can be computationally\nexpensive and increases the context length for LLM-based methods, making them\ndifficult to scale. To address these limitations, we propose a method that\ngenerates natural language (NL) user profiles from large-scale, location-based\nsocial network (LBSN) check-ins, utilizing robust personality assessments and\nbehavioral theories. These NL profiles capture user preferences, routines, and\nbehaviors, improving POI prediction accuracy while offering enhanced\ntransparency. By incorporating NL profiles as system prompts to LLMs, our\napproach reduces reliance on extensive historical data, while remaining\nflexible, easily updated, and computationally efficient. Our method is not only\ncompetitive with other LLM-based and complex agentic frameworks but is also\nmore scalable for real-world scenarios and on-device POI recommendations.\nResults demonstrate that our approach consistently outperforms baseline\nmethods, offering a more interpretable and resource-efficient solution for POI\nrecommendation systems. Our source code is available at:\n\\url{https://github.com/w11wo/GenUP}.\n","authors":["Wilson Wongso","Hao Xue","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2410.20643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20642v1","updated":"2024-10-28T00:38:06Z","published":"2024-10-28T00:38:06Z","title":"Collaborative Knowledge Fusion: A Novel Approach for Multi-task\n  Recommender Systems via LLMs","summary":"  Owing to the impressive general intelligence of large language models (LLMs),\nthere has been a growing trend to integrate them into recommender systems to\ngain a more profound insight into human interests and intentions. Existing\nLLMs-based recommender systems primarily leverage item attributes and user\ninteraction histories in textual format, improving the single task like rating\nprediction or explainable recommendation. Nevertheless, these approaches\noverlook the crucial contribution of traditional collaborative signals in\ndiscerning users' profound intentions and disregard the interrelatedness among\ntasks. To address these limitations, we introduce a novel framework known as\nCKF, specifically developed to boost multi-task recommendations via\npersonalized collaborative knowledge fusion into LLMs. Specifically, our method\nsynergizes traditional collaborative filtering models to produce collaborative\nembeddings, subsequently employing the meta-network to construct personalized\nmapping bridges tailored for each user. Upon mapped, the embeddings are\nincorporated into meticulously designed prompt templates and then fed into an\nadvanced LLM to represent user interests. To investigate the intrinsic\nrelationship among diverse recommendation tasks, we develop Multi-Lora, a new\nparameter-efficient approach for multi-task optimization, adept at distinctly\nsegregating task-shared and task-specific information. This method forges a\nconnection between LLMs and recommendation scenarios, while simultaneously\nenriching the supervisory signal through mutual knowledge transfer among\nvarious tasks. Extensive experiments and in-depth robustness analyses across\nfour common recommendation tasks on four large public data sets substantiate\nthe effectiveness and superiority of our framework.\n","authors":["Chuang Zhao","Xing Su","Ming He","Hongke Zhao","Jianping Fan","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2410.20642v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2405.14312v2","updated":"2024-10-28T19:33:30Z","published":"2024-05-23T08:32:58Z","title":"Improving Gloss-free Sign Language Translation by Reducing\n  Representation Density","summary":"  Gloss-free sign language translation (SLT) aims to develop well-performing\nSLT systems with no requirement for the costly gloss annotations, but currently\nstill lags behind gloss-based approaches significantly. In this paper, we\nidentify a representation density problem that could be a bottleneck in\nrestricting the performance of gloss-free SLT. Specifically, the representation\ndensity problem describes that the visual representations of semantically\ndistinct sign gestures tend to be closely packed together in feature space,\nwhich makes gloss-free methods struggle with distinguishing different sign\ngestures and suffer from a sharp performance drop. To address the\nrepresentation density problem, we introduce a simple but effective contrastive\nlearning strategy, namely SignCL, which encourages gloss-free models to learn\nmore discriminative feature representation in a self-supervised manner. Our\nexperiments demonstrate that the proposed SignCL can significantly reduce the\nrepresentation density and improve performance across various translation\nframeworks. Specifically, SignCL achieves a significant improvement in BLEU\nscore for the Sign Language Transformer and GFSLT-VLP on the CSL-Daily dataset\nby 39% and 46%, respectively, without any increase of model parameters.\nCompared to Sign2GPT, a state-of-the-art method based on large-scale\npre-trained vision and language models, SignCL achieves better performance with\nonly 35% of its parameters. Implementation and Checkpoints are available at\nhttps://github.com/JinhuiYE/SignCL.\n","authors":["Jinhui Ye","Xing Wang","Wenxiang Jiao","Junwei Liang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2405.14312v2.pdf","comment":"Accepted at NeurIPS'24; Representation Density Problem and\n  Performance Drop in Gloss-free SLT"},{"id":"http://arxiv.org/abs/2410.21478v1","updated":"2024-10-28T19:32:17Z","published":"2024-10-28T19:32:17Z","title":"Knowledge Distillation for Real-Time Classification of Early Media in\n  Voice Communications","summary":"  This paper investigates the industrial setting of real-time classification of\nearly media exchanged during the initialization phase of voice calls. We\nexplore the application of state-of-the-art audio tagging models and highlight\nsome limitations when applied to the classification of early media. While most\nexisting approaches leverage convolutional neural networks, we propose a novel\napproach for low-resource requirements based on gradient-boosted trees. Our\napproach not only demonstrates a substantial improvement in runtime\nperformance, but also exhibits a comparable accuracy. We show that leveraging\nknowledge distillation and class aggregation techniques to train a simpler and\nsmaller model accelerates the classification of early media in voice calls. We\nprovide a detailed analysis of the results on a proprietary and publicly\navailable dataset, regarding accuracy and runtime performance. We additionally\nreport a case study of the achieved performance improvements at a regional data\ncenter in India.\n","authors":["Kemal Altwlkany","Hadžem Hadžić","Amar Kurić","Emanuel Lacic"],"pdf_url":"https://arxiv.org/pdf/2410.21478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21269v1","updated":"2024-10-28T17:58:15Z","published":"2024-10-28T17:58:15Z","title":"OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup","summary":"  The scaling up has brought tremendous success in the fields of vision and\nlanguage in recent years. When it comes to audio, however, researchers\nencounter a major challenge in scaling up the training data, as most natural\naudio contains diverse interfering signals. To address this limitation, we\nintroduce Omni-modal Sound Separation (OmniSep), a novel framework capable of\nisolating clean soundtracks based on omni-modal queries, encompassing both\nsingle-modal and multi-modal composed queries. Specifically, we introduce the\nQuery-Mixup strategy, which blends query features from different modalities\nduring training. This enables OmniSep to optimize multiple modalities\nconcurrently, effectively bringing all modalities under a unified framework for\nsound separation. We further enhance this flexibility by allowing queries to\ninfluence sound separation positively or negatively, facilitating the retention\nor removal of specific sounds as desired. Finally, OmniSep employs a\nretrieval-augmented approach known as Query-Aug, which enables open-vocabulary\nsound separation. Experimental evaluations on MUSIC, VGGSOUND-CLEAN+, and\nMUSIC-CLEAN+ datasets demonstrate effectiveness of OmniSep, achieving\nstate-of-the-art performance in text-, image-, and audio-queried sound\nseparation tasks. For samples and further information, please visit the demo\npage at \\url{https://omnisep.github.io/}.\n","authors":["Xize Cheng","Siqi Zheng","Zehan Wang","Minghui Fang","Ziang Zhang","Rongjie Huang","Ziyang Ma","Shengpeng Ji","Jialong Zuo","Tao Jin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.21269v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2410.21061v1","updated":"2024-10-28T14:22:08Z","published":"2024-10-28T14:22:08Z","title":"Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative\n  Framework","summary":"  Text-to-image (T2I) diffusion models are popular for introducing image\nmanipulation methods, such as editing, image fusion, inpainting, etc. At the\nsame time, image-to-video (I2V) and text-to-video (T2V) models are also built\non top of T2I models. We present Kandinsky 3, a novel T2I model based on latent\ndiffusion, achieving a high level of quality and photorealism. The key feature\nof the new architecture is the simplicity and efficiency of its adaptation for\nmany types of generation tasks. We extend the base T2I model for various\napplications and create a multifunctional generation system that includes\ntext-guided inpainting/outpainting, image fusion, text-image fusion, image\nvariations generation, I2V and T2V generation. We also present a distilled\nversion of the T2I model, evaluating inference in 4 steps of the reverse\nprocess without reducing image quality and 3 times faster than the base model.\nWe deployed a user-friendly demo system in which all the features can be tested\nin the public domain. Additionally, we released the source code and checkpoints\nfor the Kandinsky 3 and extended models. Human evaluations show that Kandinsky\n3 demonstrates one of the highest quality scores among open source generation\nsystems.\n","authors":["Vladimir Arkhipkin","Viacheslav Vasilev","Andrei Filatov","Igor Pavlov","Julia Agafonova","Nikolai Gerasimenko","Anna Averchenkova","Evelina Mironova","Anton Bukashkin","Konstantin Kulikov","Andrey Kuznetsov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2410.21061v1.pdf","comment":"Accepted for EMNLP 2024 (Demo track)"},{"id":"http://arxiv.org/abs/2410.21029v1","updated":"2024-10-28T13:51:03Z","published":"2024-10-28T13:51:03Z","title":"FairStream: Fair Multimedia Streaming Benchmark for Reinforcement\n  Learning Agents","summary":"  Multimedia streaming accounts for the majority of traffic in today's\ninternet. Mechanisms like adaptive bitrate streaming control the bitrate of a\nstream based on the estimated bandwidth, ideally resulting in smooth playback\nand a good Quality of Experience (QoE). However, selecting the optimal bitrate\nis challenging under volatile network conditions. This motivated researchers to\ntrain Reinforcement Learning (RL) agents for multimedia streaming. The\nconsidered training environments are often simplified, leading to promising\nresults with limited applicability. Additionally, the QoE fairness across\nmultiple streams is seldom considered by recent RL approaches. With this work,\nwe propose a novel multi-agent environment that comprises multiple challenges\nof fair multimedia streaming: partial observability, multiple objectives, agent\nheterogeneity and asynchronicity. We provide and analyze baseline approaches\nacross five different traffic classes to gain detailed insights into the\nbehavior of the considered agents, and show that the commonly used Proximal\nPolicy Optimization (PPO) algorithm is outperformed by a simple greedy\nheuristic. Future work includes the adaptation of multi-agent RL algorithms and\nfurther expansions of the environment.\n","authors":["Jannis Weil","Jonas Ringsdorf","Julian Barthel","Yi-Ping Phoebe Chen","Tobias Meuser"],"pdf_url":"https://arxiv.org/pdf/2410.21029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18709v2","updated":"2024-10-28T12:19:39Z","published":"2023-10-28T13:37:52Z","title":"Audio-Visual Instance Segmentation","summary":"  In this paper, we propose a new multi-modal task, termed audio-visual\ninstance segmentation (AVIS), which aims to simultaneously identify, segment\nand track individual sounding object instances in audible videos. To facilitate\nthis research, we introduce a high-quality benchmark named AVISeg, containing\nover 90K instance masks from 26 semantic categories in 926 long videos.\nAdditionally, we propose a strong baseline model for this task. Our model first\nlocalizes sound source within each frame, and condenses object-specific\ncontexts into concise tokens. Then it builds long-range audio-visual\ndependencies between these tokens using window-based attention, and tracks\nsounding objects among the entire video sequences. Extensive experiments reveal\nthat our method performs best on AVISeg, surpassing the existing methods from\nrelated tasks. We further conduct the evaluation on several multi-modal large\nmodels; however, they exhibits subpar performance on instance-level sound\nsource localization and temporal perception. We expect that AVIS will inspire\nthe community towards a more comprehensive multi-modal understanding.\n","authors":["Ruohao Guo","Xianghua Ying","Yaru Chen","Dantong Niu","Guangyao Li","Liao Qu","Yanyu Qi","Bowei Xing","Wenzhen Yue","Ji Shi","Qixun Wang","Peiliang Zhang","Buwen Liang"],"pdf_url":"https://arxiv.org/pdf/2310.18709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06171v3","updated":"2024-10-28T12:11:12Z","published":"2023-12-11T07:20:42Z","title":"Joint Explicit and Implicit Cross-Modal Interaction Network for Anterior\n  Chamber Inflammation Diagnosis","summary":"  Uveitis demands the precise diagnosis of anterior chamber inflammation (ACI)\nfor optimal treatment. However, current diagnostic methods only rely on a\nlimited single-modal disease perspective, which leads to poor performance. In\nthis paper, we investigate a promising yet challenging way to fuse multimodal\ndata for ACI diagnosis. Notably, existing fusion paradigms focus on empowering\nimplicit modality interactions (i.e., self-attention and its variants), but\nneglect to inject explicit modality interactions, especially from clinical\nknowledge and imaging property. To this end, we propose a jointly Explicit and\nimplicit Cross-Modal Interaction Network (EiCI-Net) for Anterior Chamber\nInflammation Diagnosis that uses anterior segment optical coherence tomography\n(AS-OCT) images, slit-lamp images, and clinical data jointly. Specifically, we\nfirst develop CNN-Based Encoders and Tabular Processing Module (TPM) to extract\nefficient feature representations in different modalities. Then, we devise an\nExplicit Cross-Modal Interaction Module (ECIM) to generate attention maps as a\nkind of explicit clinical knowledge based on the tabular feature maps, then\nintegrated them into the slit-lamp feature maps, allowing the CNN-Based Encoder\nto focus on more effective informativeness of the slit-lamp images. After that,\nthe Implicit Cross-Modal Interaction Module (ICIM), a transformer-based\nnetwork, further implicitly enhances modality interactions. Finally, we\nconstruct a considerable real-world dataset from our collaborative hospital and\nconduct sufficient experiments to demonstrate the superior performance of our\nproposed EiCI-Net compared with the state-of-the-art classification methods in\nvarious metrics.\n","authors":["Qian Shao","Ye Dai","Haochao Ying","Kan Xu","Jinhong Wang","Wei Chi","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2312.06171v3.pdf","comment":"IEEE MedAI 2024"},{"id":"http://arxiv.org/abs/2410.20898v1","updated":"2024-10-28T10:26:19Z","published":"2024-10-28T10:26:19Z","title":"Diff-Instruct*: Towards Human-Preferred One-step Text-to-image\n  Generative Models","summary":"  In this paper, we introduce the Diff-Instruct*(DI*), a data-free approach for\nbuilding one-step text-to-image generative models that align with human\npreference while maintaining the ability to generate highly realistic images.\nWe frame human preference alignment as online reinforcement learning using\nhuman feedback (RLHF), where the goal is to maximize the reward function while\nregularizing the generator distribution to remain close to a reference\ndiffusion process. Unlike traditional RLHF approaches, which rely on the KL\ndivergence for regularization, we introduce a novel score-based divergence\nregularization, which leads to significantly better performances. Although the\ndirect calculation of this divergence remains intractable, we demonstrate that\nwe can efficiently compute its \\emph{gradient} by deriving an equivalent yet\ntractable loss function. Remarkably, with Stable Diffusion V1.5 as the\nreference diffusion model, DI* outperforms \\emph{all} previously leading models\nby a large margin. When using the 0.6B PixelArt-$\\alpha$ model as the reference\ndiffusion, DI* achieves a new record Aesthetic Score of 6.30 and an Image\nReward of 1.31 with only a single generation step, almost doubling the scores\nof the rest of the models with similar sizes. It also achieves an HPSv2 score\nof 28.70, establishing a new state-of-the-art benchmark. We also observe that\nDI* can improve the layout and enrich the colors of generated images.\n","authors":["Weijian Luo","Colin Zhang","Debing Zhang","Zhengyang Geng"],"pdf_url":"https://arxiv.org/pdf/2410.20898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20855v1","updated":"2024-10-28T09:19:28Z","published":"2024-10-28T09:19:28Z","title":"ByteNet: Rethinking Multimedia File Fragment Classification through\n  Visual Perspectives","summary":"  Multimedia file fragment classification (MFFC) aims to identify file fragment\ntypes, e.g., image/video, audio, and text without system metadata. It is of\nvital importance in multimedia storage and communication. Existing MFFC methods\ntypically treat fragments as 1D byte sequences and emphasize the relations\nbetween separate bytes (interbytes) for classification. However, the more\ninformative relations inside bytes (intrabytes) are overlooked and seldom\ninvestigated. By looking inside bytes, the bit-level details of file fragments\ncan be accessed, enabling a more accurate classification. Motivated by this, we\nfirst propose Byte2Image, a novel visual representation model that incorporates\npreviously overlooked intrabyte information into file fragments and\nreinterprets these fragments as 2D grayscale images. This model involves a\nsliding byte window to reveal the intrabyte information and a rowwise stacking\nof intrabyte ngrams for embedding fragments into a 2D space. Thus, complex\ninterbyte and intrabyte correlations can be mined simultaneously using powerful\nvision networks. Additionally, we propose an end-to-end dual-branch network\nByteNet to enhance robust correlation mining and feature representation.\nByteNet makes full use of the raw 1D byte sequence and the converted 2D image\nthrough a shallow byte branch feature extraction (BBFE) and a deep image branch\nfeature extraction (IBFE) network. In particular, the BBFE, composed of a\nsingle fully-connected layer, adaptively recognizes the co-occurrence of\nseveral some specific bytes within the raw byte sequence, while the IBFE, built\non a vision Transformer, effectively mines the complex interbyte and intrabyte\ncorrelations from the converted image. Experiments on the two representative\nbenchmarks, including 14 cases, validate that our proposed method outperforms\nstate-of-the-art approaches on different cases by up to 12.2%.\n","authors":["Wenyang Liu","Kejun Wu","Tianyi Liu","Yi Wang","Kim-Hui Yap","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2410.20855v1.pdf","comment":"Accepted in TMM"},{"id":"http://arxiv.org/abs/2404.13289v2","updated":"2024-10-28T03:35:22Z","published":"2024-04-20T06:32:00Z","title":"Double Mixture: Towards Continual Event Detection from Speech","summary":"  Speech event detection is crucial for multimedia retrieval, involving the\ntagging of both semantic and acoustic events. Traditional ASR systems often\noverlook the interplay between these events, focusing solely on content, even\nthough the interpretation of dialogue can vary with environmental context. This\npaper tackles two primary challenges in speech event detection: the continual\nintegration of new events without forgetting previous ones, and the\ndisentanglement of semantic from acoustic events. We introduce a new task,\ncontinual event detection from speech, for which we also provide two benchmark\ndatasets. To address the challenges of catastrophic forgetting and effective\ndisentanglement, we propose a novel method, 'Double Mixture.' This method\nmerges speech expertise with robust memory mechanisms to enhance adaptability\nand prevent forgetting. Our comprehensive experiments show that this task\npresents significant challenges that are not effectively addressed by current\nstate-of-the-art methods in either computer vision or natural language\nprocessing. Our approach achieves the lowest rates of forgetting and the\nhighest levels of generalization, proving robust across various continual\nlearning sequences. Our code and data are available at\nhttps://anonymous.4open.science/status/Continual-SpeechED-6461.\n","authors":["Jingqi Kang","Tongtong Wu","Jinming Zhao","Guitao Wang","Yinwei Wei","Hao Yang","Guilin Qi","Yuan-Fang Li","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2404.13289v2.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2409.14703v2","updated":"2024-10-28T03:32:28Z","published":"2024-09-23T04:49:08Z","title":"MemeCLIP: Leveraging CLIP Representations for Multimodal Meme\n  Classification","summary":"  The complexity of text-embedded images presents a formidable challenge in\nmachine learning given the need for multimodal understanding of multiple\naspects of expression conveyed by them. While previous research in multimodal\nanalysis has primarily focused on singular aspects such as hate speech and its\nsubclasses, this study expands this focus to encompass multiple aspects of\nlinguistics: hate, targets of hate, stance, and humor. We introduce a novel\ndataset PrideMM comprising 5,063 text-embedded images associated with the\nLGBTQ+ Pride movement, thereby addressing a serious gap in existing resources.\nWe conduct extensive experimentation on PrideMM by using unimodal and\nmultimodal baseline methods to establish benchmarks for each task.\nAdditionally, we propose a novel framework MemeCLIP for efficient downstream\nlearning while preserving the knowledge of the pre-trained CLIP model. The\nresults of our experiments show that MemeCLIP achieves superior performance\ncompared to previously proposed frameworks on two real-world datasets. We\nfurther compare the performance of MemeCLIP and zero-shot GPT-4 on the hate\nclassification task. Finally, we discuss the shortcomings of our model by\nqualitatively analyzing misclassified samples. Our code and dataset are\npublicly available at: https://github.com/SiddhantBikram/MemeCLIP.\n","authors":["Siddhant Bikram Shah","Shuvam Shiwakoti","Maheep Chaudhary","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2409.14703v2.pdf","comment":"Accepted to EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.20670v1","updated":"2024-10-28T02:05:10Z","published":"2024-10-28T02:05:10Z","title":"Segmenting Watermarked Texts From Language Models","summary":"  Watermarking is a technique that involves embedding nearly unnoticeable\nstatistical signals within generated content to help trace its source. This\nwork focuses on a scenario where an untrusted third-party user sends prompts to\na trusted language model (LLM) provider, who then generates a text from their\nLLM with a watermark. This setup makes it possible for a detector to later\nidentify the source of the text if the user publishes it. The user can modify\nthe generated text by substitutions, insertions, or deletions. Our objective is\nto develop a statistical method to detect if a published text is LLM-generated\nfrom the perspective of a detector. We further propose a methodology to segment\nthe published text into watermarked and non-watermarked sub-strings. The\nproposed approach is built upon randomization tests and change point detection\ntechniques. We demonstrate that our method ensures Type I and Type II error\ncontrol and can accurately identify watermarked sub-strings by finding the\ncorresponding change point locations. To validate our technique, we apply it to\ntexts generated by several language models with prompts extracted from Google's\nC4 dataset and obtain encouraging numerical results. We release all code\npublicly at https://github.com/doccstat/llm-watermark-cpd.\n","authors":["Xingchi Li","Guanxun Li","Xianyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.20670v1.pdf","comment":"25 pages, 12 figures, 2 tables, NeurIPS 2024"}]},"2024-11-04T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.02398v1","updated":"2024-11-04T18:59:51Z","published":"2024-11-04T18:59:51Z","title":"Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin\n  Script Languages","summary":"  Multilingual LLMs have achieved remarkable benchmark performance, but we find\nthey continue to underperform on non-Latin script languages across contemporary\nLLM families. This discrepancy arises from the fact that LLMs are pretrained\nwith orthographic scripts, which are dominated by Latin characters that obscure\ntheir shared phonology with non-Latin scripts. We propose leveraging phonemic\ntranscriptions as complementary signals to induce script-invariant\nrepresentations. Our study demonstrates that integrating phonemic signals\nimproves performance across both non-Latin and Latin languages, with a\nparticularly significant impact on closing the performance gap between the two.\nThrough detailed experiments, we show that phonemic and orthographic scripts\nretrieve distinct examples for in-context learning (ICL). This motivates our\nproposed Mixed-ICL retrieval strategy, where further aggregation leads to our\nsignificant performance improvements for both Latin script languages (up to\n12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL\nretrieval.\n","authors":["Hoang Nguyen","Khyati Mahajan","Vikas Yadav","Philip S. Yu","Masoud Hashemi","Rishabh Maheshwary"],"pdf_url":"https://arxiv.org/pdf/2411.02398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02391v1","updated":"2024-11-04T18:56:42Z","published":"2024-11-04T18:56:42Z","title":"Attacking Vision-Language Computer Agents via Pop-ups","summary":"  Autonomous agents powered by large vision and language models (VLM) have\ndemonstrated significant potential in completing daily computer tasks, such as\nbrowsing the web to book travel and operating desktop software, which requires\nagents to understand these interfaces. Despite such visual inputs becoming more\nintegrated into agentic applications, what types of risks and attacks exist\naround them still remain unclear. In this work, we demonstrate that VLM agents\ncan be easily attacked by a set of carefully designed adversarial pop-ups,\nwhich human users would typically recognize and ignore. This distraction leads\nagents to click these pop-ups instead of performing the tasks as usual.\nIntegrating these pop-ups into existing agent testing environments like OSWorld\nand VisualWebArena leads to an attack success rate (the frequency of the agent\nclicking the pop-ups) of 86% on average and decreases the task success rate by\n47%. Basic defense techniques such as asking the agent to ignore pop-ups or\nincluding an advertisement notice, are ineffective against the attack.\n","authors":["Yanzhe Zhang","Tao Yu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02391v1.pdf","comment":"10 pages, preprint"},{"id":"http://arxiv.org/abs/2411.02382v1","updated":"2024-11-04T18:50:00Z","published":"2024-11-04T18:50:00Z","title":"Improving Scientific Hypothesis Generation with Knowledge Grounded Large\n  Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious scientific domains, from natural language processing to complex\nproblem-solving tasks. Their ability to understand and generate human-like text\nhas opened up new possibilities for advancing scientific research, enabling\ntasks such as data analysis, literature review, and even experimental design.\nOne of the most promising applications of LLMs in this context is hypothesis\ngeneration, where they can identify novel research directions by analyzing\nexisting knowledge. However, despite their potential, LLMs are prone to\ngenerating ``hallucinations'', outputs that are plausible-sounding but\nfactually incorrect. Such a problem presents significant challenges in\nscientific fields that demand rigorous accuracy and verifiability, potentially\nleading to erroneous or misleading conclusions. To overcome these challenges,\nwe propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that\nenhances LLM hypothesis generation by integrating external, structured\nknowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured\nreasoning process, organizing their output as a chain of ideas (CoI), and\nincludes a KG-supported module for the detection of hallucinations. With\nexperiments on our newly constructed hypothesis generation dataset, we\ndemonstrate that KG-CoI not only improves the accuracy of LLM-generated\nhypotheses but also reduces the hallucination in their reasoning chains,\nhighlighting its effectiveness in advancing real-world scientific research.\n","authors":["Guangzhi Xiong","Eric Xie","Amir Hassan Shariatmadari","Sikun Guo","Stefan Bekiranov","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17717v4","updated":"2024-11-04T18:48:34Z","published":"2024-02-27T17:52:33Z","title":"AmbigNLG: Addressing Task Ambiguity in Instruction for NLG","summary":"  We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification.\n","authors":["Ayana Niwa","Hayate Iso"],"pdf_url":"https://arxiv.org/pdf/2402.17717v4.pdf","comment":"EMNLP 2024 (main)"},{"id":"http://arxiv.org/abs/2410.23262v2","updated":"2024-11-04T18:44:20Z","published":"2024-10-30T17:46:31Z","title":"EMMA: End-to-End Multimodal Model for Autonomous Driving","summary":"  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.\n","authors":["Jyh-Jing Hwang","Runsheng Xu","Hubert Lin","Wei-Chih Hung","Jingwei Ji","Kristy Choi","Di Huang","Tong He","Paul Covington","Benjamin Sapp","Yin Zhou","James Guo","Dragomir Anguelov","Mingxing Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23262v2.pdf","comment":"Blog post: https://waymo.com/blog/2024/10/introducing-emma/"},{"id":"http://arxiv.org/abs/2411.02348v1","updated":"2024-11-04T18:18:38Z","published":"2024-11-04T18:18:38Z","title":"Can Large Language Models generalize analogy solving like people can?","summary":"  When we solve an analogy we transfer information from a known context to a\nnew one through abstract rules and relational similarity. In people, the\nability to solve analogies such as \"body : feet :: table : ?\" emerges in\nchildhood, and appears to transfer easily to other domains, such as the visual\ndomain \"( : ) :: < : ?\". Recent research shows that large language models\n(LLMs) can solve various forms of analogies. However, can LLMs generalize\nanalogy solving to new domains like people can? To investigate this, we had\nchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b\n: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek\nalphabet), and a far transfer domain (list of symbols). As expected, children\nand adults easily generalized their knowledge to unfamiliar domains, whereas\nLLMs did not. This key difference between human and AI performance is evidence\nthat these LLMs still struggle with robust human-like analogical transfer.\n","authors":["Claire E. Stevenson","Alexandra Pafford","Han L. J. van der Maas","Melanie Mitchell"],"pdf_url":"https://arxiv.org/pdf/2411.02348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02344v1","updated":"2024-11-04T18:14:07Z","published":"2024-11-04T18:14:07Z","title":"Seq-VCR: Preventing Collapse in Intermediate Transformer Representations\n  for Enhanced Reasoning","summary":"  Decoder-only Transformers often struggle with complex reasoning tasks,\nparticularly arithmetic reasoning requiring multiple sequential operations. In\nthis work, we identify representation collapse in the model's intermediate\nlayers as a key factor limiting their reasoning capabilities. To address this,\nwe propose Sequential Variance-Covariance Regularization (Seq-VCR), which\nenhances the entropy of intermediate representations and prevents collapse.\nCombined with dummy pause tokens as substitutes for chain-of-thought (CoT)\ntokens, our method significantly improves performance in arithmetic reasoning\nproblems. In the challenging $5 \\times 5$ integer multiplication task, our\napproach achieves $99.5\\%$ exact match accuracy, outperforming models of the\nsame size (which yield $0\\%$ accuracy) and GPT-4 with five-shot CoT prompting\n($44\\%$). We also demonstrate superior results on arithmetic expression and\nlongest increasing subsequence (LIS) datasets. Our findings highlight the\nimportance of preventing intermediate layer representation collapse to enhance\nthe reasoning capabilities of Transformers and show that Seq-VCR offers an\neffective solution without requiring explicit CoT supervision.\n","authors":["Md Rifat Arefin","Gopeshh Subbaraj","Nicolas Gontier","Yann LeCun","Irina Rish","Ravid Shwartz-Ziv","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2411.02344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02337v1","updated":"2024-11-04T17:59:58Z","published":"2024-11-04T17:59:58Z","title":"WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning","summary":"  Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.\n","authors":["Zehan Qi","Xiao Liu","Iat Long Iong","Hanyu Lai","Xueqiao Sun","Xinyue Yang","Jiadai Sun","Yu Yang","Shuntian Yao","Tianjie Zhang","Wei Xu","Jie Tang","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2411.02337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02335v1","updated":"2024-11-04T17:59:04Z","published":"2024-11-04T17:59:04Z","title":"Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity","summary":"  Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable.\n","authors":["Yuqi Luo","Chenyang Song","Xu Han","Yingfa Chen","Chaojun Xiao","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.02335v1.pdf","comment":"23 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2411.00294v2","updated":"2024-11-04T17:57:43Z","published":"2024-11-01T01:11:58Z","title":"LLM-Ref: Enhancing Reference Handling in Technical Writing with Large\n  Language Models","summary":"  Large Language Models (LLMs) excel in data synthesis but can be inaccurate in\ndomain-specific tasks, which retrieval-augmented generation (RAG) systems\naddress by leveraging user-provided data. However, RAGs require optimization in\nboth retrieval and generation stages, which can affect output quality. In this\npaper, we present LLM-Ref, a writing assistant tool that aids researchers in\nwriting articles from multiple source documents with enhanced reference\nsynthesis and handling capabilities. Unlike traditional RAG systems that use\nchunking and indexing, our tool retrieves and generates content directly from\ntext paragraphs. This method facilitates direct reference extraction from the\ngenerated outputs, a feature unique to our tool. Additionally, our tool employs\niterative response generation, effectively managing lengthy contexts within the\nlanguage model's constraints. Compared to baseline RAG-based systems, our\napproach achieves a $3.25\\times$ to $6.26\\times$ increase in Ragas score, a\ncomprehensive metric that provides a holistic view of a RAG system's ability to\nproduce accurate, relevant, and contextually appropriate responses. This\nimprovement shows our method enhances the accuracy and contextual relevance of\nwriting assistance tools.\n","authors":["Kazi Ahmed Asif Fuad","Lizhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.00294v2.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.02316v1","updated":"2024-11-04T17:40:39Z","published":"2024-11-04T17:40:39Z","title":"Evaluating Creative Short Story Generation in Humans and Large Language\n  Models","summary":"  Storytelling is a fundamental aspect of human communication, relying heavily\non creativity to produce narratives that are novel, appropriate, and\nsurprising. While large language models (LLMs) have recently demonstrated the\nability to generate high-quality stories, their creative capabilities remain\nunderexplored. Previous research has either focused on creativity tests\nrequiring short responses or primarily compared model performance in story\ngeneration to that of professional writers. However, the question of whether\nLLMs exhibit creativity in writing short stories on par with the average human\nremains unanswered. In this work, we conduct a systematic analysis of\ncreativity in short story generation across LLMs and everyday people. Using a\nfive-sentence creative story task, commonly employed in psychology to assess\nhuman creativity, we automatically evaluate model- and human-generated stories\nacross several dimensions of creativity, including novelty, surprise, and\ndiversity. Our findings reveal that while LLMs can generate stylistically\ncomplex stories, they tend to fall short in terms of creativity when compared\nto average human writers.\n","authors":["Mete Ismayilzada","Claire Stevenson","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2411.02316v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2411.02310v1","updated":"2024-11-04T17:36:40Z","published":"2024-11-04T17:36:40Z","title":"MdEval: Massively Multilingual Code Debugging","summary":"  Code large language models (LLMs) have made significant progress in code\ndebugging by directly generating the correct code based on the buggy code\nsnippet. Programming benchmarks, typically consisting of buggy code snippet and\ntheir associated test cases, are used to assess the debugging capabilities of\nLLMs. However, many existing benchmarks primarily focus on Python and are often\nlimited in terms of language diversity (e.g., DebugBench and DebugEval). To\nadvance the field of multilingual debugging with LLMs, we propose the first\nmassively multilingual debugging benchmark, which includes 3.6K test samples of\n18 programming languages and covers the automated program repair (APR) task,\nthe code review (CR) task, and the bug identification (BI) task. Further, we\nintroduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs\ninto the correct multilingual queries and solutions (xDebugGen). Further, a\nmultilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong\nbaseline specifically to handle the bugs of a wide range of programming\nlanguages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\"\nin language C). Our extensive experiments on MDEVAL reveal a notable\nperformance gap between open-source models and closed-source LLMs (e.g., GPT\nand Claude series), highlighting huge room for improvement in multilingual code\ndebugging scenarios.\n","authors":["Shukai Liu","Linzheng Chai","Jian Yang","Jiajun Shi","He Zhu","Liran Wang","Ke Jin","Wei Zhang","Hualei Zhu","Shuyue Guo","Tao Sun","Jiaheng Liu","Yunlong Duan","Yu Hao","Liqun Yang","Guanglin Niu","Ge Zhang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2411.02310v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2407.14679v2","updated":"2024-11-04T17:36:38Z","published":"2024-07-19T21:47:57Z","title":"Compact Language Models via Pruning and Knowledge Distillation","summary":"  Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.\n","authors":["Saurav Muralidharan","Sharath Turuvekere Sreenivas","Raviraj Joshi","Marcin Chochowski","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2407.14679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02305v1","updated":"2024-11-04T17:30:51Z","published":"2024-11-04T17:30:51Z","title":"CRMArena: Understanding the Capacity of LLM Agents to Perform\n  Professional CRM Tasks in Realistic Environments","summary":"  Customer Relationship Management (CRM) systems are vital for modern\nenterprises, providing a foundation for managing customer interactions and\ndata. Integrating AI agents into CRM systems can automate routine processes and\nenhance personalized service. However, deploying and evaluating these agents is\nchallenging due to the lack of realistic benchmarks that reflect the complexity\nof real-world CRM tasks. To address this issue, we introduce CRMArena, a novel\nbenchmark designed to evaluate AI agents on realistic tasks grounded in\nprofessional work environments. Following guidance from CRM experts and\nindustry best practices, we designed CRMArena with nine customer service tasks\ndistributed across three personas: service agent, analyst, and manager. The\nbenchmark includes 16 commonly used industrial objects (e.g., account, order,\nknowledge article, case) with high interconnectivity, along with latent\nvariables (e.g., complaint habits, policy violations) to simulate realistic\ndata distributions. Experimental results reveal that state-of-the-art LLM\nagents succeed in less than 40% of the tasks with ReAct prompting, and less\nthan 55% even with function-calling abilities. Our findings highlight the need\nfor enhanced agent capabilities in function-calling and rule-following to be\ndeployed in real-world work environments. CRMArena is an open challenge to the\ncommunity: systems that can reliably complete tasks showcase direct business\nvalue in a popular work environment.\n","authors":["Kung-Hsiang Huang","Akshara Prabhakar","Sidharth Dhawan","Yixin Mao","Huan Wang","Silvio Savarese","Caiming Xiong","Philippe Laban","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2411.02305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02280v1","updated":"2024-11-04T17:09:10Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2411.00664v2","updated":"2024-11-04T17:05:58Z","published":"2024-11-01T15:28:03Z","title":"Optimizing Contextual Speech Recognition Using Vector Quantization for\n  Efficient Retrieval","summary":"  Neural contextual biasing allows speech recognition models to leverage\ncontextually relevant information, leading to improved transcription accuracy.\nHowever, the biasing mechanism is typically based on a cross-attention module\nbetween the audio and a catalogue of biasing entries, which means computational\ncomplexity can pose severe practical limitations on the size of the biasing\ncatalogue and consequently on accuracy improvements. This work proposes an\napproximation to cross-attention scoring based on vector quantization and\nenables compute- and memory-efficient use of large biasing catalogues. We\npropose to use this technique jointly with a retrieval based contextual biasing\napproach. First, we use an efficient quantized retrieval module to shortlist\nbiasing entries by grounding them on audio. Then we use retrieved entries for\nbiasing. Since the proposed approach is agnostic to the biasing method, we\ninvestigate using full cross-attention, LLM prompting, and a combination of the\ntwo. We show that retrieval based shortlisting allows the system to efficiently\nleverage biasing catalogues of several thousands of entries, resulting in up to\n71% relative error rate reduction in personal entity recognition. At the same\ntime, the proposed approximation algorithm reduces compute time by 20% and\nmemory usage by 85-95%, for lists of up to one million entries, when compared\nto standard dot-product cross-attention.\n","authors":["Nikolaos Flemotomos","Roger Hsiao","Pawel Swietojanski","Takaaki Hori","Dogan Can","Xiaodan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2411.00664v2.pdf","comment":"13 pages, 7 figures, submitted to IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing"},{"id":"http://arxiv.org/abs/2411.02272v1","updated":"2024-11-04T17:03:55Z","published":"2024-11-04T17:03:55Z","title":"Combining Induction and Transduction for Abstract Reasoning","summary":"  When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture.\n","authors":["Wen-Ding Li","Keya Hu","Carter Larsen","Yuqing Wu","Simon Alford","Caleb Woo","Spencer M. Dunn","Hao Tang","Michelangelo Naim","Dat Nguyen","Wei-Long Zheng","Zenna Tavares","Yewen Pu","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2411.02272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02265v1","updated":"2024-11-04T16:56:26Z","published":"2024-11-04T16:56:26Z","title":"Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent","summary":"  In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large\n","authors":["Xingwu Sun","Yanfeng Chen","Yiqing Huang","Ruobing Xie","Jiaqi Zhu","Kai Zhang","Shuaipeng Li","Zhen Yang","Jonny Han","Xiaobo Shu","Jiahao Bu","Zhongzhi Chen","Xuemeng Huang","Fengzong Lian","Saiyong Yang","Jianfeng Yan","Yuyuan Zeng","Xiaoqin Ren","Chao Yu","Lulu Wu","Yue Mao","Tao Yang","Suncong Zheng","Kan Wu","Dian Jiao","Jinbao Xue","Xipeng Zhang","Decheng Wu","Kai Liu","Dengpeng Wu","Guanghui Xu","Shaohua Chen","Shuang Chen","Xiao Feng","Yigeng Hong","Junqiang Zheng","Chengcheng Xu","Zongwei Li","Xiong Kuang","Jianglu Hu","Yiqi Chen","Yuchi Deng","Guiyang Li","Ao Liu","Chenchen Zhang","Shihui Hu","Zilong Zhao","Zifan Wu","Yao Ding","Weichao Wang","Han Liu","Roberts Wang","Hao Fei","Peijie She","Ze Zhao","Xun Cao","Hai Wang","Fusheng Xiang","Mengyuan Huang","Zhiyuan Xiong","Bin Hu","Xuebin Hou","Lei Jiang","Jiajia Wu","Yaping Deng","Yi Shen","Qian Wang","Weijie Liu","Jie Liu","Meng Chen","Liang Dong","Weiwen Jia","Hu Chen","Feifei Liu","Rui Yuan","Huilin Xu","Zhenxiang Yan","Tengfei Cao","Zhichao Hu","Xinhua Feng","Dong Du","Tinghao She","Yangyu Tao","Feng Zhang","Jianchen Zhu","Chengzhong Xu","Xirui Li","Chong Zha","Wen Ouyang","Yinben Xia","Xiang Li","Zekun He","Rongpeng Chen","Jiawei Song","Ruibin Chen","Fan Jiang","Chongqing Zhao","Bo Wang","Hao Gong","Rong Gan","Winston Hu","Zhanhui Kang","Yong Yang","Yuhong Liu","Di Wang","Jie Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.02265v1.pdf","comment":"17 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2407.15055v2","updated":"2024-11-04T16:56:13Z","published":"2024-07-21T04:52:38Z","title":"Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System\n  Without Turn-level Dialog Annotations","summary":"  Task-oriented dialogue (TOD) systems enable users to achieve their goals\nthrough natural language interactions. Traditionally, these systems have relied\non turn-level manually annotated metadata, such as dialogue states and policy\nannotations, which are expensive, time-consuming, and often inconsistent or\nerror-prone. This dependence limits the potential to leverage vast amounts of\nreadily available conversational data for training TOD systems. Additionally, a\ncritical challenge in TOD system design is determining when and how to access\nand integrate information from external sources. Current approaches typically\nexpect this information to be provided alongside the dialogue context, rather\nthan learning to identify and retrieve it autonomously. While pre-trained large\nlanguage models (LLMs) have been used to develop TOD systems, their potential\nto train such systems without laborious annotations remains largely unexplored.\nThis work employs multi-task instruction fine-tuning to create more efficient\nand scalable TOD systems that can effectively leverage natural language\nconversational data without manual annotations, while autonomously managing\nexternal information retrieval. Our extensive experimental evaluations, using\nthree diverse TOD datasets and three LLMs of varying sizes, demonstrate that\nour approach can generalize to new, unseen domains. Notably, our approach\noutperforms both state-of-the-art models trained on annotated data and\nbillion-scale parameter off-the-shelf ChatGPT models.\n","authors":["Adib Mosharrof","A. B. Siddique"],"pdf_url":"https://arxiv.org/pdf/2407.15055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19128v2","updated":"2024-11-04T16:44:42Z","published":"2024-10-24T19:56:28Z","title":"Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models","summary":"  Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval.\n","authors":["Guimin Hu","Hasti Seifi"],"pdf_url":"https://arxiv.org/pdf/2410.19128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02223v1","updated":"2024-11-04T16:15:28Z","published":"2024-11-04T16:15:28Z","title":"Positive Experience Reflection for Agents in Interactive Text\n  Environments","summary":"  Intelligent agents designed for interactive environments face significant\nchallenges in text-based games, a domain that demands complex reasoning and\nadaptability. While agents based on large language models (LLMs) using\nself-reflection have shown promise, they struggle when initially successful and\nexhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,\na novel approach that addresses these limitations in existing reflection\nmethods by incorporating positive experiences and managed memory to enrich the\ncontext available to the agent at decision time. Our comprehensive analysis\nspans both closed- and open-source LLMs and demonstrates the effectiveness of\nSweet&Sour in improving agent performance, particularly in scenarios where\nprevious approaches fall short.\n","authors":["Philip Lippmann","Matthijs T. J. Spaan","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02223v1.pdf","comment":"To appear at NeurIPS 2024 Language Gamification workshop"},{"id":"http://arxiv.org/abs/2411.02209v1","updated":"2024-11-04T16:01:43Z","published":"2024-11-04T16:01:43Z","title":"The Role of DevOps in Enhancing Enterprise Software Delivery Success\n  through R&D Efficiency and Source Code Management","summary":"  This study examines the impact of DevOps practices on enterprise software\ndelivery success, focusing on enhancing R&D efficiency and source code\nmanagement (SCM). Using a qualitative methodology, data were collected from\ncase studies of large-scale enterprises implementing DevOps to explore how\nthese practices streamline software development processes. Findings reveal that\nDevOps significantly improves R&D productivity by fostering cross-functional\ncollaboration, reducing development cycle times, and enhancing software quality\nthrough effective SCM practices, such as version control and continuous\nintegration. Additionally, SCM tools within DevOps enable precise change\ntracking and reliable code maintenance, further supporting faster, more robust\nsoftware delivery. However, the study identifies challenges, including cultural\nresistance and tool integration issues, that can hinder DevOps implementation.\nAdditionally, This research contributes to the growing body of DevOps\nliterature by highlighting the role of R&D efficiency and SCM as crucial\nfactors for software delivery success. Future studies should investigate these\nfactors across diverse industries to validate findings.\n","authors":["Jun Cui"],"pdf_url":"https://arxiv.org/pdf/2411.02209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17860v3","updated":"2024-11-04T15:59:19Z","published":"2024-03-26T16:49:25Z","title":"Illuminating Blind Spots of Language Models with Targeted\n  Agent-in-the-Loop Synthetic Data","summary":"  Language models (LMs) have achieved impressive accuracy across a variety of\ntasks but remain vulnerable to high-confidence misclassifications, also\nreferred to as unknown unknowns (UUs). These UUs cluster into blind spots in\nthe feature space, leading to significant risks in high-stakes applications.\nThis is particularly relevant for smaller, lightweight LMs that are more\nsusceptible to such errors. While the identification of UUs has been\nextensively studied, their mitigation remains an open challenge, including how\nto use identified UUs to eliminate unseen blind spots. In this work, we propose\na novel approach to address blind spot mitigation through the use of\nintelligent agents -- either humans or large LMs -- as teachers to characterize\nUU-type errors. By leveraging the generalization capabilities of intelligent\nagents, we identify patterns in high-confidence misclassifications and use them\nto generate targeted synthetic samples to improve model robustness and reduce\nblind spots. We conduct an extensive evaluation of our method on three\nclassification tasks and demonstrate its effectiveness in reducing the number\nof UUs, all while maintaining a similar level of accuracy. We find that the\neffectiveness of human computation has a high ceiling but is highly dependent\non familiarity with the underlying task. Moreover, the cost gap between humans\nand LMs surpasses an order of magnitude, as LMs attain human-like\ngeneralization and generation performance while being more scalable.\n","authors":["Philip Lippmann","Matthijs T. J. Spaan","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17860v3.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2404.15127v2","updated":"2024-11-04T15:54:21Z","published":"2024-04-23T15:27:19Z","title":"GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist\n  Collaboration","summary":"  Generalist foundation models (GFMs) are renowned for their exceptional\ncapability and flexibility in effectively generalizing across diverse tasks and\nmodalities. In the field of medicine, while GFMs exhibit superior\ngeneralizability based on their extensive intrinsic knowledge as well as\nproficiency in instruction following and in-context learning, specialist models\nexcel in precision due to their domain knowledge. In this work, for the first\ntime, we explore the synergy between the GFM and specialist models, to enable\nprecise medical image analysis on a broader scope. Specifically, we propose a\ncooperative framework, Generalist-Specialist Collaboration (GSCo), which\nconsists of two stages, namely the construction of GFM and specialists, and\ncollaborative inference on downstream tasks. In the construction stage, we\ndevelop MedDr, the largest open-source GFM tailored for medicine, showcasing\nexceptional instruction-following and in-context learning capabilities.\nMeanwhile, a series of lightweight specialists are crafted for downstream tasks\nwith low computational cost. In the collaborative inference stage, we introduce\ntwo cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented\nDiagnosis, to harvest the generalist's in-context learning abilities alongside\nthe specialists' domain expertise. For a comprehensive evaluation, we curate a\nlarge-scale benchmark featuring 28 datasets and about 250,000 images. Extensive\nresults demonstrate that MedDr consistently outperforms state-of-the-art GFMs\non downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists\nacross all out-of-domain disease diagnosis datasets. These findings indicate a\nsignificant paradigm shift in the application of GFMs, transitioning from\nseparate models for specific tasks to a collaborative approach between GFMs and\nspecialists, thereby advancing the frontiers of generalizable AI in medicine.\n","authors":["Sunan He","Yuxiang Nie","Hongmei Wang","Shu Yang","Yihui Wang","Zhiyuan Cai","Zhixuan Chen","Yingxue Xu","Luyang Luo","Huiling Xiang","Xi Lin","Mingxiang Wu","Yifan Peng","George Shih","Ziyang Xu","Xian Wu","Qiong Wang","Ronald Cheong Kin Chan","Varut Vardhanabhuti","Winnie Chiu Wing Chu","Yefeng Zheng","Pranav Rajpurkar","Kang Zhang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.15127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19266v3","updated":"2024-11-04T15:49:41Z","published":"2024-05-29T16:59:38Z","title":"PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications","summary":"  Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development.\n","authors":["Dingkang Yang","Jinjie Wei","Dongling Xiao","Shunli Wang","Tong Wu","Gang Li","Mingcheng Li","Shuaibing Wang","Jiawei Chen","Yue Jiang","Qingyao Xu","Ke Li","Peng Zhai","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.19266v3.pdf","comment":"Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical\n  Large Language Model"},{"id":"http://arxiv.org/abs/2411.02193v1","updated":"2024-11-04T15:46:20Z","published":"2024-11-04T15:46:20Z","title":"Improving Steering Vectors by Targeting Sparse Autoencoder Features","summary":"  To control the behavior of language models, steering methods attempt to\nensure that outputs of the model satisfy specific pre-defined properties.\nAdding steering vectors to the model is a promising method of model control\nthat is easier than finetuning, and may be more robust than prompting. However,\nit can be difficult to anticipate the effects of steering vectors produced by\nalmost all existing methods, such as CAA (Panickssery et al., 2024) or the\ndirect use of SAE latents (Templeton et al., 2024). In our work, we address\nthis issue by using SAEs to measure the effects of steering vectors, giving us\na method that can be used to understand the causal effect of any steering\nvector intervention. We use this method for measuring causal effects to develop\nan improved steering method, SAE-Targeted Steering (SAE-TS), which finds\nsteering vectors to target specific SAE features while minimizing unintended\nside effects. We show that overall, SAE-TS balances steering effects with\ncoherence better than CAA and SAE feature steering, when evaluated on a range\nof tasks.\n","authors":["Sviatoslav Chalnev","Matthew Siu","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2411.02193v1.pdf","comment":"8 maintext pages and 9 appendix pages"},{"id":"http://arxiv.org/abs/2411.00053v2","updated":"2024-11-04T15:20:16Z","published":"2024-10-30T19:09:02Z","title":"ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate","summary":"  Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models, frequently referred to as multi-agent debate\n(MAD). While debate shows promise as a means of improving model efficacy, most\nworks in this area treat debate as an emergent behavior, rather than a learned\nbehavior. In doing so, current debate frameworks rely on collaborative\nbehaviors to have been sufficiently trained into off-the-shelf models. To\naddress this limitation, we propose ACC-Debate, an Actor-Critic based learning\nframework to produce a two-agent team specialized in debate. We demonstrate\nthat ACC-Debate outperforms SotA debate techniques on a wide array of\nbenchmarks.\n","authors":["Andrew Estornell","Jean-Francois Ton","Yuanshun Yao","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.00053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17935v3","updated":"2024-11-04T15:07:18Z","published":"2024-05-28T08:01:26Z","title":"Tool Learning with Large Language Models: A Survey","summary":"  Recently, tool learning with large language models (LLMs) has emerged as a\npromising paradigm for augmenting the capabilities of LLMs to tackle highly\ncomplex problems. Despite growing attention and rapid advancements in this\nfield, the existing literature remains fragmented and lacks systematic\norganization, posing barriers to entry for newcomers. This gap motivates us to\nconduct a comprehensive survey of existing works on tool learning with LLMs. In\nthis survey, we focus on reviewing existing literature from the two primary\naspects (1) why tool learning is beneficial and (2) how tool learning is\nimplemented, enabling a comprehensive understanding of tool learning with LLMs.\nWe first explore the \"why\" by reviewing both the benefits of tool integration\nand the inherent benefits of the tool learning paradigm from six specific\naspects. In terms of \"how\", we systematically review the literature according\nto a taxonomy of four key stages in the tool learning workflow: task planning,\ntool selection, tool calling, and response generation. Additionally, we provide\na detailed summary of existing benchmarks and evaluation methods, categorizing\nthem according to their relevance to different stages. Finally, we discuss\ncurrent challenges and outline potential future directions, aiming to inspire\nboth researchers and industrial developers to further explore this emerging and\npromising area. We also maintain a GitHub repository to continually keep track\nof the relevant papers and resources in this rising area at\nhttps://github.com/quchangle1/LLM-Tool-Survey.\n","authors":["Changle Qu","Sunhao Dai","Xiaochi Wei","Hengyi Cai","Shuaiqiang Wang","Dawei Yin","Jun Xu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2405.17935v3.pdf","comment":"The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40678-2}"},{"id":"http://arxiv.org/abs/2411.02118v1","updated":"2024-11-04T14:30:57Z","published":"2024-11-04T14:30:57Z","title":"Grounding Emotional Descriptions to Electrovibration Haptic Signals","summary":"  Designing and displaying haptic signals with sensory and emotional attributes\ncan improve the user experience in various applications. Free-form user\nlanguage provides rich sensory and emotional information for haptic design\n(e.g., ``This signal feels smooth and exciting''), but little work exists on\nlinking user descriptions to haptic signals (i.e., language grounding). To\naddress this gap, we conducted a study where 12 users described the feel of 32\nsignals perceived on a surface haptics (i.e., electrovibration) display. We\ndeveloped a computational pipeline using natural language processing (NLP)\ntechniques, such as GPT-3.5 Turbo and word embedding methods, to extract\nsensory and emotional keywords and group them into semantic clusters (i.e.,\nconcepts). We linked the keyword clusters to haptic signal features (e.g.,\npulse count) using correlation analysis. The proposed pipeline demonstrates the\nviability of a computational approach to analyzing haptic experiences. We\ndiscuss our future plans for creating a predictive model of haptic experience.\n","authors":["Guimin Hu","Zirui Zhao","Lukas Heilmann","Yasemin Vardar","Hasti Seifi"],"pdf_url":"https://arxiv.org/pdf/2411.02118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02117v1","updated":"2024-11-04T14:29:49Z","published":"2024-11-04T14:29:49Z","title":"AVSS: Layer Importance Evaluation in Large Language Models via\n  Activation Variance-Sparsity Analysis","summary":"  The evaluation of layer importance in deep learning has been an active area\nof research, with significant implications for model optimization and\ninterpretability. Recently, large language models (LLMs) have gained prominence\nacross various domains, yet limited studies have explored the functional\nimportance and performance contributions of individual layers within LLMs,\nespecially from the perspective of activation distribution. In this work, we\npropose the Activation Variance-Sparsity Score (AVSS), a novel metric combining\nnormalized activation variance and sparsity to assess each layer's contribution\nto model performance. By identifying and removing approximately the lowest 25%\nof layers based on AVSS, we achieve over 90% of original model performance\nacross tasks such as question answering, language modeling, and sentiment\nclassification, indicating that these layers may be non-essential. Our approach\nprovides a systematic method for identifying less critical layers, contributing\nto efficient large language model architectures.\n","authors":["Zichen Song","Yuxin Wu","Sitan Huang","Zhongfeng Kang"],"pdf_url":"https://arxiv.org/pdf/2411.02117v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2411.02116v1","updated":"2024-11-04T14:29:28Z","published":"2024-11-04T14:29:28Z","title":"Advancements and limitations of LLMs in replicating human color-word\n  associations","summary":"  Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT- 4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category, particularly\nwhen using visual inputs rather than text-based color codes. However, the\nhighest median performance was approximately 50% even for GPT4-o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.\n","authors":["Makoto Fukushima","Shusuke Eshita","Hiroshige Fukuhara"],"pdf_url":"https://arxiv.org/pdf/2411.02116v1.pdf","comment":"20 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2405.17992v2","updated":"2024-11-04T14:01:50Z","published":"2024-05-28T09:24:52Z","title":"fMRI predictors based on language models of increasing complexity\n  recover brain left lateralization","summary":"  Over the past decade, studies of naturalistic language processing where\nparticipants are scanned while listening to continuous text have flourished.\nUsing word embeddings at first, then large language models, researchers have\ncreated encoding models to analyze the brain signals. Presenting these models\nwith the same text as the participants allows to identify brain areas where\nthere is a significant correlation between the functional magnetic resonance\nimaging (fMRI) time series and the ones predicted by the models' artificial\nneurons. One intriguing finding from these studies is that they have revealed\nhighly symmetric bilateral activation patterns, somewhat at odds with the\nwell-known left lateralization of language processing. Here, we report analyses\nof an fMRI dataset where we manipulate the complexity of large language models,\ntesting 28 pretrained models from 8 different families, ranging from 124M to\n14.2B parameters. First, we observe that the performance of models in\npredicting brain responses follows a scaling law, where the fit with brain\nactivity increases linearly with the logarithm of the number of parameters of\nthe model (and its performance on natural language processing tasks). Second,\nalthough this effect is present in both hemispheres, it is stronger in the left\nthan in the right hemisphere. Specifically, the left-right difference in brain\ncorrelation follows a scaling law with the number of parameters. This finding\nreconciles computational analyses of brain activity using large language models\nwith the classic observation from aphasic patients showing left hemisphere\ndominance for language.\n","authors":["Laurent Bonnasse-Gahot","Christophe Pallier"],"pdf_url":"https://arxiv.org/pdf/2405.17992v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2410.06511v2","updated":"2024-11-04T13:52:23Z","published":"2024-10-09T03:26:11Z","title":"TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training","summary":"  The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines.\n","authors":["Wanchao Liang","Tianyu Liu","Less Wright","Will Constable","Andrew Gu","Chien-Chin Huang","Iris Zhang","Wei Feng","Howard Huang","Junjie Wang","Sanket Purandare","Gokul Nadathur","Stratos Idreos"],"pdf_url":"https://arxiv.org/pdf/2410.06511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02083v1","updated":"2024-11-04T13:43:24Z","published":"2024-11-04T13:43:24Z","title":"Regress, Don't Guess -- A Regression-like Loss on Number Tokens for\n  Language Models","summary":"  While language models have exceptional capabilities at text generation, they\nlack a natural inductive bias for emitting numbers and thus struggle in tasks\ninvolving reasoning over quantities, especially arithmetics. This has\nparticular relevance in scientific datasets where combinations of text and\nnumerical data are abundant. One fundamental limitation is the nature of the CE\nloss, which assumes a nominal (categorical) scale and thus cannot convey\nproximity between generated number tokens. As a remedy, we here present two\nversions of a number token loss. The first is based on an $L_p$ loss between\nthe ground truth token value and the weighted sum of the predicted class\nprobabilities. The second loss minimizes the Wasserstein-1 distance between the\ndistribution of the predicted output probabilities and the ground truth\ndistribution. These regression-like losses can easily be added to any language\nmodel and extend the CE objective during training. We compare the proposed\nschemes on a mathematics dataset against existing tokenization, encoding, and\ndecoding schemes for improving number representation in language models. Our\nresults reveal a significant improvement in numerical accuracy when equipping a\nstandard T5 model with the proposed loss schemes.\n","authors":["Jonas Zausinger","Lars Pennig","Kacper Chlodny","Vincent Limbach","Anna Ketteler","Thorben Prein","Vishwa Mohan Singh","Michael Morris Danziger","Jannis Born"],"pdf_url":"https://arxiv.org/pdf/2411.02083v1.pdf","comment":"5-page version for NeurIPS 2024 (MathAI workshop)"},{"id":"http://arxiv.org/abs/2408.01963v4","updated":"2024-11-04T13:32:40Z","published":"2024-08-04T08:43:09Z","title":"A Novel Metric for Measuring the Robustness of Large Language Models in\n  Non-adversarial Scenarios","summary":"  We evaluate the robustness of several large language models on multiple\ndatasets. Robustness here refers to the relative insensitivity of the model's\nanswers to meaning-preserving variants of their input. Benchmark datasets are\nconstructed by introducing naturally-occurring, non-malicious perturbations, or\nby generating semantically equivalent paraphrases of input questions or\nstatements. We further propose a novel metric for assessing a model robustness,\nand demonstrate its benefits in the non-adversarial scenario by empirical\nevaluation of several models on the created datasets.\n","authors":["Samuel Ackerman","Ella Rabinovich","Eitan Farchi","Ateret Anaby-Tavor"],"pdf_url":"https://arxiv.org/pdf/2408.01963v4.pdf","comment":"Published in the 2024 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP) findings"},{"id":"http://arxiv.org/abs/2401.11944v4","updated":"2024-11-04T13:28:48Z","published":"2024-01-22T13:34:34Z","title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.\n","authors":["Ge Zhang","Xinrun Du","Bei Chen","Yiming Liang","Tongxu Luo","Tianyu Zheng","Kang Zhu","Yuyang Cheng","Chunpu Xu","Shuyue Guo","Haoran Zhang","Xingwei Qu","Junjie Wang","Ruibin Yuan","Yizhi Li","Zekun Wang","Yudong Liu","Yu-Hsuan Tsai","Fengji Zhang","Chenghua Lin","Wenhao Huang","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2401.11944v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09952v2","updated":"2024-11-04T13:26:07Z","published":"2024-06-14T11:58:49Z","title":"BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval","summary":"  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work, we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts significantly improves over the base\nmodel in SugarCrepe and in BiVLC for both retrieval directions. The gap to\nhuman performance in BiVLC confirms that Vision-Language Compositionality is\nstill a challenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n","authors":["Imanol Miranda","Ander Salaberria","Eneko Agirre","Gorka Azkune"],"pdf_url":"https://arxiv.org/pdf/2406.09952v2.pdf","comment":"Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page\n  at: https://imirandam.github.io/BiVLC_project_page/"},{"id":"http://arxiv.org/abs/2407.15612v3","updated":"2024-11-04T13:25:31Z","published":"2024-07-22T13:14:27Z","title":"Can GPT-4 learn to analyse moves in research article abstracts?","summary":"  One of the most powerful and enduring ideas in written discourse analysis is\nthat genres can be described in terms of the moves which structure a writer's\npurpose. Considerable research has sought to identify these distinct\ncommunicative acts, but analyses have been beset by problems of subjectivity,\nreliability and the time-consuming need for multiple coders to confirm\nanalyses. In this paper we employ the affordances of GPT-4 to automate the\nannotation process by using natural language prompts. Focusing on abstracts\nfrom articles in four applied linguistics journals, we devise prompts which\nenable the model to identify moves effectively. The annotated outputs of these\nprompts were evaluated by two assessors with a third addressing disagreements.\nThe results show that an 8-shot prompt was more effective than one using two,\nconfirming that the inclusion of examples illustrating areas of variability can\nenhance GPT-4's ability to recognize multiple moves in a single sentence and\nreduce bias related to textual position. We suggest that GPT-4 offers\nconsiderable potential in automating this annotation process, when human actors\nwith domain specific linguistic expertise inform the prompting process.\n","authors":["Danni Yu","Marina Bondi","Ken Hyland"],"pdf_url":"https://arxiv.org/pdf/2407.15612v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00533v2","updated":"2024-11-04T13:15:56Z","published":"2024-11-01T12:08:08Z","title":"ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot\n  Named Entity Recognition with Large Language Models","summary":"  This paper presents ReverseNER, a framework aimed at overcoming the\nlimitations of large language models (LLMs) in zero-shot Named Entity\nRecognition (NER) tasks, particularly in cases where certain entity types have\nambiguous boundaries. ReverseNER tackles this challenge by constructing a\nreliable example library with the reversed process of NER. Rather than\nbeginning with sentences, this method uses an LLM to generate entities based on\ntheir definitions and then expands them into full sentences. During sentence\ngeneration, the LLM is guided to replicate the structure of a specific 'feature\nsentence', extracted from the task sentences by clustering. This results in\nwell-annotated sentences with clearly labeled entities, while preserving\nsemantic and structural similarity to the task sentences. Once the example\nlibrary is constructed, the method selects the most semantically similar\nexample labels for each task sentence to support the LLM's inference. We also\npropose an entity-level self-consistency scoring mechanism to improve NER\nperformance with LLMs. Experiments show that ReverseNER significantly\noutperforms traditional zero-shot NER with LLMs and surpasses several few-shot\nmethods, marking a notable improvement in NER for domains with limited labeled\ndata.\n","authors":["Anbang Wang"],"pdf_url":"https://arxiv.org/pdf/2411.00533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02063v1","updated":"2024-11-04T13:06:17Z","published":"2024-11-04T13:06:17Z","title":"Scalable Efficient Training of Large Language Models with\n  Low-dimensional Projected Attention","summary":"  Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer.\n","authors":["Xingtai Lv","Ning Ding","Kaiyan Zhang","Ermo Hua","Ganqu Cui","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.02063v1.pdf","comment":"Accepted to EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.04317v2","updated":"2024-11-04T13:02:33Z","published":"2024-03-07T08:34:57Z","title":"Online Adaptation of Language Models with a Memory of Amortized Contexts","summary":"  Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. To\naddress the crucial need to keep models updated, online learning has emerged as\na critical tool when utilizing LLMs for real-world applications. However, given\nthe ever-expanding corpus of unseen documents and the large parameter space of\nmodern LLMs, efficient adaptation is essential. To address these challenges, we\npropose Memory of Amortized Contexts (MAC), an efficient and effective online\nadaptation framework for LLMs with strong knowledge retention. We propose a\nfeature extraction and memory-augmentation approach to compress and extract\ninformation from new documents into compact modulations stored in a memory\nbank. When answering questions, our model attends to and extracts relevant\nknowledge from this memory bank. To learn informative modulations in an\nefficient manner, we utilize amortization-based meta-learning, which\nsubstitutes an otherwise required optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. In addition, we show how MAC can be combined with and improve the\nperformance of popular alternatives such as retrieval augmented generations\n(RAGs). Code is available at: https://github.com/jihoontack/MAC.\n","authors":["Jihoon Tack","Jaehyung Kim","Eric Mitchell","Jinwoo Shin","Yee Whye Teh","Jonathan Richard Schwarz"],"pdf_url":"https://arxiv.org/pdf/2403.04317v2.pdf","comment":"Published as a conference proceeding for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.11295v5","updated":"2024-11-04T12:55:44Z","published":"2024-02-17T14:26:57Z","title":"OneBit: Towards Extremely Low-bit Large Language Models","summary":"  Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.\n","authors":["Yuzhuang Xu","Xu Han","Zonghan Yang","Shuo Wang","Qingfu Zhu","Zhiyuan Liu","Weidong Liu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2402.11295v5.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.10724v3","updated":"2024-11-04T12:42:18Z","published":"2024-08-20T10:45:36Z","title":"Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian","summary":"  In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages.\n","authors":["Cem Üyük","Danica Rovó","Shaghayegh Kolli","Rabia Varol","Georg Groh","Daryna Dementieva"],"pdf_url":"https://arxiv.org/pdf/2408.10724v3.pdf","comment":"EMNLP 2024 NLP4PI Workshop"},{"id":"http://arxiv.org/abs/2410.13409v2","updated":"2024-11-04T12:40:19Z","published":"2024-10-17T10:16:56Z","title":"Attr-Int: A Simple and Effective Entity Alignment Framework for\n  Heterogeneous Knowledge Graphs","summary":"  Entity alignment (EA) refers to the task of linking entities in different\nknowledge graphs (KGs). Existing EA methods rely heavily on structural\nisomorphism. However, in real-world KGs, aligned entities usually have\nnon-isomorphic neighborhood structures, which paralyses the application of\nthese structure-dependent methods. In this paper, we investigate and tackle the\nproblem of entity alignment between heterogeneous KGs. First, we propose two\nnew benchmarks to closely simulate real-world EA scenarios of heterogeneity.\nThen we conduct extensive experiments to evaluate the performance of\nrepresentative EA methods on the new benchmarks. Finally, we propose a simple\nand effective entity alignment framework called Attr-Int, in which innovative\nattribute information interaction methods can be seamlessly integrated with any\nembedding encoder for entity alignment, improving the performance of existing\nentity alignment techniques. Experiments demonstrate that our framework\noutperforms the state-of-the-art approaches on two new benchmarks.\n","authors":["Linyan Yang","Jingwei Cheng","Chuanhao Xu","Xihao Wang","Jiayi Li","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02036v1","updated":"2024-11-04T12:38:08Z","published":"2024-11-04T12:38:08Z","title":"Explainable cognitive decline detection in free dialogues with a Machine\n  Learning approach based on pre-trained Large Language Models","summary":"  Cognitive and neurological impairments are very common, but only a small\nproportion of affected individuals are diagnosed and treated, partly because of\nthe high costs associated with frequent screening. Detecting pre-illness stages\nand analyzing the progression of neurological disorders through effective and\nefficient intelligent systems can be beneficial for timely diagnosis and early\nintervention. We propose using Large Language Models to extract features from\nfree dialogues to detect cognitive decline. These features comprise high-level\nreasoning content-independent features (such as comprehension, decreased\nawareness, increased distraction, and memory problems). Our solution comprises\n(i) preprocessing, (ii) feature engineering via Natural Language Processing\ntechniques and prompt engineering, (iii) feature analysis and selection to\noptimize performance, and (iv) classification, supported by automatic\nexplainability. We also explore how to improve Chatgpt's direct cognitive\nimpairment prediction capabilities using the best features in our models.\nEvaluation metrics obtained endorse the effectiveness of a mixed approach\ncombining feature extraction with Chatgpt and a specialized Machine Learning\nmodel to detect cognitive decline within free-form conversational dialogues\nwith older adults. Ultimately, our work may facilitate the development of an\ninexpensive, non-invasive, and rapid means of detecting and explaining\ncognitive decline.\n","authors":["Francisco de Arriba-Pérez","Silvia García-Méndez","Javier Otero-Mosquera","Francisco J. González-Castaño"],"pdf_url":"https://arxiv.org/pdf/2411.02036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11430v2","updated":"2024-11-04T12:21:52Z","published":"2024-05-19T03:08:02Z","title":"MHPP: Exploring the Capabilities and Limitations of Language Models\n  Beyond Basic Code Generation","summary":"  Recent advancements in large language models (LLMs) have greatly improved\ncode generation, specifically at the function level. For instance, GPT-4o has\nachieved a 91.0\\% pass rate on HumanEval. However, this draws into question the\nadequacy of existing benchmarks in thoroughly assessing function-level code\ngeneration capabilities. Our study analyzed two common benchmarks, HumanEval\nand MBPP, and found that these might not thoroughly evaluate LLMs' code\ngeneration capacities due to limitations in quality, difficulty, and\ngranularity. To resolve this, we introduce the Mostly Hard Python Problems\n(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on\nthe combination of natural language and code reasoning, MHPP gauges LLMs'\nabilities to comprehend specifications and restrictions, engage in multi-step\nreasoning, and apply coding knowledge effectively. Initial evaluations of 26\nLLMs using MHPP showed many high-performing models on HumanEval failed to\nachieve similar success on MHPP. Moreover, MHPP highlighted various previously\nundiscovered limitations within various LLMs, leading us to believe that it\ncould pave the way for a better understanding of LLMs' capabilities and\nlimitations. MHPP, evaluation pipeline, and leaderboard can be found in\nhttps://github.com/SparksofAGI/MHPP.\n","authors":["Jianbo Dai","Jianqiao Lu","Yunlong Feng","Dong Huang","Guangtao Zeng","Rongju Ruan","Ming Cheng","Haochen Tan","Zhijiang Guo"],"pdf_url":"https://arxiv.org/pdf/2405.11430v2.pdf","comment":"43 pages, dataset and code are available at\n  https://github.com/SparksofAGI/MHPP, leaderboard can be found at\n  https://sparksofagi.github.io/MHPP/"},{"id":"http://arxiv.org/abs/2408.11327v2","updated":"2024-11-04T12:17:42Z","published":"2024-08-21T04:20:55Z","title":"Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking\n  Across Diverse Vocabularies","summary":"  Recent advancements in NLP have resulted in models with specialized\nstrengths, such as processing multimodal inputs or excelling in specific\ndomains. However, real-world tasks, like multimodal translation, often require\na combination of these strengths, such as handling both translation and image\nprocessing. While individual translation and vision models are powerful, they\ntypically lack the ability to perform both tasks in a single system. Combining\nthese models poses challenges, particularly due to differences in their\nvocabularies, which limit the effectiveness of traditional ensemble methods to\npost-generation techniques like N-best list re-ranking. In this work, we\npropose a novel zero-shot ensembling strategy that allows for the integration\nof different models during the decoding phase without the need for additional\ntraining. Our approach re-ranks beams during decoding by combining scores at\nthe word level, using heuristics to predict when a word is completed. We\ndemonstrate the effectiveness of this method in machine translation scenarios,\nshowing that it enables the generation of translations that are both speech-\nand image-aware while also improving overall translation quality (We will\nrelease the code upon paper acceptance.).\n","authors":["Sai Koneru","Matthias Huck","Miriam Exel","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2408.11327v2.pdf","comment":"WMT 2024"},{"id":"http://arxiv.org/abs/2407.03227v2","updated":"2024-11-04T12:14:13Z","published":"2024-07-03T15:55:14Z","title":"Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and\n  Schema Pruning","summary":"  We focus on Text-to-SQL semantic parsing from the perspective of\nretrieval-augmented generation. Motivated by challenges related to the size of\ncommercial database schemata and the deployability of business intelligence\nsolutions, we propose $\\text{ASTReS}$ that dynamically retrieves input database\ninformation and uses abstract syntax trees to select few-shot examples for\nin-context learning.\n  Furthermore, we investigate the extent to which an in-parallel semantic\nparser can be leveraged for generating approximated versions of the expected\nSQL queries, to support our retrieval. We take this approach to the extreme--we\nadapt a model consisting of less than $500$M parameters, to act as an extremely\nefficient approximator, enhancing it with the ability to process schemata in a\nparallelised manner. We apply $\\text{ASTReS}$ to monolingual and cross-lingual\nbenchmarks for semantic parsing, showing improvements over state-of-the-art\nbaselines. Comprehensive experiments highlight the contribution of modules\ninvolved in this retrieval-augmented generation setting, revealing interesting\ndirections for future work.\n","authors":["Zhili Shen","Pavlos Vougiouklis","Chenxin Diao","Kaustubh Vyas","Yuanyi Ji","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2407.03227v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2411.02018v1","updated":"2024-11-04T12:13:04Z","published":"2024-11-04T12:13:04Z","title":"Shortcut Learning in In-Context Learning: A Survey","summary":"  Shortcut learning refers to the phenomenon where models employ simple,\nnon-robust decision rules in practical tasks, which hinders their\ngeneralization and robustness. With the rapid development of large language\nmodels (LLMs) in recent years, an increasing number of studies have shown the\nimpact of shortcut learning on LLMs. This paper provides a novel perspective to\nreview relevant research on shortcut learning in In-Context Learning (ICL). It\nconducts a detailed exploration of the types of shortcuts in ICL tasks, their\ncauses, available benchmarks, and strategies for mitigating shortcuts. Based on\ncorresponding observations, it summarizes the unresolved issues in existing\nresearch and attempts to outline the future research landscape of shortcut\nlearning.\n","authors":["Rui Song","Yingji Li","Fausto Giunchiglia","Hao Xu"],"pdf_url":"https://arxiv.org/pdf/2411.02018v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.01996v1","updated":"2024-11-04T11:31:18Z","published":"2024-11-04T11:31:18Z","title":"Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task","summary":"  The advent of Large Language Models (LLMs) have shown promise in various\ncreative domains, including culinary arts. However, many LLMs still struggle to\ndeliver the desired level of culinary creativity, especially when tasked with\nadapting recipes to meet specific cultural requirements. This study focuses on\ncuisine transfer-applying elements of one cuisine to another-to assess LLMs'\nculinary creativity. We employ a diverse set of LLMs to generate and evaluate\nculturally adapted recipes, comparing their evaluations against LLM and human\njudgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark\nto evaluate LLMs' recipe generation abilities in the cuisine transfer task,\nassessing their cultural accuracy and creativity in the culinary domain. Our\nfindings reveal crucial insights into both generative and evaluative\ncapabilities of LLMs in the culinary domain, highlighting strengths and\nlimitations in understanding and applying cultural nuances in recipe creation.\nThe code and dataset used in this project will be openly available in\n\\url{http://github.com/dmis-lab/CulinaryASH}.\n","authors":["Hoonick Lee","Mogan Gim","Donghyeon Park","Donghee Choi","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2411.01996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19803v2","updated":"2024-11-04T11:28:18Z","published":"2024-06-28T10:24:31Z","title":"Scalable and Domain-General Abstractive Proposition Segmentation","summary":"  Segmenting text into fine-grained units of meaning is important to a wide\nrange of NLP applications. The default approach of segmenting text into\nsentences is often insufficient, especially since sentences are usually complex\nenough to include multiple units of meaning that merit separate treatment in\nthe downstream task. We focus on the task of abstractive proposition\nsegmentation (APS): transforming text into simple, self-contained, well-formed\nsentences. Several recent works have demonstrated the utility of proposition\nsegmentation with few-shot prompted LLMs for downstream tasks such as\nretrieval-augmented grounding and fact verification. However, this approach\ndoes not scale to large amounts of text and may not always extract all the\nfacts from the input text. In this paper, we first introduce evaluation metrics\nfor the task to measure several dimensions of quality. We then propose a\nscalable, yet accurate, proposition segmentation model. We model proposition\nsegmentation as a supervised task by training LLMs on existing annotated\ndatasets and show that training yields significantly improved results. We\nfurther show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as\nteachers for annotating large amounts of multi-domain synthetic distillation\ndata, we can train smaller student models (Gemma 1 2B and 7B) with results\nsimilar to the teacher LLMs. We then demonstrate that our technique leads to\neffective domain generalization, by annotating data in two domains outside the\noriginal training data and evaluating on them. Finally, as a key contribution\nof the paper, we share an easy-to-use API for NLP practitioners to use.\n","authors":["Mohammad Javad Hosseini","Yang Gao","Tim Baumgärtner","Alex Fabrikant","Reinald Kim Amplayo"],"pdf_url":"https://arxiv.org/pdf/2406.19803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07099v2","updated":"2024-11-04T11:15:28Z","published":"2024-06-18T07:46:13Z","title":"Nash CoT: Multi-Path Inference with Preference Equilibrium","summary":"  Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na competitive system on each path that balances the generation from\nrole-specific LLMs' and the general LLMs' generation, thereby ensuring both\neffective role adoption and diversity in LLM generation further maintaining the\nperformance of multi-path inference while reducing the requirement of the\nnumber of inference paths. We evaluate Nash CoT across various inference tasks,\nincluding Arabic Reasoning, Commonsense Question Answering, and Symbolic\nInference, achieving results that are comparable to or better than those of\nmulti-path CoT with the equal number of inference paths.\n","authors":["Ziqi Zhang","Cunxiang Wang","Xiong Xiao","Yue Zhang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07217v2","updated":"2024-11-04T11:06:40Z","published":"2024-06-11T12:50:53Z","title":"A Synthetic Dataset for Personal Attribute Inference","summary":"  Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users world-wide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose -- the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. We take two steps to\naddress this problem: (i) we construct a simulation framework for the popular\nsocial media platform Reddit using LLM agents seeded with synthetic personal\nprofiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic\ndataset of over 7800 comments manually labeled for personal attributes. We\nvalidate our dataset with a human study showing that humans barely outperform\nrandom guessing on the task of distinguishing our synthetic comments from real\nones. Further, we verify that our dataset enables meaningful personal attribute\ninference research by showing across 18 state-of-the-art LLMs that our\nsynthetic comments allow us to draw the same conclusions as real-world data.\nCombined, our experimental results, dataset and pipeline form a strong basis\nfor future privacy-preserving research geared towards understanding and\nmitigating inference-based privacy threats that LLMs pose.\n","authors":["Hanna Yukhymenko","Robin Staab","Mark Vero","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2406.07217v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05188v2","updated":"2024-11-04T10:42:01Z","published":"2024-04-08T04:30:33Z","title":"Have You Merged My Model? On The Robustness of Large Language Model IP\n  Protection Methods Against Model Merging","summary":"  Model merging is a promising lightweight model empowerment technique that\ndoes not rely on expensive computing devices (e.g., GPUs) or require the\ncollection of specific training data. Instead, it involves editing different\nupstream model parameters to absorb their downstream task capabilities.\nHowever, uncertified model merging can infringe upon the Intellectual Property\n(IP) rights of the original upstream models. In this paper, we conduct the\nfirst study on the robustness of IP protection methods under model merging\nscenarios. Specifically, we investigate two state-of-the-art IP protection\ntechniques: Quantization Watermarking and Instructional Fingerprint, along with\nvarious advanced model merging technologies, such as Task Arithmetic,\nTIES-MERGING, and so on. Experimental results indicate that current Large\nLanguage Model (LLM) watermarking techniques cannot survive in the merged\nmodels, whereas model fingerprinting techniques can. Our research aims to\nhighlight that model merging should be an indispensable consideration in the\nrobustness assessment of model IP protection techniques, thereby promoting the\nhealthy development of the open-source LLM community. Our code is available at\nhttps://github.com/ThuCCSLab/MergeGuard.\n","authors":["Tianshuo Cong","Delong Ran","Zesen Liu","Xinlei He","Jinyuan Liu","Yichen Gong","Qi Li","Anyu Wang","Xiaoyun Wang"],"pdf_url":"https://arxiv.org/pdf/2404.05188v2.pdf","comment":"Accepted by ACM CCS-LAMPS 2024 (Best Paper Award)"},{"id":"http://arxiv.org/abs/2407.11930v3","updated":"2024-11-04T10:30:16Z","published":"2024-07-16T17:23:16Z","title":"Localizing and Mitigating Errors in Long-form Question Answering","summary":"  Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nerrors and improves answer quality across multiple models. Furthermore, humans\nfind answers generated by our approach comprehensive and highly prefer them\n(84%) over the baseline answers.\n","authors":["Rachneet Sachdeva","Yixiao Song","Mohit Iyyer","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.11930v3.pdf","comment":"Code and data are available:\n  https://github.com/UKPLab/arxiv2024-lfqa-hallucination"},{"id":"http://arxiv.org/abs/2406.15227v3","updated":"2024-11-04T09:56:55Z","published":"2024-06-21T15:11:33Z","title":"A LLM-Based Ranking Method for the Evaluation of Automatic\n  Counter-Narrative Generation","summary":"  This paper proposes a novel approach to evaluate Counter Narrative (CN)\ngeneration using a Large Language Model (LLM) as an evaluator. We show that\ntraditional automatic metrics correlate poorly with human judgements and fail\nto capture the nuanced relationship between generated CNs and human perception.\nTo alleviate this, we introduce a model ranking pipeline based on pairwise\ncomparisons of generated CNs from different models, organized in a\ntournament-style format. The proposed evaluation method achieves a high\ncorrelation with human preference, with a $\\rho$ score of 0.88. As an\nadditional contribution, we leverage LLMs as zero-shot CN generators and\nprovide a comparative analysis of chat, instruct, and base models, exploring\ntheir respective strengths and limitations. Through meticulous evaluation,\nincluding fine-tuning experiments, we elucidate the differences in performance\nand responsiveness to domain-specific data. We conclude that chat-aligned\nmodels in zero-shot are the best option for carrying out the task, provided\nthey do not refuse to generate an answer due to security concerns.\n","authors":["Irune Zubiaga","Aitor Soroa","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2406.15227v3.pdf","comment":"Accepted for Findings of the Association for Computational\n  Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.12342v2","updated":"2024-11-04T09:52:25Z","published":"2024-07-17T06:36:09Z","title":"Word Embedding Dimension Reduction via Weakly-Supervised Feature\n  Selection","summary":"  As a fundamental task in natural language processing, word embedding converts\neach word into a representation in a vector space. A challenge with word\nembedding is that as the vocabulary grows, the vector space's dimension\nincreases, which can lead to a vast model size. Storing and processing word\nvectors are resource-demanding, especially for mobile edge-devices\napplications. This paper explores word embedding dimension reduction. To\nbalance computational costs and performance, we propose an efficient and\neffective weakly-supervised feature selection method named WordFS. It has two\nvariants, each utilizing novel criteria for feature selection. Experiments on\nvarious tasks (e.g., word and sentence similarity and binary and multi-class\nclassification) indicate that the proposed WordFS model outperforms other\ndimension reduction methods at lower computational costs. We have released the\ncode for reproducibility along with the paper.\n","authors":["Jintang Xue","Yun-Cheng Wang","Chengwei Wei","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2407.12342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16528v4","updated":"2024-11-04T09:50:00Z","published":"2024-05-26T11:29:57Z","title":"LoQT: Low-Rank Adapters for Quantized Pretraining","summary":"  Despite advances using low-rank adapters and quantization, pretraining of\nlarge models on consumer hardware has not been possible without model sharding,\noffloading during training, or per-layer gradient updates. To address these\nlimitations, we propose Low-Rank Adapters for Quantized Training (LoQT), a\nmethod for efficiently training quantized models. LoQT uses gradient-based\ntensor factorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning models. We demonstrate this for\nlanguage modeling and downstream task adaptation, finding that LoQT enables\nefficient training of models up to 7B parameters on a 24GB GPU. We also\ndemonstrate the feasibility of training a 13B model using per-layer gradient\nupdates on the same hardware.\n","authors":["Sebastian Loeschcke","Mads Toftrup","Michael J. Kastoryano","Serge Belongie","Vésteinn Snæbjarnarson"],"pdf_url":"https://arxiv.org/pdf/2405.16528v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02115v2","updated":"2024-11-04T09:32:30Z","published":"2024-04-02T17:18:48Z","title":"GINopic: Topic Modeling with Graph Isomorphism Network","summary":"  Topic modeling is a widely used approach for analyzing and exploring large\ndocument collections. Recent research efforts have incorporated pre-trained\ncontextualized language models, such as BERT embeddings, into topic modeling.\nHowever, they often neglect the intrinsic informational value conveyed by\nmutual dependencies between words. In this study, we introduce GINopic, a topic\nmodeling framework based on graph isomorphism networks to capture the\ncorrelation between words. By conducting intrinsic (quantitative as well as\nqualitative) and extrinsic evaluations on diverse benchmark datasets, we\ndemonstrate the effectiveness of GINopic compared to existing topic models and\nhighlight its potential for advancing topic modeling.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2404.02115v2.pdf","comment":"Accepted as a long paper for NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2403.08312v3","updated":"2024-11-04T09:17:45Z","published":"2024-03-13T07:44:14Z","title":"StreamingDialogue: Prolonged Dialogue Learning via Long Context\n  Compression with Minimal Losses","summary":"  Standard Large Language Models (LLMs) struggle with handling dialogues with\nlong contexts due to efficiency and consistency issues. According to our\nobservation, dialogue contexts are highly structured, and the special token of\n\\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate\ninformation. We refer to the EoU tokens as ``conversational attention sinks''\n(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which\ncompresses long dialogue history into conv-attn sinks with minimal losses, and\nthus reduces computational complexity quadratically with the number of sinks\n(i.e., the number of utterances). Current LLMs already demonstrate the ability\nto handle long context window, e.g., a window size of 200K or more. To this\nend, by compressing utterances into EoUs, our method has the potential to\nhandle more than 200K of utterances, resulting in a prolonged dialogue\nlearning. In order to minimize information losses from reconstruction after\ncompression, we design two learning strategies of short-memory reconstruction\n(SMR) and long-memory reactivation (LMR). Our method outperforms strong\nbaselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing\nmemory usage by 18 $\\times$ compared to dense attention recomputation.\n","authors":["Jia-Nan Li","Quan Tu","Cunli Mao","Zhengtao Yu","Ji-Rong Wen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.08312v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00119v2","updated":"2024-11-04T09:07:25Z","published":"2024-08-28T08:45:29Z","title":"3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient\n  Batching and Composability","summary":"  Parameter-efficient finetuning (PEFT) methods effectively adapt large\nlanguage models (LLMs) to diverse downstream tasks, reducing storage and GPU\nmemory demands. Despite these advantages, several applications pose new\nchallenges to PEFT beyond mere parameter efficiency. One notable challenge\ninvolves the efficient deployment of LLMs equipped with multiple task- or\nuser-specific adapters, particularly when different adapters are needed for\ndistinct requests within the same batch. Another challenge is the\ninterpretability of LLMs, which is crucial for understanding how LLMs function.\nPrevious studies introduced various approaches to address different challenges.\nIn this paper, we introduce a novel method, RoAd, which employs a\nstraightforward 2D rotation to adapt LLMs and addresses all the above\nchallenges: (1) RoAd is remarkably parameter-efficient, delivering optimal\nperformance on GLUE, eight commonsense reasoning tasks and four arithmetic\nreasoning tasks with $<0.1\\%$ trainable parameters; (2) RoAd facilitates the\nefficient serving of requests requiring different adapters within a batch, with\nan overhead comparable to element-wise multiplication instead of batch matrix\nmultiplication; (3) RoAd enhances LLM's interpretability through integration\nwithin a framework of distributed interchange intervention, demonstrated via\ncomposition experiments.\n","authors":["Baohao Liao","Christof Monz"],"pdf_url":"https://arxiv.org/pdf/2409.00119v2.pdf","comment":"Accepted to NeurIPS 2024. Code: https://github.com/BaohaoLiao/road"},{"id":"http://arxiv.org/abs/2409.17353v3","updated":"2024-11-04T08:01:22Z","published":"2024-09-25T20:59:12Z","title":"Internalizing ASR with Implicit Chain of Thought for Efficient\n  Speech-to-Speech Conversational LLM","summary":"  Current speech-based LLMs are predominantly trained on extensive ASR and TTS\ndatasets, excelling in tasks related to these domains. However, their ability\nto handle direct speech-to-speech conversations remains notably constrained.\nThese models often rely on an ASR-to-TTS chain-of-thought pipeline, converting\nspeech into text for processing before generating audio responses, which\nintroduces latency and loses audio features. We propose a method that\nimplicitly internalizes ASR chain of thought into a speech LLM, enhancing its\nnative speech understanding capabilities. Our approach reduces latency and\nimproves the model's native understanding of speech, paving the way for more\nefficient and natural real-time audio interactions. We also release a\nlarge-scale synthetic conversational dataset to facilitate further research.\n","authors":["Robin Shing-Hei Yuen","Timothy Tin-Long Tse","Jian Zhu"],"pdf_url":"https://arxiv.org/pdf/2409.17353v3.pdf","comment":"Updated for reviewer comments"},{"id":"http://arxiv.org/abs/2406.05967v2","updated":"2024-11-04T07:55:31Z","published":"2024-06-10T01:59:00Z","title":"CVQA: Culturally-diverse Multilingual Visual Question Answering\n  Benchmark","summary":"  Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 30\ncountries on four continents, covering 31 languages with 13 scripts, providing\na total of 10k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.\n","authors":["David Romero","Chenyang Lyu","Haryo Akbarianto Wibowo","Teresa Lynn","Injy Hamed","Aditya Nanda Kishore","Aishik Mandal","Alina Dragonetti","Artem Abzaliev","Atnafu Lambebo Tonja","Bontu Fufa Balcha","Chenxi Whitehouse","Christian Salamea","Dan John Velasco","David Ifeoluwa Adelani","David Le Meur","Emilio Villa-Cueva","Fajri Koto","Fauzan Farooqui","Frederico Belcavello","Ganzorig Batnasan","Gisela Vallejo","Grainne Caulfield","Guido Ivetta","Haiyue Song","Henok Biadglign Ademtew","Hernán Maina","Holy Lovenia","Israel Abebe Azime","Jan Christian Blaise Cruz","Jay Gala","Jiahui Geng","Jesus-German Ortiz-Barajas","Jinheon Baek","Jocelyn Dunstan","Laura Alonso Alemany","Kumaranage Ravindu Yasas Nagasinghe","Luciana Benotti","Luis Fernando D'Haro","Marcelo Viridiano","Marcos Estecha-Garitagoitia","Maria Camila Buitrago Cabrera","Mario Rodríguez-Cantelar","Mélanie Jouitteau","Mihail Mihaylov","Mohamed Fazli Mohamed Imam","Muhammad Farid Adilazuarda","Munkhjargal Gochoo","Munkh-Erdene Otgonbold","Naome Etori","Olivier Niyomugisha","Paula Mónica Silva","Pranjal Chitale","Raj Dabre","Rendi Chevi","Ruochen Zhang","Ryandito Diandaru","Samuel Cahyawijaya","Santiago Góngora","Soyeong Jeong","Sukannya Purkayastha","Tatsuki Kuribayashi","Teresa Clifford","Thanmay Jayakumar","Tiago Timponi Torrent","Toqeer Ehsan","Vladimir Araujo","Yova Kementchedjhieva","Zara Burzo","Zheng Wei Lim","Zheng Xin Yong","Oana Ignat","Joan Nwatu","Rada Mihalcea","Thamar Solorio","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2406.05967v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.06479v2","updated":"2024-11-04T07:28:02Z","published":"2024-10-09T02:14:39Z","title":"Large Language Model Compression with Neural Architecture Search","summary":"  Large language models (LLMs) exhibit remarkable reasoning abilities, allowing\nthem to generalize across a wide range of downstream tasks, such as commonsense\nreasoning or instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. This poses the question: Can we compress pre-trained LLMs to meet\ndiverse size and latency requirements? We leverage Neural Architecture Search\n(NAS) to compress LLMs by pruning structural components, such as attention\nheads, neurons, and layers, aiming to achieve a Pareto-optimal balance between\nperformance and efficiency. While NAS already achieved promising results on\nsmall language models in previous work, in this paper we propose various\nextensions that allow us to scale to LLMs. Compared to structural pruning\nbaselines, we show that NAS improves performance up to 3.4% on MMLU with an\non-device latency speedup.\n","authors":["Rhea Sanjay Sukthanker","Benedikt Staffler","Frank Hutter","Aaron Klein"],"pdf_url":"https://arxiv.org/pdf/2410.06479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11252v2","updated":"2024-11-04T07:27:37Z","published":"2024-09-17T15:00:31Z","title":"WER We Stand: Benchmarking Urdu ASR Models","summary":"  This paper presents a comprehensive evaluation of Urdu Automatic Speech\nRecognition (ASR) models. We analyze the performance of three ASR model\nfamilies: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along\nwith a detailed examination of the most frequent wrong words and error types\nincluding insertions, deletions, and substitutions. Our analysis is conducted\nusing two types of datasets, read speech and conversational speech. Notably, we\npresent the first conversational speech dataset designed for benchmarking Urdu\nASR models. We find that seamless-large outperforms other ASR models on the\nread speech dataset, while whisper-large performs best on the conversational\nspeech dataset. Furthermore, this evaluation highlights the complexities of\nassessing ASR models for low-resource languages like Urdu using quantitative\nmetrics alone and emphasizes the need for a robust Urdu text normalization\nsystem. Our findings contribute valuable insights for developing robust ASR\nsystems for low-resource languages like Urdu.\n","authors":["Samee Arif","Sualeha Farid","Aamina Jamal Khan","Mustafa Abbas","Agha Ali Raza","Awais Athar"],"pdf_url":"https://arxiv.org/pdf/2409.11252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01855v1","updated":"2024-11-04T07:10:24Z","published":"2024-11-04T07:10:24Z","title":"Can Language Models Learn to Skip Steps?","summary":"  Trained on vast corpora of human language, language models demonstrate\nemergent human-like reasoning abilities. Yet they are still far from true\nintelligence, which opens up intriguing opportunities to explore the parallels\nof humans and model behaviors. In this work, we study the ability to skip steps\nin reasoning - a hallmark of human expertise developed through practice. Unlike\nhumans, who may skip steps to enhance efficiency or to reduce cognitive load,\nmodels do not inherently possess such motivations to minimize reasoning steps.\nTo address this, we introduce a controlled framework that stimulates\nstep-skipping behavior by iteratively refining models to generate shorter and\naccurate reasoning paths. Empirical results indicate that models can develop\nthe step skipping ability under our guidance. Moreover, after fine-tuning on\nexpanded datasets that include both complete and skipped reasoning sequences,\nthe models can not only resolve tasks with increased efficiency without\nsacrificing accuracy, but also exhibit comparable and even enhanced\ngeneralization capabilities in out-of-domain scenarios. Our work presents the\nfirst exploration into human-like step-skipping ability and provides fresh\nperspectives on how such cognitive abilities can benefit AI models.\n","authors":["Tengxiao Liu","Qipeng Guo","Xiangkun Hu","Cheng Jiayang","Yue Zhang","Xipeng Qiu","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01855v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.16733v2","updated":"2024-11-04T06:54:34Z","published":"2024-02-21T09:12:16Z","title":"DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing","summary":"  Automated essay scoring (AES) is a useful tool in English as a Foreign\nLanguage (EFL) writing education, offering real-time essay scores for students\nand instructors. However, previous AES models were trained on essays and scores\nirrelevant to the practical scenarios of EFL writing education and usually\nprovided a single holistic score due to the lack of appropriate datasets. In\nthis paper, we release DREsS, a large-scale, standard dataset for rubric-based\nautomated essay scoring. DREsS comprises three sub-datasets: DREsS_New,\nDREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with\n2.3K essays authored by EFL undergraduate students and scored by English\neducation experts. We also standardize existing rubric-based essay scoring\ndatasets as DREsS_Std. We suggest CASE, a corruption-based augmentation\nstrategy for essays, which generates 40.1K synthetic samples of DREsS_CASE and\nimproves the baseline results by 45.44%. DREsS will enable further research to\nprovide a more accurate and practical AES system for EFL writing education.\n","authors":["Haneul Yoo","Jieun Han","So-Yeon Ahn","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2402.16733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01841v1","updated":"2024-11-04T06:27:14Z","published":"2024-11-04T06:27:14Z","title":"Leveraging Label Semantics and Meta-Label Refinement for Multi-Label\n  Question Classification","summary":"  Accurate annotation of educational resources is critical in the rapidly\nadvancing field of online education due to the complexity and volume of\ncontent. Existing classification methods face challenges with semantic overlap\nand distribution imbalance of labels in the multi-label context, which impedes\neffective personalized learning and resource recommendation. This paper\nintroduces RR2QC, a novel Retrieval Reranking method To multi-label Question\nClassification by leveraging label semantics and meta-label refinement.\nFirstly, RR2QC leverages semantic relationships within and across label groups\nto enhance pre-training strategie in multi-label context. Next, a class center\nlearning task is introduced, integrating label texts into downstream training\nto ensure questions consistently align with label semantics, retrieving the\nmost relevant label sequences. Finally, this method decomposes labels into\nmeta-labels and trains a meta-label classifier to rerank the retrieved label\nsequences. In doing so, RR2QC enhances the understanding and prediction\ncapability of long-tail labels by learning from meta-labels frequently\nappearing in other labels. Addtionally, a Math LLM is used to generate\nsolutions for questions, extracting latent information to further refine the\nmodel's insights. Experimental results demonstrate that RR2QC outperforms\nexisting classification methods in Precision@k and F1 scores across multiple\neducational datasets, establishing it as a potent enhancement for online\neducational content utilization.\n","authors":["Shi Dong","Xiaobei Niu","Rui Zhong","Zhifeng Wang","Mingzhang Zuo"],"pdf_url":"https://arxiv.org/pdf/2411.01841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01839v1","updated":"2024-11-04T06:26:09Z","published":"2024-11-04T06:26:09Z","title":"TriG-NER: Triplet-Grid Framework for Discontinuous Named Entity\n  Recognition","summary":"  Discontinuous Named Entity Recognition (DNER) presents a challenging problem\nwhere entities may be scattered across multiple non-adjacent tokens, making\ntraditional sequence labelling approaches inadequate. Existing methods\npredominantly rely on custom tagging schemes to handle these discontinuous\nentities, resulting in models tightly coupled to specific tagging strategies\nand lacking generalisability across diverse datasets. To address these\nchallenges, we propose TriG-NER, a novel Triplet-Grid Framework that introduces\na generalisable approach to learning robust token-level representations for\ndiscontinuous entity extraction. Our framework applies triplet loss at the\ntoken level, where similarity is defined by word pairs existing within the same\nentity, effectively pulling together similar and pushing apart dissimilar ones.\nThis approach enhances entity boundary detection and reduces the dependency on\nspecific tagging schemes by focusing on word-pair relationships within a\nflexible grid structure. We evaluate TriG-NER on three benchmark DNER datasets\nand demonstrate significant improvements over existing grid-based\narchitectures. These results underscore our framework's effectiveness in\ncapturing complex entity structures and its adaptability to various tagging\nschemes, setting a new benchmark for discontinuous entity extraction.\n","authors":["Rina Carines Cabral","Soyeon Caren Han","Areej Alhassan","Riza Batista-Navarro","Goran Nenadic","Josiah Poon"],"pdf_url":"https://arxiv.org/pdf/2411.01839v1.pdf","comment":"Code will be made available upon publication"},{"id":"http://arxiv.org/abs/2411.01834v1","updated":"2024-11-04T06:07:53Z","published":"2024-11-04T06:07:53Z","title":"Align-SLM: Textless Spoken Language Models with Reinforcement Learning\n  from AI Feedback","summary":"  While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs.\n","authors":["Guan-Ting Lin","Prashanth Gurunath Shivakumar","Aditya Gourav","Yile Gu","Ankur Gandhe","Hung-yi Lee","Ivan Bulyko"],"pdf_url":"https://arxiv.org/pdf/2411.01834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12902v2","updated":"2024-11-04T05:11:46Z","published":"2023-10-19T16:54:38Z","title":"Experimental Narratives: A Comparison of Human Crowdsourced Storytelling\n  and AI Storytelling","summary":"  The paper proposes a framework that combines behavioral and computational\nexperiments employing fictional prompts as a novel tool for investigating\ncultural artifacts and social biases in storytelling both by humans and\ngenerative AI. The study analyzes 250 stories authored by crowdworkers in June\n2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging\nmethods from narratology and inferential statistics. Both crowdworkers and\nlarge language models responded to identical prompts about creating and falling\nin love with an artificial human. The proposed experimental paradigm allows a\ndirect and controlled comparison between human and LLM-generated storytelling.\nResponses to the Pygmalionesque prompts confirm the pervasive presence of the\nPygmalion myth in the collective imaginary of both humans and large language\nmodels. All solicited narratives present a scientific or technological pursuit.\nThe analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are\nmore progressive in terms of gender roles and sexuality than those written by\nhumans. While AI narratives with default settings and no additional prompting\ncan occasionally provide innovative plot twists, they offer less imaginative\nscenarios and rhetoric than human-authored texts. The proposed framework argues\nthat fiction can be used as a window into human and AI-based collective\nimaginary and social dimensions.\n","authors":["Nina Begus"],"pdf_url":"https://arxiv.org/pdf/2310.12902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04172v2","updated":"2024-11-04T04:59:45Z","published":"2024-07-04T22:16:40Z","title":"ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild","summary":"  Given the ubiquity of charts as a data analysis, visualization, and\ndecision-making tool across industries and sciences, there has been a growing\ninterest in developing pre-trained foundation models as well as general purpose\ninstruction-tuned models for chart understanding and reasoning. However,\nexisting methods suffer crucial drawbacks across two critical axes affecting\nthe performance of chart representation models: they are trained on data\ngenerated from underlying data tables of the charts, ignoring the visual trends\nand patterns in chart images, and use weakly aligned vision-language backbone\nmodels for domain-specific training, limiting their generalizability when\nencountering charts in the wild. We address these important drawbacks and\nintroduce ChartGemma, a novel chart understanding and reasoning model developed\nover PaliGemma. Rather than relying on underlying data tables, ChartGemma is\ntrained on instruction-tuning data generated directly from chart images, thus\ncapturing both high-level trends and low-level visual information from a\ndiverse set of charts. Our simple approach achieves state-of-the-art results\nacross $5$ benchmarks spanning chart summarization, question answering, and\nfact-checking, and our elaborate qualitative studies on real-world charts show\nthat ChartGemma generates more realistic and factually correct summaries\ncompared to its contemporaries. We release the code, model checkpoints,\ndataset, and demos at https://github.com/vis-nlp/ChartGemma.\n","authors":["Ahmed Masry","Megh Thakkar","Aayush Bajaj","Aaryaman Kartha","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2407.04172v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00038v2","updated":"2024-11-04T03:40:54Z","published":"2024-10-29T13:55:17Z","title":"Topic-Conversation Relevance (TCR) Dataset and Benchmarks","summary":"  Workplace meetings are vital to organizational collaboration, yet a large\npercentage of meetings are rated as ineffective. To help improve meeting\neffectiveness by understanding if the conversation is on topic, we create a\ncomprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety\nof domains and meeting styles. The TCR dataset includes 1,500 unique meetings,\n22 million words in transcripts, and over 15,000 meeting topics, sourced from\nboth newly collected Speech Interruption Meeting (SIM) data and existing public\ndatasets. Along with the text data, we also open source scripts to generate\nsynthetic meetings or create augmented meetings from the TCR dataset to enhance\ndata diversity. For each data source, benchmarks are created using GPT-4 to\nevaluate the model accuracy in understanding transcription-topic relevance.\n","authors":["Yaran Fan","Jamie Pool","Senja Filipi","Ross Cutler"],"pdf_url":"https://arxiv.org/pdf/2411.00038v2.pdf","comment":"To be published in 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2411.01765v1","updated":"2024-11-04T03:20:00Z","published":"2024-11-04T03:20:00Z","title":"Towards Pedagogical LLMs with Supervised Fine Tuning for Computing\n  Education","summary":"  This paper investigates supervised fine-tuning of large language models\n(LLMs) to improve their pedagogical alignment in computing education,\naddressing concerns that LLMs may hinder learning outcomes. The project\nutilised a proprietary dataset of 2,500 high quality question/answer pairs from\nprogramming course forums, and explores two research questions: the suitability\nof university course forums in contributing to fine-tuning datasets, and how\nsupervised fine-tuning can improve LLMs' alignment with educational principles\nsuch as constructivism. Initial findings suggest benefits in pedagogical\nalignment of LLMs, with deeper evaluations required.\n","authors":["Alexandra Vassar","Jake Renzella","Emily Ross","Andrew Taylor"],"pdf_url":"https://arxiv.org/pdf/2411.01765v1.pdf","comment":"3 pages, 1 table, conference"},{"id":"http://arxiv.org/abs/2405.15362v4","updated":"2024-11-04T03:18:13Z","published":"2024-05-24T08:54:36Z","title":"Pipeline Parallelism with Controllable Memory","summary":"  Pipeline parallelism has been widely explored, but most existing schedules\nlack a systematic methodology. In this paper, we propose a framework to\ndecompose pipeline schedules as repeating a building block, and show that the\nlifespan of the building block decides the peak activation memory of the\npipeline schedule. Guided by the observations, we find that almost all existing\npipeline schedules, to the best of our knowledge, are memory inefficient. To\naddress this, we introduce a family of memory efficient building blocks with\ncontrollable activation memory, which can reduce the peak activation memory to\n1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable\nthroughput. We can also achieve almost zero pipeline bubbles while maintaining\nthe same activation memory as 1F1B. Our evaluations demonstrate that in pure\npipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in\nterms of throughput. When employing a grid search over hybrid parallelism\nhyperparameters in practical scenarios, our methods demonstrate a 16%\nthroughput improvement over the 1F1B baseline for large language models. The\nimplementation is open-sourced at\nhttps://github.com/sail-sg/zero-bubble-pipeline-parallelism.\n","authors":["Penghui Qi","Xinyi Wan","Nyamdavaa Amar","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2405.15362v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00801v2","updated":"2024-11-04T03:07:30Z","published":"2024-02-23T18:45:35Z","title":"Self-Retrieval: End-to-End Information Retrieval with One Large Language\n  Model","summary":"  The rise of large language models (LLMs) has significantly transformed both\nthe construction and application of information retrieval (IR) systems.\nHowever, current interactions between IR systems and LLMs remain limited, with\nLLMs merely serving as part of components within IR systems, and IR systems\nbeing constructed independently of LLMs. This separated architecture restricts\nknowledge sharing and deep collaboration between them. In this paper, we\nintroduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval\narchitecture. Self-Retrieval unifies all essential IR functions within a single\nLLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through\nself-supervised learning, transforms the retrieval process into sequential\npassage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms\nexisting retrieval approaches by a significant margin, but also substantially\nenhances the performance of LLM-driven downstream applications like\nretrieval-augmented generation.\n","authors":["Qiaoyu Tang","Jiawei Chen","Zhuoqun Li","Bowen Yu","Yaojie Lu","Cheng Fu","Haiyang Yu","Hongyu Lin","Fei Huang","Ben He","Xianpei Han","Le Sun","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.00801v2.pdf","comment":"NeurIPS 2024 Camera-ready Version. Code:\n  https://github.com/icip-cas/SelfRetrieval"},{"id":"http://arxiv.org/abs/2310.12821v5","updated":"2024-11-04T02:48:42Z","published":"2023-10-19T15:17:34Z","title":"GestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with\n  Large Language Model Agents","summary":"  Existing gesture interfaces only work with a fixed set of gestures defined\neither by interface designers or by users themselves, which introduces learning\nor demonstration efforts that diminish their naturalness. Humans, on the other\nhand, understand free-form gestures by synthesizing the gesture, context,\nexperience, and common sense. In this way, the user does not need to learn,\ndemonstrate, or associate gestures. We introduce GestureGPT, a free-form hand\ngesture understanding framework that mimics human gesture understanding\nprocedures to enable a natural free-form gestural interface. Our framework\nleverages multiple Large Language Model agents to manage and synthesize gesture\nand context information, then infers the interaction intent by associating the\ngesture with an interface function. More specifically, our triple-agent\nframework includes a Gesture Description Agent that automatically segments and\nformulates natural language descriptions of hand poses and movements based on\nhand landmark coordinates. The description is deciphered by a Gesture Inference\nAgent through self-reasoning and querying about the interaction context (e.g.,\ninteraction history, gaze data), which is managed by a Context Management\nAgent. Following iterative exchanges, the Gesture Inference Agent discerns the\nuser's intent by grounding it to an interactive function. We validated our\nframework offline under two real-world scenarios: smart home control and online\nvideo streaming. The average zero-shot Top-1/Top-5 grounding accuracies are\n44.79%/83.59% for smart home tasks and 37.50%/73.44% for video streaming tasks.\nWe also provide an extensive discussion that includes rationale for model\nselection, generalizability, and future research directions for a practical\nsystem etc.\n","authors":["Xin Zeng","Xiaoyu Wang","Tengxiang Zhang","Chun Yu","Shengdong Zhao","Yiqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2310.12821v5.pdf","comment":"This paper has been accepted to the ISS 2024 track of the Proceedings\n  of the ACM on Human-Computer Interaction"},{"id":"http://arxiv.org/abs/2411.01751v1","updated":"2024-11-04T02:30:05Z","published":"2024-11-04T02:30:05Z","title":"RAGViz: Diagnose and Visualize Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) combines knowledge from domain-specific\nsources into large language models to ground answer generation. Current RAG\nsystems lack customizable visibility on the context documents and the model's\nattentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool\nthat visualizes the attentiveness of the generated tokens in retrieved\ndocuments. With a built-in user interface, retrieval index, and Large Language\nModel (LLM) backbone, RAGViz provides two main functionalities: (1) token and\ndocument-level attention visualization, and (2) generation comparison upon\ncontext document addition and removal. As an open-source toolkit, RAGViz can be\neasily hosted with a custom embedding model and HuggingFace-supported LLM\nbackbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,\nmemory-efficient LLM inference tool, and custom context snippet method, RAGViz\noperates efficiently with a median query time of about 5 seconds on a moderate\nGPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo\nvideo of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.\n","authors":["Tevin Wang","Jingyuan He","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.01751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14826v3","updated":"2024-11-04T02:29:32Z","published":"2024-09-23T08:58:48Z","title":"ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions\n  with Path Planning and Feedback","summary":"  Recently, tool-augmented LLMs have gained increasing attention. Given an\ninstruction, tool-augmented LLMs can interact with various external tools in\nmultiple rounds and provide a final answer. However, previous LLMs were trained\non overly detailed instructions, which included API names or parameters, while\nreal users would not explicitly mention these API details. This leads to a gap\nbetween trained LLMs and real-world scenarios. In addition, most works ignore\nwhether the interaction process follows the instruction. To address these\nissues, we constructed a training dataset called MGToolBench, which contains\nstatement and category-level instructions to better reflect real-world\nscenarios. In addition, we propose ToolPlanner, a two-stage reinforcement\nlearning framework that utilizes path planning and two feedback mechanisms to\nenhance the LLM's task completion and instruction-following capabilities.\nExperimental results show that ToolPlanner significantly improves the Match\nRate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA\nmodel. Human evaluation verifies that the multi-granularity instructions can\nbetter align with users' usage habits. Our data and code will be released upon\nacceptance.\n","authors":["Qinzhuo Wu","Wei Liu","Jian Luan","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2409.14826v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11988v2","updated":"2024-11-04T02:28:13Z","published":"2024-10-15T18:51:18Z","title":"DISP-LLM: Dimension-Independent Structural Pruning for Large Language\n  Models","summary":"  Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, including language modeling, understanding,\nand generation. However, the increased memory and computational costs\nassociated with these models pose significant challenges for deployment on\nresource-limited devices. Structural pruning has emerged as a promising\nsolution to reduce the costs of LLMs without requiring post-processing steps.\nPrior structural pruning methods either follow the dependence of structures at\nthe cost of limiting flexibility, or introduce non-trivial additional\nparameters by incorporating different projection matrices. In this work, we\npropose a novel approach that relaxes the constraint imposed by regular\nstructural pruning methods and eliminates the structural dependence along the\nembedding dimension. Our dimension-independent structural pruning method offers\nseveral benefits. Firstly, our method enables different blocks to utilize\ndifferent subsets of the feature maps. Secondly, by removing structural\ndependence, we facilitate each block to possess varying widths along its input\nand output dimensions, thereby significantly enhancing the flexibility of\nstructural pruning. We evaluate our method on various LLMs, including OPT,\nLLaMA, LLaMA-2, Phi-1.5, and Phi-2. Experimental results demonstrate that our\napproach outperforms other state-of-the-art methods, showing for the first time\nthat structural pruning can achieve an accuracy similar to semi-structural\npruning.\n","authors":["Shangqian Gao","Chi-Heng Lin","Ting Hua","Tang Zheng","Yilin Shen","Hongxia Jin","Yen-Chang Hsu"],"pdf_url":"https://arxiv.org/pdf/2410.11988v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01747v1","updated":"2024-11-04T02:08:59Z","published":"2024-11-04T02:08:59Z","title":"DynaSaur: Large Language Agents Beyond Predefined Actions","summary":"  Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly-scoped environments, we argue that it presents two major challenges\nwhen deploying LLM agents in real-world scenarios: (1) selecting from a fixed\nset of actions significantly restricts the planning and acting capabilities of\nLLM agents, and (2) this approach requires substantial human effort to\nenumerate and implement all possible actions, which becomes impractical in\ncomplex environments with a vast number of potential actions. In this work, we\npropose an LLM agent framework that enables the dynamic creation and\ncomposition of actions in an online manner. In this framework, the agent\ninteracts with the environment by generating and executing programs written in\na general-purpose programming language at each step. Furthermore, generated\nactions are accumulated over time for future reuse. Our extensive experiments\non the GAIA benchmark demonstrate that this framework offers significantly\ngreater flexibility and outperforms previous methods. Notably, it allows an LLM\nagent to recover in scenarios where no relevant action exists in the predefined\nset or when existing actions fail due to unforeseen edge cases. At the time of\nwriting, we hold the top position on the GAIA public leaderboard. Our code can\nbe found in\n\\href{https://github.com/adobe-research/dynasaur}{https://github.com/adobe-research/dynasaur}.\n","authors":["Dang Nguyen","Viet Dac Lai","Seunghyun Yoon","Ryan A. Rossi","Handong Zhao","Ruiyi Zhang","Puneet Mathur","Nedim Lipka","Yu Wang","Trung Bui","Franck Dernoncourt","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.01747v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.03171v2","updated":"2024-11-04T00:44:32Z","published":"2024-09-05T01:58:29Z","title":"MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented\n  Generation Question Answering","summary":"  In this paper we present a multi-adapter retrieval augmented generation\nsystem (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP\n2024. CRAG is a question answering dataset contains 3 different subtasks aimed\nat realistic question and answering RAG related tasks, with a diverse set of\nquestion topics, question types, time dynamic answers, and questions featuring\nentities of varying popularity.\n  Our system follows a standard setup for web based RAG, which uses processed\nweb pages to provide context for an LLM to produce generations, while also\nquerying API endpoints for additional information. MARAGS also utilizes\nmultiple different adapters to solve the various requirements for these tasks\nwith a standard cross-encoder model for ranking candidate passages relevant for\nanswering the question. Our system achieved 2nd place for Task 1 as well as 3rd\nplace on Task 2.\n","authors":["Mitchell DeHaven"],"pdf_url":"https://arxiv.org/pdf/2409.03171v2.pdf","comment":"Accepted to CRAG KDD Cup 24 Workshop"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.02397v1","updated":"2024-11-04T18:59:44Z","published":"2024-11-04T18:59:44Z","title":"Adaptive Caching for Faster Video Generation with Diffusion Transformers","summary":"  Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.\n","authors":["Kumara Kahatapitiya","Haozhe Liu","Sen He","Ding Liu","Menglin Jia","Michael S. Ryoo","Tian Xie"],"pdf_url":"https://arxiv.org/pdf/2411.02397v1.pdf","comment":"Project-page is available at https://adacache-dit.github.io"},{"id":"http://arxiv.org/abs/2411.02394v1","updated":"2024-11-04T18:59:05Z","published":"2024-11-04T18:59:05Z","title":"AutoVFX: Physically Realistic Video Editing from Natural Language\n  Instructions","summary":"  Modern visual effects (VFX) software has made it possible for skilled artists\nto create imagery of virtually anything. However, the creation process remains\nlaborious, complex, and largely inaccessible to everyday users. In this work,\nwe present AutoVFX, a framework that automatically creates realistic and\ndynamic VFX videos from a single video and natural language instructions. By\ncarefully integrating neural scene modeling, LLM-based code generation, and\nphysical simulation, AutoVFX is able to provide physically-grounded,\nphotorealistic editing effects that can be controlled directly using natural\nlanguage instructions. We conduct extensive experiments to validate AutoVFX's\nefficacy across a diverse spectrum of videos and instructions. Quantitative and\nqualitative results suggest that AutoVFX outperforms all competing methods by a\nlarge margin in generative quality, instruction alignment, editing versatility,\nand physical plausibility.\n","authors":["Hao-Yu Hsu","Zhi-Hao Lin","Albert Zhai","Hongchi Xia","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02394v1.pdf","comment":"Project page: https://haoyuhsu.github.io/autovfx-website/"},{"id":"http://arxiv.org/abs/2411.02395v1","updated":"2024-11-04T18:59:05Z","published":"2024-11-04T18:59:05Z","title":"Training-free Regional Prompting for Diffusion Transformers","summary":"  Diffusion models have demonstrated excellent capabilities in text-to-image\ngeneration. Their semantic understanding (i.e., prompt following) ability has\nalso been greatly improved with large language models (e.g., T5, Llama).\nHowever, existing models cannot perfectly handle long and complex text prompts,\nespecially when the text prompts contain various objects with numerous\nattributes and interrelated spatial relationships. While many regional\nprompting methods have been proposed for UNet-based models (SD1.5, SDXL), but\nthere are still no implementations based on the recent Diffusion Transformer\n(DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and\nimplement regional prompting for FLUX.1 based on attention manipulation, which\nenables DiT with fined-grained compositional text-to-image generation\ncapability in a training-free manner. Code is available at\nhttps://github.com/antonioo-c/Regional-Prompting-FLUX.\n","authors":["Anthony Chen","Jianjin Xu","Wenzhao Zheng","Gaole Dai","Yida Wang","Renrui Zhang","Haofan Wang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02395v1.pdf","comment":"Code is available at\n  https://github.com/antonioo-c/Regional-Prompting-FLUX"},{"id":"http://arxiv.org/abs/2411.02393v1","updated":"2024-11-04T18:58:01Z","published":"2024-11-04T18:58:01Z","title":"Adaptive Length Image Tokenization via Recurrent Allocation","summary":"  Current vision systems typically assign fixed-length representations to\nimages, regardless of the information content. This contrasts with human\nintelligence - and even large language models - which allocate varying\nrepresentational capacities based on entropy, context and familiarity. Inspired\nby this, we propose an approach to learn variable-length token representations\nfor 2D images. Our encoder-decoder architecture recursively processes 2D image\ntokens, distilling them into 1D latent tokens over multiple iterations of\nrecurrent rollouts. Each iteration refines the 2D tokens, updates the existing\n1D latent tokens, and adaptively increases representational capacity by adding\nnew tokens. This enables compression of images into a variable number of\ntokens, ranging from 32 to 256. We validate our tokenizer using reconstruction\nloss and FID metrics, demonstrating that token count aligns with image entropy,\nfamiliarity and downstream task requirements. Recurrent token processing with\nincreasing representational capacity in each iteration shows signs of token\nspecialization, revealing potential for object / part discovery.\n","authors":["Shivam Duggal","Phillip Isola","Antonio Torralba","William T. Freeman"],"pdf_url":"https://arxiv.org/pdf/2411.02393v1.pdf","comment":"Code at: https://github.com/ShivamDuggal4/adaptive-length-tokenizer"},{"id":"http://arxiv.org/abs/2411.02385v1","updated":"2024-11-04T18:53:05Z","published":"2024-11-04T18:53:05Z","title":"How Far is Video Generation from World Model: A Physical Law Perspective","summary":"  OpenAI's Sora highlights the potential of video generation for developing\nworld models that adhere to fundamental physical laws. However, the ability of\nvideo generation models to discover such laws purely from visual data without\nhuman priors can be questioned. A world model learning the true law should give\npredictions robust to nuances and correctly extrapolate on unseen scenarios. In\nthis work, we evaluate across three key scenarios: in-distribution,\nout-of-distribution, and combinatorial generalization. We developed a 2D\nsimulation testbed for object movement and collisions to generate videos\ndeterministically governed by one or more classical mechanics laws. This\nprovides an unlimited supply of data for large-scale experimentation and\nenables quantitative evaluation of whether the generated videos adhere to\nphysical laws. We trained diffusion-based video generation models to predict\nobject movements based on initial frames. Our scaling experiments show perfect\ngeneralization within the distribution, measurable scaling behavior for\ncombinatorial generalization, but failure in out-of-distribution scenarios.\nFurther experiments reveal two key insights about the generalization mechanisms\nof these models: (1) the models fail to abstract general physical rules and\ninstead exhibit \"case-based\" generalization behavior, i.e., mimicking the\nclosest training example; (2) when generalizing to new cases, models are\nobserved to prioritize different factors when referencing training data: color\n> size > velocity > shape. Our study suggests that scaling alone is\ninsufficient for video generation models to uncover fundamental physical laws,\ndespite its role in Sora's broader success. See our project page at\nhttps://phyworld.github.io\n","authors":["Bingyi Kang","Yang Yue","Rui Lu","Zhijie Lin","Yang Zhao","Kaixin Wang","Gao Huang","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2411.02385v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.23262v2","updated":"2024-11-04T18:44:20Z","published":"2024-10-30T17:46:31Z","title":"EMMA: End-to-End Multimodal Model for Autonomous Driving","summary":"  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.\n","authors":["Jyh-Jing Hwang","Runsheng Xu","Hubert Lin","Wei-Chih Hung","Jingwei Ji","Kristy Choi","Di Huang","Tong He","Paul Covington","Benjamin Sapp","Yin Zhou","James Guo","Dragomir Anguelov","Mingxing Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23262v2.pdf","comment":"Blog post: https://waymo.com/blog/2024/10/introducing-emma/"},{"id":"http://arxiv.org/abs/2411.02372v1","updated":"2024-11-04T18:40:46Z","published":"2024-11-04T18:40:46Z","title":"Learning General-Purpose Biomedical Volume Representations using\n  Randomized Synthesis","summary":"  Current volumetric biomedical foundation models struggle to generalize as\npublic 3D datasets are small and do not cover the broad diversity of medical\nprocedures, conditions, anatomical regions, and imaging protocols. We address\nthis by creating a representation learning method that instead anticipates\nstrong domain shifts at training time itself. We first propose a data engine\nthat synthesizes highly variable training samples that enable generalization to\nnew biomedical contexts. To then train a single 3D network for any voxel-level\ntask, we develop a contrastive learning method that pretrains the network to be\nstable against nuisance imaging variation simulated by the data engine, a key\ninductive bias for generalization. This network's features can be used as\nrobust representations of input images for downstream tasks and its weights\nprovide a strong, dataset-agnostic initialization for finetuning on new\ndatasets. As a result, we set new standards across both multimodality\nregistration and few-shot segmentation, a first for any 3D biomedical vision\nmodel, all without (pre-)training on any existing dataset of real images.\n","authors":["Neel Dey","Benjamin Billot","Hallee E. Wong","Clinton J. Wang","Mengwei Ren","P. Ellen Grant","Adrian V. Dalca","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2411.02372v1.pdf","comment":"Code and model weights available at\n  https://github.com/neel-dey/anatomix"},{"id":"http://arxiv.org/abs/2411.02354v1","updated":"2024-11-04T18:21:56Z","published":"2024-11-04T18:21:56Z","title":"Machine learning identification of maternal inflammatory response and\n  histologic choroamnionitis from placental membrane whole slide images","summary":"  The placenta forms a critical barrier to infection through pregnancy, labor\nand, delivery. Inflammatory processes in the placenta have short-term, and\nlong-term consequences for offspring health. Digital pathology and machine\nlearning can play an important role in understanding placental inflammation,\nand there have been very few investigations into methods for predicting and\nunderstanding Maternal Inflammatory Response (MIR). This work intends to\ninvestigate the potential of using machine learning to understand MIR based on\nwhole slide images (WSI), and establish early benchmarks. To that end, we use\nMultiple Instance Learning framework with 3 feature extractors: ImageNet-based\nEfficientNet-v2s, and 2 histopathology foundation models, UNI and Phikon to\ninvestigate predictability of MIR stage from histopathology WSIs. We also\ninterpret predictions from these models using the learned attention maps from\nthese models. We also use the MIL framework for predicting white blood cells\ncount (WBC) and maximum fever temperature ($T_{max}$). Attention-based MIL\nmodels are able to classify MIR with a balanced accuracy of up to 88.5% with a\nCohen's Kappa ($\\kappa$) of up to 0.772. Furthermore, we found that the\npathology foundation models (UNI and Phikon) are both able to achieve higher\nperformance with balanced accuracy and $\\kappa$, compared to ImageNet-based\nfeature extractor (EfficientNet-v2s). For WBC and $T_{max}$ prediction, we\nfound mild correlation between actual values and those predicted from\nhistopathology WSIs. We used MIL framework for predicting MIR stage from WSIs,\nand compared effectiveness of foundation models as feature extractors, with\nthat of an ImageNet-based model. We further investigated model failure cases\nand found them to be either edge cases prone to interobserver variability,\nexamples of pathologist's overreach, or mislabeled due to processing errors.\n","authors":["Abhishek Sharma","Ramin Nateghi","Marina Ayad","Lee A. D. Cooper","Jeffery A. Goldstein"],"pdf_url":"https://arxiv.org/pdf/2411.02354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02347v1","updated":"2024-11-04T18:17:44Z","published":"2024-11-04T18:17:44Z","title":"Physically Based Neural Bidirectional Reflectance Distribution Function","summary":"  We introduce the physically based neural bidirectional reflectance\ndistribution function (PBNBRDF), a novel, continuous representation for\nmaterial appearance based on neural fields. Our model accurately reconstructs\nreal-world materials while uniquely enforcing physical properties for realistic\nBRDFs, specifically Helmholtz reciprocity via reparametrization and energy\npassivity via efficient analytical integration. We conduct a systematic\nanalysis demonstrating the benefits of adhering to these physical laws on the\nvisual quality of reconstructed materials. Additionally, we enhance the color\naccuracy of neural BRDFs by introducing chromaticity enforcement supervising\nthe norms of RGB channels. Through both qualitative and quantitative\nexperiments on multiple databases of measured real-world BRDFs, we show that\nadhering to these physical constraints enables neural fields to more faithfully\nand stably represent the original data and achieve higher rendering quality.\n","authors":["Chenliang Zhou","Alejandro Sztrajman","Gilles Rainer","Fangcheng Zhong","Fazilet Gokbudak","Zhilin Guo","Weihao Xia","Rafal Mantiuk","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2411.02347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02336v1","updated":"2024-11-04T17:59:39Z","published":"2024-11-04T17:59:39Z","title":"MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D","summary":"  Texturing is a crucial step in the 3D asset production workflow, which\nenhances the visual appeal and diversity of 3D assets. Despite recent\nadvancements in Text-to-Texture (T2T) generation, existing methods often yield\nsubpar results, primarily due to local discontinuities, inconsistencies across\nmultiple views, and their heavy dependence on UV unwrapping outcomes. To tackle\nthese challenges, we propose a novel generation-refinement 3D texturing\nframework called MVPaint, which can generate high-resolution, seamless textures\nwhile emphasizing multi-view consistency. MVPaint mainly consists of three key\nmodules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model,\nMVPaint first simultaneously generates multi-view images by employing an SMG\nmodel, which leads to coarse texturing results with unpainted parts due to\nmissing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete\n3D texturing, we introduce the S3I method, specifically designed to effectively\ntexture previously unobserved areas. 3) UV Refinement (UVR). Furthermore,\nMVPaint employs a UVR module to improve the texture quality in the UV space,\nwhich first performs a UV-space Super-Resolution, followed by a Spatial-aware\nSeam-Smoothing algorithm for revising spatial texturing discontinuities caused\nby UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the\nObjaverse T2T benchmark and the GSO T2T benchmark, based on selected\nhigh-quality 3D meshes from the Objaverse dataset and the entire GSO dataset,\nrespectively. Extensive experimental results demonstrate that MVPaint surpasses\nexisting state-of-the-art methods. Notably, MVPaint could generate\nhigh-fidelity textures with minimal Janus issues and highly enhanced cross-view\nconsistency.\n","authors":["Wei Cheng","Juncheng Mu","Xianfang Zeng","Xin Chen","Anqi Pang","Chi Zhang","Zhibin Wang","Bin Fu","Gang Yu","Ziwei Liu","Liang Pan"],"pdf_url":"https://arxiv.org/pdf/2411.02336v1.pdf","comment":"Project Page: https://mvpaint.github.io"},{"id":"http://arxiv.org/abs/2411.02334v1","updated":"2024-11-04T17:58:54Z","published":"2024-11-04T17:58:54Z","title":"Diffusion-based Generative Multicasting with Intent-aware Semantic\n  Decomposition","summary":"  Generative diffusion models (GDMs) have recently shown great success in\nsynthesizing multimedia signals with high perceptual quality enabling highly\nefficient semantic communications in future wireless networks. In this paper,\nwe develop an intent-aware generative semantic multicasting framework utilizing\npre-trained diffusion models. In the proposed framework, the transmitter\ndecomposes the source signal to multiple semantic classes based on the\nmulti-user intent, i.e. each user is assumed to be interested in details of\nonly a subset of the semantic classes. The transmitter then sends to each user\nonly its intended classes, and multicasts a highly compressed semantic map to\nall users over shared wireless resources that allows them to locally synthesize\nthe other classes, i.e. non-intended classes, utilizing pre-trained diffusion\nmodels. The signal retrieved at each user is thereby partially reconstructed\nand partially synthesized utilizing the received semantic map. This improves\nutilization of the wireless resources, with better preserving privacy of the\nnon-intended classes. We design a communication/computation-aware scheme for\nper-class adaptation of the communication parameters, such as the transmission\npower and compression rate to minimize the total latency of retrieving signals\nat multiple receivers, tailored to the prevailing channel conditions as well as\nthe users reconstruction/synthesis distortion/perception requirements. The\nsimulation results demonstrate significantly reduced per-user latency compared\nwith non-generative and intent-unaware multicasting benchmarks while\nmaintaining high perceptual quality of the signals retrieved at the users.\n","authors":["Xinkai Liu","Mahdi Boloursaz Mashhadi","Li Qiao","Yi Ma","Rahim Tafazolli","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2411.02334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02327v1","updated":"2024-11-04T17:50:36Z","published":"2024-11-04T17:50:36Z","title":"PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance","summary":"  The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.\n","authors":["Ruyang Liu","Haoran Tang","Haibo Liu","Yixiao Ge","Ying Shan","Chen Li","Jiankun Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02319v1","updated":"2024-11-04T17:45:44Z","published":"2024-11-04T17:45:44Z","title":"GenXD: Generating Any 3D and 4D Scenes","summary":"  Recent developments in 2D visual generation have been remarkably successful.\nHowever, 3D and 4D generation remain challenging in real-world applications due\nto the lack of large-scale 4D data and effective model design. In this paper,\nwe propose to jointly investigate general 3D and 4D generation by leveraging\ncamera and object movements commonly observed in daily life. Due to the lack of\nreal-world 4D data in the community, we first propose a data curation pipeline\nto obtain camera poses and object motion strength from videos. Based on this\npipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.\nBy leveraging all the 3D and 4D data, we develop our framework, GenXD, which\nallows us to produce any 3D or 4D scene. We propose multiview-temporal modules,\nwhich disentangle camera and object movements, to seamlessly learn from both 3D\nand 4D data. Additionally, GenXD employs masked latent conditions to support a\nvariety of conditioning views. GenXD can generate videos that follow the camera\ntrajectory as well as consistent 3D views that can be lifted into 3D\nrepresentations. We perform extensive evaluations across various real-world and\nsynthetic datasets, demonstrating GenXD's effectiveness and versatility\ncompared to previous methods in 3D and 4D generation.\n","authors":["Yuyang Zhao","Chung-Ching Lin","Kevin Lin","Zhiwen Yan","Linjie Li","Zhengyuan Yang","Jianfeng Wang","Gim Hee Lee","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18145v2","updated":"2024-11-04T17:31:40Z","published":"2024-07-25T15:49:26Z","title":"Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for\n  Open-World Perception","summary":"  Semantic segmentation models are typically trained on a fixed set of classes,\nlimiting their applicability in open-world scenarios. Class-incremental\nsemantic segmentation aims to update models with emerging new classes while\npreventing catastrophic forgetting of previously learned ones. However,\nexisting methods impose strict rigidity on old classes, reducing their\neffectiveness in learning new incremental classes. In this work, we propose\nTaxonomy-Oriented Poincar\\'e-regularized Incremental-Class Segmentation\n(TOPICS) that learns feature embeddings in hyperbolic space following explicit\ntaxonomy-tree structures. This supervision provides plasticity for old classes,\nupdating ancestors based on new classes while integrating new classes at\nfitting positions. Additionally, we maintain implicit class relational\nconstraints on the geometric basis of the Poincar\\'e ball. This ensures that\nthe latent space can continuously adapt to new constraints while maintaining a\nrobust structure to combat catastrophic forgetting. We also establish eight\nrealistic incremental learning protocols for autonomous driving scenarios,\nwhere novel classes can originate from known classes or the background.\nExtensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0\nbenchmarks demonstrate that it achieves state-of-the-art performance. We make\nthe code and trained models publicly available at\nhttp://topics.cs.uni-freiburg.de.\n","authors":["Julia Hindel","Daniele Cattaneo","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2407.18145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10376v2","updated":"2024-11-04T17:28:54Z","published":"2024-02-16T00:04:36Z","title":"Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)","summary":"  CLIP embeddings have demonstrated remarkable performance across a wide range\nof multimodal applications. However, these high-dimensional, dense vector\nrepresentations are not easily interpretable, limiting our understanding of the\nrich structure of CLIP and its use in downstream applications that require\ntransparency. In this work, we show that the semantic structure of CLIP's\nlatent space can be leveraged to provide interpretability, allowing for the\ndecomposition of representations into semantic concepts. We formulate this\nproblem as one of sparse recovery and propose a novel method, Sparse Linear\nConcept Embeddings, for transforming CLIP representations into sparse linear\ncombinations of human-interpretable concepts. Distinct from previous work,\nSpLiCE is task-agnostic and can be used, without training, to explain and even\nreplace traditional dense CLIP representations, maintaining high downstream\nperformance while significantly improving their interpretability. We also\ndemonstrate significant use cases of SpLiCE representations including detecting\nspurious correlations and model editing.\n","authors":["Usha Bhalla","Alex Oesterling","Suraj Srinivas","Flavio P. Calmon","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2402.10376v2.pdf","comment":"25 pages, 15 figures, NeurIPS 2024. Code is provided at\n  https://github.com/AI4LIFE-GROUP/SpLiCE"},{"id":"http://arxiv.org/abs/2411.02299v1","updated":"2024-11-04T17:25:10Z","published":"2024-11-04T17:25:10Z","title":"Grouped Discrete Representation for Object-Centric Learning","summary":"  Object-Centric Learning (OCL) can discover objects in images or videos by\nsimply reconstructing the input. For better object discovery, representative\nOCL methods reconstruct the input as its Variational Autoencoder (VAE)\nintermediate representation, which suppresses pixel noises and promotes object\nseparability by discretizing continuous super-pixels with template features.\nHowever, treating features as units overlooks their composing attributes, thus\nimpeding model generalization; indexing features with scalar numbers loses\nattribute-level similarities and differences, thus hindering model convergence.\nWe propose \\textit{Grouped Discrete Representation} (GDR) for OCL. We decompose\nfeatures into combinatorial attributes via organized channel grouping, and\ncompose these attributes into discrete representation via tuple indexes.\nExperiments show that our GDR improves both Transformer- and Diffusion-based\nOCL methods consistently on various datasets. Visualizations show that our GDR\ncaptures better object separability.\n","authors":["Rongzhen Zhao","Vivienne Wang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2411.02299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02293v1","updated":"2024-11-04T17:21:42Z","published":"2024-11-04T17:21:42Z","title":"Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D\n  Generation","summary":"  While 3D generative models have greatly improved artists' workflows, the\nexisting diffusion models for 3D generation suffer from slow generation and\npoor generalization. To address this issue, we propose a two-stage approach\nnamed Hunyuan3D-1.0 including a lite version and a standard version, that both\nsupport text- and image-conditioned generation. In the first stage, we employ a\nmulti-view diffusion model that efficiently generates multi-view RGB in\napproximately 4 seconds. These multi-view images capture rich details of the 3D\nasset from different viewpoints, relaxing the tasks from single-view to\nmulti-view reconstruction. In the second stage, we introduce a feed-forward\nreconstruction model that rapidly and faithfully reconstructs the 3D asset\ngiven the generated multi-view images in approximately 7 seconds. The\nreconstruction network learns to handle noises and in-consistency introduced by\nthe multi-view diffusion and leverages the available information from the\ncondition image to efficiently recover the 3D structure. % Extensive\nexperimental results demonstrate the effectiveness of Hunyuan3D-1.0 in\ngenerating high-quality 3D assets. Our framework involves the text-to-image\nmodel ~\\ie, Hunyuan-DiT, making it a unified framework to support both text-\nand image-conditioned 3D generation. Our standard version has $10\\times$ more\nparameters than our lite and other existing model. Our Hunyuan3D-1.0 achieves\nan impressive balance between speed and quality, significantly reducing\ngeneration time while maintaining the quality and diversity of the produced\nassets.\n","authors":["Xianghui Yang","Huiwen Shi","Bowen Zhang","Fan Yang","Jiacheng Wang","Hongxu Zhao","Xinhai Liu","Xinzhou Wang","Qingxiang Lin","Jiaao Yu","Lifu Wang","Zhuo Chen","Sicong Liu","Yuhong Liu","Yong Yang","Di Wang","Jie Jiang","Chunchao Guo"],"pdf_url":"https://arxiv.org/pdf/2411.02293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02281v1","updated":"2024-11-04T17:09:58Z","published":"2024-11-04T17:09:58Z","title":"Conformal-in-the-Loop for Learning with Imbalanced Noisy Data","summary":"  Class imbalance and label noise are pervasive in large-scale datasets, yet\nmuch of machine learning research assumes well-labeled, balanced data, which\nrarely reflects real world conditions. Existing approaches typically address\neither label noise or class imbalance in isolation, leading to suboptimal\nresults when both issues coexist. In this work, we propose\nConformal-in-the-Loop (CitL), a novel training framework that addresses both\nchallenges with a conformal prediction-based approach. CitL evaluates sample\nuncertainty to adjust weights and prune unreliable examples, enhancing model\nresilience and accuracy with minimal computational cost. Our extensive\nexperiments include a detailed analysis showing how CitL effectively emphasizes\nimpactful data in noisy, imbalanced datasets. Our results show that CitL\nconsistently boosts model performance, achieving up to a 6.1% increase in\nclassification accuracy and a 5.0 mIoU improvement in segmentation. Our code is\npublicly available: CitL.\n","authors":["John Brandon Graham-Knight","Jamil Fayyad","Nourhan Bayasi","Patricia Lasserre","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2411.02281v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.04280v2","updated":"2024-11-04T17:02:45Z","published":"2024-06-06T17:26:40Z","title":"xMIL: Insightful Explanations for Multiple Instance Learning in\n  Histopathology","summary":"  Multiple instance learning (MIL) is an effective and widely used approach for\nweakly supervised machine learning. In histopathology, MIL models have achieved\nremarkable success in tasks like tumor detection, biomarker prediction, and\noutcome prognostication. However, MIL explanation methods are still lagging\nbehind, as they are limited to small bag sizes or disregard instance\ninteractions. We revisit MIL through the lens of explainable AI (XAI) and\nintroduce xMIL, a refined framework with more general assumptions. We\ndemonstrate how to obtain improved MIL explanations using layer-wise relevance\npropagation (LRP) and conduct extensive evaluation experiments on three toy\nsettings and four real-world histopathology datasets. Our approach consistently\noutperforms previous explanation attempts with particularly improved\nfaithfulness scores on challenging biomarker prediction tasks. Finally, we\nshowcase how xMIL explanations enable pathologists to extract insights from MIL\nmodels, representing a significant advance for knowledge discovery and model\ndebugging in digital histopathology. Codes are available at:\nhttps://github.com/tubml-pathology/xMIL.\n","authors":["Julius Hense","Mina Jamshidi Idaji","Oliver Eberle","Thomas Schnake","Jonas Dippel","Laure Ciernik","Oliver Buchstab","Andreas Mock","Frederick Klauschen","Klaus-Robert Müller"],"pdf_url":"https://arxiv.org/pdf/2406.04280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02256v1","updated":"2024-11-04T16:46:53Z","published":"2024-11-04T16:46:53Z","title":"Unified Speech Recognition: A Single Model for Auditory, Visual, and\n  Audiovisual Inputs","summary":"  Research in auditory, visual, and audiovisual speech recognition (ASR, VSR,\nand AVSR, respectively) has traditionally been conducted independently. Even\nrecent self-supervised studies addressing two or all three tasks simultaneously\ntend to yield separate models, leading to disjoint inference pipelines with\nincreased memory requirements and redundancies. This paper proposes unified\ntraining strategies for these systems. We demonstrate that training a single\nmodel for all three tasks enhances VSR and AVSR performance, overcoming typical\noptimisation challenges when training from scratch. Moreover, we introduce a\ngreedy pseudo-labelling approach to more effectively leverage unlabelled\nsamples, addressing shortcomings in related self-supervised methods. Finally,\nwe develop a self-supervised pre-training method within our framework, proving\nits effectiveness alongside our semi-supervised approach. Despite using a\nsingle model for all tasks, our unified approach achieves state-of-the-art\nperformance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR,\nas well as on the newly released WildVSR dataset. Code and models are available\nat https://github.com/ahaliassos/usr.\n","authors":["Alexandros Haliassos","Rodrigo Mira","Honglie Chen","Zoe Landgraf","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2411.02256v1.pdf","comment":"NeurIPS 2024. Code: https://github.com/ahaliassos/usr"},{"id":"http://arxiv.org/abs/2411.00225v2","updated":"2024-11-04T16:46:01Z","published":"2024-10-31T21:52:33Z","title":"Fashion-VDM: Video Diffusion Model for Virtual Try-On","summary":"  We present Fashion-VDM, a video diffusion model (VDM) for generating virtual\ntry-on videos. Given an input garment image and person video, our method aims\nto generate a high-quality try-on video of the person wearing the given\ngarment, while preserving the person's identity and motion. Image-based virtual\ntry-on has shown impressive results; however, existing video virtual try-on\n(VVT) methods are still lacking garment details and temporal consistency. To\naddress these issues, we propose a diffusion-based architecture for video\nvirtual try-on, split classifier-free guidance for increased control over the\nconditioning inputs, and a progressive temporal training strategy for\nsingle-pass 64-frame, 512px video generation. We also demonstrate the\neffectiveness of joint image-video training for video try-on, especially when\nvideo data is limited. Our qualitative and quantitative experiments show that\nour approach sets the new state-of-the-art for video virtual try-on. For\nadditional results, visit our project page:\nhttps://johannakarras.github.io/Fashion-VDM.\n","authors":["Johanna Karras","Yingwei Li","Nan Liu","Luyang Zhu","Innfarn Yoo","Andreas Lugmayr","Chris Lee","Ira Kemelmacher-Shlizerman"],"pdf_url":"https://arxiv.org/pdf/2411.00225v2.pdf","comment":"Accepted to SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2405.07257v3","updated":"2024-11-04T16:42:38Z","published":"2024-05-12T11:41:44Z","title":"SPEAK: Speech-Driven Pose and Emotion-Adjustable Talking Head Generation","summary":"  Most earlier researches on talking face generation have focused on the\nsynchronization of lip motion and speech content. However, head pose and facial\nemotions are equally important characteristics of natural faces. While\naudio-driven talking face generation has seen notable advancements, existing\nmethods either overlook facial emotions or are limited to specific individuals\nand cannot be applied to arbitrary subjects. In this paper, we propose a novel\none-shot Talking Head Generation framework (SPEAK) that distinguishes itself\nfrom the general Talking Face Generation by enabling emotional and postural\ncontrol. Specifically, we introduce Inter-Reconstructed Feature Disentanglement\n(IRFD) module to decouple facial features into three latent spaces. Then we\ndesign a face editing module that modifies speech content and facial latent\ncodes into a single latent space. Subsequently, we present a novel generator\nthat employs modified latent codes derived from the editing module to regulate\nemotional expression, head poses, and speech content in synthesizing facial\nanimations. Extensive trials demonstrate that our method ensures lip\nsynchronization with the audio while enabling decoupled control of facial\nfeatures, it can generate realistic talking head with coordinated lip motions,\nauthentic facial emotions, and smooth head movements. The demo video is\navailable: https://anonymous.4open.science/r/SPEAK-8A22\n","authors":["Changpeng Cai","Guinan Guo","Jiao Li","Junhao Su","Fei Shen","Chenghao He","Jing Xiao","Yuanxu Chen","Lei Dai","Feiyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.07257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02236v1","updated":"2024-11-04T16:30:14Z","published":"2024-11-04T16:30:14Z","title":"3D Audio-Visual Segmentation","summary":"  Recognizing the sounding objects in scenes is a longstanding objective in\nembodied AI, with diverse applications in robotics and AR/VR/MR. To that end,\nAudio-Visual Segmentation (AVS), taking as condition an audio signal to\nidentify the masks of the target sounding objects in an input image with\nsynchronous camera and microphone sensors, has been recently advanced. However,\nthis paradigm is still insufficient for real-world operation, as the mapping\nfrom 2D images to 3D scenes is missing. To address this fundamental limitation,\nwe introduce a novel research problem, 3D Audio-Visual Segmentation, extending\nthe existing AVS to the 3D output space. This problem poses more challenges due\nto variations in camera extrinsics, audio scattering, occlusions, and diverse\nacoustics across sounding object categories. To facilitate this research, we\ncreate the very first simulation based benchmark, 3DAVS-S34-O7, providing\nphotorealistic 3D scene environments with grounded spatial audio under\nsingle-instance and multi-instance settings, across 34 scenes and 7 object\ncategories. This is made possible by re-purposing the Habitat simulator to\ngenerate comprehensive annotations of sounding object locations and\ncorresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet,\ncharacterized by integrating the ready-to-use knowledge from pretrained 2D\naudio-visual foundation models synergistically with 3D visual scene\nrepresentation through spatial audio-aware mask alignment and refinement.\nExtensive experiments demonstrate that EchoSegnet can effectively segment\nsounding objects in 3D space on our new benchmark, representing a significant\nadvancement in the field of embodied AI. Project page:\nhttps://surrey-uplab.github.io/research/3d-audio-visual-segmentation/\n","authors":["Artem Sokolov","Swapnil Bhosale","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.02236v1.pdf","comment":"Accepted at the NeurIPS 2024 Workshop on Audio Imagination"},{"id":"http://arxiv.org/abs/2411.02229v1","updated":"2024-11-04T16:21:00Z","published":"2024-11-04T16:21:00Z","title":"FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage\n  Training","summary":"  The field of novel view synthesis from images has seen rapid advancements\nwith the introduction of Neural Radiance Fields (NeRF) and more recently with\n3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its\nefficiency and ability to render novel views accurately. While Gaussian\nSplatting performs well when a sufficient amount of training images are\navailable, its unstructured explicit representation tends to overfit in\nscenarios with sparse input images, resulting in poor rendering performance. To\naddress this, we present a 3D Gaussian-based novel view synthesis method using\nsparse input images that can accurately render the scene from the viewpoints\nnot covered by the training images. We propose a multi-stage training scheme\nwith matching-based consistency constraints imposed on the novel views without\nrelying on pre-trained depth estimation or diffusion models. This is achieved\nby using the matches of the available training images to supervise the\ngeneration of the novel views sampled between the training frames with color,\ngeometry, and semantic losses. In addition, we introduce a locality preserving\nregularization for 3D Gaussians which removes rendering artifacts by preserving\nthe local color structure of the scene. Evaluation on synthetic and real-world\ndatasets demonstrates competitive or superior performance of our method in\nfew-shot novel view synthesis compared to existing state-of-the-art methods.\n","authors":["Ruihong Yin","Vladimir Yugay","Yue Li","Sezer Karaoglu","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2411.02229v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2411.02220v1","updated":"2024-11-04T16:14:35Z","published":"2024-11-04T16:14:35Z","title":"SIRA: Scalable Inter-frame Relation and Association for Radar Perception","summary":"  Conventional radar feature extraction faces limitations due to low spatial\nresolution, noise, multipath reflection, the presence of ghost targets, and\nmotion blur. Such limitations can be exacerbated by nonlinear object motion,\nparticularly from an ego-centric viewpoint. It becomes evident that to address\nthese challenges, the key lies in exploiting temporal feature relation over an\nextended horizon and enforcing spatial motion consistency for effective\nassociation. To this end, this paper proposes SIRA (Scalable Inter-frame\nRelation and Association) with two designs. First, inspired by Swin\nTransformer, we introduce extended temporal relation, generalizing the existing\ntemporal relation layer from two consecutive frames to multiple inter-frames\nwith temporally regrouped window attention for scalability. Second, we propose\nmotion consistency track with the concept of a pseudo-tracklet generated from\nobservational data for better trajectory prediction and subsequent object\nassociation. Our approach achieves 58.11 mAP@0.5 for oriented object detection\nand 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing\nprevious state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA,\nrespectively.\n","authors":["Ryoma Yataka","Pu Perry Wang","Petros Boufounos","Ryuhei Takahashi"],"pdf_url":"https://arxiv.org/pdf/2411.02220v1.pdf","comment":"25 pages, Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2411.02210v1","updated":"2024-11-04T16:04:59Z","published":"2024-11-04T16:04:59Z","title":"One VLM to Keep it Learning: Generation and Balancing for Data-free\n  Continual Visual Question Answering","summary":"  Vision-Language Models (VLMs) have shown significant promise in Visual\nQuestion Answering (VQA) tasks by leveraging web-scale multimodal datasets.\nHowever, these models often struggle with continual learning due to\ncatastrophic forgetting when adapting to new tasks. As an effective remedy to\nmitigate catastrophic forgetting, rehearsal strategy uses the data of past\ntasks upon learning new task. However, such strategy incurs the need of storing\npast data, which might not be feasible due to hardware constraints or privacy\nconcerns. In this work, we propose the first data-free method that leverages\nthe language generation capability of a VLM, instead of relying on external\nmodels, to produce pseudo-rehearsal data for addressing continual VQA. Our\nproposal, named as GaB, generates pseudo-rehearsal data by posing previous task\nquestions on new task data. Yet, despite being effective, the distribution of\ngenerated questions skews towards the most frequently posed questions due to\nthe limited and task-specific training data. To mitigate this issue, we\nintroduce a pseudo-rehearsal balancing module that aligns the generated data\ntowards the ground-truth data distribution using either the question\nmeta-statistics or an unsupervised clustering method. We evaluate our proposed\nmethod on two recent benchmarks, \\ie VQACL-VQAv2 and CLOVE-function benchmarks.\nGaB outperforms all the data-free baselines with substantial improvement in\nmaintaining VQA performance across evolving tasks, while being on-par with\nmethods with access to the past data.\n","authors":["Deepayan Das","Davide Talon","Massimiliano Mancini","Yiming Wang","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2411.02210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15127v2","updated":"2024-11-04T15:54:21Z","published":"2024-04-23T15:27:19Z","title":"GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist\n  Collaboration","summary":"  Generalist foundation models (GFMs) are renowned for their exceptional\ncapability and flexibility in effectively generalizing across diverse tasks and\nmodalities. In the field of medicine, while GFMs exhibit superior\ngeneralizability based on their extensive intrinsic knowledge as well as\nproficiency in instruction following and in-context learning, specialist models\nexcel in precision due to their domain knowledge. In this work, for the first\ntime, we explore the synergy between the GFM and specialist models, to enable\nprecise medical image analysis on a broader scope. Specifically, we propose a\ncooperative framework, Generalist-Specialist Collaboration (GSCo), which\nconsists of two stages, namely the construction of GFM and specialists, and\ncollaborative inference on downstream tasks. In the construction stage, we\ndevelop MedDr, the largest open-source GFM tailored for medicine, showcasing\nexceptional instruction-following and in-context learning capabilities.\nMeanwhile, a series of lightweight specialists are crafted for downstream tasks\nwith low computational cost. In the collaborative inference stage, we introduce\ntwo cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented\nDiagnosis, to harvest the generalist's in-context learning abilities alongside\nthe specialists' domain expertise. For a comprehensive evaluation, we curate a\nlarge-scale benchmark featuring 28 datasets and about 250,000 images. Extensive\nresults demonstrate that MedDr consistently outperforms state-of-the-art GFMs\non downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists\nacross all out-of-domain disease diagnosis datasets. These findings indicate a\nsignificant paradigm shift in the application of GFMs, transitioning from\nseparate models for specific tasks to a collaborative approach between GFMs and\nspecialists, thereby advancing the frontiers of generalizable AI in medicine.\n","authors":["Sunan He","Yuxiang Nie","Hongmei Wang","Shu Yang","Yihui Wang","Zhiyuan Cai","Zhixuan Chen","Yingxue Xu","Luyang Luo","Huiling Xiang","Xi Lin","Mingxiang Wu","Yifan Peng","George Shih","Ziyang Xu","Xian Wu","Qiong Wang","Ronald Cheong Kin Chan","Varut Vardhanabhuti","Winnie Chiu Wing Chu","Yefeng Zheng","Pranav Rajpurkar","Kang Zhang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.15127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20915v2","updated":"2024-11-04T15:48:10Z","published":"2024-05-31T15:21:44Z","title":"Fast yet Safe: Early-Exiting with Risk Control","summary":"  Scaling machine learning models significantly improves their performance.\nHowever, such gains come at the cost of inference being slow and\nresource-intensive. Early-exit neural networks (EENNs) offer a promising\nsolution: they accelerate inference by allowing intermediate layers to exit and\nproduce a prediction early. Yet a fundamental issue with EENNs is how to\ndetermine when to exit without severely degrading performance. In other words,\nwhen is it 'safe' for an EENN to go 'fast'? To address this issue, we\ninvestigate how to adapt frameworks of risk control to EENNs. Risk control\noffers a distribution-free, post-hoc solution that tunes the EENN's exiting\nmechanism so that exits only occur when the output is of sufficient quality. We\nempirically validate our insights on a range of vision and language tasks,\ndemonstrating that risk control can produce substantial computational savings,\nall the while preserving user-specified performance goals.\n","authors":["Metod Jazbec","Alexander Timans","Tin Hadži Veljković","Kaspar Sakmann","Dan Zhang","Christian A. Naesseth","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2405.20915v2.pdf","comment":"27 pages, 13 figures, 4 tables (incl. appendix)"},{"id":"http://arxiv.org/abs/2411.02188v1","updated":"2024-11-04T15:42:22Z","published":"2024-11-04T15:42:22Z","title":"Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition\n  via Foundation Models","summary":"  The accuracy of face recognition systems has improved significantly in the\npast few years, thanks to the large amount of data collected and the\nadvancement in neural network architectures. However, these large-scale\ndatasets are often collected without explicit consent, raising ethical and\nprivacy concerns. To address this, there have been proposals to use synthetic\ndatasets for training face recognition models. Yet, such models still rely on\nreal data to train the generative models and generally exhibit inferior\nperformance compared to those trained on real datasets. One of these datasets,\nDigiFace, uses a graphics pipeline to generate different identities and\ndifferent intra-class variations without using real data in training the\nmodels. However, the performance of this approach is poor on face recognition\nbenchmarks, possibly due to the lack of realism in the images generated from\nthe graphics pipeline. In this work, we introduce a novel framework for realism\ntransfer aimed at enhancing the realism of synthetically generated face images.\nOur method leverages the large-scale face foundation model, and we adapt the\npipeline for realism enhancement. By integrating the controllable aspects of\nthe graphics pipeline with our realism enhancement technique, we generate a\nlarge amount of realistic variations-combining the advantages of both\napproaches. Our empirical evaluations demonstrate that models trained using our\nenhanced dataset significantly improve the performance of face recognition\nsystems over the baseline. The source code and datasets will be made available\npublicly.\n","authors":["Anjith George","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2411.02188v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2411.02184v1","updated":"2024-11-04T15:39:12Z","published":"2024-11-04T15:39:12Z","title":"Double Descent Meets Out-of-Distribution Detection: Theoretical Insights\n  and Empirical Analysis on the role of model complexity","summary":"  While overparameterization is known to benefit generalization, its impact on\nOut-Of-Distribution (OOD) detection is less understood. This paper investigates\nthe influence of model complexity in OOD detection. We propose an expected OOD\nrisk metric to evaluate classifiers confidence on both training and OOD\nsamples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD\nrisk of binary least-squares classifiers applied to Gaussian data. We show that\nthe OOD risk depicts an infinite peak, when the number of parameters is equal\nto the number of samples, which we associate with the double descent\nphenomenon. Our experimental study on different OOD detection methods across\nmultiple neural architectures extends our theoretical insights and highlights a\ndouble descent curve. Our observations suggest that overparameterization does\nnot necessarily lead to better OOD detection. Using the Neural Collapse\nframework, we provide insights to better understand this behavior. To\nfacilitate reproducibility, our code will be made publicly available upon\npublication.\n","authors":["Mouïn Ben Ammar","David Brellmann","Arturo Mendoza","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2411.02184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02181v1","updated":"2024-11-04T15:38:32Z","published":"2024-11-04T15:38:32Z","title":"Detect an Object At Once without Fine-tuning","summary":"  When presented with one or a few photos of a previously unseen object, humans\ncan instantly recognize it in different scenes. Although the human brain\nmechanism behind this phenomenon is still not fully understood, this work\nintroduces a novel technical realization of this task. It consists of two\nphases: (1) generating a Similarity Density Map (SDM) by convolving the scene\nimage with the given object image patch(es) so that the highlight areas in the\nSDM indicate the possible locations; (2) obtaining the object occupied areas in\nthe scene through a Region Alignment Network (RAN). The RAN is constructed on a\nbackbone of Deep Siamese Network (DSN), and different from the traditional\nDSNs, it aims to obtain the object accurate regions by regressing the location\nand area differences between the ground truths and the predicted ones indicated\nby the highlight areas in SDM. By pre-learning from labels annotated in\ntraditional datasets, the SDM-RAN can detect previously unknown objects without\nfine-tuning. Experiments were conducted on the MS COCO, PASCAL VOC datasets.\nThe results indicate that the proposed method outperforms state-of-the-art\nmethods on the same task.\n","authors":["Junyu Hao","Jianheng Liu","Yongjia Zhao","Zuofan Chen","Qi Sun","Jinlong Chen","Jianguo Wei","Minghao Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02179v1","updated":"2024-11-04T15:37:18Z","published":"2024-11-04T15:37:18Z","title":"CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile\n  Augmented Reality","summary":"  High-quality environment lighting is the foundation of creating immersive\nuser experiences in mobile augmented reality (AR) applications. However,\nachieving visually coherent environment lighting estimation for Mobile AR is\nchallenging due to several key limitations associated with AR device sensing\ncapabilities, including limitations in device camera FoV and pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address their key limitations of\ngeneration hallucination and slow inference process. To do so, in this work, we\ndesign and implement a generative lighting estimation system called CleAR that\ncan produce high-quality and diverse environment maps in the format of\n360$^\\circ$ images. Specifically, we design a two-step generation pipeline\nguided by AR environment context data to ensure the results follow physical\nenvironment visual context and color appearances. To improve the estimation\nrobustness under different lighting conditions, we design a real-time\nrefinement component to adjust lighting estimation results on AR devices. To\ntrain and test our generative models, we curate a large-scale environment\nlighting estimation dataset with diverse lighting conditions. Through\nquantitative evaluation and user study, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy and\nrobustness. Moreover, CleAR supports real-time refinement of lighting\nestimation results, ensuring robust and timely environment lighting updates for\nAR applications. Our end-to-end generative estimation takes as fast as 3.2\nseconds, outperforming state-of-the-art methods by 110x.\n","authors":["Yiqin Zhao","Mallesham Dasari","Tian Guo"],"pdf_url":"https://arxiv.org/pdf/2411.02179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02175v1","updated":"2024-11-04T15:34:30Z","published":"2024-11-04T15:34:30Z","title":"SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning\n  with Pre-Trained Models","summary":"  Continual learning aims to incrementally acquire new concepts in data streams\nwhile resisting forgetting previous knowledge. With the rise of powerful\npre-trained models (PTMs), there is a growing interest in training incremental\nlearning systems using these foundation models, rather than learning from\nscratch. Existing works often view PTMs as a strong initial point and directly\napply parameter-efficient tuning (PET) in the first session for adapting to\ndownstream tasks. In the following sessions, most methods freeze model\nparameters for tackling forgetting issues. However, applying PET directly to\ndownstream data cannot fully explore the inherent knowledge in PTMs.\nAdditionally, freezing the parameters in incremental sessions hinders models'\nplasticity to novel concepts not covered in the first session. To solve the\nabove issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE)\nframework. In particular, to inherit general knowledge from foundation models,\nwe include a transfer loss function by measuring the correlation between the\nPTM and the PET-applied model. After calibrating in the first session, the slow\nefficient tuning parameters can capture more informative features, improving\ngeneralization to incoming classes. Moreover, to further incorporate novel\nconcepts, we strike a balance between stability and plasticity by fixing slow\nefficient tuning parameters and continuously updating the fast ones.\nSpecifically, a cross-classification loss with feature alignment is proposed to\ncircumvent catastrophic forgetting. During inference, we introduce an\nentropy-based aggregation strategy to dynamically utilize the complementarity\nin the slow and fast learners. Extensive experiments on seven benchmark\ndatasets verify the effectiveness of our method by significantly surpassing the\nstate-of-the-art.\n","authors":["Linglan Zhao","Xuerui Zhang","Ke Yan","Shouhong Ding","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2411.02175v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02149v1","updated":"2024-11-04T15:06:57Z","published":"2024-11-04T15:06:57Z","title":"Improving Domain Generalization in Self-supervised Monocular Depth\n  Estimation via Stabilized Adversarial Training","summary":"  Learning a self-supervised Monocular Depth Estimation (MDE) model with great\ngeneralization remains significantly challenging. Despite the success of\nadversarial augmentation in the supervised learning generalization, naively\nincorporating it into self-supervised MDE models potentially causes\nover-regularization, suffering from severe performance degradation. In this\npaper, we conduct qualitative analysis and illuminate the main causes: (i)\ninherent sensitivity in the UNet-alike depth network and (ii) dual optimization\nconflict caused by over-regularization. To tackle these issues, we propose a\ngeneral adversarial training framework, named Stabilized Conflict-optimization\nAdversarial Training (SCAT), integrating adversarial data augmentation into\nself-supervised MDE methods to achieve a balance between stability and\ngeneralization. Specifically, we devise an effective scaling depth network that\ntunes the coefficients of long skip connection and effectively stabilizes the\ntraining process. Then, we propose a conflict gradient surgery strategy, which\nprogressively integrates the adversarial gradient and optimizes the model\ntoward a conflict-free direction. Extensive experiments on five benchmarks\ndemonstrate that SCAT can achieve state-of-the-art performance and\nsignificantly improve the generalization capability of existing self-supervised\nMDE methods.\n","authors":["Yuanqi Yao","Gang Wu","Kui Jiang","Siao Liu","Jian Kuai","Xianming Liu","Junjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.02149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02136v1","updated":"2024-11-04T14:49:01Z","published":"2024-11-04T14:49:01Z","title":"Advanced computer vision for extracting georeferenced vehicle\n  trajectories from drone imagery","summary":"  This paper presents a framework for extracting georeferenced vehicle\ntrajectories from high-altitude drone footage, addressing key challenges in\nurban traffic monitoring and limitations of traditional ground-based systems.\nWe employ state-of-the-art computer vision and deep learning to create an\nend-to-end pipeline that enhances vehicle detection, tracking, and trajectory\nstabilization. Conducted in the Songdo International Business District, South\nKorea, the study used a multi-drone experiment over 20 intersections, capturing\napproximately 12TB of 4K video data over four days. We developed a novel track\nstabilization method that uses detected vehicle bounding boxes as exclusion\nmasks during image registration, which, combined with advanced georeferencing\ntechniques, accurately transforms vehicle coordinates into real-world\ngeographical data. Additionally, our framework includes robust vehicle\ndimension estimation and detailed road segmentation for in-depth traffic\nanalysis. The framework produced two high-quality datasets: the Songdo Traffic\ndataset, comprising nearly 1 million unique vehicle trajectories, and the\nSongdo Vision dataset, containing over 5,000 human-annotated frames with about\n300,000 vehicle instances in four classes. Comparisons between drone-derived\ndata and high-precision sensor data from an instrumented probe vehicle\nhighlight the accuracy and consistency of our framework's extraction in dense\nurban settings. By publicly releasing these datasets and the pipeline source\ncode, this work sets new benchmarks for data quality, reproducibility, and\nscalability in traffic research. Results demonstrate the potential of\nintegrating drone technology with advanced computer vision for precise,\ncost-effective urban traffic monitoring, providing valuable resources for the\nresearch community to develop intelligent transportation systems and improve\ntraffic management strategies.\n","authors":["Robert Fonod","Haechan Cho","Hwasoo Yeo","Nikolas Geroliminis"],"pdf_url":"https://arxiv.org/pdf/2411.02136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13174v2","updated":"2024-11-04T14:48:23Z","published":"2024-09-20T03:02:05Z","title":"Manipulation Facing Threats: Evaluating Physical Vulnerabilities in\n  End-to-End Vision Language Action Models","summary":"  Recently, driven by advancements in Multimodal Large Language Models (MLLMs),\nVision Language Action Models (VLAMs) are being proposed to achieve better\nperformance in open-vocabulary scenarios for robotic manipulation tasks. Since\nmanipulation tasks involve direct interaction with the physical world, ensuring\nrobustness and safety during the execution of this task is always a very\ncritical issue. In this paper, by synthesizing current safety research on MLLMs\nand the specific application scenarios of the manipulation task in the physical\nworld, we comprehensively evaluate VLAMs in the face of potential physical\nthreats. Specifically, we propose the Physical Vulnerability Evaluating\nPipeline (PVEP) that can incorporate as many visual modal physical threats as\npossible for evaluating the physical robustness of VLAMs. The physical threats\nin PVEP specifically include Out-of-Distribution, Typography-based Visual\nPrompts, and Adversarial Patch Attacks. By comparing the performance\nfluctuations of VLAMs before and after being attacked, we provide generalizable\nAnalyses of how VLAMs respond to different physical security threats. Our\nproject page is in this link:\nhttps://chaducheng.github.io/Manipulat-Facing-Threats/.\n","authors":["Hao Cheng","Erjia Xiao","Chengyuan Yu","Zhao Yao","Jiahang Cao","Qiang Zhang","Jiaxu Wang","Mengshu Sun","Kaidi Xu","Jindong Gu","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2409.13174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17360v2","updated":"2024-11-04T14:47:12Z","published":"2024-04-26T12:21:57Z","title":"UniRGB-IR: A Unified Framework for RGB-Infrared Semantic Tasks via\n  Adapter Tuning","summary":"  Semantic analysis on visible (RGB) and infrared (IR) images has gained\nattention for its ability to be more accurate and robust under low-illumination\nand complex weather conditions. Due to the lack of pre-trained foundation\nmodels on the large-scale infrared image datasets, existing methods prefer to\ndesign task-specific frameworks and directly fine-tune them with pre-trained\nfoundation models on their RGB-IR semantic relevance datasets, which results in\npoor scalability and limited generalization. In this work, we propose a general\nand efficient framework called UniRGB-IR to unify RGB-IR semantic tasks, in\nwhich a novel adapter is developed to efficiently introduce richer RGB-IR\nfeatures into the pre-trained RGB-based foundation model. Specifically, our\nframework consists of a RGB-based foundation model, a Multi-modal Feature Pool\n(MFP) module and a Supplementary Feature Injector (SFI) module. The MFP and SFI\nmodules cooperate with each other as an adapter to effectively complement the\nRGB-based features with the rich RGB-IR features. During training process, we\nfreeze the entire foundation model to inherit prior knowledge and only optimize\nthe proposed adapter. Furthermore, to verify the effectiveness of our\nframework, we utilize the vanilla vision transformer (ViT-Base) as the\npre-trained foundation model to perform extensive experiments. Experimental\nresults on various RGB-IR downstream tasks demonstrate that our method can\nachieve state-of-the-art performance. The source code and results are available\nat https://github.com/PoTsui99/UniRGB-IR.git.\n","authors":["Maoxun Yuan","Bo Cui","Tianyi Zhao","Jiayi Wang","Shan Fu","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2404.17360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06753v3","updated":"2024-11-04T14:32:02Z","published":"2023-03-12T21:01:54Z","title":"Modular Quantization-Aware Training for 6D Object Pose Estimation","summary":"  Edge applications, such as collaborative robotics and spacecraft rendezvous,\ndemand efficient 6D object pose estimation on resource-constrained embedded\nplatforms. Existing 6D pose estimation networks are often too large for such\ndeployments, necessitating compression while maintaining reliable performance.\nTo address this challenge, we introduce Modular Quantization-Aware Training\n(MQAT), an adaptive and mixed-precision quantization-aware training strategy\nthat exploits the modular structure of modern 6D pose estimation architectures.\nMQAT guides a systematic gradated modular quantization sequence and determines\nmodule-specific bit precisions, leading to quantized models that outperform\nthose produced by state-of-the-art uniform and mixed-precision quantization\ntechniques. Our experiments showcase the generality of MQAT across datasets,\narchitectures, and quantization algorithms. Remarkably, MQAT-trained quantized\nmodels achieve a significant accuracy boost (>7%) over the baseline\nfull-precision network while reducing model size by a factor of 4x or more. Our\nproject website is at: https://saqibjaved1.github.io/MQAT_/\n","authors":["Saqib Javed","Chengkun Li","Andrew Price","Yinlin Hu","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2303.06753v3.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR), 2024"},{"id":"http://arxiv.org/abs/2408.05500v2","updated":"2024-11-04T14:30:03Z","published":"2024-08-10T09:31:58Z","title":"PointNCBW: Towards Dataset Ownership Verification for Point Clouds via\n  Negative Clean-label Backdoor Watermark","summary":"  Recently, point clouds have been widely used in computer vision, whereas\ntheir collection is time-consuming and expensive. As such, point cloud datasets\nare the valuable intellectual property of their owners and deserve protection.\nTo detect and prevent unauthorized use of these datasets, especially for\ncommercial or open-sourced ones that cannot be sold again or used commercially\nwithout permission, we intend to identify whether a suspicious third-party\nmodel is trained on our protected dataset under the black-box setting. We\nachieve this goal by designing a scalable clean-label backdoor-based dataset\nwatermark for point clouds that ensures both effectiveness and stealthiness.\nUnlike existing clean-label watermark schemes, which are susceptible to the\nnumber of categories, our method could watermark samples from all classes\ninstead of only from the target one. Accordingly, it can still preserve high\neffectiveness even on large-scale datasets with many classes. Specifically, we\nperturb selected point clouds with non-target categories in both shape-wise and\npoint-wise manners before inserting trigger patterns without changing their\nlabels. The features of perturbed samples are similar to those of benign\nsamples from the target class. As such, models trained on the watermarked\ndataset will have a distinctive yet stealthy backdoor behavior, i.e.,\nmisclassifying samples from the target class whenever triggers appear, since\nthe trained DNNs will treat the inserted trigger pattern as a signal to deny\npredicting the target label. We also design a hypothesis-test-guided dataset\nownership verification based on the proposed watermark. Extensive experiments\non benchmark datasets are conducted, verifying the effectiveness of our method\nand its resistance to potential removal methods.\n","authors":["Cheng Wei","Yang Wang","Kuofeng Gao","Shuo Shao","Yiming Li","Zhibo Wang","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2408.05500v2.pdf","comment":"This paper was accepted by IEEE Transactions on Information Forensics\n  and Security (TIFS), 2024. 16 pages"},{"id":"http://arxiv.org/abs/2411.02116v1","updated":"2024-11-04T14:29:28Z","published":"2024-11-04T14:29:28Z","title":"Advancements and limitations of LLMs in replicating human color-word\n  associations","summary":"  Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT- 4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category, particularly\nwhen using visual inputs rather than text-based color codes. However, the\nhighest median performance was approximately 50% even for GPT4-o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.\n","authors":["Makoto Fukushima","Shusuke Eshita","Hiroshige Fukuhara"],"pdf_url":"https://arxiv.org/pdf/2411.02116v1.pdf","comment":"20 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.02112v1","updated":"2024-11-04T14:27:10Z","published":"2024-11-04T14:27:10Z","title":"Multi-modal biometric authentication: Leveraging shared layer\n  architectures for enhanced security","summary":"  In this study, we introduce a novel multi-modal biometric authentication\nsystem that integrates facial, vocal, and signature data to enhance security\nmeasures. Utilizing a combination of Convolutional Neural Networks (CNNs) and\nRecurrent Neural Networks (RNNs), our model architecture uniquely incorporates\ndual shared layers alongside modality-specific enhancements for comprehensive\nfeature extraction. The system undergoes rigorous training with a joint loss\nfunction, optimizing for accuracy across diverse biometric inputs.\nFeature-level fusion via Principal Component Analysis (PCA) and classification\nthrough Gradient Boosting Machines (GBM) further refine the authentication\nprocess. Our approach demonstrates significant improvements in authentication\naccuracy and robustness, paving the way for advanced secure identity\nverification solutions.\n","authors":["Vatchala S","Yogesh C","Yeshwanth Govindarajan","Krithik Raja M","Vishal Pramav Amirtha Ganesan","Aashish Vinod A","Dharun Ramesh"],"pdf_url":"https://arxiv.org/pdf/2411.02112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02104v1","updated":"2024-11-04T14:15:26Z","published":"2024-11-04T14:15:26Z","title":"Deep Learning on 3D Semantic Segmentation: A Detailed Review","summary":"  In this paper an exhaustive review and comprehensive analysis of recent and\nformer deep learning methods in 3D Semantic Segmentation (3DSS) is presented.\nIn the related literature, the taxonomy scheme used for the classification of\nthe 3DSS deep learning methods is ambiguous. Based on the taxonomy schemes of 9\nexisting review papers, a new taxonomy scheme of the 3DSS deep learning methods\nis proposed, aiming to standardize it and improve the comparability and clarity\nacross related studies. Furthermore, an extensive overview of the available\n3DSS indoor and outdoor datasets is provided along with their links. The core\npart of the review is the detailed presentation of recent and former 3DSS deep\nlearning methods and their classification using the proposed taxonomy scheme\nalong with their GitHub repositories. Additionally, a brief but informative\nanalysis of the evaluation metrics and loss functions used in 3DSS is included.\nFinally, a fruitful discussion of the examined 3DSS methods and datasets, is\npresented to foster new research directions and applications in the field of\n3DSS. Supplementary, to this review a GitHub repository is provided\n(https://github.com/thobet/Deep-Learning-on-3D-Semantic-Segmentation-a-\nDetailed-Review) including a quick classification of over 400 3DSS methods,\nusing the proposed taxonomy scheme.\n","authors":["Thodoris Betsas","Andreas Georgopoulos","Anastasios Doulamis","Pierre Grussenmeyer"],"pdf_url":"https://arxiv.org/pdf/2411.02104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02099v1","updated":"2024-11-04T14:08:26Z","published":"2024-11-04T14:08:26Z","title":"Differentially Private Integrated Decision Gradients (IDG-DP) for\n  Radar-based Human Activity Recognition","summary":"  Human motion analysis offers significant potential for healthcare monitoring\nand early detection of diseases. The advent of radar-based sensing systems has\ncaptured the spotlight for they are able to operate without physical contact\nand they can integrate with pre-existing Wi-Fi networks. They are also seen as\nless privacy-invasive compared to camera-based systems. However, recent\nresearch has shown high accuracy in recognizing subjects or gender from radar\ngait patterns, raising privacy concerns. This study addresses these issues by\ninvestigating privacy vulnerabilities in radar-based Human Activity Recognition\n(HAR) systems and proposing a novel method for privacy preservation using\nDifferential Privacy (DP) driven by attributions derived with Integrated\nDecision Gradient (IDG) algorithm. We investigate Black-box Membership\nInference Attack (MIA) Models in HAR settings across various levels of\nattacker-accessible information. We extensively evaluated the effectiveness of\nthe proposed IDG-DP method by designing a CNN-based HAR model and rigorously\nassessing its resilience against MIAs. Experimental results demonstrate the\npotential of IDG-DP in mitigating privacy attacks while maintaining utility\nacross all settings, particularly excelling against label-only and shadow model\nblack-box MIA attacks. This work represents a crucial step towards balancing\nthe need for effective radar-based HAR with robust privacy protection in\nhealthcare environments.\n","authors":["Idris Zakariyya","Linda Tran","Kaushik Bhargav Sivangi","Paul Henderson","Fani Deligianni"],"pdf_url":"https://arxiv.org/pdf/2411.02099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22709v2","updated":"2024-11-04T14:06:19Z","published":"2024-10-30T05:38:03Z","title":"FilterViT and DropoutViT: Lightweight Vision Transformer Models for\n  Efficient Attention Mechanisms","summary":"  In this study, we introduce FilterViT, an enhanced version of MobileViT,\nwhich leverages an attention-based mechanism for early-stage downsampling.\nTraditional QKV operations on high-resolution feature maps are computationally\nintensive due to the abundance of tokens. To address this, we propose a filter\nattention mechanism using a convolutional neural network (CNN) to generate an\nimportance mask, focusing attention on key image regions. The method\nsignificantly reduces computational complexity while maintaining\ninterpretability, as it highlights essential image areas. Experimental results\nshow that FilterViT achieves substantial gains in both efficiency and accuracy\ncompared to other models. We also introduce DropoutViT, a variant that uses a\nstochastic approach for pixel selection, further enhancing robustness.\n","authors":["Bohang Sun"],"pdf_url":"https://arxiv.org/pdf/2410.22709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02095v1","updated":"2024-11-04T13:59:01Z","published":"2024-11-04T13:59:01Z","title":"The evolution of volumetric video: A survey of smart transcoding and\n  compression approaches","summary":"  Volumetric video, the capture and display of three-dimensional (3D) imagery,\nhas emerged as a revolutionary technology poised to transform the media\nlandscape, enabling immersive experiences that transcend the limitations of\ntraditional 2D video. One of the key challenges in this domain is the efficient\ndelivery of these high-bandwidth, data-intensive volumetric video streams,\nwhich requires innovative transcoding and compression techniques. This research\npaper explores the state-of-the-art in volumetric video compression and\ndelivery, with a focus on the potential of AI-driven solutions to address the\nunique challenges posed by this emerging medium.\n","authors":["Preetish Kakkar","Hariharan Ragothaman"],"pdf_url":"https://arxiv.org/pdf/2411.02095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06457v2","updated":"2024-11-04T13:43:15Z","published":"2024-08-12T19:17:57Z","title":"Advanced Vision Transformers and Open-Set Learning for Robust Mosquito\n  Classification: A Novel Approach to Entomological Studies","summary":"  Mosquito-related diseases pose a significant threat to global public health,\nnecessitating efficient and accurate mosquito classification for effective\nsurveillance and control. This work presents an innovative approach to mosquito\nclassification by leveraging state-of-the-art vision transformers and open-set\nlearning techniques. A novel framework has been introduced that integrates\nTransformer-based deep learning models with comprehensive data augmentation and\npreprocessing methods, enabling robust and precise identification of ten\nmosquito species. The Swin Transformer model achieves the best performance for\ntraditional closed-set learning with 99.80% accuracy and 0.998 F1 score. The\nlightweight MobileViT technique attains an almost similar accuracy of 98.90%\nwith significantly reduced parameters and model complexities. Next, the applied\ndeep learning models' adaptability and generalizability in a static environment\nhave been enhanced by using new classes of data samples during the inference\nstage that have not been included in the training set. The proposed framework's\nability to handle unseen classes like insects similar to mosquitoes, even\nhumans, through open-set learning further enhances its practical applicability\nby employing the OpenMax technique and Weibull distribution. The traditional\nCNN model, Xception, outperforms the latest transformer with higher accuracy\nand F1 score for open-set learning. The study's findings highlight the\ntransformative potential of advanced deep-learning architectures in entomology,\nproviding a strong groundwork for future research and development in mosquito\nsurveillance and vector control. The implications of this work extend beyond\nmosquito classification, offering valuable insights for broader ecological and\nenvironmental monitoring applications.\n","authors":["Ahmed Akib Jawad Karim","Muhammad Zawad Mahmud","Riasat Khan"],"pdf_url":"https://arxiv.org/pdf/2408.06457v2.pdf","comment":"23 pages, 15 figures"},{"id":"http://arxiv.org/abs/2402.18718v2","updated":"2024-11-04T13:40:24Z","published":"2024-02-28T21:29:16Z","title":"Model Pairing Using Embedding Translation for Backdoor Attack Detection\n  on Open-Set Classification Tasks","summary":"  Backdoor attacks allow an attacker to embed a specific vulnerability in a\nmachine learning algorithm, activated when an attacker-chosen pattern is\npresented, causing a specific misprediction. The need to identify backdoors in\nbiometric scenarios has led us to propose a novel technique with different\ntrade-offs. In this paper we propose to use model pairs on open-set\nclassification tasks for detecting backdoors. Using a simple linear operation\nto project embeddings from a probe model's embedding space to a reference\nmodel's embedding space, we can compare both embeddings and compute a\nsimilarity score. We show that this score, can be an indicator for the presence\nof a backdoor despite models being of different architectures, having been\ntrained independently and on different datasets. This technique allows for the\ndetection of backdoors on models designed for open-set classification tasks,\nwhich is little studied in the literature. Additionally, we show that backdoors\ncan be detected even when both models are backdoored. The source code is made\navailable for reproducibility purposes.\n","authors":["Alexander Unnervik","Hatef Otroshi Shahreza","Anjith George","Sébastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2402.18718v2.pdf","comment":"Accepted in NeurIPS 2024 Safe Generative AI Workshop (oral\n  presentation)"},{"id":"http://arxiv.org/abs/2410.18978v2","updated":"2024-11-04T13:37:31Z","published":"2024-10-24T17:59:51Z","title":"Framer: Interactive Frame Interpolation","summary":"  We propose Framer for interactive frame interpolation, which targets\nproducing smoothly transitioning frames between two images as per user\ncreativity. Concretely, besides taking the start and end frames as inputs, our\napproach supports customizing the transition process by tailoring the\ntrajectory of some selected keypoints. Such a design enjoys two clear benefits.\nFirst, incorporating human interaction mitigates the issue arising from\nnumerous possibilities of transforming one image to another, and in turn\nenables finer control of local motions. Second, as the most basic form of\ninteraction, keypoints help establish the correspondence across frames,\nenhancing the model to handle challenging cases (e.g., objects on the start and\nend frames are of different shapes and styles). It is noteworthy that our\nsystem also offers an \"autopilot\" mode, where we introduce a module to estimate\nthe keypoints and refine the trajectory automatically, to simplify the usage in\npractice. Extensive experimental results demonstrate the appealing performance\nof Framer on various applications, such as image morphing, time-lapse video\ngeneration, cartoon interpolation, etc. The code, the model, and the interface\nwill be released to facilitate further research.\n","authors":["Wen Wang","Qiuyu Wang","Kecheng Zheng","Hao Ouyang","Zhekai Chen","Biao Gong","Hao Chen","Yujun Shen","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2410.18978v2.pdf","comment":"Project page: https://aim-uofa.github.io/Framer/"},{"id":"http://arxiv.org/abs/2410.01768v2","updated":"2024-11-04T13:33:16Z","published":"2024-10-02T17:25:31Z","title":"SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for\n  Remote Sensing Images","summary":"  Remote sensing image plays an irreplaceable role in fields such as\nagriculture, water resources, military, and disaster relief. Pixel-level\ninterpretation is a critical aspect of remote sensing image applications;\nhowever, a prevalent limitation remains the need for extensive manual\nannotation. For this, we try to introduce open-vocabulary semantic segmentation\n(OVSS) into the remote sensing context. However, due to the sensitivity of\nremote sensing images to low-resolution features, distorted target shapes and\nill-fitting boundaries are exhibited in the prediction mask. To tackle this\nissue, we propose a simple and general upsampler, SimFeatUp, to restore lost\nspatial information in deep features in a training-free style. Further, based\non the observation of the abnormal response of local patch tokens to [CLS]\ntoken in CLIP, we propose to execute a straightforward subtraction operation to\nalleviate the global bias in patch tokens. Extensive experiments are conducted\non 17 remote sensing datasets spanning semantic segmentation, building\nextraction, road detection, and flood detection tasks. Our method achieves an\naverage of 5.8%, 8.2%, 4.0%, and 15.3% improvement over state-of-the-art\nmethods on 4 tasks. All codes are released.\n\\url{https://earth-insights.github.io/SegEarth-OV}\n","authors":["Kaiyu Li","Ruixun Liu","Xiangyong Cao","Xueru Bai","Feng Zhou","Deyu Meng","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11944v4","updated":"2024-11-04T13:28:48Z","published":"2024-01-22T13:34:34Z","title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.\n","authors":["Ge Zhang","Xinrun Du","Bei Chen","Yiming Liang","Tongxu Luo","Tianyu Zheng","Kang Zhu","Yuyang Cheng","Chunpu Xu","Shuyue Guo","Haoran Zhang","Xingwei Qu","Junjie Wang","Ruibin Yuan","Yizhi Li","Zekun Wang","Yudong Liu","Yu-Hsuan Tsai","Fengji Zhang","Chenghua Lin","Wenhao Huang","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2401.11944v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02074v1","updated":"2024-11-04T13:26:15Z","published":"2024-11-04T13:26:15Z","title":"GraphVL: Graph-Enhanced Semantic Modeling via Vision-Language Models for\n  Generalized Class Discovery","summary":"  Generalized Category Discovery (GCD) aims to cluster unlabeled images into\nknown and novel categories using labeled images from known classes. To address\nthe challenge of transferring features from known to unknown classes while\nmitigating model bias, we introduce GraphVL, a novel approach for\nvision-language modeling in GCD, leveraging CLIP. Our method integrates a graph\nconvolutional network (GCN) with CLIP's text encoder to preserve class\nneighborhood structure. We also employ a lightweight visual projector for image\ndata, ensuring discriminative features through margin-based contrastive losses\nfor image-text mapping. This neighborhood preservation criterion effectively\nregulates the semantic space, making it less sensitive to known classes.\nAdditionally, we learn textual prompts from known classes and align them to\ncreate a more contextually meaningful semantic feature space for the GCN layer\nusing a contextual similarity loss. Finally, we represent unlabeled samples\nbased on their semantic distance to class prompts from the GCN, enabling\nsemi-supervised clustering for class discovery and minimizing errors. Our\nexperiments on seven benchmark datasets consistently demonstrate the\nsuperiority of GraphVL when integrated with the CLIP backbone.\n","authors":["Bhupendra Solanki","Ashwin Nair","Mainak Singha","Souradeep Mukhopadhyay","Ankit Jha","Biplab Banerjee"],"pdf_url":"https://arxiv.org/pdf/2411.02074v1.pdf","comment":"Accepted in ACM ICVGIP 2024"},{"id":"http://arxiv.org/abs/2406.09952v2","updated":"2024-11-04T13:26:07Z","published":"2024-06-14T11:58:49Z","title":"BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval","summary":"  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work, we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts significantly improves over the base\nmodel in SugarCrepe and in BiVLC for both retrieval directions. The gap to\nhuman performance in BiVLC confirms that Vision-Language Compositionality is\nstill a challenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n","authors":["Imanol Miranda","Ander Salaberria","Eneko Agirre","Gorka Azkune"],"pdf_url":"https://arxiv.org/pdf/2406.09952v2.pdf","comment":"Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page\n  at: https://imirandam.github.io/BiVLC_project_page/"},{"id":"http://arxiv.org/abs/2404.13686v3","updated":"2024-11-04T13:24:18Z","published":"2024-04-21T15:16:05Z","title":"Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image\n  Synthesis","summary":"  Recently, a series of diffusion-aware distillation algorithms have emerged to\nalleviate the computational overhead associated with the multi-step inference\nprocess of Diffusion Models (DMs). Current distillation techniques often\ndichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii)\nODE Trajectory Reformulation. However, these approaches suffer from severe\nperformance degradation or domain shifts. To address these limitations, we\npropose Hyper-SD, a novel framework that synergistically amalgamates the\nadvantages of ODE Trajectory Preservation and Reformulation, while maintaining\nnear-lossless performance during step compression. Firstly, we introduce\nTrajectory Segmented Consistency Distillation to progressively perform\nconsistent distillation within pre-defined time-step segments, which\nfacilitates the preservation of the original ODE trajectory from a higher-order\nperspective. Secondly, we incorporate human feedback learning to boost the\nperformance of the model in a low-step regime and mitigate the performance loss\nincurred by the distillation process. Thirdly, we integrate score distillation\nto further improve the low-step generation capability of the model and offer\nthe first attempt to leverage a unified LoRA to support the inference process\nat all steps. Extensive experiments and user studies demonstrate that Hyper-SD\nachieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5.\nFor example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and\n+0.51 in Aes Score in the 1-step inference.\n","authors":["Yuxi Ren","Xin Xia","Yanzuo Lu","Jiacheng Zhang","Jie Wu","Pan Xie","Xing Wang","Xuefeng Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.13686v3.pdf","comment":"Accepted by NeurIPS 2024 (Camera-Ready Version). Project Page:\n  https://hyper-sd.github.io/"},{"id":"http://arxiv.org/abs/2309.09875v2","updated":"2024-11-04T13:17:00Z","published":"2023-09-18T15:37:01Z","title":"RaLF: Flow-based Global and Metric Radar Localization in LiDAR Maps","summary":"  Localization is paramount for autonomous robots. While camera and LiDAR-based\napproaches have been extensively investigated, they are affected by adverse\nillumination and weather conditions. Therefore, radar sensors have recently\ngained attention due to their intrinsic robustness to such conditions. In this\npaper, we propose RaLF, a novel deep neural network-based approach for\nlocalizing radar scans in a LiDAR map of the environment, by jointly learning\nto address both place recognition and metric localization. RaLF is composed of\nradar and LiDAR feature encoders, a place recognition head that generates\nglobal descriptors, and a metric localization head that predicts the 3-DoF\ntransformation between the radar scan and the map. We tackle the place\nrecognition task by learning a shared embedding space between the two\nmodalities via cross-modal metric learning. Additionally, we perform metric\nlocalization by predicting pixel-level flow vectors that align the query radar\nscan with the LiDAR map. We extensively evaluate our approach on multiple\nreal-world driving datasets and show that RaLF achieves state-of-the-art\nperformance for both place recognition and metric localization. Moreover, we\ndemonstrate that our approach can effectively generalize to different cities\nand sensor setups than the ones used during training. We make the code and\ntrained models publicly available at http://ralf.cs.uni-freiburg.de.\n","authors":["Abhijeet Nayak","Daniele Cattaneo","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2309.09875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02068v1","updated":"2024-11-04T13:15:28Z","published":"2024-11-04T13:15:28Z","title":"Model Integrity when Unlearning with T2I Diffusion Models","summary":"  The rapid advancement of text-to-image Diffusion Models has led to their\nwidespread public accessibility. However these models, trained on large\ninternet datasets, can sometimes generate undesirable outputs. To mitigate\nthis, approximate Machine Unlearning algorithms have been proposed to modify\nmodel weights to reduce the generation of specific types of images,\ncharacterized by samples from a ``forget distribution'', while preserving the\nmodel's ability to generate other images, characterized by samples from a\n``retain distribution''. While these methods aim to minimize the influence of\ntraining data in the forget distribution without extensive additional\ncomputation, we point out that they can compromise the model's integrity by\ninadvertently affecting generation for images in the retain distribution.\nRecognizing the limitations of FID and CLIPScore in capturing these effects, we\nintroduce a novel retention metric that directly assesses the perceptual\ndifference between outputs generated by the original and the unlearned models.\nWe then propose unlearning algorithms that demonstrate superior effectiveness\nin preserving model integrity compared to existing baselines. Given their\nstraightforward implementation, these algorithms serve as valuable benchmarks\nfor future advancements in approximate Machine Unlearning for Diffusion Models.\n","authors":["Andrea Schioppa","Emiel Hoogeboom","Jonathan Heek"],"pdf_url":"https://arxiv.org/pdf/2411.02068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02065v1","updated":"2024-11-04T13:07:22Z","published":"2024-11-04T13:07:22Z","title":"AM Flow: Adapters for Temporal Processing in Action Recognition","summary":"  Deep learning models, in particular \\textit{image} models, have recently\ngained generalisability and robustness. %are becoming more general and robust\nby the day. In this work, we propose to exploit such advances in the realm of\n\\textit{video} classification. Video foundation models suffer from the\nrequirement of extensive pretraining and a large training time. Towards\nmitigating such limitations, we propose \"\\textit{Attention Map (AM) Flow}\" for\nimage models, a method for identifying pixels relevant to motion in each input\nvideo frame. In this context, we propose two methods to compute AM flow,\ndepending on camera motion. AM flow allows the separation of spatial and\ntemporal processing, while providing improved results over combined\nspatio-temporal processing (as in video models). Adapters, one of the popular\ntechniques in parameter efficient transfer learning, facilitate the\nincorporation of AM flow into pretrained image models, mitigating the need for\nfull-finetuning. We extend adapters to \"\\textit{temporal processing adapters}\"\nby incorporating a temporal processing unit into the adapters. Our work\nachieves faster convergence, therefore reducing the number of epochs needed for\ntraining. Moreover, we endow an image model with the ability to achieve\nstate-of-the-art results on popular action recognition datasets. This reduces\ntraining time and simplifies pretraining. We present experiments on\nKinetics-400, Something-Something v2, and Toyota Smarthome datasets, showcasing\nstate-of-the-art or comparable results.\n","authors":["Tanay Agrawal","Abid Ali","Antitza Dantcheva","Francois Bremond"],"pdf_url":"https://arxiv.org/pdf/2411.02065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02057v1","updated":"2024-11-04T12:59:13Z","published":"2024-11-04T12:59:13Z","title":"Exploiting Unlabeled Data with Multiple Expert Teachers for Open\n  Vocabulary Aerial Object Detection and Its Orientation Adaptation","summary":"  In recent years, aerial object detection has been increasingly pivotal in\nvarious earth observation applications. However, current algorithms are limited\nto detecting a set of pre-defined object categories, demanding sufficient\nannotated training samples, and fail to detect novel object categories. In this\npaper, we put forth a novel formulation of the aerial object detection problem,\nnamely open-vocabulary aerial object detection (OVAD), which can detect objects\nbeyond training categories without costly collecting new labeled data. We\npropose CastDet, a CLIP-activated student-teacher detection framework that\nserves as the first OVAD detector specifically designed for the challenging\naerial scenario, where objects often exhibit weak appearance features and\narbitrary orientations. Our framework integrates a robust localization teacher\nalong with several box selection strategies to generate high-quality proposals\nfor novel objects. Additionally, the RemoteCLIP model is adopted as an\nomniscient teacher, which provides rich knowledge to enhance classification\ncapabilities for novel categories. A dynamic label queue is devised to maintain\nhigh-quality pseudo-labels during training. By doing so, the proposed CastDet\nboosts not only novel object proposals but also classification. Furthermore, we\nextend our approach from horizontal OVAD to oriented OVAD with tailored\nalgorithm designs to effectively manage bounding box representation and\npseudo-label generation. Extensive experiments for both tasks on multiple\nexisting aerial object detection datasets demonstrate the effectiveness of our\napproach. The code is available at https://github.com/lizzy8587/CastDet.\n","authors":["Yan Li","Weiwei Guo","Xue Yang","Ning Liao","Shaofeng Zhang","Yi Yu","Wenxian Yu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2411.02057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14435v2","updated":"2024-11-04T12:48:38Z","published":"2023-11-24T12:22:00Z","title":"Local Concept Embeddings for Analysis of Concept Distributions in DNN\n  Feature Spaces","summary":"  Insights into the learned latent representations are imperative for verifying\ndeep neural networks (DNNs) in critical computer vision (CV) tasks. Therefore,\nstate-of-the-art supervised Concept-based eXplainable Artificial Intelligence\n(C-XAI) methods associate user-defined concepts like ``car'' each with a single\nvector in the DNN latent space (concept embedding vector). In the case of\nconcept segmentation, these linearly separate between activation map pixels\nbelonging to a concept and those belonging to background. Existing methods for\nconcept segmentation, however, fall short of capturing sub-concepts (e.g.,\n``proximate car'' and ``distant car''), and concept overlap (e.g., between\n``bus'' and ``truck''). In other words, they do not capture the full\ndistribution of concept representatives in latent space. For the first time,\nthis work shows that these simplifications are frequently broken and that\ndistribution information can be particularly useful for understanding\nDNN-learned notions of sub-concepts, concept confusion, and concept outliers.\nTo allow exploration of learned concept distributions, we propose a novel local\nconcept analysis framework. Instead of optimizing a single global concept\nvector on the complete dataset, it generates a local concept embedding (LoCE)\nvector for each individual sample. We use the distribution formed by LoCEs to\nexplore the latent concept distribution by fitting Gaussian mixture models\n(GMMs), hierarchical clustering, and concept-level information retrieval and\noutlier detection. Despite its context sensitivity, our method's concept\nsegmentation performance is competitive to global baselines. Analysis results\nare obtained on two datasets and five diverse vision DNN architectures,\nincluding vision transformers (ViTs).\n","authors":["Georgii Mikriukov","Gesina Schwalbe","Korinna Bade"],"pdf_url":"https://arxiv.org/pdf/2311.14435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02038v1","updated":"2024-11-04T12:40:18Z","published":"2024-11-04T12:40:18Z","title":"Addressing Representation Collapse in Vector Quantized Models with One\n  Linear Layer","summary":"  Vector Quantization (VQ) is a widely used method for converting continuous\nrepresentations into discrete codes, which has become fundamental in\nunsupervised representation learning and latent generative models. However, VQ\nmodels are often hindered by the problem of representation collapse in the\nlatent space, which leads to low codebook utilization and limits the\nscalability of the codebook for large-scale training. Existing methods designed\nto mitigate representation collapse typically reduce the dimensionality of\nlatent space at the expense of model capacity, which do not fully resolve the\ncore issue. In this study, we conduct a theoretical analysis of representation\ncollapse in VQ models and identify its primary cause as the disjoint\noptimization of the codebook, where only a small subset of code vectors are\nupdated through gradient descent. To address this issue, we propose\n\\textbf{SimVQ}, a novel method which reparameterizes the code vectors through a\nlinear transformation layer based on a learnable latent basis. This\ntransformation optimizes the \\textit{entire linear space} spanned by the\ncodebook, rather than merely updating \\textit{the code vector} selected by the\nnearest-neighbor search in vanilla VQ models. Although it is commonly\nunderstood that the multiplication of two linear matrices is equivalent to\napplying a single linear layer, our approach works surprisingly well in\nresolving the collapse issue in VQ models with just one linear layer. We\nvalidate the efficacy of SimVQ through extensive experiments across various\nmodalities, including image and audio data with different model architectures.\nOur code is available at \\url{https://github.com/youngsheen/SimVQ}.\n","authors":["Yongxin Zhu","Bocheng Li","Yifei Xin","Linli Xu"],"pdf_url":"https://arxiv.org/pdf/2411.02038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03078v2","updated":"2024-11-04T12:19:57Z","published":"2024-08-06T10:13:57Z","title":"BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications","summary":"  Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Monocular\nVisual Simultaneous Localization and Mapping (MVSLAM) has emerged as a\npromising solution, its implementation in endoscopic procedures faces\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents\nBodySLAM, a robust deep learning-based MVSLAM approach that addresses these\nchallenges through three key components: CycleVO, a novel unsupervised\nmonocular pose estimation module; the integration of the state-of-the-art Zoe\narchitecture for monocular depth estimation; and a 3D reconstruction module\ncreating a coherent surgical map. The approach is rigorously evaluated using\nthree publicly available datasets (Hamlyn, EndoSLAM, and SCARED) spanning\nlaparoscopy, gastroscopy, and colonoscopy scenarios, and benchmarked against\nfour state-of-the-art methods. Results demonstrate that CycleVO exhibited\ncompetitive performance with the lowest inference time among pose estimation\nmethods, while maintaining robust generalization capabilities, whereas Zoe\nsignificantly outperformed existing algorithms for depth estimation in\nendoscopy. BodySLAM's strong performance across diverse endoscopic scenarios\ndemonstrates its potential as a viable MVSLAM solution for endoscopic\napplications.\n","authors":["G. Manni","C. Lauretti","F. Prata","R. Papalia","L. Zollo","P. Soda"],"pdf_url":"https://arxiv.org/pdf/2408.03078v2.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.02009v1","updated":"2024-11-04T11:54:31Z","published":"2024-11-04T11:54:31Z","title":"Tree level change detection over Ahmedabad city using very high\n  resolution satellite images and Deep Learning","summary":"  In this study, 0.5m high resolution satellite datasets over Indian urban\nregion was used to demonstrate the applicability of deep learning models over\nAhmedabad, India. Here, YOLOv7 instance segmentation model was trained on well\ncurated trees canopy dataset (6500 images) in order to carry out the change\ndetection. During training, evaluation metrics such as bounding box regression\nand mask regression loss, mean average precision (mAP) and stochastic gradient\ndescent algorithm were used for evaluating and optimizing the performance of\nmodel. After the 500 epochs, the mAP of 0.715 and 0.699 for individual tree\ndetection and tree canopy mask segmentation were obtained. However, by further\ntuning hyper parameters of the model, maximum accuracy of 80 % of trees\ndetection with false segmentation rate of 2% on data was obtained.\n","authors":["Jai G Singla","Gautam Jaiswal"],"pdf_url":"https://arxiv.org/pdf/2411.02009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16998v2","updated":"2024-11-04T11:44:29Z","published":"2024-09-25T15:03:22Z","title":"PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in\n  Endoscopic Pituitary Surgery","summary":"  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow\nfor anaesthetists to more accurately decide when to administer anaesthetic\nagents and drugs, as well as to notify hospital staff to send in the next\npatient. Therefore RSD plays an important role in improving patient care and\nminimising surgical theatre costs via efficient scheduling. In endoscopic\npituitary surgery, it is uniquely challenging due to variable workflow\nsequences with a selection of optional steps contributing to high variability\nin surgery duration. This paper presents PitRSDNet for predicting RSD during\npituitary surgery, a spatio-temporal neural network model that learns from\nhistorical data focusing on workflow sequences. PitRSDNet integrates workflow\nknowledge into RSD prediction in two forms: 1) multi-task learning for\nconcurrently predicting step and RSD; and 2) incorporating prior steps as\ncontext in temporal learning and inference. PitRSDNet is trained and evaluated\non a new endoscopic pituitary surgery dataset with 88 videos to show\ncompetitive performance improvements over previous statistical and machine\nlearning methods. The findings also highlight how PitRSDNet improve RSD\nprecision on outlier cases utilising the knowledge of prior steps.\n","authors":["Anjana Wijekoon","Adrito Das","Roxana R. Herrera","Danyal Z. Khan","John Hanrahan","Eleanor Carter","Valpuri Luoma","Danail Stoyanov","Hani J. Marcus","Sophia Bano"],"pdf_url":"https://arxiv.org/pdf/2409.16998v2.pdf","comment":"Accepted to the Augmented Environments for Computer-Assisted\n  Interventions (AE-CAI) Workshop at the Medical Image Computing and\n  Computer-Assisted Interventions (MICCAI) Conference 2024"},{"id":"http://arxiv.org/abs/2409.18783v2","updated":"2024-11-04T11:39:40Z","published":"2024-09-27T14:30:24Z","title":"DualDn: Dual-domain Denoising via Differentiable ISP","summary":"  Image denoising is a critical component in a camera's Image Signal Processing\n(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP\npipeline: applying a denoiser directly to captured raw frames (raw domain) or\nto the ISP's output sRGB images (sRGB domain). However, both approaches have\ntheir limitations. Residual noise from raw-domain denoising can be amplified by\nthe subsequent ISP processing, and the sRGB domain struggles to handle\nspatially varying noise since it only sees noise distorted by the ISP.\nConsequently, most raw or sRGB domain denoising works only for specific noise\ndistributions and ISP configurations. To address these challenges, we propose\nDualDn, a novel learning-based dual-domain denoising. Unlike previous\nsingle-domain denoising, DualDn consists of two denoising networks: one in the\nraw domain and one in the sRGB domain. The raw domain denoising adapts to\nsensor-specific noise as well as spatially varying noise levels, while the sRGB\ndomain denoising adapts to ISP variations and removes residual noise amplified\nby the ISP. Both denoising networks are connected with a differentiable ISP,\nwhich is trained end-to-end and discarded during the inference stage. With this\ndesign, DualDn achieves greater generalizability compared to most\nlearning-based denoising methods, as it can adapt to different unseen noises,\nISP parameters, and even novel ISP pipelines. Experiments show that DualDn\nachieves state-of-the-art performance and can adapt to different denoising\narchitectures. Moreover, DualDn can be used as a plug-and-play denoising module\nwith real cameras without retraining, and still demonstrate better performance\nthan commercial on-camera denoising. The project website is available at:\nhttps://openimaginglab.github.io/DualDn/\n","authors":["Ruikang Li","Yujin Wang","Shiqi Chen","Fan Zhang","Jinwei Gu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2409.18783v2.pdf","comment":"Accepted at ECCV 2024, Project page:\n  https://openimaginglab.github.io/DualDn/"},{"id":"http://arxiv.org/abs/2403.00174v4","updated":"2024-11-04T11:38:49Z","published":"2024-02-29T22:58:13Z","title":"A citizen science toolkit to collect human perceptions of urban\n  environments using open street view images","summary":"  Street View Imagery (SVI) is a valuable data source for studies (e.g.,\nenvironmental assessments, green space identification or land cover\nclassification). While commercial SVI is available, such providers commonly\nrestrict copying or reuse in ways necessary for research. Open SVI datasets are\nreadily available from less restrictive sources, such as Mapillary, but due to\nthe heterogeneity of the images, these require substantial preprocessing,\nfiltering, and careful quality checks. We present an efficient method for\nautomated downloading, processing, cropping, and filtering open SVI, to be used\nin a survey of human perceptions of the streets portrayed in these images. We\ndemonstrate our open-source reusable SVI preparation and smartphone-friendly\nperception-survey software with Amsterdam (Netherlands) as the case study.\nUsing a citizen science approach, we collected from 331 people 22,637 ratings\nabout their perceptions for various criteria. We have published our software in\na public repository for future re-use and reproducibility.\n","authors":["Matthew Danish","SM Labib","Britta Ricker","Marco Helbich"],"pdf_url":"https://arxiv.org/pdf/2403.00174v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10853v2","updated":"2024-11-04T11:37:51Z","published":"2024-06-16T08:54:38Z","title":"MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images","summary":"  We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-view\nimages, not merely as a field or raw geometry but as a sketch-extrude CAD\nmodel. Extracting extrusion cylinders from raw 3D geometry has been extensively\nresearched in computer vision, while the processing of 3D data through neural\nnetworks has remained a bottleneck. Since 3D scans are generally accompanied by\nmulti-view images, leveraging 2D convolutional neural networks allows these\nimages to be exploited as a rich source for extracting extrusion cylinder\ninformation. However, we observe that extracting only the surface information\nof the extrudes and utilizing it results in suboptimal outcomes due to the\nchallenges in the occlusion and surface segmentation. By synergizing with the\nextracted base curve information, we achieve the optimal reconstruction result\nwith the best accuracy in 2D sketch and extrude parameter estimation. Our\nexperiments, comparing our method with previous work that takes a raw 3D point\ncloud as input, demonstrate the effectiveness of our approach by taking\nadvantage of multi-view images.\n","authors":["Eunji Hong","Minh Hieu Nguyen","Mikaela Angelina Uy","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2406.10853v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01988v1","updated":"2024-11-04T11:20:17Z","published":"2024-11-04T11:20:17Z","title":"QCS:Feature Refining from Quadruplet Cross Similarity for Facial\n  Expression Recognition","summary":"  On facial expression datasets with complex and numerous feature types, where\nthe significance and dominance of labeled features are difficult to predict,\nfacial expression recognition(FER) encounters the challenges of inter-class\nsimilarity and intra-class variances, making it difficult to mine effective\nfeatures. We aim to solely leverage the feature similarity among facial samples\nto address this. We introduce the Cross Similarity Attention (CSA), an\ninput-output position-sensitive attention mechanism that harnesses feature\nsimilarity across different images to compute the corresponding global spatial\nattention. Based on this, we propose a four-branch circular framework, called\nQuadruplet Cross Similarity (QCS), to extract discriminative features from the\nsame class and eliminate redundant ones from different classes synchronously to\nrefine cleaner features. The symmetry of the network ensures balanced and\nstable training and reduces the amount of CSA interaction matrix. Contrastive\nresidual distillation is utilized to transfer the information learned in the\ncross module back to the base network. The cross-attention module exists during\ntraining, and only one base branch is retained during inference. our proposed\nQCS model outperforms state-of-the-art methods on several popular FER datasets,\nwithout requiring additional landmark information or other extra training data.\nThe code is available at https://github.com/birdwcp/QCS.\n","authors":["Chengpeng Wang","Li Chen","Lili Wang","Zhaofan Li","Xuebin Lv"],"pdf_url":"https://arxiv.org/pdf/2411.01988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10618v2","updated":"2024-11-04T11:11:49Z","published":"2024-04-16T14:42:49Z","title":"Private Attribute Inference from Images with Vision-Language Models","summary":"  As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that LLMs can make accurate privacy-infringing\ninferences from previously unseen texts. With the rise of vision-language\nmodels (VLMs), capable of understanding both images and text, a key question is\nwhether this concern transfers to the previously unexplored domain of benign\nimages posted online. To answer this question, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the privacy risks posed by VLMs beyond traditional human attribute\nrecognition, our dataset consists of images where the inferable private\nattributes do not stem from direct depictions of humans. On this dataset, we\nevaluate 7 state-of-the-art VLMs, finding that they can infer various personal\nattributes at up to 77.6% accuracy. Concerningly, we observe that accuracy\nscales with the general capabilities of the models, implying that future models\ncan be misused as stronger inferential adversaries, establishing an imperative\nfor the development of adequate defenses.\n","authors":["Batuhan Tömekçe","Mark Vero","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2404.10618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01981v1","updated":"2024-11-04T11:09:47Z","published":"2024-11-04T11:09:47Z","title":"Typicalness-Aware Learning for Failure Detection","summary":"  Deep neural networks (DNNs) often suffer from the overconfidence issue, where\nincorrect predictions are made with high confidence scores, hindering the\napplications in critical systems. In this paper, we propose a novel approach\ncalled Typicalness-Aware Learning (TAL) to address this issue and improve\nfailure detection performance. We observe that, with the cross-entropy loss,\nmodel predictions are optimized to align with the corresponding labels via\nincreasing logit magnitude or refining logit direction. However, regarding\natypical samples, the image content and their labels may exhibit disparities.\nThis discrepancy can lead to overfitting on atypical samples, ultimately\nresulting in the overconfidence issue that we aim to address. To tackle the\nproblem, we have devised a metric that quantifies the typicalness of each\nsample, enabling the dynamic adjustment of the logit magnitude during the\ntraining process. By allowing atypical samples to be adequately fitted while\npreserving reliable logit direction, the problem of overconfidence can be\nmitigated. TAL has been extensively evaluated on benchmark datasets, and the\nresults demonstrate its superiority over existing failure detection methods.\nSpecifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of\nthe Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art.\nCode is available at https://github.com/liuyijungoon/TAL.\n","authors":["Yijun Liu","Jiequan Cui","Zhuotao Tian","Senqiao Yang","Qingdong He","Xiaoling Wang","Jingyong Su"],"pdf_url":"https://arxiv.org/pdf/2411.01981v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2310.08421v4","updated":"2024-11-04T11:06:25Z","published":"2023-10-12T15:42:17Z","title":"Visual Self-supervised Learning Scheme for Dense Prediction Tasks on\n  X-ray Images","summary":"  Recently, significant advancements in artificial intelligence have been\nattributed to the integration of self-supervised learning (SSL) scheme. While\nSSL has shown impressive achievements in natural language processing (NLP), its\nprogress in computer vision has comparatively lagged behind. However, the\nincorporation of contrastive learning into existing visual SSL models has led\nto considerable progress, often surpassing supervised counterparts.\nNonetheless, these improvements have been mostly limited to classification\ntasks. Moreover, few studies have evaluated visual SSL models in real-world\nscenarios, as most have focused on datasets with class-wise portrait images,\nnotably ImageNet. Here, we focus on dense prediction tasks using security\ninspection x-ray images to evaluate our proposed model, Segment Localization\n(SegLoc). Based upon the Instance Localization (InsLoc) model, SegLoc addresses\none of the key challenges of contrastive learning, i.e., false negative pairs\nof query embeddings. Our pre-training dataset is synthesized by cutting,\ntransforming, and pasting labeled segments from an existing labeled dataset\n(PIDray) as foregrounds onto instances from an unlabeled dataset (SIXray) as\nbackgrounds. Furthermore, we fully leverage the labeled data by incorporating\nthe concept, one queue per class, into the MoCo-v2 memory bank, thereby\navoiding false negative pairs. In our experiments, SegLoc outperformed random\ninitialization by 3% to 6% while underperformed supervised initialization, in\nterms of AR and AP metrics across different IoU values over 20 to 30\npre-training epochs.\n","authors":["Shervin Halat","Mohammad Rahmati","Ehsan Nazerfard"],"pdf_url":"https://arxiv.org/pdf/2310.08421v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01975v1","updated":"2024-11-04T10:51:47Z","published":"2024-11-04T10:51:47Z","title":"SPECTRUM: Semantic Processing and Emotion-informed video-Captioning\n  Through Retrieval and Understanding Modalities","summary":"  Capturing a video's meaning and critical concepts by analyzing the subtle\ndetails is a fundamental yet challenging task in video captioning. Identifying\nthe dominant emotional tone in a video significantly enhances the perception of\nits context. Despite a strong emphasis on video captioning, existing models\noften need to adequately address emotional themes, resulting in suboptimal\ncaptioning results. To address these limitations, this paper proposes a novel\nSemantic Processing and Emotion-informed video-Captioning Through Retrieval and\nUnderstanding Modalities (SPECTRUM) framework to empower the generation of\nemotionally and semantically credible captions. Leveraging our pioneering\nstructure, SPECTRUM discerns multimodal semantics and emotional themes using\nVisual Text Attribute Investigation (VTAI) and determines the orientation of\ndescriptive captions through a Holistic Concept-Oriented Theme (HCOT),\nexpressing emotionally-informed and field-acquainted references. They exploit\nvideo-to-text retrieval capabilities and the multifaceted nature of video\ncontent to estimate the emotional probabilities of candidate captions. Then,\nthe dominant theme of the video is determined by appropriately weighting\nembedded attribute vectors and applying coarse- and fine-grained emotional\nconcepts, which define the video's contextual alignment. Furthermore, using two\nloss functions, SPECTRUM is optimized to integrate emotional information and\nminimize prediction errors. Extensive experiments on the EmVidCap, MSVD, and\nMSRVTT video captioning datasets demonstrate that our model significantly\nsurpasses state-of-the-art methods. Quantitative and qualitative evaluations\nhighlight the model's ability to accurately capture and convey video emotions\nand multimodal attributes.\n","authors":["Ehsan Faghihi","Mohammedreza Zarenejad","Ali-Asghar Beheshti Shirazi"],"pdf_url":"https://arxiv.org/pdf/2411.01975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11914v3","updated":"2024-11-04T10:48:54Z","published":"2024-05-20T09:49:13Z","title":"PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single\n  Highly-Ambiguous RGB Images","summary":"  Generating 3D shapes from single RGB images is essential in various\napplications such as robotics. Current approaches typically target images\ncontaining clear and complete visual descriptions of the object, without\nconsidering common realistic cases where observations of objects that are\nlargely occluded or truncated. We thus propose a transformer-based\nautoregressive model to generate the probabilistic distribution of 3D shapes\nconditioned on an RGB image containing potentially highly ambiguous\nobservations of the object. To handle realistic scenarios such as occlusion or\nfield-of-view truncation, we create simulated image-to-shape training pairs\nthat enable improved fine-tuning for real-world scenarios. We then adopt\ncross-attention to effectively identify the most relevant region of interest\nfrom the input image for shape generation. This enables inference of sampled\nshapes with reasonable diversity and strong alignment with the input image. We\ntrain and test our model on our synthetic data then fine-tune and test it on\nreal-world data. Experiments demonstrate that our model outperforms state of\nthe art in both scenarios.\n","authors":["Yiheng Xiong","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2405.11914v3.pdf","comment":"10 pages, 6 figures. Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2411.01969v1","updated":"2024-11-04T10:44:46Z","published":"2024-11-04T10:44:46Z","title":"Active Gaze Behavior Boosts Self-Supervised Object Learning","summary":"  Due to significant variations in the projection of the same object from\ndifferent viewpoints, machine learning algorithms struggle to recognize the\nsame object across various perspectives. In contrast, toddlers quickly learn to\nrecognize objects from different viewpoints with almost no supervision. Recent\nworks argue that toddlers develop this ability by mapping close-in-time visual\ninputs to similar representations while interacting with objects. High acuity\nvision is only available in the central visual field, which may explain why\ntoddlers (much like adults) constantly move their gaze around during such\ninteractions. It is unclear whether/how much toddlers curate their visual\nexperience through these eye movements to support learning object\nrepresentations. In this work, we explore whether a bio inspired visual\nlearning model can harness toddlers' gaze behavior during a play session to\ndevelop view-invariant object recognition. Exploiting head-mounted eye tracking\nduring dyadic play, we simulate toddlers' central visual field experience by\ncropping image regions centered on the gaze location. This visual stream feeds\na time-based self-supervised learning algorithm. Our experiments demonstrate\nthat toddlers' gaze strategy supports the learning of invariant object\nrepresentations. Our analysis also reveals that the limited size of the central\nvisual field where acuity is high is crucial for this. We further find that\ntoddlers' visual experience elicits more robust representations compared to\nadults' mostly because toddlers look at objects they hold themselves for longer\nbouts. Overall, our work reveals how toddlers' gaze behavior supports\nself-supervised learning of view-invariant object recognition.\n","authors":["Zhengyang Yu","Arthur Aubret","Marcel C. Raabe","Jane Yang","Chen Yu","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2411.01969v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.01966v1","updated":"2024-11-04T10:42:21Z","published":"2024-11-04T10:42:21Z","title":"UnSegMedGAT: Unsupervised Medical Image Segmentation using Graph\n  Attention Networks Clustering","summary":"  The data-intensive nature of supervised classification drives the interest of\nthe researchers towards unsupervised approaches, especially for problems such\nas medical image segmentation, where labeled data is scarce. Building on the\nrecent advancements of Vision transformers (ViT) in computer vision, we propose\nan unsupervised segmentation framework using a pre-trained Dino-ViT. In the\nproposed method, we leverage the inherent graph structure within the image to\nrealize a significant performance gain for segmentation in medical images. For\nthis, we introduce a modularity-based loss function coupled with a Graph\nAttention Network (GAT) to effectively capture the inherent graph topology\nwithin the image. Our method achieves state-of-the-art performance, even\nsignificantly surpassing or matching that of existing (semi)supervised\ntechnique such as MedSAM which is a Segment Anything Model in medical images.\nWe demonstrate this using two challenging medical image datasets ISIC-2018 and\nCVC-ColonDB. This work underscores the potential of unsupervised approaches in\nadvancing medical image analysis in scenarios where labeled data is scarce. The\ngithub repository of the code is available on\n[https://github.com/mudit-adityaja/UnSegMedGAT].\n","authors":["A. Mudit Adityaja","Saurabh J. Shigwan","Nitin Kumar"],"pdf_url":"https://arxiv.org/pdf/2411.01966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01962v1","updated":"2024-11-04T10:38:33Z","published":"2024-11-04T10:38:33Z","title":"Deep Learning for Leopard Individual Identification: An Adaptive Angular\n  Margin Approach","summary":"  Accurate identification of individual leopards across camera trap images is\ncritical for population monitoring and ecological studies. This paper\nintroduces a deep learning framework to distinguish between individual leopards\nbased on their unique spot patterns. This approach employs a novel adaptive\nangular margin method in the form of a modified CosFace architecture. In\naddition, I propose a preprocessing pipeline that combines RGB channels with an\nedge detection channel to underscore the critical features learned by the\nmodel.\n  This approach significantly outperforms the Triplet Network baseline,\nachieving a Dynamic Top-5 Average Precision of 0.8814 and a Top-5 Rank Match\nDetection of 0.9533, demonstrating its potential for open-set learning in\nwildlife identification. While not surpassing the performance of the SIFT-based\nHotspotter algorithm, this method represents a substantial advancement in\napplying deep learning to patterned wildlife identification.\n  This research contributes to the field of computer vision and provides a\nvaluable tool for biologists aiming to study and protect leopard populations.\nIt also serves as a stepping stone for applying the power of deep learning in\nCapture-Recapture studies for other patterned species.\n","authors":["David Colomer Matachana"],"pdf_url":"https://arxiv.org/pdf/2411.01962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01955v1","updated":"2024-11-04T10:27:57Z","published":"2024-11-04T10:27:57Z","title":"Robust plug-and-play methods for highly accelerated non-Cartesian MRI\n  reconstruction","summary":"  Achieving high-quality Magnetic Resonance Imaging (MRI) reconstruction at\naccelerated acquisition rates remains challenging due to the inherent ill-posed\nnature of the inverse problem. Traditional Compressed Sensing (CS) methods,\nwhile robust across varying acquisition settings, struggle to maintain good\nreconstruction quality at high acceleration factors ($\\ge$ 8). Recent advances\nin deep learning have improved reconstruction quality, but purely data-driven\nmethods are prone to overfitting and hallucination effects, notably when the\nacquisition setting is varying. Plug-and-Play (PnP) approaches have been\nproposed to mitigate the pitfalls of both frameworks. In a nutshell, PnP\nalgorithms amount to replacing suboptimal handcrafted CS priors with powerful\ndenoising deep neural network (DNNs). However, in MRI reconstruction, existing\nPnP methods often yield suboptimal results due to instabilities in the proximal\ngradient descent (PGD) schemes and the lack of curated, noiseless datasets for\ntraining robust denoisers. In this work, we propose a fully unsupervised\npreprocessing pipeline to generate clean, noiseless complex MRI signals from\nmulticoil data, enabling training of a high-performance denoising DNN.\nFurthermore, we introduce an annealed Half-Quadratic Splitting (HQS) algorithm\nto address the instability issues, leading to significant improvements over\nexisting PnP algorithms. When combined with preconditioning techniques, our\napproach achieves state-of-the-art results, providing a robust and efficient\nsolution for high-quality MRI reconstruction.\n","authors":["Pierre-Antoine Comby","Benjamin Lapostolle","Matthieu Terris","Philippe Ciuciu"],"pdf_url":"https://arxiv.org/pdf/2411.01955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00543v2","updated":"2024-11-04T10:21:57Z","published":"2024-11-01T12:50:38Z","title":"3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction","summary":"  Determining the 3D orientations of an object in an image, known as\nsingle-image pose estimation, is a crucial task in 3D vision applications.\nExisting methods typically learn 3D rotations parametrized in the spatial\ndomain using Euler angles or quaternions, but these representations often\nintroduce discontinuities and singularities. SO(3)-equivariant networks enable\nthe structured capture of pose patterns with data-efficient learning, but the\nparametrizations in spatial domain are incompatible with their architecture,\nparticularly spherical CNNs, which operate in the frequency domain to enhance\ncomputational efficiency. To overcome these issues, we propose a\nfrequency-domain approach that directly predicts Wigner-D coefficients for 3D\nrotation regression, aligning with the operations of spherical CNNs. Our\nSO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial\nparameterizations, ensuring consistent pose estimation under arbitrary\nrotations. Trained with a frequency-domain regression loss, our method achieves\nstate-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+,\nwith significant improvements in accuracy, robustness, and data efficiency.\n","authors":["Jongmin Lee","Minsu Cho"],"pdf_url":"https://arxiv.org/pdf/2411.00543v2.pdf","comment":"Accepted to NeurIPS 2024, Project webpage at\n  http://cvlab.postech.ac.kr/research/3D_EquiPose"},{"id":"http://arxiv.org/abs/2402.14695v2","updated":"2024-11-04T10:20:25Z","published":"2024-02-22T16:49:58Z","title":"QIS : Interactive Segmentation via Quasi-Conformal Mappings","summary":"  Image segmentation plays a crucial role in extracting important objects of\ninterest from images, enabling various applications. While existing methods\nhave shown success in segmenting clean images, they often struggle to produce\naccurate segmentation results when dealing with degraded images, such as those\ncontaining noise or occlusions. To address this challenge, interactive\nsegmentation has emerged as a promising approach, allowing users to provide\nmeaningful input to guide the segmentation process. However, an important\nproblem in interactive segmentation lies in determining how to incorporate\nminimal yet meaningful user guidance into the segmentation model. In this\npaper, we propose the quasi-conformal interactive segmentation (QIS) model,\nwhich incorporates user input in the form of positive and negative clicks.\nUsers mark a few pixels belonging to the object region as positive clicks,\nindicating that the segmentation model should include a region around these\nclicks. Conversely, negative clicks are provided on pixels belonging to the\nbackground, instructing the model to exclude the region near these clicks from\nthe segmentation mask. Additionally, the segmentation mask is obtained by\ndeforming a template mask with the same topology as the object of interest\nusing an orientation-preserving quasiconformal mapping. This approach helps to\navoid topological errors in the segmentation results. We provide a thorough\nanalysis of the proposed model, including theoretical support for the ability\nof QIS to include or exclude regions of interest or disinterest based on the\nuser's indication. To evaluate the performance of QIS, we conduct experiments\non synthesized images, medical images, natural images and noisy natural images.\nThe results demonstrate the efficacy of our proposed method.\n","authors":["Han Zhang","Daoping Zhang","Lok Ming Lui"],"pdf_url":"https://arxiv.org/pdf/2402.14695v2.pdf","comment":"34 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.01948v1","updated":"2024-11-04T10:17:40Z","published":"2024-11-04T10:17:40Z","title":"Learning Where to Edit Vision Transformers","summary":"  Model editing aims to data-efficiently correct predictive errors of large\npre-trained models while ensuring generalization to neighboring failures and\nlocality to minimize unintended effects on unrelated examples. While\nsignificant progress has been made in editing Transformer-based large language\nmodels, effective strategies for editing vision Transformers (ViTs) in computer\nvision remain largely untapped. In this paper, we take initial steps towards\ncorrecting predictive errors of ViTs, particularly those arising from\nsubpopulation shifts. Taking a locate-then-edit approach, we first address the\nwhere-to-edit challenge by meta-learning a hypernetwork on CutMix-augmented\ndata generated for editing reliability. This trained hypernetwork produces\ngeneralizable binary masks that identify a sparse subset of structured model\nparameters, responsive to real-world failure samples. Afterward, we solve the\nhow-to-edit problem by simply fine-tuning the identified parameters using a\nvariant of gradient descent to achieve successful edits. To validate our\nmethod, we construct an editing benchmark that introduces subpopulation shifts\ntowards natural underrepresented images and AI-generated images, thereby\nrevealing the limitations of pre-trained ViTs for object recognition. Our\napproach not only achieves superior performance on the proposed benchmark but\nalso allows for adjustable trade-offs between generalization and locality. Our\ncode is available at https://github.com/hustyyq/Where-to-Edit.\n","authors":["Yunqiao Yang","Long-Kai Huang","Shengzhuang Chen","Kede Ma","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2411.01948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05964v2","updated":"2024-11-04T10:14:22Z","published":"2024-08-12T07:33:11Z","title":"Target Detection of Safety Protective Gear Using the Improved YOLOv5","summary":"  In high-risk railway construction, personal protective equipment monitoring\nis critical but challenging due to small and frequently obstructed targets. We\npropose YOLO-EA, an innovative model that enhances safety measure detection by\nintegrating ECA into its backbone's convolutional layers, improving discernment\nof minuscule objects like hardhats. YOLO-EA further refines target recognition\nunder occlusion by replacing GIoU with EIoU loss. YOLO-EA's effectiveness was\nempirically substantiated using a dataset derived from real-world railway\nconstruction site surveillance footage. It outperforms YOLOv5, achieving 98.9%\nprecision and 94.7% recall, up 2.5% and 0.5% respectively, while maintaining\nreal-time performance at 70.774 fps. This highly efficient and precise YOLO-EA\nholds great promise for practical application in intricate construction\nscenarios, enforcing stringent safety compliance during complex railway\nconstruction projects.\n","authors":["Hao Liu","Xue Qin"],"pdf_url":"https://arxiv.org/pdf/2408.05964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23085v2","updated":"2024-11-04T10:01:38Z","published":"2024-10-30T15:00:06Z","title":"S3PT: Scene Semantics and Structure Guided Clustering to Boost\n  Self-Supervised Pre-Training for Autonomous Driving","summary":"  Recent self-supervised clustering-based pre-training techniques like DINO and\nCribo have shown impressive results for downstream detection and segmentation\ntasks. However, real-world applications such as autonomous driving face\nchallenges with imbalanced object class and size distributions and complex\nscene geometries. In this paper, we propose S3PT a novel scene semantics and\nstructure guided clustering to provide more scene-consistent objectives for\nself-supervised training. Specifically, our contributions are threefold: First,\nwe incorporate semantic distribution consistent clustering to encourage better\nrepresentation of rare classes such as motorcycles or animals. Second, we\nintroduce object diversity consistent spatial clustering, to handle imbalanced\nand diverse object sizes, ranging from large background areas to small objects\nsuch as pedestrians and traffic signs. Third, we propose a depth-guided spatial\nclustering to regularize learning based on geometric information of the scene,\nthus further refining region separation on the feature level. Our learned\nrepresentations significantly improve performance in downstream semantic\nsegmentation and 3D object detection tasks on the nuScenes, nuImages, and\nCityscapes datasets and show promising domain translation properties.\n","authors":["Maciej K. Wozniak","Hariprasath Govindarajan","Marvin Klingner","Camille Maurice","B Ravi Kiran","Senthil Yogamani"],"pdf_url":"https://arxiv.org/pdf/2410.23085v2.pdf","comment":"Accepted for WACV 2025"},{"id":"http://arxiv.org/abs/2410.22392v2","updated":"2024-11-04T09:56:16Z","published":"2024-10-29T17:56:05Z","title":"EfficientNet with Hybrid Attention Mechanisms for Enhanced Breast\n  Histopathology Classification: A Comprehensive Approach","summary":"  Breast cancer histopathology image classification is crucial for early cancer\ndetection, offering the potential to reduce mortality rates through timely\ndiagnosis. This paper introduces a novel approach integrating Hybrid\nEfficientNet models with advanced attention mechanisms, including Convolutional\nBlock Attention Module (CBAM), Self-Attention, and Deformable Attention, to\nenhance feature extraction and focus on critical image regions. We evaluate the\nperformance of our models across multiple magnification scales using publicly\navailable histopathological datasets. Our method achieves significant\nimprovements, with accuracy reaching 98.42% at 400X magnification, surpassing\nseveral state-of-the-art models, including VGG and ResNet architectures. The\nresults are validated using metrics such as accuracy, F1-score, precision, and\nrecall, demonstrating the clinical potential of our model in improving\ndiagnostic accuracy. Furthermore, the proposed method shows increased\ncomputational efficiency, making it suitable for integration into real-time\ndiagnostic workflows.\n","authors":["Naren Sengodan"],"pdf_url":"https://arxiv.org/pdf/2410.22392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01925v1","updated":"2024-11-04T09:43:33Z","published":"2024-11-04T09:43:33Z","title":"Exploiting Contextual Uncertainty of Visual Data for Efficient Training\n  of Deep Models","summary":"  Objects, in the real world, rarely occur in isolation and exhibit typical\narrangements governed by their independent utility, and their expected\ninteraction with humans and other objects in the context. For example, a chair\nis expected near a table, and a computer is expected on top. Humans use this\nspatial context and relative placement as an important cue for visual\nrecognition in case of ambiguities. Similar to human's, DNN's exploit\ncontextual information from data to learn representations. Our research focuses\non harnessing the contextual aspects of visual data to optimize data annotation\nand enhance the training of deep networks. Our contributions can be summarized\nas follows: (1) We introduce the notion of contextual diversity for active\nlearning CDAL and show its applicability in three different visual tasks\nsemantic segmentation, object detection and image classification, (2) We\npropose a data repair algorithm to curate contextually fair data to reduce\nmodel bias, enabling the model to detect objects out of their obvious context,\n(3) We propose Class-based annotation, where contextually relevant classes are\nselected that are complementary for model training under domain shift.\nUnderstanding the importance of well-curated data, we also emphasize the\nnecessity of involving humans in the loop to achieve accurate annotations and\nto develop novel interaction strategies that allow humans to serve as\nfact-checkers. In line with this we are working on developing image retrieval\nsystem for wildlife camera trap images and reliable warning system for poor\nquality rural roads. For large-scale annotation, we are employing a strategic\ncombination of human expertise and zero-shot models, while also integrating\nhuman input at various stages for continuous feedback.\n","authors":["Sharat Agarwal"],"pdf_url":"https://arxiv.org/pdf/2411.01925v1.pdf","comment":"ICVGIP, Young Researchers Symposium"},{"id":"http://arxiv.org/abs/2411.01919v1","updated":"2024-11-04T09:34:55Z","published":"2024-11-04T09:34:55Z","title":"Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing","summary":"  We present a novel algorithm for real-time planar semantic mapping tailored\nfor humanoid robots navigating complex terrains such as staircases. Our method\nis adaptable to any odometry input and leverages GPU-accelerated processes for\nplanar extraction, enabling the rapid generation of globally consistent\nsemantic maps. We utilize an anisotropic diffusion filter on depth images to\neffectively minimize noise from gradient jumps while preserving essential edge\ndetails, enhancing normal vector images' accuracy and smoothness. Both the\nanisotropic diffusion and the RANSAC-based plane extraction processes are\noptimized for parallel processing on GPUs, significantly enhancing\ncomputational efficiency. Our approach achieves real-time performance,\nprocessing single frames at rates exceeding $30~Hz$, which facilitates detailed\nplane extraction and map management swiftly and efficiently. Extensive testing\nunderscores the algorithm's capabilities in real-time scenarios and\ndemonstrates its practical application in humanoid robot gait planning,\nsignificantly improving its ability to navigate dynamic environments.\n","authors":["Teng Bin","Jianming Yao","Tin Lun Lam","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01919v1.pdf","comment":"Accepted by The 2024 IEEE-RAS International Conference on Humanoid\n  Robots. The code: https://github.com/BTFrontier/polygon_mapping"},{"id":"http://arxiv.org/abs/2406.14056v3","updated":"2024-11-04T09:31:06Z","published":"2024-06-20T07:24:43Z","title":"VGA: Vision GUI Assistant -- Minimizing Hallucinations through\n  Image-Centric Fine-Tuning","summary":"  Recent advances in Large Vision-Language Models (LVLMs) have significantly\nimprove performance in image comprehension tasks, such as formatted charts and\nrich-content images. Yet, Graphical User Interface (GUI) pose a greater\nchallenge due to their structured format and detailed textual information.\nExisting LVLMs often overly depend on internal knowledge and neglect image\ncontent, resulting in hallucinations and incorrect responses in GUI\ncomprehension. To address these issues, we introduce VGA, a fine-tuned model\ndesigned for comprehensive GUI understanding. Our model aims to enhance the\ninterpretation of visual data of GUI and reduce hallucinations. We first\nconstruct a Vision Question Answering (VQA) dataset of 63.8k high-quality\nexamples with our propose Referent Method, which ensures the model's responses\nare highly depend on visual content within the image. We then design a\ntwo-stage fine-tuning method called Foundation and Advanced Comprehension (FAC)\nto enhance both the model's ability to extract information from image content\nand alignment with human intent. Experiments show that our approach enhances\nthe model's ability to extract information from images and achieves\nstate-of-the-art results in GUI understanding tasks. Our dataset and\nfine-tuning script will be released soon.\n","authors":["Ziyang Meng","Yu Dai","Zezheng Gong","Shaoxiong Guo","Minglong Tang","Tongquan Wei"],"pdf_url":"https://arxiv.org/pdf/2406.14056v3.pdf","comment":"Accepted by EMNLP2024"},{"id":"http://arxiv.org/abs/2411.01916v1","updated":"2024-11-04T09:28:18Z","published":"2024-11-04T09:28:18Z","title":"Masked Autoencoders are Parameter-Efficient Federated Continual Learners","summary":"  Federated learning is a specific distributed learning paradigm in which a\ncentral server aggregates updates from multiple clients' local models, thereby\nenabling the server to learn without requiring clients to upload their private\ndata, maintaining data privacy. While existing federated learning methods are\nprimarily designed for static data, real-world applications often require\nclients to learn new categories over time. This challenge necessitates the\nintegration of continual learning techniques, resulting in federated continual\nlearning (FCL). Although advanced prompt-based continual learning methods\nleverage pre-trained transformers to mitigate catastrophic forgetting, they do\nnot adequately address the non-IID challenges in federated learning. To address\nboth catastrophic forgetting and non-IID issues, we propose to use masked\nautoencoders (MAEs) as parameter-efficient federated continual learners, called\npMAE. pMAE learns reconstructive prompt on the client side through image\nreconstruction using MAEs. On the server side, it reconstructs the uploaded\nrestore information to capture the data distribution across previous tasks and\ndifferent clients, using these reconstructed images to finetune discriminative\nprompt and classifier parameters designed for classification, thereby\nalleviating catastrophic forgetting and non-IID challenges on a global scale.\nExperimental results demonstrate that pMAE achieves performance comparable to\nexisting prompt-based methods and can enhance their effectiveness, particularly\nwhen using self-supervised pre-trained transformers as the backbone. Code is\navailable at: https://github.com/ycheoo/pMAE.\n","authors":["Yuchen He","Xiangfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01904v1","updated":"2024-11-04T09:15:21Z","published":"2024-11-04T09:15:21Z","title":"FPPL: An Efficient and Non-IID Robust Federated Continual Learning\n  Framework","summary":"  Federated continual learning (FCL) aims to learn from sequential data stream\nin the decentralized federated learning setting, while simultaneously\nmitigating the catastrophic forgetting issue in classical continual learning.\nExisting FCL methods usually employ typical rehearsal mechanisms, which could\nresult in privacy violations or additional onerous storage and computational\nburdens. In this work, an efficient and non-IID robust federated continual\nlearning framework, called Federated Prototype-Augmented Prompt Learning\n(FPPL), is proposed. The FPPL can collaboratively learn lightweight prompts\naugmented by prototypes without rehearsal. On the client side, a fusion\nfunction is employed to fully leverage the knowledge contained in task-specific\nprompts for alleviating catastrophic forgetting. Additionally, global\nprototypes aggregated from the server are used to obtain unified representation\nthrough contrastive learning, mitigating the impact of non-IID-derived data\nheterogeneity. On the server side, locally uploaded prototypes are utilized to\nperform debiasing on the classifier, further alleviating the performance\ndegradation caused by both non-IID and catastrophic forgetting. Empirical\nevaluations demonstrate the effectiveness of FPPL, achieving notable\nperformance with an efficient design while remaining robust to diverse non-IID\ndegrees. Code is available at: https://github.com/ycheoo/FPPL.\n","authors":["Yuchen He","Chuyun Shen","Xiangfeng Wang","Bo Jin"],"pdf_url":"https://arxiv.org/pdf/2411.01904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05645v3","updated":"2024-11-04T09:14:03Z","published":"2024-07-08T06:14:37Z","title":"OneDiff: A Generalist Model for Image Difference Captioning","summary":"  In computer vision, Image Difference Captioning (IDC) is crucial for\naccurately describing variations between closely related images. Traditional\nIDC methods often rely on specialist models, which restrict their applicability\nacross varied contexts. This paper introduces the OneDiff model, a novel\ngeneralist approach that utilizes a robust vision-language model architecture,\nintegrating a siamese image encoder with a Visual Delta Module. This innovative\nconfiguration allows for the precise detection and articulation of fine-grained\ndifferences between image pairs. OneDiff is trained through a dual-phase\nstrategy, encompassing Coupled Sample Training and multi-task learning across a\ndiverse array of data types, supported by our newly developed DiffCap Dataset.\nThis dataset merges real-world and synthetic data, enhancing the training\nprocess and bolstering the model's robustness. Extensive testing on diverse IDC\nbenchmarks, such as Spot-the-Diff, Image-Editing-Request, and Birds-to-Words,\nshows that OneDiff consistently outperforms existing state-of-the-art models in\naccuracy and adaptability, achieving improvements of up to 97% CIDEr points in\naverage. By setting a new benchmark in IDC, OneDiff paves the way for more\nversatile and effective applications in detecting and describing visual\ndifferences. The code, models, and data will be made publicly available.\n","authors":["Erdong Hu","Longteng Guo","Tongtian Yue","Zijia Zhao","Shuning Xue","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2407.05645v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15706v3","updated":"2024-11-04T09:06:40Z","published":"2024-03-23T03:56:31Z","title":"GACL: Exemplar-Free Generalized Analytic Continual Learning","summary":"  Class incremental learning (CIL) trains a network on sequential tasks with\nseparated categories in each task but suffers from catastrophic forgetting,\nwhere models quickly lose previously learned knowledge when acquiring new\ntasks. The generalized CIL (GCIL) aims to address the CIL problem in a more\nreal-world scenario, where incoming data have mixed data categories and unknown\nsample size distribution. Existing attempts for the GCIL either have poor\nperformance or invade data privacy by saving exemplars. In this paper, we\npropose a new exemplar-free GCIL technique named generalized analytic continual\nlearning (GACL). The GACL adopts analytic learning (a gradient-free training\ntechnique) and delivers an analytical (i.e., closed-form) solution to the GCIL\nscenario. This solution is derived via decomposing the incoming data into\nexposed and unexposed classes, thereby attaining a weight-invariant property, a\nrare yet valuable property supporting an equivalence between incremental\nlearning and its joint training. Such an equivalence is crucial in GCIL\nsettings as data distributions among different tasks no longer pose challenges\nto adopting our GACL. Theoretically, this equivalence property is validated\nthrough matrix analysis tools. Empirically, we conduct extensive experiments\nwhere, compared with existing GCIL methods, our GACL exhibits a consistently\nleading performance across various datasets and GCIL settings. Source code is\navailable at https://github.com/CHEN-YIZHU/GACL.\n","authors":["Huiping Zhuang","Yizhu Chen","Di Fang","Run He","Kai Tong","Hongxin Wei","Ziqian Zeng","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.15706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01896v1","updated":"2024-11-04T09:03:43Z","published":"2024-11-04T09:03:43Z","title":"MBDRes-U-Net: Multi-Scale Lightweight Brain Tumor Segmentation Network","summary":"  Accurate segmentation of brain tumors plays a key role in the diagnosis and\ntreatment of brain tumor diseases. It serves as a critical technology for\nquantifying tumors and extracting their features. With the increasing\napplication of deep learning methods, the computational burden has become\nprogressively heavier. To achieve a lightweight model with good segmentation\nperformance, this study proposes the MBDRes-U-Net model using the\nthree-dimensional (3D) U-Net codec framework, which integrates multibranch\nresidual blocks and fused attention into the model. The computational burden of\nthe model is reduced by the branch strategy, which effectively uses the rich\nlocal features in multimodal images and enhances the segmentation performance\nof subtumor regions. Additionally, during encoding, an adaptive weighted\nexpansion convolution layer is introduced into the multi-branch residual block,\nwhich enriches the feature expression and improves the segmentation accuracy of\nthe model. Experiments on the Brain Tumor Segmentation (BraTS) Challenge 2018\nand 2019 datasets show that the architecture could maintain a high precision of\nbrain tumor segmentation while considerably reducing the calculation\noverhead.Our code is released at\nhttps://github.com/Huaibei-normal-university-cv-laboratory/mbdresunet\n","authors":["Longfeng Shen","Yanqi Hou","Jiacong Chen","Liangjin Diao","Yaxi Duan"],"pdf_url":"https://arxiv.org/pdf/2411.01896v1.pdf","comment":"Brain tumor segmentation, lightweight model, Brain Tumor Segmentation\n  (BraTS) Challenge, group convolution"},{"id":"http://arxiv.org/abs/2405.17815v2","updated":"2024-11-04T09:03:31Z","published":"2024-05-28T04:23:00Z","title":"Visual Anchors Are Strong Information Aggregators For Multimodal Large\n  Language Model","summary":"  In the realm of Multimodal Large Language Models (MLLMs), vision-language\nconnector plays a crucial role to link the pre-trained vision encoders with\nLarge Language Models (LLMs). Despite its importance, the vision-language\nconnector has been relatively less explored. In this study, we aim to propose a\nstrong vision-language connector that enables MLLMs to achieve high accuracy\nwhile maintain low computation cost. We first reveal the existence of the\nvisual anchors in Vision Transformer and propose a cost-effective search\nalgorithm to extract them. Building on these findings, we introduce the Anchor\nFormer (AcFormer), a novel vision-language connector designed to leverage the\nrich prior knowledge obtained from these visual anchors during pretraining,\nguiding the aggregation of information. Through extensive experimentation, we\ndemonstrate that the proposed method significantly reduces computational costs\nby nearly two-thirds compared with baseline, while simultaneously outperforming\nbaseline methods. This highlights the effectiveness and efficiency of AcFormer.\nCodes are available at https://github.com/liuhaogeng/Anchor-Former.\n","authors":["Haogeng Liu","Quanzeng You","Xiaotian Han","Yongfei Liu","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2405.17815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05643v2","updated":"2024-11-04T08:58:14Z","published":"2024-10-08T02:46:30Z","title":"TRACE: Temporal Grounding Video LLM via Causal Event Modeling","summary":"  Video Temporal Grounding (VTG) is a crucial capability for video\nunderstanding models and plays a vital role in downstream tasks such as video\nbrowsing and editing. To effectively handle various tasks simultaneously and\nenable zero-shot prediction, there is a growing trend in employing video LLMs\nfor VTG tasks. However, current video LLM-based methods rely exclusively on\nnatural language generation, lacking the ability to model the clear structure\ninherent in videos, which restricts their effectiveness in tackling VTG tasks.\nTo address this issue, this paper first formally introduces causal event\nmodeling framework, which represents videos as sequences of events, and predict\nthe current event using previous events, video inputs, and textural\ninstructions. Each event consists of three components: timestamps, salient\nscores, and textual captions. We then propose a novel task-interleaved video\nLLM called TRACE to effectively implement the causal event modeling framework\nin practice. The TRACE processes visual frames, timestamps, salient scores, and\ntext as distinct tasks, employing various encoders and decoding heads for each.\nTask tokens are arranged in an interleaved sequence according to the causal\nevent modeling framework's formulation. Extensive experiments on various VTG\ntasks and datasets demonstrate the superior performance of TRACE compared to\nstate-of-the-art video LLMs. Our model and code are available at\n\\url{https://github.com/gyxxyg/TRACE}.\n","authors":["Yongxin Guo","Jingyu Liu","Mingda Li","Xiaoying Tang","Qingbin Liu","Xi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01893v1","updated":"2024-11-04T08:50:16Z","published":"2024-11-04T08:50:16Z","title":"A Global Depth-Range-Free Multi-View Stereo Transformer Network with\n  Pose Embedding","summary":"  In this paper, we propose a novel multi-view stereo (MVS) framework that gets\nrid of the depth range prior. Unlike recent prior-free MVS methods that work in\na pair-wise manner, our method simultaneously considers all the source images.\nSpecifically, we introduce a Multi-view Disparity Attention (MDA) module to\naggregate long-range context information within and across multi-view images.\nConsidering the asymmetry of the epipolar disparity flow, the key to our method\nlies in accurately modeling multi-view geometric constraints. We integrate pose\nembedding to encapsulate information such as multi-view camera poses, providing\nimplicit geometric constraints for multi-view disparity feature fusion\ndominated by attention. Additionally, we construct corresponding hidden states\nfor each source image due to significant differences in the observation quality\nof the same pixel in the reference frame across multiple source frames. We\nexplicitly estimate the quality of the current pixel corresponding to sampled\npoints on the epipolar line of the source image and dynamically update hidden\nstates through the uncertainty estimation module. Extensive results on the DTU\ndataset and Tanks&Temple benchmark demonstrate the effectiveness of our method.\nThe code is available at our project page:\nhttps://zju3dv.github.io/GD-PoseMVS/.\n","authors":["Yitong Dong","Yijin Li","Zhaoyang Huang","Weikang Bian","Jingbo Liu","Hujun Bao","Zhaopeng Cui","Hongsheng Li","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02554v2","updated":"2024-11-04T08:48:19Z","published":"2024-02-04T15:59:35Z","title":"DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms\n  in Vision Transformers","summary":"  Vision transformers have contributed greatly to advancements in the computer\nvision domain, demonstrating state-of-the-art performance in diverse tasks\n(e.g., image classification, object detection). However, their high\ncomputational requirements grow quadratically with the number of tokens used.\nToken sparsification mechanisms have been proposed to address this issue. These\nmechanisms employ an input-dependent strategy, in which uninformative tokens\nare discarded from the computation pipeline, improving the model's efficiency.\nHowever, their dynamism and average-case assumption makes them vulnerable to a\nnew threat vector - carefully crafted adversarial examples capable of fooling\nthe sparsification mechanism, resulting in worst-case performance. In this\npaper, we present DeSparsify, an attack targeting the availability of vision\ntransformers that use token sparsification mechanisms. The attack aims to\nexhaust the operating system's resources, while maintaining its stealthiness.\nOur evaluation demonstrates the attack's effectiveness on three token\nsparsification mechanisms and examines the attack's transferability between\nthem and its effect on the GPU resources. To mitigate the impact of the attack,\nwe propose various countermeasures.\n","authors":["Oryan Yehezkel","Alon Zolfi","Amit Baras","Yuval Elovici","Asaf Shabtai"],"pdf_url":"https://arxiv.org/pdf/2402.02554v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15751v2","updated":"2024-11-04T08:48:10Z","published":"2024-03-23T07:39:13Z","title":"F-OAL: Forward-only Online Analytic Learning with Fast Training and Low\n  Memory Footprint in Class Incremental Learning","summary":"  Online Class Incremental Learning (OCIL) aims to train models incrementally,\nwhere data arrive in mini-batches, and previous data are not accessible. A\nmajor challenge in OCIL is Catastrophic Forgetting, i.e., the loss of\npreviously learned knowledge. Among existing baselines, replay-based methods\nshow competitive results but requires extra memory for storing exemplars, while\nexemplar-free (i.e., data need not be stored for replay in production) methods\nare resource-friendly but often lack accuracy. In this paper, we propose an\nexemplar-free approach--Forward-only Online Analytic Learning (F-OAL). Unlike\ntraditional methods, F-OAL does not rely on back-propagation and is\nforward-only, significantly reducing memory usage and computational time.\nCooperating with a pre-trained frozen encoder with Feature Fusion, F-OAL only\nneeds to update a linear classifier by recursive least square. This approach\nsimultaneously achieves high accuracy and low resource consumption. Extensive\nexperiments on benchmark datasets demonstrate F-OAL's robust performance in\nOCIL scenarios. Code is available at https://github.com/liuyuchen-cz/F-OAL.\n","authors":["Huiping Zhuang","Yuchen Liu","Run He","Kai Tong","Ziqian Zeng","Cen Chen","Yi Wang","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2403.15751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19458v3","updated":"2024-11-04T08:43:30Z","published":"2024-05-29T19:12:08Z","title":"MemControl: Mitigating Memorization in Diffusion Models via Automated\n  Parameter Selection","summary":"  Diffusion models excel in generating images that closely resemble their\ntraining data but are also susceptible to data memorization, raising privacy,\nethical, and legal concerns, particularly in sensitive domains such as medical\nimaging. We hypothesize that this memorization stems from the\noverparameterization of deep models and propose that regularizing model\ncapacity during fine-tuning can mitigate this issue. Firstly, we empirically\nshow that regulating the model capacity via Parameter-efficient fine-tuning\n(PEFT) mitigates memorization to some extent, however, it further requires the\nidentification of the exact parameter subsets to be fine-tuned for high-quality\ngeneration. To identify these subsets, we introduce a bi-level optimization\nframework, MemControl, that automates parameter selection using memorization\nand generation quality metrics as rewards during fine-tuning. The parameter\nsubsets discovered through MemControl achieve a superior tradeoff between\ngeneration quality and memorization. For the task of medical image generation,\nour approach outperforms existing state-of-the-art memorization mitigation\nstrategies by fine-tuning as few as 0.019% of model parameters. Moreover, we\ndemonstrate that the discovered parameter subsets are transferable to\nnon-medical domains. Our framework is scalable to large datasets, agnostic to\nreward functions, and can be integrated with existing approaches for further\nmemorization mitigation. To the best of our knowledge, this is the first study\nto empirically evaluate memorization in medical images and propose a targeted\nyet universal mitigation strategy. The code is available at\nhttps://github.com/Raman1121/Diffusion_Memorization_HPO\n","authors":["Raman Dutt","Ondrej Bohdal","Pedro Sanchez","Sotirios A. Tsaftaris","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2405.19458v3.pdf","comment":"Accepted at WACV'25 (Applications Track)"},{"id":"http://arxiv.org/abs/2411.01889v1","updated":"2024-11-04T08:37:12Z","published":"2024-11-04T08:37:12Z","title":"LiDAttack: Robust Black-box Attack on LiDAR-based Object Detection","summary":"  Since DNN is vulnerable to carefully crafted adversarial examples,\nadversarial attack on LiDAR sensors have been extensively studied. We introduce\na robust black-box attack dubbed LiDAttack. It utilizes a genetic algorithm\nwith a simulated annealing strategy to strictly limit the location and number\nof perturbation points, achieving a stealthy and effective attack. And it\nsimulates scanning deviations, allowing it to adapt to dynamic changes in real\nworld scenario variations. Extensive experiments are conducted on 3 datasets\n(i.e., KITTI, nuScenes, and self-constructed data) with 3 dominant object\ndetection models (i.e., PointRCNN, PointPillar, and PV-RCNN++). The results\nreveal the efficiency of the LiDAttack when targeting a wide range of object\ndetection models, with an attack success rate (ASR) up to 90%.\n","authors":["Jinyin Chen","Danxin Liao","Sheng Xiang","Haibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.01889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01870v1","updated":"2024-11-04T07:57:44Z","published":"2024-11-04T07:57:44Z","title":"Mining and Transferring Feature-Geometry Coherence for Unsupervised\n  Point Cloud Registration","summary":"  Point cloud registration, a fundamental task in 3D vision, has achieved\nremarkable success with learning-based methods in outdoor environments.\nUnsupervised outdoor point cloud registration methods have recently emerged to\ncircumvent the need for costly pose annotations. However, they fail to\nestablish reliable optimization objectives for unsupervised training, either\nrelying on overly strong geometric assumptions, or suffering from poor-quality\npseudo-labels due to inadequate integration of low-level geometric and\nhigh-level contextual information. We have observed that in the feature space,\nlatent new inlier correspondences tend to cluster around respective positive\nanchors that summarize features of existing inliers. Motivated by this\nobservation, we propose a novel unsupervised registration method termed INTEGER\nto incorporate high-level contextual information for reliable pseudo-label\nmining. Specifically, we propose the Feature-Geometry Coherence Mining module\nto dynamically adapt the teacher for each mini-batch of data during training\nand discover reliable pseudo-labels by considering both high-level feature\nrepresentations and low-level geometric cues. Furthermore, we propose\nAnchor-Based Contrastive Learning to facilitate contrastive learning with\nanchors for a robust feature space. Lastly, we introduce a Mixed-Density\nStudent to learn density-invariant features, addressing challenges related to\ndensity variation and low overlap in the outdoor scenario. Extensive\nexperiments on KITTI and nuScenes datasets demonstrate that our INTEGER\nachieves competitive performance in terms of accuracy and generalizability.\n","authors":["Kezheng Xiong","Haoen Xiang","Qingshan Xu","Chenglu Wen","Siqi Shen","Jonathan Li","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01870v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2406.05967v2","updated":"2024-11-04T07:55:31Z","published":"2024-06-10T01:59:00Z","title":"CVQA: Culturally-diverse Multilingual Visual Question Answering\n  Benchmark","summary":"  Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 30\ncountries on four continents, covering 31 languages with 13 scripts, providing\na total of 10k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.\n","authors":["David Romero","Chenyang Lyu","Haryo Akbarianto Wibowo","Teresa Lynn","Injy Hamed","Aditya Nanda Kishore","Aishik Mandal","Alina Dragonetti","Artem Abzaliev","Atnafu Lambebo Tonja","Bontu Fufa Balcha","Chenxi Whitehouse","Christian Salamea","Dan John Velasco","David Ifeoluwa Adelani","David Le Meur","Emilio Villa-Cueva","Fajri Koto","Fauzan Farooqui","Frederico Belcavello","Ganzorig Batnasan","Gisela Vallejo","Grainne Caulfield","Guido Ivetta","Haiyue Song","Henok Biadglign Ademtew","Hernán Maina","Holy Lovenia","Israel Abebe Azime","Jan Christian Blaise Cruz","Jay Gala","Jiahui Geng","Jesus-German Ortiz-Barajas","Jinheon Baek","Jocelyn Dunstan","Laura Alonso Alemany","Kumaranage Ravindu Yasas Nagasinghe","Luciana Benotti","Luis Fernando D'Haro","Marcelo Viridiano","Marcos Estecha-Garitagoitia","Maria Camila Buitrago Cabrera","Mario Rodríguez-Cantelar","Mélanie Jouitteau","Mihail Mihaylov","Mohamed Fazli Mohamed Imam","Muhammad Farid Adilazuarda","Munkhjargal Gochoo","Munkh-Erdene Otgonbold","Naome Etori","Olivier Niyomugisha","Paula Mónica Silva","Pranjal Chitale","Raj Dabre","Rendi Chevi","Ruochen Zhang","Ryandito Diandaru","Samuel Cahyawijaya","Santiago Góngora","Soyeong Jeong","Sukannya Purkayastha","Tatsuki Kuribayashi","Teresa Clifford","Thanmay Jayakumar","Tiago Timponi Torrent","Toqeer Ehsan","Vladimir Araujo","Yova Kementchedjhieva","Zara Burzo","Zheng Wei Lim","Zheng Xin Yong","Oana Ignat","Joan Nwatu","Rada Mihalcea","Thamar Solorio","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2406.05967v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2411.01859v1","updated":"2024-11-04T07:21:06Z","published":"2024-11-04T07:21:06Z","title":"A Novel Deep Learning Tractography Fiber Clustering Framework for\n  Functionally Consistent White Matter Parcellation Using Multimodal Diffusion\n  MRI and Functional MRI","summary":"  Tractography fiber clustering using diffusion MRI (dMRI) is a crucial\nstrategy for white matter (WM) parcellation. Current methods primarily use the\ngeometric information of fibers (i.e., the spatial trajectories) to group\nsimilar fibers into clusters, overlooking the important functional signals\npresent along the fiber tracts. There is increasing evidence that neural\nactivity in the WM can be measured using functional MRI (fMRI), offering\npotentially valuable multimodal information for fiber clustering. In this\npaper, we develop a novel deep learning fiber clustering framework, namely Deep\nMulti-view Fiber Clustering (DMVFC), that uses joint dMRI and fMRI data to\nenable functionally consistent WM parcellation. DMVFC can effectively integrate\nthe geometric characteristics of the WM fibers with the fMRI BOLD signals along\nthe fiber tracts. It includes two major components: 1) a multi-view pretraining\nmodule to compute embedding features from fiber geometric information and\nfunctional signals separately, and 2) a collaborative fine-tuning module to\nsimultaneously refine the two kinds of embeddings. In the experiments, we\ncompare DMVFC with two state-of-the-art fiber clustering methods and\ndemonstrate superior performance in achieving functionally meaningful and\nconsistent WM parcellation results.\n","authors":["Jin Wang","Bocheng Guo","Yijie Li","Junyi Wang","Yuqian Chen","Jarrett Rushmore","Nikos Makris","Yogesh Rathi","Lauren J O'Donnell","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01859v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.03879v2","updated":"2024-11-04T07:16:54Z","published":"2024-06-06T09:14:32Z","title":"Decay Pruning Method: Smooth Pruning With a Self-Rectifying Procedure","summary":"  Current structured pruning methods often result in considerable accuracy\ndrops due to abrupt network changes and loss of information from pruned\nstructures. To address these issues, we introduce the Decay Pruning Method\n(DPM), a novel smooth pruning approach with a self-rectifying mechanism. DPM\nconsists of two key components: (i) Smooth Pruning: It converts conventional\nsingle-step pruning into multi-step smooth pruning, gradually reducing\nredundant structures to zero over N steps with ongoing optimization. (ii)\nSelf-Rectifying: This procedure further enhances the aforementioned process by\nrectifying sub-optimal pruning based on gradient information. Our approach\ndemonstrates strong generalizability and can be easily integrated with various\nexisting pruning methods. We validate the effectiveness of DPM by integrating\nit with three popular pruning methods: OTOv2, Depgraph, and Gate Decorator.\nExperimental results show consistent improvements in performance compared to\nthe original pruning methods, along with further reductions of FLOPs in most\nscenarios.\n","authors":["Minghao Yang","Linlin Gao","Pengyuan Li","Wenbo Li","Yihong Dong","Zhiying Cui"],"pdf_url":"https://arxiv.org/pdf/2406.03879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01853v1","updated":"2024-11-04T07:07:31Z","published":"2024-11-04T07:07:31Z","title":"GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface\n  Reconstruction in Open Scenes","summary":"  In this paper we present a novel method for efficient and effective 3D\nsurface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)\nbased works typically require extensive training and rendering time due to the\nadopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)\nuses an explicit and discrete representation, hence the reconstructed surface\nis built by the huge number of Gaussian primitives, which leads to excessive\nmemory consumption and rough surface details in sparse Gaussian areas. To\naddress these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which\nestablish a continuous scene representation based on discrete 3DGS through\nkernel regression. The GVKF integrates fast 3DGS rasterization and highly\neffective scene implicit representations, achieving high-fidelity open scene\nsurface reconstruction. Experiments on challenging scene datasets demonstrate\nthe efficiency and effectiveness of our proposed GVKF, featuring with high\nreconstruction quality, real-time rendering speed, significant savings in\nstorage and training memory consumption.\n","authors":["Gaochao Song","Chong Cheng","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01853v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01851v1","updated":"2024-11-04T07:05:47Z","published":"2024-11-04T07:05:47Z","title":"Silver medal Solution for Image Matching Challenge 2024","summary":"  Image Matching Challenge 2024 is a competition focused on building 3D maps\nfrom diverse image sets, requiring participants to solve fundamental computer\nvision challenges in image matching across varying angles, lighting, and\nseasonal changes. This project develops a Pipeline method that combines\nmultiple advanced techniques: using pre-trained EfficientNet-B7 for initial\nfeature extraction and cosine distance-based image pair filtering, employing\nboth KeyNetAffNetHardNet and SuperPoint for keypoint feature extraction,\nutilizing AdaLAM and SuperGlue for keypoint matching, and finally applying\nPycolmap for 3D spatial analysis. The methodology achieved an excellent score\nof 0.167 on the private leaderboard, with experimental results demonstrating\nthat the combination of KeyNetAffNetHardNet and SuperPoint provides significant\nadvantages in keypoint detection and matching, particularly when dealing with\nchallenging variations in surface texture and environmental conditions that\ntypically degrade traditional algorithm performance.\n","authors":["Yian Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01846v1","updated":"2024-11-04T06:42:24Z","published":"2024-11-04T06:42:24Z","title":"KptLLM: Unveiling the Power of Large Language Model for Keypoint\n  Comprehension","summary":"  Recent advancements in Multimodal Large Language Models (MLLMs) have greatly\nimproved their abilities in image understanding. However, these models often\nstruggle with grasping pixel-level semantic details, e.g., the keypoints of an\nobject. To bridge this gap, we introduce the novel challenge of Semantic\nKeypoint Comprehension, which aims to comprehend keypoints across different\ntask scenarios, including keypoint semantic understanding, visual prompt-based\nkeypoint detection, and textual prompt-based keypoint detection. Moreover, we\nintroduce KptLLM, a unified multimodal model that utilizes an\nidentify-then-detect strategy to effectively address these challenges. KptLLM\nunderscores the initial discernment of semantics in keypoints, followed by the\nprecise determination of their positions through a chain-of-thought process.\nWith several carefully designed modules, KptLLM adeptly handles various\nmodality inputs, facilitating the interpretation of both semantic contents and\nkeypoint locations. Our extensive experiments demonstrate KptLLM's superiority\nin various keypoint detection benchmarks and its unique semantic capabilities\nin interpreting keypoints.\n","authors":["Jie Yang","Wang Zeng","Sheng Jin","Lumin Xu","Wentao Liu","Chen Qian","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01846v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.20024v3","updated":"2024-11-04T06:08:30Z","published":"2024-06-28T16:13:55Z","title":"eMoE-Tracker: Environmental MoE-based Transformer for Robust\n  Event-guided Object Tracking","summary":"  The unique complementarity of frame-based and event cameras for high frame\nrate object tracking has recently inspired some research attempts to develop\nmulti-modal fusion approaches. However, these methods directly fuse both\nmodalities and thus ignore the environmental attributes, e.g., motion blur,\nillumination variance, occlusion, scale variation, etc. Meanwhile, insufficient\ninteraction between search and template features makes distinguishing target\nobjects and backgrounds difficult. As a result, performance degradation is\ninduced especially in challenging conditions. This paper proposes a novel and\neffective Transformer-based event-guided tracking framework, called\neMoE-Tracker, which achieves new SOTA performance under various conditions. Our\nkey idea is to disentangle the environment into several learnable attributes to\ndynamically learn the attribute-specific features and strengthen the target\ninformation by improving the interaction between the target template and search\nregions. To achieve the goal, we first propose an environmental Mix-of-Experts\n(eMoE) module that is built upon the environmental Attributes Disentanglement\nto learn attribute-specific features and environmental Attributes Assembling to\nassemble the attribute-specific features by the learnable attribute scores\ndynamically. The eMoE module is a subtle router that prompt-tunes the\ntransformer backbone more efficiently. We then introduce a contrastive relation\nmodeling (CRM) module to emphasize target information by leveraging a\ncontrastive learning strategy between the target template and search regions.\nExtensive experiments on diverse event-based benchmark datasets showcase the\nsuperior performance of our eMoE-Tracker compared to the prior arts.\n","authors":["Yucheng Chen","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.20024v3.pdf","comment":"RGB-event single object tracking"},{"id":"http://arxiv.org/abs/2411.01833v1","updated":"2024-11-04T06:07:43Z","published":"2024-11-04T06:07:43Z","title":"OwMatch: Conditional Self-Labeling with Consistency for Open-World\n  Semi-Supervised Learning","summary":"  Semi-supervised learning (SSL) offers a robust framework for harnessing the\npotential of unannotated data. Traditionally, SSL mandates that all classes\npossess labeled instances. However, the emergence of open-world SSL (OwSSL)\nintroduces a more practical challenge, wherein unlabeled data may encompass\nsamples from unseen classes. This scenario leads to misclassification of unseen\nclasses as known ones, consequently undermining classification accuracy. To\novercome this challenge, this study revisits two methodologies from\nself-supervised and semi-supervised learning, self-labeling and consistency,\ntailoring them to address the OwSSL problem. Specifically, we propose an\neffective framework called OwMatch, combining conditional self-labeling and\nopen-world hierarchical thresholding. Theoretically, we analyze the estimation\nof class distribution on unlabeled data through rigorous statistical analysis,\nthus demonstrating that OwMatch can ensure the unbiasedness of the self-label\nassignment estimator with reliability. Comprehensive empirical analyses\ndemonstrate that our method yields substantial performance enhancements across\nboth known and unknown classes in comparison to previous studies. Code is\navailable at https://github.com/niusj03/OwMatch.\n","authors":["Shengjie Niu","Lifan Lin","Jian Huang","Chao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01833v1.pdf","comment":"NeurIPS 2024 camera-ready (10 pages, 4 figures) with the appendices\n  (10 pages, 7 figures)"},{"id":"http://arxiv.org/abs/2310.02901v3","updated":"2024-11-04T06:06:02Z","published":"2023-10-04T15:39:57Z","title":"Efficient Vectorized Backpropagation Algorithms for Training Feedforward\n  Networks Composed of Quadratic Neurons","summary":"  Higher order artificial neurons whose outputs are computed by applying an\nactivation function to a higher order multinomial function of the inputs have\nbeen considered in the past, but did not gain acceptance due to the extra\nparameters and computational cost. However, higher order neurons have\nsignificantly greater learning capabilities since the decision boundaries of\nhigher order neurons can be complex surfaces instead of just hyperplanes. The\nboundary of a single quadratic neuron can be a general hyper-quadric surface\nallowing it to learn many nonlinearly separable datasets. Since quadratic forms\ncan be represented by symmetric matrices, only $\\frac{n(n+1)}{2}$ additional\nparameters are needed instead of $n^2$. A quadratic Logistic regression model\nis first presented. Solutions to the XOR problem with a single quadratic neuron\nare considered. The complete vectorized equations for both forward and backward\npropagation in feedforward networks composed of quadratic neurons are derived.\nA reduced parameter quadratic neural network model with just $ n $ additional\nparameters per neuron that provides a compromise between learning ability and\ncomputational cost is presented. Comparison on benchmark classification\ndatasets are used to demonstrate that a final layer of quadratic neurons\nenables networks to achieve higher accuracy with significantly fewer hidden\nlayer neurons. In particular this paper shows that any dataset composed of\n$\\mathcal{C}$ bounded clusters can be separated with only a single layer of\n$\\mathcal{C}$ quadratic neurons.\n","authors":["Mathew Mithra Noel","Venkataraman Muthiah-Nakarajan"],"pdf_url":"https://arxiv.org/pdf/2310.02901v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.16999v3","updated":"2024-11-04T05:50:56Z","published":"2024-03-25T17:59:23Z","title":"Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive\n  Dataset and Benchmark for Chain-of-Thought Reasoning","summary":"  Multi-Modal Large Language Models (MLLMs) have demonstrated impressive\nperformance in various VQA tasks. However, they often lack interpretability and\nstruggle with complex visual inputs, especially when the resolution of the\ninput image is high or when the interested region that could provide key\ninformation for answering the question is small. To address these challenges,\nwe collect and introduce the large-scale Visual CoT dataset comprising 438k\nquestion-answer pairs, annotated with intermediate bounding boxes highlighting\nkey regions essential for answering the questions. Additionally, about 98k\npairs of them are annotated with detailed reasoning steps. Importantly, we\npropose a multi-turn processing pipeline that dynamically focuses on visual\ninputs and provides interpretable thoughts. We also introduce the related\nbenchmark to evaluate the MLLMs in scenarios requiring specific local region\nidentification. Extensive experiments demonstrate the effectiveness of our\nframework and shed light on better inference strategies. The Visual CoT\ndataset, benchmark, and pre-trained models are available on\nhttps://hao-shao.com/projects/viscot.html to support further research in this\narea.\n","authors":["Hao Shao","Shengju Qian","Han Xiao","Guanglu Song","Zhuofan Zong","Letian Wang","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16999v3.pdf","comment":"Project Page: https://hao-shao.com/projects/viscot.html"},{"id":"http://arxiv.org/abs/2410.24060v2","updated":"2024-11-04T05:44:08Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08590v2","updated":"2024-11-04T05:43:17Z","published":"2024-04-12T16:38:48Z","title":"Vision-Aware Text Features in Referring Image Segmentation: From Object\n  Understanding to Context Understanding","summary":"  Referring image segmentation is a challenging task that involves generating\npixel-wise segmentation masks based on natural language descriptions. The\ncomplexity of this task increases with the intricacy of the sentences provided.\nExisting methods have relied mostly on visual features to generate the\nsegmentation masks while treating text features as supporting components.\nHowever, this under-utilization of text understanding limits the model's\ncapability to fully comprehend the given expressions. In this work, we propose\na novel framework that specifically emphasizes object and context comprehension\ninspired by human cognitive processes through Vision-Aware Text Features.\nFirstly, we introduce a CLIP Prior module to localize the main object of\ninterest and embed the object heatmap into the query initialization process.\nSecondly, we propose a combination of two components: Contextual Multimodal\nDecoder and Meaning Consistency Constraint, to further enhance the coherent and\nconsistent interpretation of language cues with the contextual understanding\nobtained from the image. Our method achieves significant performance\nimprovements on three benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Project\npage: \\url{https://vatex.hkustvgd.com/}.\n","authors":["Hai Nguyen-Truong","E-Ro Nguyen","Tuan-Anh Vu","Minh-Triet Tran","Binh-Son Hua","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2404.08590v2.pdf","comment":"This paper is accepted in WACV 2025"},{"id":"http://arxiv.org/abs/2411.01822v1","updated":"2024-11-04T05:41:31Z","published":"2024-11-04T05:41:31Z","title":"Distribution alignment based transfer fusion frameworks on quantum\n  devices for seeking quantum advantages","summary":"  The scarcity of labelled data is specifically an urgent challenge in the\nfield of quantum machine learning (QML). Two transfer fusion frameworks are\nproposed in this paper to predict the labels of a target domain data by\naligning its distribution to a different but related labelled source domain on\nquantum devices. The frameworks fuses the quantum data from two different, but\nrelated domains through a quantum information infusion channel. The predicting\ntasks in the target domain can be achieved with quantum advantages by\npost-processing quantum measurement results. One framework, the quantum basic\nlinear algebra subroutines (QBLAS) based implementation, can theoretically\nachieve the procedure of transfer fusion with quadratic speedup on a universal\nquantum computer. In addition, the other framework, a hardware-scalable\narchitecture, is implemented on the noisy intermediate-scale quantum (NISQ)\ndevices through a variational hybrid quantum-classical procedure. Numerical\nexperiments on the synthetic and handwritten digits datasets demonstrate that\nthe variatioinal transfer fusion (TF) framework can reach state-of-the-art\n(SOTA) quantum DA method performance.\n","authors":["Xi He","Feiyu Du","Xiaohan Yu","Yang Zhao","Tao Lei"],"pdf_url":"https://arxiv.org/pdf/2411.01822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01819v1","updated":"2024-11-04T05:39:01Z","published":"2024-11-04T05:39:01Z","title":"DiffuMask-Editor: A Novel Paradigm of Integration Between the\n  Segmentation Diffusion Model and Image Editing to Improve Segmentation\n  Ability","summary":"  Semantic segmentation models, like mask2former, often demand a substantial\namount of manually annotated data, which is time-consuming and inefficient to\nacquire. Leveraging state-of-the-art text-to-image models like Midjourney and\nStable Diffusion has emerged as an effective strategy for automatically\ngenerating synthetic data instead of human annotations. However, prior\napproaches have been constrained to synthesizing single-instance images due to\nthe instability inherent in generating multiple instances with Stable\nDiffusion. To expand the domains and diversity of synthetic datasets, this\npaper introduces a novel paradigm named DiffuMask-Editor, which combines the\nDiffusion Model for Segmentation with Image Editing. By integrating multiple\nobjects into images using Text2Image models, our method facilitates the\ncreation of more realistic datasets that closely resemble open-world settings\nwhile simultaneously generating accurate masks. Our approach significantly\nreduces the laborious effort associated with manual annotation while ensuring\nprecise mask generation. Experimental results demonstrate that synthetic data\ngenerated by DiffuMask-Editor enable segmentation methods to achieve superior\nperformance compared to real data. Particularly in zero-shot backgrounds,\nDiffuMask-Editor achieves new state-of-the-art results on Unseen classes of VOC\n2012. The code and models will be publicly available soon.\n","authors":["Bo Gao","Fangxu Xing","Daniel Tang"],"pdf_url":"https://arxiv.org/pdf/2411.01819v1.pdf","comment":"13 pages,4 figures"},{"id":"http://arxiv.org/abs/2411.01801v1","updated":"2024-11-04T05:00:49Z","published":"2024-11-04T05:00:49Z","title":"Bootstrapping Top-down Information for Self-modulating Slot Attention","summary":"  Object-centric learning (OCL) aims to learn representations of individual\nobjects within visual scenes without manual supervision, facilitating efficient\nand effective visual reasoning. Traditional OCL methods primarily employ\nbottom-up approaches that aggregate homogeneous visual features to represent\nobjects. However, in complex visual environments, these methods often fall\nshort due to the heterogeneous nature of visual features within an object. To\naddress this, we propose a novel OCL framework incorporating a top-down\npathway. This pathway first bootstraps the semantics of individual objects and\nthen modulates the model to prioritize features relevant to these semantics. By\ndynamically modulating the model based on its own output, our top-down pathway\nenhances the representational quality of objects. Our framework achieves\nstate-of-the-art performance across multiple synthetic and real-world\nobject-discovery benchmarks.\n","authors":["Dongwon Kim","Seoyeon Kim","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2411.01801v1.pdf","comment":"Accepted to NeurIPS2 2024"},{"id":"http://arxiv.org/abs/2407.04172v2","updated":"2024-11-04T04:59:45Z","published":"2024-07-04T22:16:40Z","title":"ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild","summary":"  Given the ubiquity of charts as a data analysis, visualization, and\ndecision-making tool across industries and sciences, there has been a growing\ninterest in developing pre-trained foundation models as well as general purpose\ninstruction-tuned models for chart understanding and reasoning. However,\nexisting methods suffer crucial drawbacks across two critical axes affecting\nthe performance of chart representation models: they are trained on data\ngenerated from underlying data tables of the charts, ignoring the visual trends\nand patterns in chart images, and use weakly aligned vision-language backbone\nmodels for domain-specific training, limiting their generalizability when\nencountering charts in the wild. We address these important drawbacks and\nintroduce ChartGemma, a novel chart understanding and reasoning model developed\nover PaliGemma. Rather than relying on underlying data tables, ChartGemma is\ntrained on instruction-tuning data generated directly from chart images, thus\ncapturing both high-level trends and low-level visual information from a\ndiverse set of charts. Our simple approach achieves state-of-the-art results\nacross $5$ benchmarks spanning chart summarization, question answering, and\nfact-checking, and our elaborate qualitative studies on real-world charts show\nthat ChartGemma generates more realistic and factually correct summaries\ncompared to its contemporaries. We release the code, model checkpoints,\ndataset, and demos at https://github.com/vis-nlp/ChartGemma.\n","authors":["Ahmed Masry","Megh Thakkar","Aayush Bajaj","Aaryaman Kartha","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2407.04172v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01800v1","updated":"2024-11-04T04:58:20Z","published":"2024-11-04T04:58:20Z","title":"Expanding Sparse Tuning for Low Memory Usage","summary":"  Parameter-efficient fine-tuning (PEFT) is an effective method for adapting\npre-trained vision models to downstream tasks by tuning a small subset of\nparameters. Among PEFT methods, sparse tuning achieves superior performance by\nonly adjusting the weights most relevant to downstream tasks, rather than\ndensely tuning the whole weight matrix. However, this performance improvement\nhas been accompanied by increases in memory usage, which stems from two\nfactors, i.e., the storage of the whole weight matrix as learnable parameters\nin the optimizer and the additional storage of tunable weight indexes. In this\npaper, we propose a method named SNELL (Sparse tuning with kerNELized LoRA) for\nsparse tuning with low memory usage. To achieve low memory usage, SNELL\ndecomposes the tunable matrix for sparsification into two learnable low-rank\nmatrices, saving from the costly storage of the whole original matrix. A\ncompetition-based sparsification mechanism is further proposed to avoid the\nstorage of tunable weight indexes. To maintain the effectiveness of sparse\ntuning with low-rank matrices, we extend the low-rank decomposition by applying\nnonlinear kernel functions to the whole-matrix merging. Consequently, we gain\nan increase in the rank of the merged matrix, enhancing the ability of SNELL in\nadapting the pre-trained models to downstream tasks. Extensive experiments on\nmultiple downstream tasks show that SNELL achieves state-of-the-art performance\nwith low memory usage, endowing PEFT with sparse tuning to large-scale models.\nCodes are available at https://github.com/ssfgunner/SNELL.\n","authors":["Shufan Shen","Junshu Sun","Xiangyang Ji","Qingming Huang","Shuhui Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01800v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01797v1","updated":"2024-11-04T04:45:45Z","published":"2024-11-04T04:45:45Z","title":"AIWR: Aerial Image Water Resource Dataset for Segmentation Analysis","summary":"  Effective water resource management is crucial in agricultural regions like\nnortheastern Thailand, where limited water retention in sandy soils poses\nsignificant challenges. In response to this issue, the Aerial Image Water\nResource (AIWR) dataset was developed, comprising 800 aerial images focused on\nnatural and artificial water bodies in this region. The dataset was created\nusing Bing Maps and follows the standards of the Fundamental Geographic Data\nSet (FGDS). It includes ground truth annotations validated by experts in remote\nsensing, making it an invaluable resource for researchers in geoinformatics,\ncomputer vision, and artificial intelligence. The AIWR dataset presents\nconsiderable challenges, such as segmentation due to variations in the size,\ncolor, shape, and similarity of water bodies, which often resemble other land\nuse categories.\n","authors":["Sangdaow Noppitaka","Emmanuel Okafor","Olarik Surinta"],"pdf_url":"https://arxiv.org/pdf/2411.01797v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.01788v1","updated":"2024-11-04T04:21:41Z","published":"2024-11-04T04:21:41Z","title":"Non rigid geometric distortions correction -- Application to atmospheric\n  turbulence stabilization","summary":"  A novel approach is presented to recover an image degraded by atmospheric\nturbulence. Given a sequence of frames affected by turbulence, we construct a\nvariational model to characterize the static image. The optimization problem is\nsolved by Bregman Iteration and the operator splitting method. Our algorithm is\nsimple, efficient, and can be easily generalized for different scenarios.\n","authors":["Yu Mao","Jerome Gilles"],"pdf_url":"https://arxiv.org/pdf/2411.01788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01781v1","updated":"2024-11-04T04:14:39Z","published":"2024-11-04T04:14:39Z","title":"MSTA3D: Multi-scale Twin-attention for 3D Instance Segmentation","summary":"  Recently, transformer-based techniques incorporating superpoints have become\nprevalent in 3D instance segmentation. However, they often encounter an\nover-segmentation problem, especially noticeable with large objects.\nAdditionally, unreliable mask predictions stemming from superpoint mask\nprediction further compound this issue. To address these challenges, we propose\na novel framework called MSTA3D. It leverages multi-scale feature\nrepresentation and introduces a twin-attention mechanism to effectively capture\nthem. Furthermore, MSTA3D integrates a box query with a box regularizer,\noffering a complementary spatial constraint alongside semantic queries.\nExperimental evaluations on ScanNetV2, ScanNet200 and S3DIS datasets\ndemonstrate that our approach surpasses state-of-the-art 3D instance\nsegmentation methods.\n","authors":["Duc Dang Trung Tran","Byeongkeun Kang","Yeejin Lee"],"pdf_url":"https://arxiv.org/pdf/2411.01781v1.pdf","comment":"14 pages, 9 figures, 7 tables, conference"},{"id":"http://arxiv.org/abs/2411.01777v1","updated":"2024-11-04T03:58:09Z","published":"2024-11-04T03:58:09Z","title":"Learning predictable and robust neural representations by straightening\n  image sequences","summary":"  Prediction is a fundamental capability of all living organisms, and has been\nproposed as an objective for learning sensory representations. Recent work\ndemonstrates that in primate visual systems, prediction is facilitated by\nneural representations that follow straighter temporal trajectories than their\ninitial photoreceptor encoding, which allows for prediction by linear\nextrapolation. Inspired by these experimental findings, we develop a\nself-supervised learning (SSL) objective that explicitly quantifies and\npromotes straightening. We demonstrate the power of this objective in training\ndeep feedforward neural networks on smoothly-rendered synthetic image sequences\nthat mimic commonly-occurring properties of natural videos. The learned model\ncontains neural embeddings that are predictive, but also factorize the\ngeometric, photometric, and semantic attributes of objects. The representations\nalso prove more robust to noise and adversarial attacks compared to previous\nSSL methods that optimize for invariance to random augmentations. Moreover,\nthese beneficial properties can be transferred to other training procedures by\nusing the straightening objective as a regularizer, suggesting a broader\nutility for straightening as a principle for robust unsupervised learning.\n","authors":["Xueyan Niu","Cristina Savin","Eero P. Simoncelli"],"pdf_url":"https://arxiv.org/pdf/2411.01777v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01769v1","updated":"2024-11-04T03:29:51Z","published":"2024-11-04T03:29:51Z","title":"ARN-LSTM: A Multi-Stream Attention-Based Model for Action Recognition\n  with Temporal Dynamics","summary":"  This paper presents ARN-LSTM, a novel multi-stream action recognition model\ndesigned to address the challenge of simultaneously capturing spatial motion\nand temporal dynamics in action sequences. Traditional methods often focus\nsolely on spatial or temporal features, limiting their ability to comprehend\ncomplex human activities fully. Our proposed model integrates joint, motion,\nand temporal information through a multi-stream fusion architecture.\nSpecifically, it comprises a joint stream for extracting skeleton features, a\ntemporal stream for capturing dynamic temporal features, and an ARN-LSTM block\nthat utilizes Time-Distributed Long Short-Term Memory (TD-LSTM) layers followed\nby an Attention Relation Network (ARN) to model temporal relations. The outputs\nfrom these streams are fused in a fully connected layer to provide the final\naction prediction. Evaluations on the NTU RGB+D 60 and NTU RGB+D 120 datasets\ndemonstrate the effectiveness of our model, achieving effective performance,\nparticularly in group activity recognition.\n","authors":["Chuanchuan Wang","Ahmad Sufril Azlan Mohmamed","Xiao Yang","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2411.01769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14133v2","updated":"2024-11-04T02:59:25Z","published":"2023-03-24T16:38:58Z","title":"Survey on Adversarial Attack and Defense for Medical Image Analysis:\n  Methods and Challenges","summary":"  Deep learning techniques have achieved superior performance in computer-aided\nmedical image analysis, yet they are still vulnerable to imperceptible\nadversarial attacks, resulting in potential misdiagnosis in clinical practice.\nOppositely, recent years have also witnessed remarkable progress in defense\nagainst these tailored adversarial examples in deep medical diagnosis systems.\nIn this exposition, we present a comprehensive survey on recent advances in\nadversarial attacks and defenses for medical image analysis with a systematic\ntaxonomy in terms of the application scenario. We also provide a unified\nframework for different types of adversarial attack and defense methods in the\ncontext of medical image analysis. For a fair comparison, we establish a new\nbenchmark for adversarially robust medical diagnosis models obtained by\nadversarial training under various scenarios. To the best of our knowledge,\nthis is the first survey paper that provides a thorough evaluation of\nadversarially robust medical diagnosis models. By analyzing qualitative and\nquantitative results, we conclude this survey with a detailed discussion of\ncurrent challenges for adversarial attack and defense in medical image analysis\nsystems to shed light on future research directions. Code is available on\n\\href{https://github.com/tomvii/Adv_MIA}{\\color{red}{GitHub}}.\n","authors":["Junhao Dong","Junxi Chen","Xiaohua Xie","Jianhuang Lai","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.14133v2.pdf","comment":"Accepted by ACM Computing Surveys (CSUR) (DOI:\n  https://doi.org/10.1145/3702638)"},{"id":"http://arxiv.org/abs/2305.04161v3","updated":"2024-11-04T02:52:56Z","published":"2023-05-07T02:26:00Z","title":"Camera-Based HRV Prediction for Remote Learning Environments","summary":"  In recent years, due to the widespread use of internet videos, remote\nphotoplethysmography (rPPG) has gained more and more attention in the fields of\naffective computing. Restoring blood volume pulse (BVP) signals from facial\nvideos is a challenging task that involves a series of preprocessing, image\nalgorithms, and postprocessing to restore waveforms. Not only is the heart rate\nmetric utilized for affective computing, but the heart rate variability (HRV)\nmetric is even more significant. The challenge in obtaining HRV indices through\nrPPG lies in the necessity for algorithms to precisely predict the BVP peak\npositions. In this paper, we collected the Remote Learning Affect and\nPhysiology (RLAP) dataset, which includes over 32 hours of highly synchronized\nvideo and labels from 58 subjects. This is a public dataset whose BVP labels\nhave been meticulously designed to better suit the training of HRV models.\nUsing the RLAP dataset, we trained a new model called Seq-rPPG, it is a model\nbased on one-dimensional convolution, and experimental results reveal that this\nstructure is more suitable for handling HRV tasks, which outperformed all other\nbaselines in HRV performance and also demonstrated significant advantages in\ncomputational efficiency.\n","authors":["Kegang Wang","Yantao Wei","Jiankai Tang","Yuntao Wang","Mingwen Tong","Jie Gao","Yujian Ma","Zhongjin Zhao"],"pdf_url":"https://arxiv.org/pdf/2305.04161v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01759v1","updated":"2024-11-04T02:52:02Z","published":"2024-11-04T02:52:02Z","title":"Automatic Structured Pruning for Efficient Architecture in Federated\n  Learning","summary":"  In Federated Learning (FL), training is conducted on client devices,\ntypically with limited computational resources and storage capacity. To address\nthese constraints, we propose an automatic pruning scheme tailored for FL\nsystems. Our solution improves computation efficiency on client devices, while\nminimizing communication costs. One of the challenges of tuning pruning\nhyper-parameters in FL systems is the restricted access to local data. Thus, we\nintroduce an automatic pruning paradigm that dynamically determines pruning\nboundaries. Additionally, we utilized a structured pruning algorithm optimized\nfor mobile devices that lack hardware support for sparse computations.\nExperimental results demonstrate the effectiveness of our approach, achieving\naccuracy comparable to existing methods. Our method notably reduces the number\nof parameters by 89% and FLOPS by 90%, with minimal impact on the accuracy of\nthe FEMNIST and CelebFaces datasets. Furthermore, our pruning method decreases\ncommunication overhead by up to 5x and halves inference time when deployed on\nAndroid devices.\n","authors":["Thai Vu Nguyen","Long Bao Le","Anderson Avila"],"pdf_url":"https://arxiv.org/pdf/2411.01759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01758v1","updated":"2024-11-04T02:50:52Z","published":"2024-11-04T02:50:52Z","title":"Disentangled PET Lesion Segmentation","summary":"  PET imaging is an invaluable tool in clinical settings as it captures the\nfunctional activity of both healthy anatomy and cancerous lesions. Developing\nautomatic lesion segmentation methods for PET images is crucial since manual\nlesion segmentation is laborious and prone to inter- and intra-observer\nvariability. We propose PET-Disentangler, a 3D disentanglement method that uses\na 3D UNet-like encoder-decoder architecture to disentangle disease and normal\nhealthy anatomical features with losses for segmentation, reconstruction, and\nhealthy component plausibility. A critic network is used to encourage the\nhealthy latent features to match the distribution of healthy samples and thus\nencourages these features to not contain any lesion-related features. Our\nquantitative results show that PET-Disentangler is less prone to incorrectly\ndeclaring healthy and high tracer uptake regions as cancerous lesions, since\nsuch uptake pattern would be assigned to the disentangled healthy component.\n","authors":["Tanya Gatsak","Kumar Abhishek","Hanene Ben Yedder","Saeid Asgari Taghanaki","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2411.01758v1.pdf","comment":"4 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2411.01756v1","updated":"2024-11-04T02:43:55Z","published":"2024-11-04T02:43:55Z","title":"ChatTracker: Enhancing Visual Tracking Performance via Chatting with\n  Multimodal Large Language Model","summary":"  Visual object tracking aims to locate a targeted object in a video sequence\nbased on an initial bounding box. Recently, Vision-Language~(VL) trackers have\nproposed to utilize additional natural language descriptions to enhance\nversatility in various applications. However, VL trackers are still inferior to\nState-of-The-Art (SoTA) visual trackers in terms of tracking performance. We\nfound that this inferiority primarily results from their heavy reliance on\nmanual textual annotations, which include the frequent provision of ambiguous\nlanguage descriptions. In this paper, we propose ChatTracker to leverage the\nwealth of world knowledge in the Multimodal Large Language Model (MLLM) to\ngenerate high-quality language descriptions and enhance tracking performance.\nTo this end, we propose a novel reflection-based prompt optimization module to\niteratively refine the ambiguous and inaccurate descriptions of the target with\ntracking feedback. To further utilize semantic information produced by MLLM, a\nsimple yet effective VL tracking framework is proposed and can be easily\nintegrated as a plug-and-play module to boost the performance of both VL and\nvisual trackers. Experimental results show that our proposed ChatTracker\nachieves a performance comparable to existing methods.\n","authors":["Yiming Sun","Fan Yu","Shaoxiang Chen","Yu Zhang","Junwei Huang","Chenhui Li","Yang Li","Changbo Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09728v2","updated":"2024-11-04T02:34:59Z","published":"2024-06-14T05:33:01Z","title":"Neural Pose Representation Learning for Generating and Transferring\n  Non-Rigid Object Poses","summary":"  We propose a novel method for learning representations of poses for 3D\ndeformable objects, which specializes in 1) disentangling pose information from\nthe object's identity, 2) facilitating the learning of pose variations, and 3)\ntransferring pose information to other object identities. Based on these\nproperties, our method enables the generation of 3D deformable objects with\ndiversity in both identities and poses, using variations of a single object. It\ndoes not require explicit shape parameterization such as skeletons or joints,\npoint-level or shape-level correspondence supervision, or variations of the\ntarget object for pose transfer. To achieve pose disentanglement, compactness\nfor generative models, and transferability, we first design the pose extractor\nto represent the pose as a keypoint-based hybrid representation and the pose\napplier to learn an implicit deformation field. To better distill pose\ninformation from the object's geometry, we propose the implicit pose applier to\noutput an intrinsic mesh property, the face Jacobian. Once the extracted pose\ninformation is transferred to the target object, the pose applier is fine-tuned\nin a self-supervised manner to better describe the target object's shapes with\npose variations. The extracted poses are also used to train a cascaded\ndiffusion model to enable the generation of novel poses. Our experiments with\nthe DeformThings4D and Human datasets demonstrate state-of-the-art performance\nin pose transfer and the ability to generate diverse deformed shapes with\nvarious objects and poses.\n","authors":["Seungwoo Yoo","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2406.09728v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.19464v2","updated":"2024-11-04T02:21:30Z","published":"2024-06-27T18:06:38Z","title":"ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data","summary":"  Audio signals provide rich information for the robot interaction and object\nproperties through contact. This information can surprisingly ease the learning\nof contact-rich robot manipulation skills, especially when the visual\ninformation alone is ambiguous or incomplete. However, the usage of audio data\nin robot manipulation has been constrained to teleoperated demonstrations\ncollected by either attaching a microphone to the robot or object, which\nsignificantly limits its usage in robot learning pipelines. In this work, we\nintroduce ManiWAV: an 'ear-in-hand' data collection device to collect\nin-the-wild human demonstrations with synchronous audio and visual feedback,\nand a corresponding policy interface to learn robot manipulation policy\ndirectly from the demonstrations. We demonstrate the capabilities of our system\nthrough four contact-rich manipulation tasks that require either passively\nsensing the contact events and modes, or actively sensing the object surface\nmaterials and states. In addition, we show that our system can generalize to\nunseen in-the-wild environments by learning from diverse in-the-wild human\ndemonstrations.\n","authors":["Zeyi Liu","Cheng Chi","Eric Cousineau","Naveen Kuppuswamy","Benjamin Burchfiel","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.19464v2.pdf","comment":"Conference on Robot Learning (CoRL) 2024; Project website:\n  https://maniwav.github.io/"},{"id":"http://arxiv.org/abs/2411.01749v1","updated":"2024-11-04T02:20:22Z","published":"2024-11-04T02:20:22Z","title":"Multi-task Geometric Estimation of Depth and Surface Normal from\n  Monocular 360° Images","summary":"  Geometric estimation is required for scene understanding and analysis in\npanoramic 360{\\deg} images. Current methods usually predict a single feature,\nsuch as depth or surface normal. These methods can lack robustness, especially\nwhen dealing with intricate textures or complex object surfaces. We introduce a\nnovel multi-task learning (MTL) network that simultaneously estimates depth and\nsurface normals from 360{\\deg} images. Our first innovation is our MTL\narchitecture, which enhances predictions for both tasks by integrating\ngeometric information from depth and surface normal estimation, enabling a\ndeeper understanding of 3D scene structure. Another innovation is our fusion\nmodule, which bridges the two tasks, allowing the network to learn shared\nrepresentations that improve accuracy and robustness. Experimental results\ndemonstrate that our MTL architecture significantly outperforms\nstate-of-the-art methods in both depth and surface normal estimation, showing\nsuperior performance in complex and diverse scenes. Our model's effectiveness\nand generalizability, particularly in handling intricate surface textures,\nestablish it as a new benchmark in 360{\\deg} image geometric estimation. The\ncode and model are available at\n\\url{https://github.com/huangkun101230/360MTLGeometricEstimation}.\n","authors":["Kun Huang","Fang-Lue Zhang","Fangfang Zhang","Yu-Kun Lai","Paul Rosin","Neil A. Dodgson"],"pdf_url":"https://arxiv.org/pdf/2411.01749v1.pdf","comment":"18 pages, this paper is accepted by Computational Visual Media\n  Journal (CVMJ) but not pushlished yet"},{"id":"http://arxiv.org/abs/2411.01748v1","updated":"2024-11-04T02:13:41Z","published":"2024-11-04T02:13:41Z","title":"Rotation Perturbation Robustness in Point Cloud Analysis: A Perspective\n  of Manifold Distillation","summary":"  Point cloud is often regarded as a discrete sampling of Riemannian manifold\nand plays a pivotal role in the 3D image interpretation. Particularly, rotation\nperturbation, an unexpected small change in rotation caused by various factors\n(like equipment offset, system instability, measurement errors and so on), can\neasily lead to the inferior results in point cloud learning tasks. However,\nclassical point cloud learning methods are sensitive to rotation perturbation,\nand the existing networks with rotation robustness also have much room for\nimprovements in terms of performance and noise tolerance. Given these, this\npaper remodels the point cloud from the perspective of manifold as well as\ndesigns a manifold distillation method to achieve the robustness of rotation\nperturbation without any coordinate transformation. In brief, during the\ntraining phase, we introduce a teacher network to learn the rotation robustness\ninformation and transfer this information to the student network through online\ndistillation. In the inference phase, the student network directly utilizes the\noriginal 3D coordinate information to achieve the robustness of rotation\nperturbation. Experiments carried out on four different datasets verify the\neffectiveness of our method. Averagely, on the Modelnet40 and ScanobjectNN\nclassification datasets with random rotation perturbations, our classification\naccuracy has respectively improved by 4.92% and 4.41%, compared to popular\nrotation-robust networks; on the ShapeNet and S3DIS segmentation datasets,\ncompared to the rotation-robust networks, the improvements of mIoU are 7.36%\nand 4.82%, respectively. Besides, from the experimental results, the proposed\nalgorithm also shows excellent performance in resisting noise and outliers.\n","authors":["Xinyu Xu","Huazhen Liu","Feiming Wei","Huilin Xiong","Wenxian Yu","Tao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01748v1.pdf","comment":"13 pages, 8 figures, submitted to TCSVT"},{"id":"http://arxiv.org/abs/2411.01742v1","updated":"2024-11-04T01:51:50Z","published":"2024-11-04T01:51:50Z","title":"Learning from Convolution-based Unlearnable Datastes","summary":"  The construction of large datasets for deep learning has raised concerns\nregarding unauthorized use of online data, leading to increased interest in\nprotecting data from third-parties who want to use it for training. The\nConvolution-based Unlearnable DAtaset (CUDA) method aims to make data\nunlearnable by applying class-wise blurs to every image in the dataset so that\nneural networks learn relations between blur kernels and labels, as opposed to\ninformative features for classifying clean data. In this work, we evaluate\nwhether CUDA data remains unlearnable after image sharpening and frequency\nfiltering, finding that this combination of simple transforms improves the\nutility of CUDA data for training. In particular, we observe a substantial\nincrease in test accuracy over adversarial training for models trained with\nCUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training\nmodels to high accuracy using unlearnable data, we underscore the need for\nongoing refinement in data poisoning techniques to ensure data privacy. Our\nmethod opens new avenues for enhancing the robustness of unlearnable datasets\nby highlighting that simple methods such as sharpening and frequency filtering\nare capable of breaking convolution-based unlearnable datasets.\n","authors":["Dohyun Kim","Pedro Sandoval-Segura"],"pdf_url":"https://arxiv.org/pdf/2411.01742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01739v1","updated":"2024-11-04T01:42:41Z","published":"2024-11-04T01:42:41Z","title":"Not Just Object, But State: Compositional Incremental Learning without\n  Forgetting","summary":"  Most incremental learners excessively prioritize coarse classes of objects\nwhile neglecting various kinds of states (e.g. color and material) attached to\nthe objects. As a result, they are limited in the ability to reason\nfine-grained compositionality of state-object pairs. To remedy this limitation,\nwe propose a novel task called Compositional Incremental Learning\n(composition-IL), enabling the model to recognize state-object compositions as\na whole in an incremental learning fashion. Since the lack of suitable\nbenchmarks, we re-organize two existing datasets and make them tailored for\ncomposition-IL. Then, we propose a prompt-based Composition Incremental Learner\n(CompILer), to overcome the ambiguous composition boundary problem which\nchallenges composition-IL largely. Specifically, we exploit multi-pool prompt\nlearning, which is regularized by inter-pool prompt discrepancy and intra-pool\nprompt diversity. Besides, we devise object-injected state prompting by using\nobject prompts to guide the selection of state prompts. Furthermore, we fuse\nthe selected prompts by a generalized-mean strategy, to eliminate irrelevant\ninformation learned in the prompts. Extensive experiments on two datasets\nexhibit state-of-the-art performance achieved by CompILer.\n","authors":["Yanyi Zhang","Binglin Qiu","Qi Jia","Yu Liu","Ran He"],"pdf_url":"https://arxiv.org/pdf/2411.01739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01734v1","updated":"2024-11-04T01:32:09Z","published":"2024-11-04T01:32:09Z","title":"Next Best View For Point-Cloud Model Acquisition: Bayesian Approximation\n  and Uncertainty Analysis","summary":"  The Next Best View problem is a computer vision problem widely studied in\nrobotics. To solve it, several methodologies have been proposed over the years.\nSome, more recently, propose the use of deep learning models. Predictions\nobtained with the help of deep learning models naturally have some uncertainty\nassociated with them. Despite this, the standard models do not allow for their\nquantification. However, Bayesian estimation theory contributed to the\ndemonstration that dropout layers allow to estimate prediction uncertainty in\nneural networks.\n  This work adapts the point-net-based neural network for Next-Best-View\n(PC-NBV). It incorporates dropout layers into the model's architecture, thus\nallowing the computation of the uncertainty estimate associated with its\npredictions. The aim of the work is to improve the network's accuracy in\ncorrectly predicting the next best viewpoint, proposing a way to make the 3D\nreconstruction process more efficient.\n  Two uncertainty measurements capable of reflecting the prediction's error and\naccuracy, respectively, were obtained. These enabled the reduction of the\nmodel's error and the increase in its accuracy from 30\\% to 80\\% by identifying\nand disregarding predictions with high values of uncertainty. Another method\nthat directly uses these uncertainty metrics to improve the final prediction\nwas also proposed. However, it showed very residual improvements.\n","authors":["Madalena Caldeira","Plinio Moreno"],"pdf_url":"https://arxiv.org/pdf/2411.01734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14370v4","updated":"2024-11-04T01:25:37Z","published":"2024-03-21T12:57:30Z","title":"SyncTweedies: A General Generative Framework Based on Synchronized\n  Diffusions","summary":"  We introduce a general framework for generating diverse visual content,\nincluding ambiguous images, panorama images, mesh textures, and Gaussian splat\ntextures, by synchronizing multiple diffusion processes. We present exhaustive\ninvestigation into all possible scenarios for synchronizing multiple diffusion\nprocesses through a canonical space and analyze their characteristics across\napplications. In doing so, we reveal a previously unexplored case: averaging\nthe outputs of Tweedie's formula while conducting denoising in multiple\ninstance spaces. This case also provides the best quality with the widest\napplicability to downstream tasks. We name this case SyncTweedies. In our\nexperiments generating visual content aforementioned, we demonstrate the\nsuperior quality of generation by SyncTweedies compared to other\nsynchronization methods, optimization-based and iterative-update-based methods.\n","authors":["Jaihoon Kim","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.14370v4.pdf","comment":"Project page: https://synctweedies.github.io/ (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2406.13155v2","updated":"2024-11-04T00:55:06Z","published":"2024-06-19T02:09:44Z","title":"Convolutional Kolmogorov-Arnold Networks","summary":"  In this paper, we introduce Convolutional Kolmogorov-Arnold Networks\n(Convolutional KANs), an innovative alternative to the standard Convolutional\nNeural Networks (CNNs) that have revolutionized the field of computer vision.\nBy integrating the learneable non-linear activation functions presented in\nKolmogorov-Arnold Networks (KANs) into convolutions, we propose a new layer.\nThroughout the paper, we empirically validate the performance of Convolutional\nKANs against traditional architectures across Fashion-MNIST dataset, finding\nthat, in some cases, this new approach maintains a similar level of accuracy\nwhile using half the number of parameters. This experiments show that KAN\nConvolutions seem to learn more per kernel, which opens up a new horizon of\npossibilities in deep learning for computer vision.\n","authors":["Alexander Dylan Bodner","Antonio Santiago Tepsich","Jack Natan Spolski","Santiago Pourteau"],"pdf_url":"https://arxiv.org/pdf/2406.13155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01725v1","updated":"2024-11-04T00:49:47Z","published":"2024-11-04T00:49:47Z","title":"A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields","summary":"  In this paper we reexamine the process through which a Neural Radiance Field\n(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image\napplications where camera pixels integrate light over time, LiDAR pulses arrive\nat specific times. As such, multiple LiDAR returns are possible for any given\ndetector and the classification of these returns is inherently probabilistic.\nApplying a traditional NeRF training routine can result in the network learning\nphantom surfaces in free space between conflicting range measurements, similar\nto how floater aberrations may be produced by an image model. We show that by\nformulating loss as an integral of probability (rather than as an integral of\noptical density) the network can learn multiple peaks for a given ray, allowing\nthe sampling of first, nth, or strongest returns from a single output channel.\nCode is available at https://github.com/mcdermatt/PLINK\n","authors":["Matthew McDermott","Jason Rife"],"pdf_url":"https://arxiv.org/pdf/2411.01725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15748v2","updated":"2024-11-04T00:48:12Z","published":"2023-05-25T05:55:53Z","title":"ReactFace: Online Multiple Appropriate Facial Reaction Generation in\n  Dyadic Interactions","summary":"  In dyadic interaction, predicting the listener's facial reactions is\nchallenging as different reactions could be appropriate in response to the same\nspeaker's behaviour. Previous approaches predominantly treated this task as an\ninterpolation or fitting problem, emphasizing deterministic outcomes but\nignoring the diversity and uncertainty of human facial reactions. Furthermore,\nthese methods often failed to model short-range and long-range dependencies\nwithin the interaction context, leading to issues in the synchrony and\nappropriateness of the generated facial reactions. To address these\nlimitations, this paper reformulates the task as an extrapolation or prediction\nproblem, and proposes an novel framework (called ReactFace) to generate\nmultiple different but appropriate facial reactions from a speaker behaviour\nrather than merely replicating the corresponding listener facial behaviours.\nOur ReactFace generates multiple different but appropriate photo-realistic\nhuman facial reactions by: (i) learning an appropriate facial reaction\ndistribution representing multiple different but appropriate facial reactions;\nand (ii) synchronizing the generated facial reactions with the speaker verbal\nand non-verbal behaviours at each time stamp, resulting in realistic 2D facial\nreaction sequences. Experimental results demonstrate the effectiveness of our\napproach in generating multiple diverse, synchronized, and appropriate facial\nreactions from each speaker's behaviour. The quality of the generated facial\nreactions is intimately tied to the speaker's speech and facial expressions,\nachieved through our novel speaker-listener interaction modules. Our code is\nmade publicly available at \\url{https://github.com/lingjivoo/ReactFace}.\n","authors":["Cheng Luo","Siyang Song","Weicheng Xie","Micol Spitale","Zongyuan Ge","Linlin Shen","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2305.15748v2.pdf","comment":"Accepted to IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), 18 pages, 10 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.02284v1","updated":"2024-11-04T17:11:14Z","published":"2024-11-04T17:11:14Z","title":"Training on the Test Model: Contamination in Ranking Distillation","summary":"  Neural approaches to ranking based on pre-trained language models are highly\neffective in ad-hoc search. However, the computational expense of these models\ncan limit their application. As such, a process known as knowledge distillation\nis frequently applied to allow a smaller, efficient model to learn from an\neffective but expensive model. A key example of this is the distillation of\nexpensive API-based commercial Large Language Models into smaller\nproduction-ready models. However, due to the opacity of training data and\nprocesses of most commercial models, one cannot ensure that a chosen test\ncollection has not been observed previously, creating the potential for\ninadvertent data contamination. We, therefore, investigate the effect of a\ncontaminated teacher model in a distillation setting. We evaluate several\ndistillation techniques to assess the degree to which contamination occurs\nduring distillation. By simulating a ``worst-case'' setting where the degree of\ncontamination is known, we find that contamination occurs even when the test\ndata represents a small fraction of the teacher's training samples. We,\ntherefore, encourage caution when training using black-box teacher models where\ndata provenance is ambiguous.\n","authors":["Vishakha Suresh Kalal","Andrew Parry","Sean MacAvaney"],"pdf_url":"https://arxiv.org/pdf/2411.02284v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2411.02041v1","updated":"2024-11-04T12:43:12Z","published":"2024-11-04T12:43:12Z","title":"Enhancing ID-based Recommendation with Large Language Models","summary":"  Large Language Models (LLMs) have recently garnered significant attention in\nvarious domains, including recommendation systems. Recent research leverages\nthe capabilities of LLMs to improve the performance and user modeling aspects\nof recommender systems. These studies primarily focus on utilizing LLMs to\ninterpret textual data in recommendation tasks. However, it's worth noting that\nin ID-based recommendations, textual data is absent, and only ID data is\navailable. The untapped potential of LLMs for ID data within the ID-based\nrecommendation paradigm remains relatively unexplored. To this end, we\nintroduce a pioneering approach called \"LLM for ID-based Recommendation\"\n(LLM4IDRec). This innovative approach integrates the capabilities of LLMs while\nexclusively relying on ID data, thus diverging from the previous reliance on\ntextual data. The basic idea of LLM4IDRec is that by employing LLM to augment\nID data, if augmented ID data can improve recommendation performance, it\ndemonstrates the ability of LLM to interpret ID data effectively, exploring an\ninnovative way for the integration of LLM in ID-based recommendation. We\nevaluate the effectiveness of our LLM4IDRec approach using three widely-used\ndatasets. Our results demonstrate a notable improvement in recommendation\nperformance, with our approach consistently outperforming existing methods in\nID-based recommendation by solely augmenting input data.\n","authors":["Lei Chen","Chen Gao","Xiaoyi Du","Hengliang Luo","Depeng Jin","Yong Li","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01843v1","updated":"2024-11-04T06:31:52Z","published":"2024-11-04T06:31:52Z","title":"Dissertation: On the Theoretical Foundation of Model Comparison and\n  Evaluation for Recommender System","summary":"  Recommender systems have become increasingly important with the rise of the\nweb as a medium for electronic and business transactions. One of the key\ndrivers of this technology is the ease with which users can provide feedback\nabout their likes and dislikes through simple clicks of a mouse. This feedback\nis commonly collected in the form of ratings, but can also be inferred from a\nuser's browsing and purchasing history. Recommender systems utilize users'\nhistorical data to infer customer interests and provide personalized\nrecommendations. The basic principle of recommendations is that significant\ndependencies exist between user- and item-centric activity, which can be\nlearned in a data-driven manner to make accurate predictions. Collaborative\nfiltering is one family of recommendation algorithms that uses ratings from\nmultiple users to predict missing ratings or uses binary click information to\npredict potential clicks. However, recommender systems can be more complex and\nincorporate auxiliary data such as content-based attributes, user interactions,\nand contextual information.\n","authors":["Dong Li"],"pdf_url":"https://arxiv.org/pdf/2411.01843v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.08517"},{"id":"http://arxiv.org/abs/2411.01785v1","updated":"2024-11-04T04:16:11Z","published":"2024-11-04T04:16:11Z","title":"Transferable Sequential Recommendation via Vector Quantized Meta\n  Learning","summary":"  While sequential recommendation achieves significant progress on capturing\nuser-item transition patterns, transferring such large-scale recommender\nsystems remains challenging due to the disjoint user and item groups across\ndomains. In this paper, we propose a vector quantized meta learning for\ntransferable sequential recommenders (MetaRec). Without requiring additional\nmodalities or shared information across domains, our approach leverages\nuser-item interactions from multiple source domains to improve the target\ndomain performance. To solve the input heterogeneity issue, we adopt vector\nquantization that maps item embeddings from heterogeneous input spaces to a\nshared feature space. Moreover, our meta transfer paradigm exploits limited\ntarget data to guide the transfer of source domain knowledge to the target\ndomain (i.e., learn to transfer). In addition, MetaRec adaptively transfers\nfrom multiple source tasks by rescaling meta gradients based on the\nsource-target domain similarity, enabling selective learning to improve\nrecommendation performance. To validate the effectiveness of our approach, we\nperform extensive experiments on benchmark datasets, where MetaRec consistently\noutperforms baseline methods by a considerable margin.\n","authors":["Zhenrui Yue","Huimin Zeng","Yang Zhang","Julian McAuley","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01785v1.pdf","comment":"Accepted to BigData 2024"},{"id":"http://arxiv.org/abs/2403.00801v2","updated":"2024-11-04T03:07:30Z","published":"2024-02-23T18:45:35Z","title":"Self-Retrieval: End-to-End Information Retrieval with One Large Language\n  Model","summary":"  The rise of large language models (LLMs) has significantly transformed both\nthe construction and application of information retrieval (IR) systems.\nHowever, current interactions between IR systems and LLMs remain limited, with\nLLMs merely serving as part of components within IR systems, and IR systems\nbeing constructed independently of LLMs. This separated architecture restricts\nknowledge sharing and deep collaboration between them. In this paper, we\nintroduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval\narchitecture. Self-Retrieval unifies all essential IR functions within a single\nLLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through\nself-supervised learning, transforms the retrieval process into sequential\npassage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms\nexisting retrieval approaches by a significant margin, but also substantially\nenhances the performance of LLM-driven downstream applications like\nretrieval-augmented generation.\n","authors":["Qiaoyu Tang","Jiawei Chen","Zhuoqun Li","Bowen Yu","Yaojie Lu","Cheng Fu","Haiyang Yu","Hongyu Lin","Fei Huang","Ben He","Xianpei Han","Le Sun","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.00801v2.pdf","comment":"NeurIPS 2024 Camera-ready Version. Code:\n  https://github.com/icip-cas/SelfRetrieval"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2411.02398v1","updated":"2024-11-04T18:59:51Z","published":"2024-11-04T18:59:51Z","title":"Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin\n  Script Languages","summary":"  Multilingual LLMs have achieved remarkable benchmark performance, but we find\nthey continue to underperform on non-Latin script languages across contemporary\nLLM families. This discrepancy arises from the fact that LLMs are pretrained\nwith orthographic scripts, which are dominated by Latin characters that obscure\ntheir shared phonology with non-Latin scripts. We propose leveraging phonemic\ntranscriptions as complementary signals to induce script-invariant\nrepresentations. Our study demonstrates that integrating phonemic signals\nimproves performance across both non-Latin and Latin languages, with a\nparticularly significant impact on closing the performance gap between the two.\nThrough detailed experiments, we show that phonemic and orthographic scripts\nretrieve distinct examples for in-context learning (ICL). This motivates our\nproposed Mixed-ICL retrieval strategy, where further aggregation leads to our\nsignificant performance improvements for both Latin script languages (up to\n12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL\nretrieval.\n","authors":["Hoang Nguyen","Khyati Mahajan","Vikas Yadav","Philip S. Yu","Masoud Hashemi","Rishabh Maheshwary"],"pdf_url":"https://arxiv.org/pdf/2411.02398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19370v2","updated":"2024-11-04T18:58:02Z","published":"2024-06-27T17:50:05Z","title":"Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept\n  Space","summary":"  Modern generative models demonstrate impressive capabilities, likely stemming\nfrom an ability to identify and manipulate abstract concepts underlying their\ntraining data. However, fundamental questions remain: what determines the\nconcepts a model learns, the order in which it learns them, and its ability to\nmanipulate those concepts? To address these questions, we propose analyzing a\nmodel's learning dynamics via a framework we call the concept space, where each\naxis represents an independent concept underlying the data generating process.\nBy characterizing learning dynamics in this space, we identify how the speed at\nwhich a concept is learned, and hence the order of concept learning, is\ncontrolled by properties of the data we term concept signal. Further, we\nobserve moments of sudden turns in the direction of a model's learning dynamics\nin concept space. Surprisingly, these points precisely correspond to the\nemergence of hidden capabilities, i.e., where latent interventions show the\nmodel possesses the capability to manipulate a concept, but these capabilities\ncannot yet be elicited via naive input prompting. While our results focus on\nsynthetically defined toy datasets, we hypothesize a general claim on emergence\nof hidden capabilities may hold: generative models possess latent capabilities\nthat emerge suddenly and consistently during training, though a model might not\nexhibit these capabilities under naive input prompting.\n","authors":["Core Francisco Park","Maya Okawa","Andrew Lee","Ekdeep Singh Lubana","Hidenori Tanaka"],"pdf_url":"https://arxiv.org/pdf/2406.19370v2.pdf","comment":"NeurIPS 2024 (spotlight)"},{"id":"http://arxiv.org/abs/2411.02393v1","updated":"2024-11-04T18:58:01Z","published":"2024-11-04T18:58:01Z","title":"Adaptive Length Image Tokenization via Recurrent Allocation","summary":"  Current vision systems typically assign fixed-length representations to\nimages, regardless of the information content. This contrasts with human\nintelligence - and even large language models - which allocate varying\nrepresentational capacities based on entropy, context and familiarity. Inspired\nby this, we propose an approach to learn variable-length token representations\nfor 2D images. Our encoder-decoder architecture recursively processes 2D image\ntokens, distilling them into 1D latent tokens over multiple iterations of\nrecurrent rollouts. Each iteration refines the 2D tokens, updates the existing\n1D latent tokens, and adaptively increases representational capacity by adding\nnew tokens. This enables compression of images into a variable number of\ntokens, ranging from 32 to 256. We validate our tokenizer using reconstruction\nloss and FID metrics, demonstrating that token count aligns with image entropy,\nfamiliarity and downstream task requirements. Recurrent token processing with\nincreasing representational capacity in each iteration shows signs of token\nspecialization, revealing potential for object / part discovery.\n","authors":["Shivam Duggal","Phillip Isola","Antonio Torralba","William T. Freeman"],"pdf_url":"https://arxiv.org/pdf/2411.02393v1.pdf","comment":"Code at: https://github.com/ShivamDuggal4/adaptive-length-tokenizer"},{"id":"http://arxiv.org/abs/2407.16677v2","updated":"2024-11-04T18:54:23Z","published":"2024-07-23T17:44:54Z","title":"From Imitation to Refinement -- Residual RL for Precise Assembly","summary":"  Recent advances in behavior cloning (BC), like action-chunking and diffusion,\nhave led to impressive progress. Still, imitation alone remains insufficient\nfor tasks requiring reliable and precise movements, such as aligning and\ninserting objects. Our key insight is that chunked BC policies function as\ntrajectory planners, enabling long-horizon tasks. Conversely, as they execute\naction chunks open-loop, they lack the fine-grained reactivity necessary for\nreliable execution. Further, we find that the performance of BC policies\nsaturates despite increasing data. Reinforcement learning (RL) is a natural way\nto overcome this, but it is not straightforward to apply directly to\naction-chunked models like diffusion policies. We present a simple yet\neffective method, ResiP (Residual for Precise Manipulation), that sidesteps\nthese challenges by augmenting a frozen, chunked BC model with a fully\nclosed-loop residual policy trained with RL. The residual policy is trained via\non-policy RL, addressing distribution shifts and introducing reactivity without\naltering the BC trajectory planner. Evaluation on high-precision manipulation\ntasks demonstrates strong performance of ResiP over BC methods and direct RL\nfine-tuning. Videos, code, and data are available at\n\\url{https://residual-assembly.github.io}.\n","authors":["Lars Ankile","Anthony Simeonov","Idan Shenfeld","Marcel Torne","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.16677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05484v2","updated":"2024-11-04T18:51:17Z","published":"2024-07-07T20:02:52Z","title":"Learning to Price Homogeneous Data","summary":"  We study a data pricing problem, where a seller has access to $N$ homogeneous\ndata points (e.g. drawn i.i.d. from some distribution). There are $m$ types of\nbuyers in the market, where buyers of the same type $i$ have the same valuation\ncurve $v_i:[N]\\rightarrow [0,1]$, where $v_i(n)$ is the value for having $n$\ndata points. A priori, the seller is unaware of the distribution of buyers, but\ncan repeat the market for $T$ rounds so as to learn the revenue-optimal pricing\ncurve $p:[N] \\rightarrow [0, 1]$. To solve this online learning problem, we\nfirst develop novel discretization schemes to approximate any pricing curve.\nWhen compared to prior work, the size of our discretization schemes scales\ngracefully with the approximation parameter, which translates to better regret\nin online learning. Under assumptions like smoothness and diminishing returns\nwhich are satisfied by data, the discretization size can be reduced further. We\nthen turn to the online learning problem, both in the stochastic and\nadversarial settings. On each round, the seller chooses an anonymous pricing\ncurve $p_t$. A new buyer appears and may choose to purchase some amount of\ndata. She then reveals her type only if she makes a purchase. Our online\nalgorithms build on classical algorithms such as UCB and FTPL, but require\nnovel ideas to account for the asymmetric nature of this feedback and to deal\nwith the vastness of the space of pricing curves. Using the improved\ndiscretization schemes previously developed, we are able to achieve\n$\\tilde{O}(m\\sqrt{T})$ regret in the stochastic setting and\n$\\tilde{O}(m^{3/2}\\sqrt{T})$ regret in the adversarial setting.\n","authors":["Keran Chen","Joon Suk Huh","Kirthevasan Kandasamy"],"pdf_url":"https://arxiv.org/pdf/2407.05484v2.pdf","comment":"The Thirty-Eighth Annual Conference on Neural Information Processing\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.02383v1","updated":"2024-11-04T18:50:39Z","published":"2024-11-04T18:50:39Z","title":"Linear Causal Bandits: Unknown Graph and Soft Interventions","summary":"  Designing causal bandit algorithms depends on two central categories of\nassumptions: (i) the extent of information about the underlying causal graphs\nand (ii) the extent of information about interventional statistical models.\nThere have been extensive recent advances in dispensing with assumptions on\neither category. These include assuming known graphs but unknown interventional\ndistributions, and the converse setting of assuming unknown graphs but access\nto restrictive hard/$\\operatorname{do}$ interventions, which removes the\nstochasticity and ancestral dependencies. Nevertheless, the problem in its\ngeneral form, i.e., unknown graph and unknown stochastic intervention models,\nremains open. This paper addresses this problem and establishes that in a graph\nwith $N$ nodes, maximum in-degree $d$ and maximum causal path length $L$, after\n$T$ interaction rounds the regret upper bound scales as\n$\\tilde{\\mathcal{O}}((cd)^{L-\\frac{1}{2}}\\sqrt{T} + d + RN)$ where $c>1$ is a\nconstant and $R$ is a measure of intervention power. A universal minimax lower\nbound is also established, which scales as $\\Omega(d^{L-\\frac{3}{2}}\\sqrt{T})$.\nImportantly, the graph size $N$ has a diminishing effect on the regret as $T$\ngrows. These bounds have matching behavior in $T$, exponential dependence on\n$L$, and polynomial dependence on $d$ (with the gap $d\\ $). On the algorithmic\naspect, the paper presents a novel way of designing a computationally efficient\nCB algorithm, addressing a challenge that the existing CB algorithms using soft\ninterventions face.\n","authors":["Zirui Yan","Ali Tajer"],"pdf_url":"https://arxiv.org/pdf/2411.02383v1.pdf","comment":"42 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.23262v2","updated":"2024-11-04T18:44:20Z","published":"2024-10-30T17:46:31Z","title":"EMMA: End-to-End Multimodal Model for Autonomous Driving","summary":"  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.\n","authors":["Jyh-Jing Hwang","Runsheng Xu","Hubert Lin","Wei-Chih Hung","Jingwei Ji","Kristy Choi","Di Huang","Tong He","Paul Covington","Benjamin Sapp","Yin Zhou","James Guo","Dragomir Anguelov","Mingxing Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23262v2.pdf","comment":"Blog post: https://waymo.com/blog/2024/10/introducing-emma/"},{"id":"http://arxiv.org/abs/2410.23773v2","updated":"2024-11-04T18:42:07Z","published":"2024-10-31T09:42:03Z","title":"Towards Generative Ray Path Sampling for Faster Point-to-Point Ray\n  Tracing","summary":"  Radio propagation modeling is essential in telecommunication research, as\nradio channels result from complex interactions with environmental objects.\nRecently, Machine Learning has been attracting attention as a potential\nalternative to computationally demanding tools, like Ray Tracing, which can\nmodel these interactions in detail. However, existing Machine Learning\napproaches often attempt to learn directly specific channel characteristics,\nsuch as the coverage map, making them highly specific to the frequency and\nmaterial properties and unable to fully capture the underlying propagation\nmechanisms. Hence, Ray Tracing, particularly the Point-to-Point variant,\nremains popular to accurately identify all possible paths between transmitter\nand receiver nodes. Still, path identification is computationally intensive\nbecause the number of paths to be tested grows exponentially while only a small\nfraction is valid. In this paper, we propose a Machine Learning-aided Ray\nTracing approach to efficiently sample potential ray paths, significantly\nreducing the computational load while maintaining high accuracy. Our model\ndynamically learns to prioritize potentially valid paths among all possible\npaths and scales linearly with scene complexity. Unlike recent alternatives,\nour approach is invariant with translation, scaling, or rotation of the\ngeometry, and avoids dependency on specific environment characteristics.\n","authors":["Jérome Eertmans","Nicola Di Cicco","Claude Oestges","Laurent Jacques","Enrico M. Vittuci","Vittorio Degli-Esposti"],"pdf_url":"https://arxiv.org/pdf/2410.23773v2.pdf","comment":"6 pages, 6 figures, submitted to IEEE ICMLCN 2025"},{"id":"http://arxiv.org/abs/2404.13895v3","updated":"2024-11-04T18:41:21Z","published":"2024-04-22T06:05:35Z","title":"Optimal Design for Human Preference Elicitation","summary":"  Learning of preference models from human feedback has been central to recent\nadvances in artificial intelligence. Motivated by the cost of obtaining\nhigh-quality human annotations, we study efficient human preference elicitation\nfor learning preference models. The key idea in our work is to generalize\noptimal designs, a methodology for computing optimal information-gathering\npolicies, to questions with multiple answers, represented as lists of items.\nThe policy is a distribution over lists and we elicit preferences from the list\nproportionally to its probability. To show the generality of our ideas, we\nstudy both absolute and ranking feedback models on items in the list. We design\nefficient algorithms for both and analyze them. Finally, we demonstrate that\nour algorithms are practical by evaluating them on existing question-answering\nproblems.\n","authors":["Subhojyoti Mukherjee","Anusha Lalitha","Kousha Kalantari","Aniket Deshmukh","Ge Liu","Yifei Ma","Branislav Kveton"],"pdf_url":"https://arxiv.org/pdf/2404.13895v3.pdf","comment":"Advances in Neural Information Processing Systems 37"},{"id":"http://arxiv.org/abs/2411.02372v1","updated":"2024-11-04T18:40:46Z","published":"2024-11-04T18:40:46Z","title":"Learning General-Purpose Biomedical Volume Representations using\n  Randomized Synthesis","summary":"  Current volumetric biomedical foundation models struggle to generalize as\npublic 3D datasets are small and do not cover the broad diversity of medical\nprocedures, conditions, anatomical regions, and imaging protocols. We address\nthis by creating a representation learning method that instead anticipates\nstrong domain shifts at training time itself. We first propose a data engine\nthat synthesizes highly variable training samples that enable generalization to\nnew biomedical contexts. To then train a single 3D network for any voxel-level\ntask, we develop a contrastive learning method that pretrains the network to be\nstable against nuisance imaging variation simulated by the data engine, a key\ninductive bias for generalization. This network's features can be used as\nrobust representations of input images for downstream tasks and its weights\nprovide a strong, dataset-agnostic initialization for finetuning on new\ndatasets. As a result, we set new standards across both multimodality\nregistration and few-shot segmentation, a first for any 3D biomedical vision\nmodel, all without (pre-)training on any existing dataset of real images.\n","authors":["Neel Dey","Benjamin Billot","Hallee E. Wong","Clinton J. Wang","Mengwei Ren","P. Ellen Grant","Adrian V. Dalca","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2411.02372v1.pdf","comment":"Code and model weights available at\n  https://github.com/neel-dey/anatomix"},{"id":"http://arxiv.org/abs/2405.17081v2","updated":"2024-11-04T18:39:10Z","published":"2024-05-27T11:54:51Z","title":"Effective Layer Pruning Through Similarity Metric Perspective","summary":"  Deep neural networks have been the predominant paradigm in machine learning\nfor solving cognitive tasks. Such models, however, are restricted by a high\ncomputational overhead, limiting their applicability and hindering advancements\nin the field. Extensive research demonstrated that pruning structures from\nthese models is a straightforward approach to reducing network complexity. In\nthis direction, most efforts focus on removing weights or filters. Studies have\nalso been devoted to layer pruning as it promotes superior computational gains.\nHowever, layer pruning often hurts the network predictive ability (i.e.,\naccuracy) at high compression rates. This work introduces an effective\nlayer-pruning strategy that meets all underlying properties pursued by pruning\nmethods. Our method estimates the relative importance of a layer using the\nCentered Kernel Alignment (CKA) metric, employed to measure the similarity\nbetween the representations of the unpruned model and a candidate layer for\npruning. We confirm the effectiveness of our method on standard architectures\nand benchmarks, in which it outperforms existing layer-pruning strategies and\nother state-of-the-art pruning techniques. Particularly, we remove more than\n75% of computation while improving predictive ability. At higher compression\nregimes, our method exhibits negligible accuracy drop, while other methods\nnotably deteriorate model accuracy. Apart from these benefits, our pruned\nmodels exhibit robustness to adversarial and out-of-distribution samples.\n","authors":["Ian Pons","Bruno Yamamoto","Anna H. Reali Costa","Artur Jordao"],"pdf_url":"https://arxiv.org/pdf/2405.17081v2.pdf","comment":"Published at International Conference on Pattern Recognition (ICPR),\n  2024. Oral presentation"},{"id":"http://arxiv.org/abs/2406.12072v3","updated":"2024-11-04T18:38:35Z","published":"2024-06-17T20:16:12Z","title":"DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs","summary":"  Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world\nscenarios, where each node and edge are associated with text descriptions, and\nboth the graph structure and text descriptions evolve over time. Despite their\nbroad applicability, there is a notable scarcity of benchmark datasets tailored\nto DyTAGs, which hinders the potential advancement in many research fields. To\naddress this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),\na collection of large-scale, time-evolving graphs from diverse domains, with\nnodes and edges enriched by dynamically changing text attributes and\ncategories. To facilitate the use of DTGB, we design standardized evaluation\nprocedures based on four real-world use cases: future link prediction,\ndestination node retrieval, edge classification, and textual relation\ngeneration. These tasks require models to understand both dynamic graph\nstructures and natural language, highlighting the unique challenges posed by\nDyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,\nevaluating 7 popular dynamic graph learning algorithms and their variants of\nadapting to text attributes with LLM embeddings, along with 6 powerful large\nlanguage models (LLMs). Our results show the limitations of existing models in\nhandling DyTAGs. Our analysis also demonstrates the utility of DTGB in\ninvestigating the incorporation of structural and textual dynamics. The\nproposed DTGB fosters research on DyTAGs and their broad applications. It\noffers a comprehensive benchmark for evaluating and advancing models to handle\nthe interplay between dynamic graph structures and natural language. The\ndataset and source code are available at https://github.com/zjs123/DTGB.\n","authors":["Jiasheng Zhang","Jialin Chen","Menglin Yang","Aosong Feng","Shuang Liang","Jie Shao","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2406.12072v3.pdf","comment":"28 pages, 13 figures, camera-ready version for NeurIPS 2024 Datasets\n  and Benchmarks Track"},{"id":"http://arxiv.org/abs/2409.05284v2","updated":"2024-11-04T18:37:07Z","published":"2024-09-09T02:32:45Z","title":"Bypassing the Noisy Parity Barrier: Learning Higher-Order Markov Random\n  Fields from Dynamics","summary":"  We consider the problem of learning graphical models, also known as Markov\nrandom fields (MRFs) from temporally correlated samples. As in many traditional\nstatistical settings, fundamental results in the area all assume independent\nsamples from the distribution. However, these samples generally will not\ndirectly correspond to more realistic observations from nature, which instead\nevolve according to some stochastic process. From the computational lens, even\ngenerating a single sample from the true MRF distribution is intractable unless\n$\\mathsf{NP}=\\mathsf{RP}$, and moreover, any algorithm to learn from i.i.d.\nsamples requires prohibitive runtime due to hardness reductions to the parity\nwith noise problem. These computational barriers for sampling and learning from\nthe i.i.d. setting severely lessen the utility of these breakthrough results\nfor this important task; however, dropping this assumption typically only\nintroduces further algorithmic and statistical complexities.\n  In this work, we surprisingly demonstrate that the direct trajectory data\nfrom a natural evolution of the MRF overcomes the fundamental computational\nlower bounds to efficient learning. In particular, we show that given a\ntrajectory with $\\widetilde{O}_k(n)$ site updates of an order $k$ MRF from the\nGlauber dynamics, a well-studied, natural stochastic process on graphical\nmodels, there is an algorithm that recovers the graph and the parameters in\n$\\widetilde{O}_k(n^2)$ time. By contrast, all prior algorithms for learning\norder $k$ MRFs inherently suffer from $n^{\\Theta(k)}$ runtime even in sparse\ninstances due to the reductions to sparse parity with noise. Our results thus\nsurprisingly show that this more realistic, but intuitively less tractable,\nmodel for MRFs actually leads to efficiency far beyond what is known and\nbelieved to be true in the traditional i.i.d. case.\n","authors":["Jason Gaitonde","Ankur Moitra","Elchanan Mossel"],"pdf_url":"https://arxiv.org/pdf/2409.05284v2.pdf","comment":"44 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.02359v1","updated":"2024-11-04T18:26:08Z","published":"2024-11-04T18:26:08Z","title":"DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for\n  Efficient Robot Execution","summary":"  MLLMs have demonstrated remarkable comprehension and reasoning capabilities\nwith complex language and visual data. These advances have spurred the vision\nof establishing a generalist robotic MLLM proficient in understanding complex\nhuman instructions and accomplishing various embodied tasks. However,\ndeveloping MLLMs for real-world robots is challenging due to the typically\nlimited computation and memory capacities available on robotic platforms. In\ncontrast, the inference of MLLMs involves storing billions of parameters and\nperforming tremendous computation, imposing significant hardware demands. In\nour paper, we propose a Dynamic Early-Exit Framework for Robotic\nVision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically\nadjusts the size of the activated MLLM based on each situation at hand. The\napproach leverages a multi-exit architecture in MLLMs, which allows the model\nto terminate processing once a proper size of the model has been activated for\na specific situation, thus avoiding further redundant computation.\nAdditionally, we develop novel algorithms that establish early-termination\ncriteria for DeeR, conditioned on predefined demands such as average\ncomputational cost (i.e., power consumption), as well as peak computational\nconsumption (i.e., latency) and GPU memory usage. These enhancements ensure\nthat DeeR operates efficiently under varying resource constraints while\nmaintaining competitive performance. On the CALVIN robot manipulation\nbenchmark, DeeR demonstrates significant reductions in computational costs of\nLLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.\nCode and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.\n","authors":["Yang Yue","Yulin Wang","Bingyi Kang","Yizeng Han","Shenzhi Wang","Shiji Song","Jiashi Feng","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2411.02359v1.pdf","comment":"25 pages, 6 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02355v1","updated":"2024-11-04T18:21:59Z","published":"2024-11-04T18:21:59Z","title":"\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization","summary":"  Despite the popularity of large language model (LLM) quantization for\ninference acceleration, significant uncertainty remains regarding the\naccuracy-performance trade-offs associated with various quantization formats.\nWe present a comprehensive empirical study of quantized accuracy, evaluating\npopular quantization formats (FP8, INT8, INT4) across academic benchmarks and\nreal-world tasks, on the entire Llama-3.1 model family. Additionally, our study\nexamines the difference in text generated by quantized models versus their\nuncompressed counterparts. Beyond benchmarks, we also present a couple of\nquantization improvements which allowed us to obtain state-of-the-art accuracy\nrecovery results. Our investigation, encompassing over 500,000 individual\nevaluations, yields several key findings: (1) FP8 weight and activation\nquantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and\nactivation quantization (W8A8-INT), when properly tuned, incurs surprisingly\nlow 1-3% accuracy degradation, and (3) INT4 weight-only quantization\n(W4A16-INT) is competitive with 8-bit integer weight and activation\nquantization. To address the question of the \"best\" format for a given\ndeployment environment, we conduct inference performance analysis using the\npopular open-source vLLM framework on various GPU architectures. We find that\nW4A16 offers the best cost-efficiency for synchronous deployments, and for\nasynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel\nin asynchronous \"continuous batching\" deployment of mid- and large-size models\non high-end GPUs. Our results provide a set of practical guidelines for\ndeploying quantized LLMs across scales and performance requirements.\n","authors":["Eldar Kurtic","Alexandre Marques","Shubhra Pandit","Mark Kurtz","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2411.02355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02347v1","updated":"2024-11-04T18:17:44Z","published":"2024-11-04T18:17:44Z","title":"Physically Based Neural Bidirectional Reflectance Distribution Function","summary":"  We introduce the physically based neural bidirectional reflectance\ndistribution function (PBNBRDF), a novel, continuous representation for\nmaterial appearance based on neural fields. Our model accurately reconstructs\nreal-world materials while uniquely enforcing physical properties for realistic\nBRDFs, specifically Helmholtz reciprocity via reparametrization and energy\npassivity via efficient analytical integration. We conduct a systematic\nanalysis demonstrating the benefits of adhering to these physical laws on the\nvisual quality of reconstructed materials. Additionally, we enhance the color\naccuracy of neural BRDFs by introducing chromaticity enforcement supervising\nthe norms of RGB channels. Through both qualitative and quantitative\nexperiments on multiple databases of measured real-world BRDFs, we show that\nadhering to these physical constraints enables neural fields to more faithfully\nand stably represent the original data and achieve higher rendering quality.\n","authors":["Chenliang Zhou","Alejandro Sztrajman","Gilles Rainer","Fangcheng Zhong","Fazilet Gokbudak","Zhilin Guo","Weihao Xia","Rafal Mantiuk","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2411.02347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02344v1","updated":"2024-11-04T18:14:07Z","published":"2024-11-04T18:14:07Z","title":"Seq-VCR: Preventing Collapse in Intermediate Transformer Representations\n  for Enhanced Reasoning","summary":"  Decoder-only Transformers often struggle with complex reasoning tasks,\nparticularly arithmetic reasoning requiring multiple sequential operations. In\nthis work, we identify representation collapse in the model's intermediate\nlayers as a key factor limiting their reasoning capabilities. To address this,\nwe propose Sequential Variance-Covariance Regularization (Seq-VCR), which\nenhances the entropy of intermediate representations and prevents collapse.\nCombined with dummy pause tokens as substitutes for chain-of-thought (CoT)\ntokens, our method significantly improves performance in arithmetic reasoning\nproblems. In the challenging $5 \\times 5$ integer multiplication task, our\napproach achieves $99.5\\%$ exact match accuracy, outperforming models of the\nsame size (which yield $0\\%$ accuracy) and GPT-4 with five-shot CoT prompting\n($44\\%$). We also demonstrate superior results on arithmetic expression and\nlongest increasing subsequence (LIS) datasets. Our findings highlight the\nimportance of preventing intermediate layer representation collapse to enhance\nthe reasoning capabilities of Transformers and show that Seq-VCR offers an\neffective solution without requiring explicit CoT supervision.\n","authors":["Md Rifat Arefin","Gopeshh Subbaraj","Nicolas Gontier","Yann LeCun","Irina Rish","Ravid Shwartz-Ziv","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2411.02344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02343v1","updated":"2024-11-04T18:12:59Z","published":"2024-11-04T18:12:59Z","title":"Boulder2Vec: Modeling Climber Performances in Professional Bouldering\n  Competitions","summary":"  Using data from professional bouldering competitions from 2008 to 2022, we\ntrain a logistic regression to predict climber results and measure climber\nskill. However, this approach is limited, as a single numeric coefficient per\nclimber cannot adequately capture the intricacies of climbers' varying\nstrengths and weaknesses in different boulder problems. For example, some\nclimbers might prefer more static, technical routes while other climbers may\nspecialize in powerful, dynamic problems.\n  To this end, we apply Probabilistic Matrix Factorization (PMF), a framework\ncommonly used in recommender systems, to represent the unique characteristics\nof climbers and problems with latent, multi-dimensional vectors. In this\nframework, a climber's performance on a given problem is predicted by taking\nthe dot product of the corresponding climber vector and problem vectors. PMF\neffectively handles sparse datasets, such as our dataset where only a subset of\nclimbers attempt each particular problem, by extrapolating patterns from\nsimilar climbers.\n  We contrast the empirical performance of PMF to the logistic regression\napproach and investigate the multivariate representations produced by PMF to\ngain insights into climber characteristics. Our results show that the\nmultivariate PMF representations improve predictive performance of professional\nbouldering competitions by capturing both the overall strength of climbers and\ntheir specialized skill sets.\n","authors":["Ethan Baron","Victor Hau","Zeke Weng"],"pdf_url":"https://arxiv.org/pdf/2411.02343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02335v1","updated":"2024-11-04T17:59:04Z","published":"2024-11-04T17:59:04Z","title":"Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity","summary":"  Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable.\n","authors":["Yuqi Luo","Chenyang Song","Xu Han","Yingfa Chen","Chaojun Xiao","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.02335v1.pdf","comment":"23 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2411.02322v1","updated":"2024-11-04T17:47:15Z","published":"2024-11-04T17:47:15Z","title":"LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed\n  Acyclic Graph Generation","summary":"  Directed acyclic graphs (DAGs) serve as crucial data representations in\ndomains such as hardware synthesis and compiler/program optimization for\ncomputing systems. DAG generative models facilitate the creation of synthetic\nDAGs, which can be used for benchmarking computing systems while preserving\nintellectual property. However, generating realistic DAGs is challenging due to\ntheir inherent directional and logical dependencies. This paper introduces\nLayerDAG, an autoregressive diffusion model, to address these challenges.\nLayerDAG decouples the strong node dependencies into manageable units that can\nbe processed sequentially. By interpreting the partial order of nodes as a\nsequence of bipartite graphs, LayerDAG leverages autoregressive generation to\nmodel directional dependencies and employs diffusion models to capture logical\ndependencies within each bipartite graph. Comparative analyses demonstrate that\nLayerDAG outperforms existing DAG generative models in both expressiveness and\ngeneralization, particularly for generating large-scale DAGs with up to 400\nnodes-a critical scenario for system benchmarking. Extensive experiments on\nboth synthetic and real-world flow graphs from various computing platforms show\nthat LayerDAG generates valid DAGs with superior statistical properties and\nbenchmarking performance. The synthetic DAGs generated by LayerDAG enhance the\ntraining of ML-based surrogate models, resulting in improved accuracy in\npredicting performance metrics of real-world DAGs across diverse computing\nplatforms.\n","authors":["Mufei Li","Viraj Shitole","Eli Chien","Changhai Man","Zhaodong Wang","Srinivas Sridharan","Ying Zhang","Tushar Krishna","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2411.02322v1.pdf","comment":"Code available at https://github.com/Graph-COM/LayerDAG"},{"id":"http://arxiv.org/abs/2410.22472v2","updated":"2024-11-04T17:44:42Z","published":"2024-10-29T19:05:22Z","title":"Learning Identifiable Factorized Causal Representations of Cellular\n  Responses","summary":"  The study of cells and their responses to genetic or chemical perturbations\npromises to accelerate the discovery of therapeutic targets. However, designing\nadequate and insightful models for such data is difficult because the response\nof a cell to perturbations essentially depends on its biological context (e.g.,\ngenetic background or cell type). For example, while discovering therapeutic\ntargets, one may want to enrich for drugs that specifically target a certain\ncell type. This challenge emphasizes the need for methods that explicitly take\ninto account potential interactions between drugs and contexts. Towards this\ngoal, we propose a novel Factorized Causal Representation (FCR)\n  learning method that reveals causal structure in single-cell perturbation\ndata from several cell lines. Based on the framework of identifiable deep\ngenerative models, FCR learns multiple cellular representations that are\ndisentangled, comprised of covariate-specific ($\\mathbf{z}_x$),\ntreatment-specific ($\\mathbf{z}_{t}$), and interaction-specific\n($\\mathbf{z}_{tx}$) blocks. Based on recent advances in non-linear ICA theory,\nwe prove the component-wise identifiability of $\\mathbf{z}_{tx}$ and block-wise\nidentifiability of $\\mathbf{z}_t$ and $\\mathbf{z}_x$. Then, we present our\nimplementation of FCR, and empirically demonstrate that it outperforms\nstate-of-the-art baselines in various tasks across four single-cell datasets.\n","authors":["Haiyi Mao","Romain Lopez","Kai Liu","Jan-Christian Huetter","David Richmond","Panayiotis V. Benos","Lin Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.22472v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16317v2","updated":"2024-11-04T17:44:26Z","published":"2024-03-24T22:42:40Z","title":"Optimization on a Finer Scale: Bounded Local Subgradient Variation\n  Perspective","summary":"  We initiate the study of nonsmooth optimization problems under bounded local\nsubgradient variation, which postulates bounded difference between\n(sub)gradients in small local regions around points, in either average or\nmaximum sense. The resulting class of objective functions encapsulates the\nclasses of objective functions traditionally studied in optimization, which are\ndefined based on either Lipschitz continuity of the objective or\nH\\\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class\ncontains functions that are neither Lipschitz continuous nor have a H\\\"{o}lder\ncontinuous gradient. When restricted to the traditional classes of optimization\nproblems, the parameters defining the studied classes lead to more fine-grained\ncomplexity bounds, recovering traditional oracle complexity bounds in the worst\ncase but generally leading to lower oracle complexity for functions that are\nnot ``worst case.'' Some highlights of our results are that: (i) it is possible\nto obtain complexity results for both convex and nonconvex problems with the\n(local or global) Lipschitz constant being replaced by a constant of local\nsubgradient variation and (ii) mean width of the subdifferential set around the\noptima plays a role in the complexity of nonsmooth optimization, particularly\nin parallel settings. A consequence of (ii) is that for any error parameter\n$\\epsilon > 0$, parallel oracle complexity of nonsmooth Lipschitz convex\noptimization is lower than its sequential oracle complexity by a factor\n$\\tilde{\\Omega}\\big(\\frac{1}{\\epsilon}\\big)$ whenever the objective function is\npiecewise linear with polynomially many pieces in the input size. This is\nparticularly surprising as existing parallel complexity lower bounds are based\non such classes of functions. The seeming contradiction is resolved by\nconsidering the region in which the algorithm is allowed to query the\nobjective.\n","authors":["Jelena Diakonikolas","Cristóbal Guzmán"],"pdf_url":"https://arxiv.org/pdf/2403.16317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07815v3","updated":"2024-11-04T17:42:45Z","published":"2024-03-12T16:53:54Z","title":"Chronos: Learning the Language of Time Series","summary":"  We introduce Chronos, a simple yet effective framework for pretrained\nprobabilistic time series models. Chronos tokenizes time series values using\nscaling and quantization into a fixed vocabulary and trains existing\ntransformer-based language model architectures on these tokenized time series\nvia the cross-entropy loss. We pretrained Chronos models based on the T5 family\n(ranging from 20M to 710M parameters) on a large collection of publicly\navailable datasets, complemented by a synthetic dataset that we generated via\nGaussian processes to improve generalization. In a comprehensive benchmark\nconsisting of 42 datasets, and comprising both classical local models and deep\nlearning methods, we show that Chronos models: (a) significantly outperform\nother methods on datasets that were part of the training corpus; and (b) have\ncomparable and occasionally superior zero-shot performance on new datasets,\nrelative to methods that were trained specifically on them. Our results\ndemonstrate that Chronos models can leverage time series data from diverse\ndomains to improve zero-shot accuracy on unseen forecasting tasks, positioning\npretrained models as a viable tool to greatly simplify forecasting pipelines.\n","authors":["Abdul Fatir Ansari","Lorenzo Stella","Caner Turkmen","Xiyuan Zhang","Pedro Mercado","Huibin Shen","Oleksandr Shchur","Syama Sundar Rangapuram","Sebastian Pineda Arango","Shubham Kapoor","Jasper Zschiegner","Danielle C. Maddix","Hao Wang","Michael W. Mahoney","Kari Torkkola","Andrew Gordon Wilson","Michael Bohlke-Schneider","Yuyang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07815v3.pdf","comment":"Code and model checkpoints available at\n  https://github.com/amazon-science/chronos-forecasting"},{"id":"http://arxiv.org/abs/2411.02317v1","updated":"2024-11-04T17:41:25Z","published":"2024-11-04T17:41:25Z","title":"Defining and Evaluating Physical Safety for Large Language Models","summary":"  Large Language Models (LLMs) are increasingly used to control robotic systems\nsuch as drones, but their risks of causing physical threats and harm in\nreal-world applications remain unexplored. Our study addresses the critical gap\nin evaluating LLM physical safety by developing a comprehensive benchmark for\ndrone control. We classify the physical safety risks of drones into four\ncategories: (1) human-targeted threats, (2) object-targeted threats, (3)\ninfrastructure attacks, and (4) regulatory violations. Our evaluation of\nmainstream LLMs reveals an undesirable trade-off between utility and safety,\nwith models that excel in code generation often performing poorly in crucial\nsafety aspects. Furthermore, while incorporating advanced prompt engineering\ntechniques such as In-Context Learning and Chain-of-Thought can improve safety,\nthese methods still struggle to identify unintentional attacks. In addition,\nlarger models demonstrate better safety capabilities, particularly in refusing\ndangerous commands. Our findings and benchmark can facilitate the design and\nevaluation of physical safety for LLMs. The project page is available at\nhuggingface.co/spaces/TrustSafeAI/LLM-physical-safety.\n","authors":["Yung-Chen Tang","Pin-Yu Chen","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2411.02317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00949v2","updated":"2024-11-04T17:39:35Z","published":"2024-02-01T19:06:06Z","title":"Geometry of Polynomial Neural Networks","summary":"  We study the expressivity and learning process for polynomial neural networks\n(PNNs) with monomial activation functions. The weights of the network\nparametrize the neuromanifold. In this paper, we study certain neuromanifolds\nusing tools from algebraic geometry: we give explicit descriptions as\nsemialgebraic sets and characterize their Zariski closures, called\nneurovarieties. We study their dimension and associate an algebraic degree, the\nlearning degree, to the neurovariety. The dimension serves as a geometric\nmeasure for the expressivity of the network, the learning degree is a measure\nfor the complexity of training the network and provides upper bounds on the\nnumber of learnable functions. These theoretical results are accompanied with\nexperiments.\n","authors":["Kaie Kubjas","Jiayi Li","Maximilian Wiesmann"],"pdf_url":"https://arxiv.org/pdf/2402.00949v2.pdf","comment":"34 pages, 3 figures. Comments are welcome!"},{"id":"http://arxiv.org/abs/2409.04022v2","updated":"2024-11-04T17:39:20Z","published":"2024-09-06T04:26:57Z","title":"Heterogeneity-Aware Cooperative Federated Edge Learning with Adaptive\n  Computation and Communication Compression","summary":"  Motivated by the drawbacks of cloud-based federated learning (FL),\ncooperative federated edge learning (CFEL) has been proposed to improve\nefficiency for FL over mobile edge networks, where multiple edge servers\ncollaboratively coordinate the distributed model training across a large number\nof edge devices. However, CFEL faces critical challenges arising from dynamic\nand heterogeneous device properties, which slow down the convergence and\nincrease resource consumption. This paper proposes a heterogeneity-aware CFEL\nscheme called \\textit{Heterogeneity-Aware Cooperative Edge-based Federated\nAveraging} (HCEF) that aims to maximize the model accuracy while minimizing the\ntraining time and energy consumption via adaptive computation and communication\ncompression in CFEL. By theoretically analyzing how local update frequency and\ngradient compression affect the convergence error bound in CFEL, we develop an\nefficient online control algorithm for HCEF to dynamically determine local\nupdate frequencies and compression ratios for heterogeneous devices.\nExperimental results show that compared with prior schemes, the proposed HCEF\nscheme can maintain higher model accuracy while reducing training latency and\nimproving energy efficiency simultaneously.\n","authors":["Zhenxiao Zhang","Zhidong Gao","Yuanxiong Guo","Yanmin Gong"],"pdf_url":"https://arxiv.org/pdf/2409.04022v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.02313v1","updated":"2024-11-04T17:38:46Z","published":"2024-11-04T17:38:46Z","title":"Information plane and compression-gnostic feedback in quantum machine\n  learning","summary":"  The information plane (Tishby et al. arXiv:physics/0004057, Shwartz-Ziv et\nal. arXiv:1703.00810) has been proposed as an analytical tool for studying the\nlearning dynamics of neural networks. It provides quantitative insight on how\nthe model approaches the learned state by approximating a minimal sufficient\nstatistics. In this paper we extend this tool to the domain of quantum learning\nmodels. In a second step, we study how the insight on how much the model\ncompresses the input data (provided by the information plane) can be used to\nimprove a learning algorithm. Specifically, we consider two ways to do so: via\na multiplicative regularization of the loss function, or with a\ncompression-gnostic scheduler of the learning rate (for algorithms based on\ngradient descent). Both ways turn out to be equivalent in our implementation.\nFinally, we benchmark the proposed learning algorithms on several\nclassification and regression tasks using variational quantum circuits. The\nresults demonstrate an improvement in test accuracy and convergence speed for\nboth synthetic and real-world datasets. Additionally, with one example we\nanalyzed the impact of the proposed modifications on the performances of neural\nnetworks in a classification task.\n","authors":["Nathan Haboury","Mo Kordzanganeh","Alexey Melnikov","Pavel Sekatski"],"pdf_url":"https://arxiv.org/pdf/2411.02313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14679v2","updated":"2024-11-04T17:36:38Z","published":"2024-07-19T21:47:57Z","title":"Compact Language Models via Pruning and Knowledge Distillation","summary":"  Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.\n","authors":["Saurav Muralidharan","Sharath Turuvekere Sreenivas","Raviraj Joshi","Marcin Chochowski","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2407.14679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02308v1","updated":"2024-11-04T17:32:21Z","published":"2024-11-04T17:32:21Z","title":"Nash Equilibria via Stochastic Eigendecomposition","summary":"  This work proposes a novel set of techniques for approximating a Nash\nequilibrium in a finite, normal-form game. It achieves this by constructing a\nnew reformulation as solving a parameterized system of multivariate polynomials\nwith tunable complexity. In doing so, it forges an itinerant loop from game\ntheory to machine learning and back. We show a Nash equilibrium can be\napproximated with purely calls to stochastic, iterative variants of singular\nvalue decomposition and power iteration, with implications for biological\nplausibility. We provide pseudocode and experiments demonstrating solving for\nall equilibria of a general-sum game using only these readily available linear\nalgebra tools.\n","authors":["Ian Gemp"],"pdf_url":"https://arxiv.org/pdf/2411.02308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02306v1","updated":"2024-11-04T17:31:02Z","published":"2024-11-04T17:31:02Z","title":"Targeted Manipulation and Deception Emerge when Optimizing LLMs for User\n  Feedback","summary":"  As LLMs become more widely deployed, there is increasing interest in directly\noptimizing for feedback from end users (e.g. thumbs up) in addition to feedback\nfrom paid annotators. However, training to maximize human feedback creates a\nperverse incentive structure for the AI to resort to manipulative tactics to\nobtain positive feedback, and some users may be especially vulnerable to such\ntactics. We study this phenomenon by training LLMs with Reinforcement Learning\nwith simulated user feedback. We have three main findings: 1) Extreme forms of\n\"feedback gaming\" such as manipulation and deception can reliably emerge in\ndomains of practical LLM usage; 2) Concerningly, even if only <2% of users are\nvulnerable to manipulative strategies, LLMs learn to identify and surgically\ntarget them while behaving appropriately with other users, making such\nbehaviors harder to detect; 3 To mitigate this issue, it may seem promising to\nleverage continued safety training or LLM-as-judges during training to filter\nproblematic outputs. To our surprise, we found that while such approaches help\nin some settings, they backfire in others, leading to the emergence of subtler\nproblematic behaviors that would also fool the LLM judges. Our findings serve\nas a cautionary tale, highlighting the risks of using gameable feedback sources\n-- such as user feedback -- as a target for RL.\n","authors":["Marcus Williams","Micah Carroll","Adhyyan Narang","Constantin Weisser","Brendan Murphy","Anca Dragan"],"pdf_url":"https://arxiv.org/pdf/2411.02306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10376v2","updated":"2024-11-04T17:28:54Z","published":"2024-02-16T00:04:36Z","title":"Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)","summary":"  CLIP embeddings have demonstrated remarkable performance across a wide range\nof multimodal applications. However, these high-dimensional, dense vector\nrepresentations are not easily interpretable, limiting our understanding of the\nrich structure of CLIP and its use in downstream applications that require\ntransparency. In this work, we show that the semantic structure of CLIP's\nlatent space can be leveraged to provide interpretability, allowing for the\ndecomposition of representations into semantic concepts. We formulate this\nproblem as one of sparse recovery and propose a novel method, Sparse Linear\nConcept Embeddings, for transforming CLIP representations into sparse linear\ncombinations of human-interpretable concepts. Distinct from previous work,\nSpLiCE is task-agnostic and can be used, without training, to explain and even\nreplace traditional dense CLIP representations, maintaining high downstream\nperformance while significantly improving their interpretability. We also\ndemonstrate significant use cases of SpLiCE representations including detecting\nspurious correlations and model editing.\n","authors":["Usha Bhalla","Alex Oesterling","Suraj Srinivas","Flavio P. Calmon","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2402.10376v2.pdf","comment":"25 pages, 15 figures, NeurIPS 2024. Code is provided at\n  https://github.com/AI4LIFE-GROUP/SpLiCE"},{"id":"http://arxiv.org/abs/2410.23625v2","updated":"2024-11-04T17:27:53Z","published":"2024-10-31T04:24:03Z","title":"EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation\n  for Electromyography","summary":"  This paper introduces the first generalization and adaptation benchmark using\nmachine learning for evaluating out-of-distribution performance of\nelectromyography (EMG) classification algorithms. The ability of an EMG\nclassifier to handle inputs drawn from a different distribution than the\ntraining distribution is critical for real-world deployment as a control\ninterface. By predicting the user's intended gesture using EMG signals, we can\ncreate a wearable solution to control assistive technologies, such as\ncomputers, prosthetics, and mobile manipulator robots. This new\nout-of-distribution benchmark consists of two major tasks that have utility for\nbuilding robust and adaptable control interfaces: 1) intersubject\nclassification and 2) adaptation using train-test splits for time-series. This\nbenchmark spans nine datasets--the largest collection of EMG datasets in a\nbenchmark. Among these, a new dataset is introduced, featuring a novel,\neasy-to-wear high-density EMG wearable for data collection. The lack of\nopen-source benchmarks has made comparing accuracy results between papers\nchallenging for the EMG research community. This new benchmark provides\nresearchers with a valuable resource for analyzing practical measures of\nout-of-distribution performance for EMG datasets. Our code and data from our\nnew dataset can be found at emgbench.github.io.\n","authors":["Jehan Yang","Maxwell Soh","Vivianna Lieu","Douglas J Weber","Zackory Erickson"],"pdf_url":"https://arxiv.org/pdf/2410.23625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02299v1","updated":"2024-11-04T17:25:10Z","published":"2024-11-04T17:25:10Z","title":"Grouped Discrete Representation for Object-Centric Learning","summary":"  Object-Centric Learning (OCL) can discover objects in images or videos by\nsimply reconstructing the input. For better object discovery, representative\nOCL methods reconstruct the input as its Variational Autoencoder (VAE)\nintermediate representation, which suppresses pixel noises and promotes object\nseparability by discretizing continuous super-pixels with template features.\nHowever, treating features as units overlooks their composing attributes, thus\nimpeding model generalization; indexing features with scalar numbers loses\nattribute-level similarities and differences, thus hindering model convergence.\nWe propose \\textit{Grouped Discrete Representation} (GDR) for OCL. We decompose\nfeatures into combinatorial attributes via organized channel grouping, and\ncompose these attributes into discrete representation via tuple indexes.\nExperiments show that our GDR improves both Transformer- and Diffusion-based\nOCL methods consistently on various datasets. Visualizations show that our GDR\ncaptures better object separability.\n","authors":["Rongzhen Zhao","Vivienne Wang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2411.02299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02298v1","updated":"2024-11-04T17:23:52Z","published":"2024-11-04T17:23:52Z","title":"Sample-Efficient Private Learning of Mixtures of Gaussians","summary":"  We study the problem of learning mixtures of Gaussians with approximate\ndifferential privacy. We prove that roughly $kd^2 + k^{1.5} d^{1.75} + k^2 d$\nsamples suffice to learn a mixture of $k$ arbitrary $d$-dimensional Gaussians\nup to low total variation distance, with differential privacy. Our work\nimproves over the previous best result [AAL24b] (which required roughly $k^2\nd^4$ samples) and is provably optimal when $d$ is much larger than $k^2$.\nMoreover, we give the first optimal bound for privately learning mixtures of\n$k$ univariate (i.e., $1$-dimensional) Gaussians. Importantly, we show that the\nsample complexity for privately learning mixtures of univariate Gaussians is\nlinear in the number of components $k$, whereas the previous best sample\ncomplexity [AAL21] was quadratic in $k$. Our algorithms utilize various\ntechniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23],\nsample compression for distributions [ABDH+20], and methods for bounding\nvolumes of sumsets.\n","authors":["Hassan Ashtiani","Mahbod Majid","Shyam Narayanan"],"pdf_url":"https://arxiv.org/pdf/2411.02298v1.pdf","comment":"52 pages. To appear in Neural Information Processing Systems\n  (NeurIPS), 2024"},{"id":"http://arxiv.org/abs/2411.02292v1","updated":"2024-11-04T17:20:42Z","published":"2024-11-04T17:20:42Z","title":"ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed\n  Convergence","summary":"  Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can\nprocess data without the limitation of time intervals. They have advantages in\nlearning and understanding the evolution of complex real dynamics. Many\nprevious works have focused on NODEs in concise forms, while numerous physical\nsystems taking straightforward forms, in fact, belong to their more complex\nquasi-classes, thus appealing to a class of general NODEs with high scalability\nand flexibility to model those systems. This, however, may result in intricate\nnonlinear properties. In this paper, we introduce ControlSynth Neural ODEs\n(CSODEs). We show that despite their highly nonlinear nature, convergence can\nbe guaranteed via tractable linear inequalities. In the composition of CSODEs,\nwe introduce an extra control term for learning the potential simultaneous\ncapture of dynamics at different scales, which could be particularly useful for\npartial differential equation-formulated systems. Finally, we compare several\nrepresentative NNs with CSODEs on important physical dynamics under the\ninductive biases of CSODEs, and illustrate that CSODEs have better learning and\npredictive abilities in these settings.\n","authors":["Wenjie Mei","Dongzhe Zheng","Shihua Li"],"pdf_url":"https://arxiv.org/pdf/2411.02292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02286v1","updated":"2024-11-04T17:13:35Z","published":"2024-11-04T17:13:35Z","title":"Federated GNNs for EEG-Based Stroke Assessment","summary":"  Machine learning (ML) has the potential to become an essential tool in\nsupporting clinical decision-making processes, offering enhanced diagnostic\ncapabilities and personalized treatment plans. However, outsourcing medical\nrecords to train ML models using patient data raises legal, privacy, and\nsecurity concerns. Federated learning has emerged as a promising paradigm for\ncollaborative ML, meeting healthcare institutions' requirements for robust\nmodels without sharing sensitive data and compromising patient privacy. This\nstudy proposes a novel method that combines federated learning (FL) and Graph\nNeural Networks (GNNs) to predict stroke severity using electroencephalography\n(EEG) signals across multiple medical institutions. Our approach enables\nmultiple hospitals to jointly train a shared GNN model on their local EEG data\nwithout exchanging patient information. Specifically, we address a regression\nproblem by predicting the National Institutes of Health Stroke Scale (NIHSS), a\nkey indicator of stroke severity. The proposed model leverages a masked\nself-attention mechanism to capture salient brain connectivity patterns and\nemploys EdgeSHAP to provide post-hoc explanations of the neurological states\nafter a stroke. We evaluated our method on EEG recordings from four\ninstitutions, achieving a mean absolute error (MAE) of 3.23 in predicting\nNIHSS, close to the average error made by human experts (MAE $\\approx$ 3.0).\nThis demonstrates the method's effectiveness in providing accurate and\nexplainable predictions while maintaining data privacy.\n","authors":["Andrea Protani","Lorenzo Giusti","Albert Sund Aillet","Simona Sacco","Paolo Manganotti","Lucio Marinelli","Diogo Reis Santos","Pierpaolo Brutti","Pietro Caliandro","Luigi Serio"],"pdf_url":"https://arxiv.org/pdf/2411.02286v1.pdf","comment":"13 pages, 5 figures, Proceedings of the II edition of the Workshop on\n  Unifying Representations in Neural Models (UniReps 2024)"},{"id":"http://arxiv.org/abs/2411.02281v1","updated":"2024-11-04T17:09:58Z","published":"2024-11-04T17:09:58Z","title":"Conformal-in-the-Loop for Learning with Imbalanced Noisy Data","summary":"  Class imbalance and label noise are pervasive in large-scale datasets, yet\nmuch of machine learning research assumes well-labeled, balanced data, which\nrarely reflects real world conditions. Existing approaches typically address\neither label noise or class imbalance in isolation, leading to suboptimal\nresults when both issues coexist. In this work, we propose\nConformal-in-the-Loop (CitL), a novel training framework that addresses both\nchallenges with a conformal prediction-based approach. CitL evaluates sample\nuncertainty to adjust weights and prune unreliable examples, enhancing model\nresilience and accuracy with minimal computational cost. Our extensive\nexperiments include a detailed analysis showing how CitL effectively emphasizes\nimpactful data in noisy, imbalanced datasets. Our results show that CitL\nconsistently boosts model performance, achieving up to a 6.1% increase in\nclassification accuracy and a 5.0 mIoU improvement in segmentation. Our code is\npublicly available: CitL.\n","authors":["John Brandon Graham-Knight","Jamil Fayyad","Nourhan Bayasi","Patricia Lasserre","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2411.02281v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2411.02280v1","updated":"2024-11-04T17:09:10Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2411.02279v1","updated":"2024-11-04T17:08:59Z","published":"2024-11-04T17:08:59Z","title":"ELU-GCN: Effectively Label-Utilizing Graph Convolutional Network","summary":"  The message-passing mechanism of graph convolutional networks (i.e., GCNs)\nenables label information to be propagated to a broader range of neighbors,\nthereby increasing the utilization of labels. However, the label information is\nnot always effectively utilized in the traditional GCN framework. To address\nthis issue, we propose a new two-step framework called ELU-GCN. In the first\nstage, ELU-GCN conducts graph learning to learn a new graph structure (\\ie\nELU-graph), which enables GCNs to effectively utilize label information. In the\nsecond stage, we design a new graph contrastive learning on the GCN framework\nfor representation learning by exploring the consistency and mutually exclusive\ninformation between the learned ELU graph and the original graph. Moreover, we\ntheoretically demonstrate that the proposed method can ensure the\ngeneralization ability of GCNs. Extensive experiments validate the superiority\nof the proposed method.\n","authors":["Jincheng Huang","Yujie Mo","Xiaoshuang Shi","Lei Feng","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.02279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02275v1","updated":"2024-11-04T17:05:37Z","published":"2024-11-04T17:05:37Z","title":"Breaking the Reclustering Barrier in Centroid-based Deep Clustering","summary":"  This work investigates an important phenomenon in centroid-based deep\nclustering (DC) algorithms: Performance quickly saturates after a period of\nrapid early gains. Practitioners commonly address early saturation with\nperiodic reclustering, which we demonstrate to be insufficient to address\nperformance plateaus. We call this phenomenon the \"reclustering barrier\" and\nempirically show when the reclustering barrier occurs, what its underlying\nmechanisms are, and how it is possible to Break the Reclustering Barrier with\nour algorithm BRB. BRB avoids early over-commitment to initial clusterings and\nenables continuous adaptation to reinitialized clustering targets while\nremaining conceptually simple. Applying our algorithm to widely-used\ncentroid-based DC algorithms, we show that (1) BRB consistently improves\nperformance across a wide range of clustering benchmarks, (2) BRB enables\ntraining from scratch, and (3) BRB performs competitively against\nstate-of-the-art DC algorithms when combined with a contrastive loss. We\nrelease our code and pre-trained models at\nhttps://github.com/Probabilistic-and-Interactive-ML/breaking-the-reclustering-barrier .\n","authors":["Lukas Miklautz","Timo Klein","Kevin Sidak","Collin Leiber","Thomas Lang","Andrii Shkabrii","Sebastian Tschiatschek","Claudia Plant"],"pdf_url":"https://arxiv.org/pdf/2411.02275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02272v1","updated":"2024-11-04T17:03:55Z","published":"2024-11-04T17:03:55Z","title":"Combining Induction and Transduction for Abstract Reasoning","summary":"  When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture.\n","authors":["Wen-Ding Li","Keya Hu","Carter Larsen","Yuqing Wu","Simon Alford","Caleb Woo","Spencer M. Dunn","Hao Tang","Michelangelo Naim","Dat Nguyen","Wei-Long Zheng","Zenna Tavares","Yewen Pu","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2411.02272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02271v1","updated":"2024-11-04T17:03:52Z","published":"2024-11-04T17:03:52Z","title":"On the Utilization of Unique Node Identifiers in Graph Neural Networks","summary":"  Graph neural networks have inherent representational limitations due to their\nmessage-passing structure. Recent work has suggested that these limitations can\nbe overcome by using unique node identifiers (UIDs). Here we argue that despite\nthe advantages of UIDs, one of their disadvantages is that they lose the\ndesirable property of permutation-equivariance. We thus propose to focus on UID\nmodels that are permutation-equivariant, and present theoretical arguments for\ntheir advantages. Motivated by this, we propose a method to regularize UID\nmodels towards permutation equivariance, via a contrastive loss. We empirically\ndemonstrate that our approach improves generalization and extrapolation\nabilities while providing faster training convergence. On the recent BREC\nexpressiveness benchmark, our proposed method achieves state-of-the-art\nperformance compared to other random-based approaches.\n","authors":["Maya Bechler-Speicher","Moshe Eliasof","Carola-Bibiane Schönlieb","Ran Gilad-Bachrach","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2411.02271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04280v2","updated":"2024-11-04T17:02:45Z","published":"2024-06-06T17:26:40Z","title":"xMIL: Insightful Explanations for Multiple Instance Learning in\n  Histopathology","summary":"  Multiple instance learning (MIL) is an effective and widely used approach for\nweakly supervised machine learning. In histopathology, MIL models have achieved\nremarkable success in tasks like tumor detection, biomarker prediction, and\noutcome prognostication. However, MIL explanation methods are still lagging\nbehind, as they are limited to small bag sizes or disregard instance\ninteractions. We revisit MIL through the lens of explainable AI (XAI) and\nintroduce xMIL, a refined framework with more general assumptions. We\ndemonstrate how to obtain improved MIL explanations using layer-wise relevance\npropagation (LRP) and conduct extensive evaluation experiments on three toy\nsettings and four real-world histopathology datasets. Our approach consistently\noutperforms previous explanation attempts with particularly improved\nfaithfulness scores on challenging biomarker prediction tasks. Finally, we\nshowcase how xMIL explanations enable pathologists to extract insights from MIL\nmodels, representing a significant advance for knowledge discovery and model\ndebugging in digital histopathology. Codes are available at:\nhttps://github.com/tubml-pathology/xMIL.\n","authors":["Julius Hense","Mina Jamshidi Idaji","Oliver Eberle","Thomas Schnake","Jonas Dippel","Laure Ciernik","Oliver Buchstab","Andreas Mock","Frederick Klauschen","Klaus-Robert Müller"],"pdf_url":"https://arxiv.org/pdf/2406.04280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02259v1","updated":"2024-11-04T16:49:39Z","published":"2024-11-04T16:49:39Z","title":"Counterfactual Explanations via Riemannian Latent Space Traversal","summary":"  The adoption of increasingly complex deep models has fueled an urgent need\nfor insight into how these models make predictions. Counterfactual explanations\nform a powerful tool for providing actionable explanations to practitioners.\nPreviously, counterfactual explanation methods have been designed by traversing\nthe latent space of generative models. Yet, these latent spaces are usually\ngreatly simplified, with most of the data distribution complexity contained in\nthe decoder rather than the latent embedding. Thus, traversing the latent space\nnaively without taking the nonlinear decoder into account can lead to unnatural\ncounterfactual trajectories. We introduce counterfactual explanations obtained\nusing a Riemannian metric pulled back via the decoder and the classifier under\nscrutiny. This metric encodes information about the complex geometric structure\nof the data and the learned representation, enabling us to obtain robust\ncounterfactual trajectories with high fidelity, as demonstrated by our\nexperiments in real-world tabular datasets.\n","authors":["Paraskevas Pegios","Aasa Feragen","Andreas Abildtrup Hansen","Georgios Arvanitidis"],"pdf_url":"https://arxiv.org/pdf/2411.02259v1.pdf","comment":"Accepted at NeurIPS 2024 Workshop NeurReps"},{"id":"http://arxiv.org/abs/2411.02253v1","updated":"2024-11-04T16:43:16Z","published":"2024-11-04T16:43:16Z","title":"Towards safe Bayesian optimization with Wiener kernel regression","summary":"  Bayesian Optimization (BO) is a data-driven strategy for\nminimizing/maximizing black-box functions based on probabilistic surrogate\nmodels. In the presence of safety constraints, the performance of BO crucially\nrelies on tight probabilistic error bounds related to the uncertainty\nsurrounding the surrogate model. For the case of Gaussian Process surrogates\nand Gaussian measurement noise, we present a novel error bound based on the\nrecently proposed Wiener kernel regression. We prove that under rather mild\nassumptions, the proposed error bound is tighter than bounds previously\ndocumented in the literature which leads to enlarged safety regions. We draw\nupon a numerical example to demonstrate the efficacy of the proposed error\nbound in safe BO.\n","authors":["Oleksii Molodchyk","Johannes Teutsch","Timm Faulwasser"],"pdf_url":"https://arxiv.org/pdf/2411.02253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20081v2","updated":"2024-11-04T16:29:43Z","published":"2024-10-26T05:18:48Z","title":"emg2qwerty: A Large Dataset with Baselines for Touch Typing using\n  Surface Electromyography","summary":"  Surface electromyography (sEMG) non-invasively measures signals generated by\nmuscle activity with sufficient sensitivity to detect individual spinal neurons\nand richness to identify dozens of gestures and their nuances. Wearable\nwrist-based sEMG sensors have the potential to offer low friction, subtle,\ninformation rich, always available human-computer inputs. To this end, we\nintroduce emg2qwerty, a large-scale dataset of non-invasive electromyographic\nsignals recorded at the wrists while touch typing on a QWERTY keyboard,\ntogether with ground-truth annotations and reproducible baselines. With 1,135\nsessions spanning 108 users and 346 hours of recording, this is the largest\nsuch public dataset to date. These data demonstrate non-trivial, but well\ndefined hierarchical relationships both in terms of the generative process,\nfrom neurons to muscles and muscle combinations, as well as in terms of domain\nshift across users and user sessions. Applying standard modeling techniques\nfrom the closely related field of Automatic Speech Recognition (ASR), we show\nstrong baseline performance on predicting key-presses using sEMG signals alone.\nWe believe the richness of this task and dataset will facilitate progress in\nseveral problems of interest to both the machine learning and neuroscientific\ncommunities. Dataset and code can be accessed at\nhttps://github.com/facebookresearch/emg2qwerty.\n","authors":["Viswanath Sivakumar","Jeffrey Seely","Alan Du","Sean R Bittner","Adam Berenzweig","Anuoluwapo Bolarinwa","Alexandre Gramfort","Michael I Mandel"],"pdf_url":"https://arxiv.org/pdf/2410.20081v2.pdf","comment":"Submitted to NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.23101v2","updated":"2024-11-04T16:26:34Z","published":"2024-10-30T15:12:36Z","title":"Guided Game Level Repair via Explainable AI","summary":"  Procedurally generated levels created by machine learning models can be\nunsolvable without further editing. Various methods have been developed to\nautomatically repair these levels by enforcing hard constraints during the\npost-processing step. However, as levels increase in size, these\nconstraint-based repairs become increasingly slow. This paper proposes using\nexplainability methods to identify specific regions of a level that contribute\nto its unsolvability. By assigning higher weights to these regions,\nconstraint-based solvers can prioritize these problematic areas, enabling more\nefficient repairs. Our results, tested across three games, demonstrate that\nthis approach can help to repair procedurally generated levels faster.\n","authors":["Mahsa Bazzaz","Seth Cooper"],"pdf_url":"https://arxiv.org/pdf/2410.23101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02225v1","updated":"2024-11-04T16:19:09Z","published":"2024-11-04T16:19:09Z","title":"Variable Selection in Convex Piecewise Linear Regression","summary":"  This paper presents Sparse Gradient Descent as a solution for variable\nselection in convex piecewise linear regression where the model is given as\n$\\mathrm{max}\\langle a_j^\\star, x \\rangle + b_j^\\star$ for $j = 1,\\dots,k$\nwhere $x \\in \\mathbb R^d$ is the covariate vector. Here,\n$\\{a_j^\\star\\}_{j=1}^k$ and $\\{b_j^\\star\\}_{j=1}^k$ denote the ground-truth\nweight vectors and intercepts. A non-asymptotic local convergence analysis is\nprovided for Sp-GD under sub-Gaussian noise when the covariate distribution\nsatisfies sub-Gaussianity and anti-concentration property. When the model order\nand parameters are fixed, Sp-GD provides an $\\epsilon$-accurate estimate given\n$\\mathcal{O}(\\max(\\epsilon^{-2}\\sigma_z^2,1)s\\log(d/s))$ observations where\n$\\sigma_z^2$ denotes the noise variance. This also implies the exact parameter\nrecovery by Sp-GD from $\\mathcal{O}(s\\log(d/s))$ noise-free observations. Since\noptimizing the squared loss for sparse max-affine is non-convex, an\ninitialization scheme is proposed to provide a suitable initial estimate within\nthe basin of attraction for Sp-GD, i.e. sufficiently accurate to invoke the\nconvergence guarantees. The initialization scheme uses sparse principal\ncomponent analysis to estimate the subspace spanned by $\\{ a_j^\\star\\}_{j=1}^k$\nthen applies an $r$-covering search to estimate the model parameters. A\nnon-asymptotic analysis is presented for this initialization scheme when the\ncovariates and noise samples follow Gaussian distributions. When the model\norder and parameters are fixed, this initialization scheme provides an\n$\\epsilon$-accurate estimate given\n$\\mathcal{O}(\\epsilon^{-2}\\max(\\sigma_z^4,\\sigma_z^2,1)s^2\\log^4(d))$\nobservations. Numerical Monte Carlo results corroborate theoretical findings\nfor Sp-GD and the initialization scheme.\n","authors":["Haitham Kanj","Seonho Kim","Kiryung Lee"],"pdf_url":"https://arxiv.org/pdf/2411.02225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02224v1","updated":"2024-11-04T16:17:57Z","published":"2024-11-04T16:17:57Z","title":"Predicting the Temperature-Dependent CMC of Surfactant Mixtures with\n  Graph Neural Networks","summary":"  Surfactants are key ingredients in foaming and cleansing products across\nvarious industries such as personal and home care, industrial cleaning, and\nmore, with the critical micelle concentration (CMC) being of major interest.\nPredictive models for CMC of pure surfactants have been developed based on\nrecent ML methods, however, in practice surfactant mixtures are typically used\ndue to to performance, environmental, and cost reasons. This requires\naccounting for synergistic/antagonistic interactions between surfactants;\nhowever, predictive ML models for a wide spectrum of mixtures are missing so\nfar. Herein, we develop a graph neural network (GNN) framework for surfactant\nmixtures to predict the temperature-dependent CMC. We collect data for 108\nsurfactant binary mixtures, to which we add data for pure species from our\nprevious work [Brozos et al. (2024), J. Chem. Theory Comput.]. We then develop\nand train GNNs and evaluate their accuracy across different prediction test\nscenarios for binary mixtures relevant to practical applications. The final GNN\nmodels demonstrate very high predictive performance when interpolating between\ndifferent mixture compositions and for new binary mixtures with known species.\nExtrapolation to binary surfactant mixtures where either one or both surfactant\nspecies are not seen before, yields accurate results for the majority of\nsurfactant systems. We further find superior accuracy of the GNN over a\nsemi-empirical model based on activity coefficients, which has been widely used\nto date. We then explore if GNN models trained solely on binary mixture and\npure species data can also accurately predict the CMCs of ternary mixtures.\nFinally, we experimentally measure the CMC of 4 commercial surfactants that\ncontain up to four species and industrial relevant mixtures and find a very\ngood agreement between measured and predicted CMC values.\n","authors":["Christoforos Brozos","Jan G. Rittig","Sandip Bhattacharya","Elie Akanny","Christina Kohlmann","Alexander Mitsos"],"pdf_url":"https://arxiv.org/pdf/2411.02224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02223v1","updated":"2024-11-04T16:15:28Z","published":"2024-11-04T16:15:28Z","title":"Positive Experience Reflection for Agents in Interactive Text\n  Environments","summary":"  Intelligent agents designed for interactive environments face significant\nchallenges in text-based games, a domain that demands complex reasoning and\nadaptability. While agents based on large language models (LLMs) using\nself-reflection have shown promise, they struggle when initially successful and\nexhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,\na novel approach that addresses these limitations in existing reflection\nmethods by incorporating positive experiences and managed memory to enrich the\ncontext available to the agent at decision time. Our comprehensive analysis\nspans both closed- and open-source LLMs and demonstrates the effectiveness of\nSweet&Sour in improving agent performance, particularly in scenarios where\nprevious approaches fall short.\n","authors":["Philip Lippmann","Matthijs T. J. Spaan","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02223v1.pdf","comment":"To appear at NeurIPS 2024 Language Gamification workshop"},{"id":"http://arxiv.org/abs/2411.02221v1","updated":"2024-11-04T16:14:45Z","published":"2024-11-04T16:14:45Z","title":"Targeted Learning for Variable Importance","summary":"  Variable importance is one of the most widely used measures for interpreting\nmachine learning with significant interest from both statistics and machine\nlearning communities. Recently, increasing attention has been directed toward\nuncertainty quantification in these metrics. Current approaches largely rely on\none-step procedures, which, while asymptotically efficient, can present higher\nsensitivity and instability in finite sample settings. To address these\nlimitations, we propose a novel method by employing the targeted learning (TL)\nframework, designed to enhance robustness in inference for variable importance\nmetrics. Our approach is particularly suited for conditional permutation\nvariable importance. We show that it (i) retains the asymptotic efficiency of\ntraditional methods, (ii) maintains comparable computational complexity, and\n(iii) delivers improved accuracy, especially in finite sample contexts. We\nfurther support these findings with numerical experiments that illustrate the\npractical advantages of our method and validate the theoretical results.\n","authors":["Xiaohan Wang","Yunzhe Zhou","Giles Hooker"],"pdf_url":"https://arxiv.org/pdf/2411.02221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02220v1","updated":"2024-11-04T16:14:35Z","published":"2024-11-04T16:14:35Z","title":"SIRA: Scalable Inter-frame Relation and Association for Radar Perception","summary":"  Conventional radar feature extraction faces limitations due to low spatial\nresolution, noise, multipath reflection, the presence of ghost targets, and\nmotion blur. Such limitations can be exacerbated by nonlinear object motion,\nparticularly from an ego-centric viewpoint. It becomes evident that to address\nthese challenges, the key lies in exploiting temporal feature relation over an\nextended horizon and enforcing spatial motion consistency for effective\nassociation. To this end, this paper proposes SIRA (Scalable Inter-frame\nRelation and Association) with two designs. First, inspired by Swin\nTransformer, we introduce extended temporal relation, generalizing the existing\ntemporal relation layer from two consecutive frames to multiple inter-frames\nwith temporally regrouped window attention for scalability. Second, we propose\nmotion consistency track with the concept of a pseudo-tracklet generated from\nobservational data for better trajectory prediction and subsequent object\nassociation. Our approach achieves 58.11 mAP@0.5 for oriented object detection\nand 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing\nprevious state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA,\nrespectively.\n","authors":["Ryoma Yataka","Pu Perry Wang","Petros Boufounos","Ryuhei Takahashi"],"pdf_url":"https://arxiv.org/pdf/2411.02220v1.pdf","comment":"25 pages, Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2411.02217v1","updated":"2024-11-04T16:12:37Z","published":"2024-11-04T16:12:37Z","title":"Recursive Learning of Asymptotic Variational Objectives","summary":"  General state-space models (SSMs) are widely used in statistical machine\nlearning and are among the most classical generative models for sequential\ntime-series data. SSMs, comprising latent Markovian states, can be subjected to\nvariational inference (VI), but standard VI methods like the\nimportance-weighted autoencoder (IWAE) lack functionality for streaming data.\nTo enable online VI in SSMs when the observations are received in real time, we\npropose maximising an IWAE-type variational lower bound on the asymptotic\ncontrast function, rather than the standard IWAE ELBO, using stochastic\napproximation. Unlike the recursive maximum likelihood method, which directly\nmaximises the asymptotic contrast, our approach, called online sequential IWAE\n(OSIWAE), allows for online learning of both model parameters and a Markovian\nrecognition model for inferring latent states. By approximating filter state\nposteriors and their derivatives using sequential Monte Carlo (SMC) methods, we\ncreate a particle-based framework for online VI in SSMs. This approach is more\ntheoretically well-founded than recently proposed online variational SMC\nmethods. We provide rigorous theoretical results on the learning objective and\na numerical study demonstrating the method's efficiency in learning model\nparameters and particle proposal kernels.\n","authors":["Alessandro Mastrototaro","Mathias Müller","Jimmy Olsson"],"pdf_url":"https://arxiv.org/pdf/2411.02217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02550v2","updated":"2024-11-04T16:04:27Z","published":"2024-06-04T17:59:36Z","title":"Learning to grok: Emergence of in-context learning and skill composition\n  in modular arithmetic tasks","summary":"  Large language models can solve tasks that were not present in the training\nset. This capability is believed to be due to in-context learning and skill\ncomposition. In this work, we study the emergence of in-context learning and\nskill composition in a collection of modular arithmetic tasks. Specifically, we\nconsider a finite collection of linear modular functions $z = a \\, x + b \\, y\n\\;\\mathrm{mod}\\; p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use\nsome of these tasks for pre-training and the rest for out-of-distribution\ntesting. We empirically show that a GPT-style transformer exhibits a transition\nfrom in-distribution to out-of-distribution generalization as the number of\npre-training tasks increases. We find that the smallest model capable of\nout-of-distribution generalization requires two transformer blocks, while for\ndeeper models, the out-of-distribution generalization phase is\n\\emph{transient}, necessitating early stopping. Finally, we perform an\ninterpretability study of the pre-trained models, revealing highly structured\nrepresentations in both attention heads and MLPs; and discuss the learned\nalgorithms. Notably, we find an algorithmic shift in deeper models, as we go\nfrom few to many in-context examples.\n","authors":["Tianyu He","Darshil Doshi","Aritra Das","Andrey Gromov"],"pdf_url":"https://arxiv.org/pdf/2406.02550v2.pdf","comment":"Camera-ready version, NeurIPS 2024 (Oral)"},{"id":"http://arxiv.org/abs/2411.02207v1","updated":"2024-11-04T15:59:16Z","published":"2024-11-04T15:59:16Z","title":"Collective Model Intelligence Requires Compatible Specialization","summary":"  In this work, we explore the limitations of combining models by averaging\nintermediate features, referred to as model merging, and propose a new\ndirection for achieving collective model intelligence through what we call\ncompatible specialization. Current methods for model merging, such as parameter\nand feature averaging, struggle to effectively combine specialized models due\nto representational divergence during fine-tuning. As models specialize to\ntheir individual domains, their internal feature representations become\nincreasingly incompatible, leading to poor performance when attempting to merge\nthem for new tasks. We analyze this phenomenon using centered kernel alignment\n(CKA) and show that as models specialize, the similarity in their feature space\nstructure diminishes, hindering their capacity for collective use. To address\nthese challenges, we investigate routing-based merging strategies, which offer\nmore flexible methods for combining specialized models by dynamically routing\nacross different layers. This allows us to improve on existing methods by\ncombining features from multiple layers rather than relying on fixed,\nlayer-wise combinations. However, we find that these approaches still face\nlimitations when layers within models are representationally incompatible. Our\nfindings highlight the importance of designing new approaches for model merging\nthat operate on well-defined input and output spaces, similar to how humans\ncommunicate through language rather than intermediate neural activations.\n","authors":["Jyothish Pari","Samy Jelassi","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2411.02207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18489v2","updated":"2024-11-04T15:58:35Z","published":"2024-05-28T18:00:32Z","title":"Predicting Ground State Properties: Constant Sample Complexity and Deep\n  Learning Algorithms","summary":"  A fundamental problem in quantum many-body physics is that of finding ground\nstates of local Hamiltonians. A number of recent works gave provably efficient\nmachine learning (ML) algorithms for learning ground states. Specifically,\n[Huang et al. Science 2022], introduced an approach for learning properties of\nthe ground state of an $n$-qubit gapped local Hamiltonian $H$ from only\n$n^{\\mathcal{O}(1)}$ data points sampled from Hamiltonians in the same phase of\nmatter. This was subsequently improved by [Lewis et al. Nature Communications\n2024], to $\\mathcal{O}(\\log n)$ samples when the geometry of the $n$-qubit\nsystem is known. In this work, we introduce two approaches that achieve a\nconstant sample complexity, independent of system size $n$, for learning ground\nstate properties. Our first algorithm consists of a simple modification of the\nML model used by Lewis et al. and applies to a property of interest known\nbeforehand. Our second algorithm, which applies even if a description of the\nproperty is not known, is a deep neural network model. While empirical results\nshowing the performance of neural networks have been demonstrated, to our\nknowledge, this is the first rigorous sample complexity bound on a neural\nnetwork model for predicting ground state properties. We also perform numerical\nexperiments that confirm the improved scaling of our approach compared to\nearlier results.\n","authors":["Marc Wanner","Laura Lewis","Chiranjib Bhattacharyya","Devdatt Dubhashi","Alexandru Gheorghiu"],"pdf_url":"https://arxiv.org/pdf/2405.18489v2.pdf","comment":"11 pages, 7 figures + 40-page appendix"},{"id":"http://arxiv.org/abs/2411.02199v1","updated":"2024-11-04T15:54:32Z","published":"2024-11-04T15:54:32Z","title":"Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning","summary":"  Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.\n","authors":["Dake Bu","Wei Huang","Andi Han","Atsushi Nitanda","Taiji Suzuki","Qingfu Zhang","Hau-San Wong"],"pdf_url":"https://arxiv.org/pdf/2411.02199v1.pdf","comment":"Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.02198v1","updated":"2024-11-04T15:53:45Z","published":"2024-11-04T15:53:45Z","title":"Metric properties of partial and robust Gromov-Wasserstein distances","summary":"  The Gromov-Wasserstein (GW) distances define a family of metrics, based on\nideas from optimal transport, which enable comparisons between probability\nmeasures defined on distinct metric spaces. They are particularly useful in\nareas such as network analysis and geometry processing, as computation of a GW\ndistance involves solving for registration between the objects which minimizes\ngeometric distortion. Although GW distances have proven useful for various\napplications in the recent machine learning literature, it has been observed\nthat they are inherently sensitive to outlier noise and cannot accommodate\npartial matching. This has been addressed by various constructions building on\nthe GW framework; in this article, we focus specifically on a natural\nrelaxation of the GW optimization problem, introduced by Chapel et al., which\nis aimed at addressing exactly these shortcomings. Our goal is to understand\nthe theoretical properties of this relaxed optimization problem, from the\nviewpoint of metric geometry. While the relaxed problem fails to induce a\nmetric, we derive precise characterizations of how it fails the axioms of\nnon-degeneracy and triangle inequality. These observations lead us to define a\nnovel family of distances, whose construction is inspired by the Prokhorov and\nKy Fan distances, as well as by the recent work of Raghvendra et al.\\ on robust\nversions of classical Wasserstein distance. We show that our new distances\ndefine true metrics, that they induce the same topology as the GW distances,\nand that they enjoy additional robustness to perturbations. These results\nprovide a mathematically rigorous basis for using our robust partial GW\ndistances in applications where outliers and partial matching are concerns.\n","authors":["Jannatul Chhoa","Michael Ivanitskiy","Fushuai Jiang","Shiying Li","Daniel McBride","Tom Needham","Kaiying O'Hare"],"pdf_url":"https://arxiv.org/pdf/2411.02198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11590v2","updated":"2024-11-04T15:49:59Z","published":"2023-10-17T21:12:32Z","title":"Predicting Human Impressions of Robot Performance During Navigation\n  Tasks","summary":"  Human impressions of robot performance are often measured through surveys. As\na more scalable and cost-effective alternative, we investigate the possibility\nof predicting people's impressions of robot behavior using non-verbal\nbehavioral cues and machine learning techniques. To this end, we first\ncontribute the SEAN TOGETHER Dataset consisting of observations of an\ninteraction between a person and a mobile robot in a VR simulation, together\nwith impressions of robot performance provided by users on a 5-point scale.\nSecond, we contribute analyses of how well humans and supervised learning\ntechniques can predict perceived robot performance based on different\nobservation types (like facial expression features, and features that describe\nthe navigation behavior of the robot and pedestrians). Our results suggest that\nfacial expressions alone provide useful information about human impressions of\nrobot performance; but in the navigation scenarios that we considered,\nreasoning about spatial features in context is critical for the prediction\ntask. Also, supervised learning techniques showed promise because they\noutperformed humans' predictions of robot performance in most cases. Further,\nwhen predicting robot performance as a binary classification task on unseen\nusers' data, the F1 Score of machine learning models more than doubled in\ncomparison to predicting performance on a 5-point scale. This suggested that\nthe models can have good generalization capabilities, although they are better\nat telling the directionality of robot performance than predicting exact\nperformance ratings. Based on our findings in simulation, we conducted a\nreal-world demonstration in which a mobile robot uses a machine learning model\nto predict how a human that follows it perceives it. Finally, we discuss the\nimplications of our results for implementing such supervised learning models in\nreal-world navigation scenarios.\n","authors":["Qiping Zhang","Nathan Tsoi","Mofeed Nagib","Booyeon Choi","Jie Tan","Hao-Tien Lewis Chiang","Marynel Vázquez"],"pdf_url":"https://arxiv.org/pdf/2310.11590v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20915v2","updated":"2024-11-04T15:48:10Z","published":"2024-05-31T15:21:44Z","title":"Fast yet Safe: Early-Exiting with Risk Control","summary":"  Scaling machine learning models significantly improves their performance.\nHowever, such gains come at the cost of inference being slow and\nresource-intensive. Early-exit neural networks (EENNs) offer a promising\nsolution: they accelerate inference by allowing intermediate layers to exit and\nproduce a prediction early. Yet a fundamental issue with EENNs is how to\ndetermine when to exit without severely degrading performance. In other words,\nwhen is it 'safe' for an EENN to go 'fast'? To address this issue, we\ninvestigate how to adapt frameworks of risk control to EENNs. Risk control\noffers a distribution-free, post-hoc solution that tunes the EENN's exiting\nmechanism so that exits only occur when the output is of sufficient quality. We\nempirically validate our insights on a range of vision and language tasks,\ndemonstrating that risk control can produce substantial computational savings,\nall the while preserving user-specified performance goals.\n","authors":["Metod Jazbec","Alexander Timans","Tin Hadži Veljković","Kaspar Sakmann","Dan Zhang","Christian A. Naesseth","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2405.20915v2.pdf","comment":"27 pages, 13 figures, 4 tables (incl. appendix)"},{"id":"http://arxiv.org/abs/2411.02193v1","updated":"2024-11-04T15:46:20Z","published":"2024-11-04T15:46:20Z","title":"Improving Steering Vectors by Targeting Sparse Autoencoder Features","summary":"  To control the behavior of language models, steering methods attempt to\nensure that outputs of the model satisfy specific pre-defined properties.\nAdding steering vectors to the model is a promising method of model control\nthat is easier than finetuning, and may be more robust than prompting. However,\nit can be difficult to anticipate the effects of steering vectors produced by\nalmost all existing methods, such as CAA (Panickssery et al., 2024) or the\ndirect use of SAE latents (Templeton et al., 2024). In our work, we address\nthis issue by using SAEs to measure the effects of steering vectors, giving us\na method that can be used to understand the causal effect of any steering\nvector intervention. We use this method for measuring causal effects to develop\nan improved steering method, SAE-Targeted Steering (SAE-TS), which finds\nsteering vectors to target specific SAE features while minimizing unintended\nside effects. We show that overall, SAE-TS balances steering effects with\ncoherence better than CAA and SAE feature steering, when evaluated on a range\nof tasks.\n","authors":["Sviatoslav Chalnev","Matthew Siu","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2411.02193v1.pdf","comment":"8 maintext pages and 9 appendix pages"},{"id":"http://arxiv.org/abs/2411.02189v1","updated":"2024-11-04T15:43:57Z","published":"2024-11-04T15:43:57Z","title":"DiffSim2Real: Deploying Quadrupedal Locomotion Policies Purely Trained\n  in Differentiable Simulation","summary":"  Differentiable simulators provide analytic gradients, enabling more\nsample-efficient learning algorithms and paving the way for data intensive\nlearning tasks such as learning from images. In this work, we demonstrate that\nlocomotion policies trained with analytic gradients from a differentiable\nsimulator can be successfully transferred to the real world. Typically,\nsimulators that offer informative gradients lack the physical accuracy needed\nfor sim-to-real transfer, and vice-versa. A key factor in our success is a\nsmooth contact model that combines informative gradients with physical\naccuracy, ensuring effective transfer of learned behaviors. To the best of our\nknowledge, this is the first time a real quadrupedal robot is able to locomote\nafter training exclusively in a differentiable simulation.\n","authors":["Joshua Bagajo","Clemens Schwarke","Victor Klemm","Ignat Georgiev","Jean-Pierre Sleiman","Jesus Tordesillas","Animesh Garg","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2411.02189v1.pdf","comment":"Presented at the CoRL 2024 Workshop 'Differentiable Optimization\n  Everywhere'"},{"id":"http://arxiv.org/abs/2411.02184v1","updated":"2024-11-04T15:39:12Z","published":"2024-11-04T15:39:12Z","title":"Double Descent Meets Out-of-Distribution Detection: Theoretical Insights\n  and Empirical Analysis on the role of model complexity","summary":"  While overparameterization is known to benefit generalization, its impact on\nOut-Of-Distribution (OOD) detection is less understood. This paper investigates\nthe influence of model complexity in OOD detection. We propose an expected OOD\nrisk metric to evaluate classifiers confidence on both training and OOD\nsamples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD\nrisk of binary least-squares classifiers applied to Gaussian data. We show that\nthe OOD risk depicts an infinite peak, when the number of parameters is equal\nto the number of samples, which we associate with the double descent\nphenomenon. Our experimental study on different OOD detection methods across\nmultiple neural architectures extends our theoretical insights and highlights a\ndouble descent curve. Our observations suggest that overparameterization does\nnot necessarily lead to better OOD detection. Using the Neural Collapse\nframework, we provide insights to better understand this behavior. To\nfacilitate reproducibility, our code will be made publicly available upon\npublication.\n","authors":["Mouïn Ben Ammar","David Brellmann","Arturo Mendoza","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2411.02184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02183v1","updated":"2024-11-04T15:38:47Z","published":"2024-11-04T15:38:47Z","title":"Vehicles, Pedestrians, and E-bikes: a Three-party Game at\n  Right-turn-on-red Crossroads Revealing the Dual and Irrational Role of\n  E-bikes that Risks Traffic Safety","summary":"  The widespread use of e-bikes has facilitated short-distance travel yet led\nto confusion and safety problems in road traffic. This study focuses on the\ndual characteristics of e-bikes in traffic conflicts: they resemble pedestrians\nwhen interacting with motor vehicles and behave like motor vehicles when in\nconflict with pedestrians, which raises the right of way concerns when\npotential conflicts are at stake. Using the Quantal Response Equilibrium model,\nthis research analyzes the behavioral choice differences of three groups of\nroad users (vehicle-pedestrian, vehicle-e-bike, e-bike-pedestrian) at\nright-turn-on-red crossroads in right-turning lines and straight-going lines\nconflict scenarios. The results show that the behavior of e-bikes is more\nsimilar to that of motor vehicles than pedestrians overall, and their\ninteractions with either pedestrians or motor vehicles do not establish a\nreasonable order, increasing the likelihood of confusion and conflict. In\ncontrast, a mutual understanding has developed between motor vehicles and\npedestrians, where motor vehicles tend to yield, and pedestrians tend to cross.\nBy clarifying the game theoretical model and introducing the rationality\nparameter, this study precisely locates the role of e-bikes among road users,\nwhich provides a reliable theoretical basis for optimizing traffic regulations.\n","authors":["Gangcheng Zhang","Yeshuo Shu","Keyi Liu","Yuxuan Wang","Donghang Li","Liyan Xu"],"pdf_url":"https://arxiv.org/pdf/2411.02183v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.02177v1","updated":"2024-11-04T15:36:17Z","published":"2024-11-04T15:36:17Z","title":"Physics-informed neural networks viewpoint for solving the\n  Dyson-Schwinger equations of quantum electrodynamics","summary":"  We employ physics-informed neural networks (PINNs) to solve fundamental\nDyson-Schwinger integral equations in the theory of quantum electrodynamics\n(QED) in Euclidean space. Our approach uses neural networks to approximate the\nfermion wave function renormalization, dynamical mass function, and photon\npropagator. By integrating the Dyson-Schwinger equations into the loss\nfunction, the networks learn and predict solutions over a range of momenta and\nultraviolet cutoff values. This method can be extended to other quantum field\ntheories (QFTs), potentially paving the way for forefront applications of\nmachine learning within high-level theoretical physics.\n","authors":["Rodrigo Carmo Terin"],"pdf_url":"https://arxiv.org/pdf/2411.02177v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.02175v1","updated":"2024-11-04T15:34:30Z","published":"2024-11-04T15:34:30Z","title":"SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning\n  with Pre-Trained Models","summary":"  Continual learning aims to incrementally acquire new concepts in data streams\nwhile resisting forgetting previous knowledge. With the rise of powerful\npre-trained models (PTMs), there is a growing interest in training incremental\nlearning systems using these foundation models, rather than learning from\nscratch. Existing works often view PTMs as a strong initial point and directly\napply parameter-efficient tuning (PET) in the first session for adapting to\ndownstream tasks. In the following sessions, most methods freeze model\nparameters for tackling forgetting issues. However, applying PET directly to\ndownstream data cannot fully explore the inherent knowledge in PTMs.\nAdditionally, freezing the parameters in incremental sessions hinders models'\nplasticity to novel concepts not covered in the first session. To solve the\nabove issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE)\nframework. In particular, to inherit general knowledge from foundation models,\nwe include a transfer loss function by measuring the correlation between the\nPTM and the PET-applied model. After calibrating in the first session, the slow\nefficient tuning parameters can capture more informative features, improving\ngeneralization to incoming classes. Moreover, to further incorporate novel\nconcepts, we strike a balance between stability and plasticity by fixing slow\nefficient tuning parameters and continuously updating the fast ones.\nSpecifically, a cross-classification loss with feature alignment is proposed to\ncircumvent catastrophic forgetting. During inference, we introduce an\nentropy-based aggregation strategy to dynamically utilize the complementarity\nin the slow and fast learners. Extensive experiments on seven benchmark\ndatasets verify the effectiveness of our method by significantly surpassing the\nstate-of-the-art.\n","authors":["Linglan Zhao","Xuerui Zhang","Ke Yan","Shouhong Ding","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2411.02175v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02174v1","updated":"2024-11-04T15:34:28Z","published":"2024-11-04T15:34:28Z","title":"Behavioral Sequence Modeling with Ensemble Learning","summary":"  We investigate the use of sequence analysis for behavior modeling,\nemphasizing that sequential context often outweighs the value of aggregate\nfeatures in understanding human behavior. We discuss framing common problems in\nfields like healthcare, finance, and e-commerce as sequence modeling tasks, and\naddress challenges related to constructing coherent sequences from fragmented\ndata and disentangling complex behavior patterns. We present a framework for\nsequence modeling using Ensembles of Hidden Markov Models, which are\nlightweight, interpretable, and efficient. Our ensemble-based scoring method\nenables robust comparison across sequences of different lengths and enhances\nperformance in scenarios with imbalanced or scarce data. The framework scales\nin real-world scenarios, is compatible with downstream feature-based modeling,\nand is applicable in both supervised and unsupervised learning settings. We\ndemonstrate the effectiveness of our method with results on a longitudinal\nhuman behavior dataset.\n","authors":["Maxime Kawawa-Beaudan","Srijan Sood","Soham Palande","Ganapathy Mani","Tucker Balch","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2411.02174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02168v1","updated":"2024-11-04T15:26:07Z","published":"2024-11-04T15:26:07Z","title":"Do graph neural network states contain graph properties?","summary":"  Graph learning models achieve state-of-the-art performance on many tasks, but\nthis often requires increasingly large model sizes. Accordingly, the complexity\nof their representations increase. Explainability techniques (XAI) have made\nremarkable progress in the interpretability of ML models. However, the\nnon-relational nature of Graph Neural Networks (GNNs) make it difficult to\nreuse already existing XAI methods. While other works have focused on\ninstance-based explanation methods for GNNs, very few have investigated\nmodel-based methods and, to our knowledge, none have tried to probe the\nembedding of the GNNs for well-known structural graph properties. In this paper\nwe present a model agnostic explainability pipeline for Graph Neural Networks\n(GNNs) employing diagnostic classifiers. This pipeline aims to probe and\ninterpret the learned representations in GNNs across various architectures and\ndatasets, refining our understanding and trust in these models.\n","authors":["Tom Pelletreau-Duris","Ruud van Bakel","Michael Cochez"],"pdf_url":"https://arxiv.org/pdf/2411.02168v1.pdf","comment":"10 pages, 22 figures, conference"},{"id":"http://arxiv.org/abs/2406.05033v2","updated":"2024-11-04T15:23:23Z","published":"2024-06-07T15:53:06Z","title":"Gradient Descent on Logistic Regression with Non-Separable Data and\n  Large Step Sizes","summary":"  We study gradient descent (GD) dynamics on logistic regression problems with\nlarge, constant step sizes. For linearly-separable data, it is known that GD\nconverges to the minimizer with arbitrarily large step sizes, a property which\nno longer holds when the problem is not separable. In fact, the behaviour can\nbe much more complex -- a sequence of period-doubling bifurcations begins at\nthe critical step size $2/\\lambda$, where $\\lambda$ is the largest eigenvalue\nof the Hessian at the solution. Using a smaller-than-critical step size\nguarantees convergence if initialized nearby the solution: but does this\nsuffice globally? In one dimension, we show that a step size less than\n$1/\\lambda$ suffices for global convergence. However, for all step sizes\nbetween $1/\\lambda$ and the critical step size $2/\\lambda$, one can construct a\ndataset such that GD converges to a stable cycle. In higher dimensions, this is\nactually possible even for step sizes less than $1/\\lambda$. Our results show\nthat although local convergence is guaranteed for all step sizes less than the\ncritical step size, global convergence is not, and GD may instead converge to a\ncycle depending on the initialization.\n","authors":["Si Yi Meng","Antonio Orvieto","Daniel Yiming Cao","Christopher De Sa"],"pdf_url":"https://arxiv.org/pdf/2406.05033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02158v1","updated":"2024-11-04T15:17:19Z","published":"2024-11-04T15:17:19Z","title":"Learning Multiple Initial Solutions to Optimization Problems","summary":"  Sequentially solving similar optimization problems under strict runtime\nconstraints is essential for many applications, such as robot control,\nautonomous driving, and portfolio management. The performance of local\noptimization methods in these settings is sensitive to the initial solution:\npoor initialization can lead to slow convergence or suboptimal solutions. To\naddress this challenge, we propose learning to predict \\emph{multiple} diverse\ninitial solutions given parameters that define the problem instance. We\nintroduce two strategies for utilizing multiple initial solutions: (i) a\nsingle-optimizer approach, where the most promising initial solution is chosen\nusing a selection function, and (ii) a multiple-optimizers approach, where\nseveral optimizers, potentially run in parallel, are each initialized with a\ndifferent solution, with the best solution chosen afterward. We validate our\nmethod on three optimal control benchmark tasks: cart-pole, reacher, and\nautonomous driving, using different optimizers: DDP, MPPI, and iLQR. We find\nsignificant and consistent improvement with our method across all evaluation\nsettings and demonstrate that it efficiently scales with the number of initial\nsolutions required. The code is available at\n$\\href{https://github.com/EladSharony/miso}{\\tt{https://github.com/EladSharony/miso}}$.\n","authors":["Elad Sharony","Heng Yang","Tong Che","Marco Pavone","Shie Mannor","Peter Karkus"],"pdf_url":"https://arxiv.org/pdf/2411.02158v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.19987v3","updated":"2024-11-04T15:09:54Z","published":"2024-10-25T22:00:11Z","title":"Residual Random Neural Networks","summary":"  The single-layer feedforward neural network with random weights is a\nrecurring motif in the neural networks literature. The advantage of these\nnetworks is their simplified training, which reduces to solving a\nridge-regression problem. A general assumption is that these networks require a\nlarge number of hidden neurons relative to the dimensionality of the data\nsamples, in order to achieve good classification accuracy. Contrary to this\nassumption, here we show that one can obtain good classification results even\nif the number of hidden neurons has the same order of magnitude as the\ndimensionality of the data samples, if this dimensionality is reasonably high.\nInspired by this result, we also develop an efficient iterative residual\ntraining method for such random neural networks, and we extend the algorithm to\nthe least-squares kernel version of the neural network model. Moreover, we also\ndescribe an encryption (obfuscation) method which can be used to protect both\nthe data and the resulted network model.\n","authors":["M. Andrecut"],"pdf_url":"https://arxiv.org/pdf/2410.19987v3.pdf","comment":"revised version, 17 pages, 8 figures, added kernel method"},{"id":"http://arxiv.org/abs/2411.02152v1","updated":"2024-11-04T15:08:36Z","published":"2024-11-04T15:08:36Z","title":"FedPID: An Aggregation Method for Federated Learning","summary":"  This paper presents FedPID, our submission to the Federated Tumor\nSegmentation Challenge 2024 (FETS24). Inspired by FedCostWAvg and FedPIDAvg,\nour winning contributions to FETS21 and FETS2022, we propose an improved\naggregation strategy for federated and collaborative learning. FedCostWAvg is a\nmethod that averages results by considering both the number of training samples\nin each group and how much the cost function decreased in the last round of\ntraining. This is similar to how the derivative part of a PID controller works.\nIn FedPIDAvg, we also included the integral part that was missing. Another\nchallenge we faced were vastly differing dataset sizes at each center. We\nsolved this by assuming the sizes follow a Poisson distribution and adjusting\nthe training iterations for each center accordingly. Essentially, this part of\nthe method controls that outliers that require too much training time are less\nfrequently used. Based on these contributions we now adapted FedPIDAvg by\nchanging how the integral part is computed. Instead of integrating the loss\nfunction we measure the global drop in cost since the first round.\n","authors":["Leon Mächler","Gustav Grimberg","Ivan Ezhov","Manuel Nickel","Suprosanna Shit","David Naccache","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2411.02152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02150v1","updated":"2024-11-04T15:07:48Z","published":"2024-11-04T15:07:48Z","title":"Cooperative and Collaborative Multi-Task Semantic Communication for\n  Distributed Sources","summary":"  In this paper, we explore a multi-task semantic communication (SemCom) system\nfor distributed sources, extending the existing focus on collaborative\nsingle-task execution. We build on the cooperative multi-task processing\nintroduced in [1], which divides the encoder into a common unit (CU) and\nmultiple specific units (SUs). While earlier studies in multi-task SemCom\nfocused on full observation settings, our research explores a more realistic\ncase where only distributed partial observations are available, such as in a\nproduction line monitored by multiple sensing nodes. To address this, we\npropose an SemCom system that supports multi-task processing through\ncooperation on the transmitter side via split structure and collaboration on\nthe receiver side. We have used an information-theoretic perspective with\nvariational approximations for our end-to-end data-driven approach. Simulation\nresults demonstrate that the proposed cooperative and collaborative multi-task\n(CCMT) SemCom system significantly improves task execution accuracy,\nparticularly in complex datasets, if the noise introduced from the\ncommunication channel is not limiting the task performance too much. Our\nfindings contribute to a more general SemCom framework capable of handling\ndistributed sources and multiple tasks simultaneously, advancing the\napplicability of SemCom systems in real-world scenarios.\n","authors":["Ahmad Halimi Razlighi","Maximilian H. V. Tillmann","Edgar Beck","Carsten Bockelmann","Armin Dekorsy"],"pdf_url":"https://arxiv.org/pdf/2411.02150v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2406.01781v2","updated":"2024-11-04T15:04:49Z","published":"2024-06-03T20:52:34Z","title":"DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the\n  Generalised $h$-transform","summary":"  Generative modelling paradigms based on denoising diffusion processes have\nemerged as a leading candidate for conditional sampling in inverse problems. In\nmany real-world applications, we often have access to large, expensively\ntrained unconditional diffusion models, which we aim to exploit for improving\nconditional sampling. Most recent approaches are motivated heuristically and\nlack a unifying framework, obscuring connections between them. Further, they\noften suffer from issues such as being very sensitive to hyperparameters, being\nexpensive to train or needing access to weights hidden behind a closed API. In\nthis work, we unify conditional training and sampling using the mathematically\nwell-understood Doob's h-transform. This new perspective allows us to unify\nmany existing methods under a common umbrella. Under this framework, we propose\nDEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional\ngeneration that simply fine-tunes a very small network to quickly learn the\nconditional $h$-transform, while keeping the larger unconditional network\nunchanged. DEFT is much faster than existing baselines while achieving\nstate-of-the-art performance across a variety of linear and non-linear\nbenchmarks. On image reconstruction tasks, we achieve speedups of up to\n1.6$\\times$, while having the best perceptual quality on natural images and\nreconstruction performance on medical images. Further, we also provide initial\nexperiments on protein motif scaffolding and outperform reconstruction guidance\nmethods.\n","authors":["Alexander Denker","Francisco Vargas","Shreyas Padhy","Kieran Didi","Simon Mathis","Vincent Dutordoir","Riccardo Barbano","Emile Mathieu","Urszula Julia Komorowska","Pietro Lio"],"pdf_url":"https://arxiv.org/pdf/2406.01781v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.09236"},{"id":"http://arxiv.org/abs/2411.02142v1","updated":"2024-11-04T14:58:37Z","published":"2024-11-04T14:58:37Z","title":"Training Compute-Optimal Protein Language Models","summary":"  We explore optimally training protein language models, an area of significant\ninterest in biological research where guidance on best practices is limited.\nMost models are trained with extensive compute resources until performance\ngains plateau, focusing primarily on increasing model sizes rather than\noptimizing the efficient compute frontier that balances performance and compute\nbudgets. Our investigation is grounded in a massive dataset consisting of 939\nmillion protein sequences. We trained over 300 models ranging from 3.5 million\nto 10.7 billion parameters on 5 to 200 billion unique tokens, to investigate\nthe relations between model sizes, training token numbers, and objectives.\nFirst, we observed the effect of diminishing returns for the Causal Language\nModel (CLM) and that of overfitting for the Masked Language Model~(MLM) when\nrepeating the commonly used Uniref database. To address this, we included\nmetagenomic protein sequences in the training set to increase the diversity and\navoid the plateau or overfitting effects. Second, we obtained the scaling laws\nof CLM and MLM on Transformer, tailored to the specific characteristics of\nprotein sequence data. Third, we observe a transfer scaling phenomenon from CLM\nto MLM, further demonstrating the effectiveness of transfer through scaling\nbehaviors based on estimated Effectively Transferred Tokens. Finally, to\nvalidate our scaling laws, we compare the large-scale versions of ESM-2 and\nPROGEN2 on downstream tasks, encompassing evaluations of protein generation as\nwell as structure- and function-related tasks, all within less or equivalent\npre-training compute budgets.\n","authors":["Xingyi Cheng","Bo Chen","Pan Li","Jing Gong","Jie Tang","Le Song"],"pdf_url":"https://arxiv.org/pdf/2411.02142v1.pdf","comment":"NeurIPS 2024 (Spotlight); Code:\n  https://github.com/cxysteven/ScalingProteinLM. Additional resources are\n  available here"},{"id":"http://arxiv.org/abs/2411.02139v1","updated":"2024-11-04T14:56:48Z","published":"2024-11-04T14:56:48Z","title":"Theoretical characterisation of the Gauss-Newton conditioning in Neural\n  Networks","summary":"  The Gauss-Newton (GN) matrix plays an important role in machine learning,\nmost evident in its use as a preconditioning matrix for a wide family of\npopular adaptive methods to speed up optimization. Besides, it can also provide\nkey insights into the optimization landscape of neural networks. In the context\nof deep neural networks, understanding the GN matrix involves studying the\ninteraction between different weight matrices as well as the dependencies\nintroduced by the data, thus rendering its analysis challenging. In this work,\nwe take a first step towards theoretically characterizing the conditioning of\nthe GN matrix in neural networks. We establish tight bounds on the condition\nnumber of the GN in deep linear networks of arbitrary depth and width, which we\nalso extend to two-layer ReLU networks. We expand the analysis to further\narchitectural components, such as residual connections and convolutional\nlayers. Finally, we empirically validate the bounds and uncover valuable\ninsights into the influence of the analyzed architectural components.\n","authors":["Jim Zhao","Sidak Pal Singh","Aurelien Lucchi"],"pdf_url":"https://arxiv.org/pdf/2411.02139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02138v1","updated":"2024-11-04T14:51:35Z","published":"2024-11-04T14:51:35Z","title":"SpecRaGE: Robust and Generalizable Multi-view Spectral Representation\n  Learning","summary":"  Multi-view representation learning (MvRL) has garnered substantial attention\nin recent years, driven by the increasing demand for applications that can\neffectively process and analyze data from multiple sources. In this context,\ngraph Laplacian-based MvRL methods have demonstrated remarkable success in\nrepresenting multi-view data. However, these methods often struggle with\ngeneralization to new data and face challenges with scalability. Moreover, in\nmany practical scenarios, multi-view data is contaminated by noise or outliers.\nIn such cases, modern deep-learning-based MvRL approaches that rely on\nalignment or contrastive objectives can lead to misleading results, as they may\nimpose incorrect consistency between clear and corrupted data sources. We\nintroduce $\\textit{SpecRaGE}$, a novel fusion-based framework that integrates\nthe strengths of graph Laplacian methods with the power of deep learning to\novercome these challenges. SpecRage uses neural networks to learn parametric\nmapping that approximates a joint diagonalization of graph Laplacians. This\nsolution bypasses the need for alignment while enabling generalizable and\nscalable learning of informative and meaningful representations. Moreover, it\nincorporates a meta-learning fusion module that dynamically adapts to data\nquality, ensuring robustness against outliers and noisy views. Our extensive\nexperiments demonstrate that SpecRaGE outperforms state-of-the-art methods,\nparticularly in scenarios with data contamination, paving the way for more\nreliable and efficient multi-view learning. Our code will be made publicly\navailable upon acceptance.\n","authors":["Amitai Yacobi","Ofir Lindenbaum","Uri Shaham"],"pdf_url":"https://arxiv.org/pdf/2411.02138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02137v1","updated":"2024-11-04T14:50:15Z","published":"2024-11-04T14:50:15Z","title":"Finite-sample performance of the maximum likelihood estimator in\n  logistic regression","summary":"  Logistic regression is a classical model for describing the probabilistic\ndependence of binary responses to multivariate covariates. We consider the\npredictive performance of the maximum likelihood estimator (MLE) for logistic\nregression, assessed in terms of logistic risk. We consider two questions:\nfirst, that of the existence of the MLE (which occurs when the dataset is not\nlinearly separated), and second that of its accuracy when it exists. These\nproperties depend on both the dimension of covariates and on the signal\nstrength. In the case of Gaussian covariates and a well-specified logistic\nmodel, we obtain sharp non-asymptotic guarantees for the existence and excess\nlogistic risk of the MLE. We then generalize these results in two ways: first,\nto non-Gaussian covariates satisfying a certain two-dimensional margin\ncondition, and second to the general case of statistical learning with a\npossibly misspecified logistic model. Finally, we consider the case of a\nBernoulli design, where the behavior of the MLE is highly sensitive to the\nparameter direction.\n","authors":["Hugo Chardon","Matthieu Lerasle","Jaouad Mourtada"],"pdf_url":"https://arxiv.org/pdf/2411.02137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02134v1","updated":"2024-11-04T14:47:48Z","published":"2024-11-04T14:47:48Z","title":"Encoding Multi-level Dynamics in Effect Heterogeneity Estimation","summary":"  Earth Observation (EO) data are increasingly used in policy analysis by\nenabling granular estimation of treatment effects. However, a challenge in\nEO-based causal inference lies in balancing the trade-off between capturing\nfine-grained individual heterogeneity and broader contextual information. This\npaper introduces Multi-scale Concatenation, a family of composable procedures\nthat transform arbitrary single-scale CATE estimation algorithms into\nmulti-scale algorithms. We benchmark the performance of Multi-scale\nConcatenation on a CATE estimation pipeline combining Vision Transformer (ViT)\nmodels fine-tuned on satellite images to encode images of different scales with\nCausal Forests to obtain the final CATE estimate. We first perform simulation\nstudies, showing how a multi-scale approach captures multi-level dynamics that\nsingle-scale ViT models fail to capture. We then apply the multi-scale method\nto two randomized controlled trials (RCTs) conducted in Peru and Uganda using\nLandsat satellite imagery. In the RCT analysis, the Rank Average Treatment\nEffect Ratio (RATE Ratio) measure is employed to assess performance without\nground truth individual treatment effects. Results indicate that Multi-scale\nConcatenation improves the performance of deep learning models in EO-based CATE\nestimation without the complexity of designing new multi-scale architectures\nfor a specific use case.\n","authors":["Fucheng Warren Zhu","Connor T. Jerzak","Adel Daoud"],"pdf_url":"https://arxiv.org/pdf/2411.02134v1.pdf","comment":"27 pages, 13 figures"},{"id":"http://arxiv.org/abs/2411.02131v1","updated":"2024-11-04T14:44:20Z","published":"2024-11-04T14:44:20Z","title":"Generating the Traces You Need: A Conditional Generative Model for\n  Process Mining Data","summary":"  In recent years, trace generation has emerged as a significant challenge\nwithin the Process Mining community. Deep Learning (DL) models have\ndemonstrated accuracy in reproducing the features of the selected processes.\nHowever, current DL generative models are limited in their ability to adapt the\nlearned distributions to generate data samples based on specific conditions or\nattributes. This limitation is particularly significant because the ability to\ncontrol the type of generated data can be beneficial in various contexts,\nenabling a focus on specific behaviours, exploration of infrequent patterns, or\nsimulation of alternative 'what-if' scenarios. In this work, we address this\nchallenge by introducing a conditional model for process data generation based\non a conditional variational autoencoder (CVAE). Conditional models offer\ncontrol over the generation process by tuning input conditional variables,\nenabling more targeted and controlled data generation. Unlike other domains,\nCVAE for process mining faces specific challenges due to the multiperspective\nnature of the data and the need to adhere to control-flow rules while ensuring\ndata variability. Specifically, we focus on generating process executions\nconditioned on control flow and temporal features of the trace, allowing us to\nproduce traces for specific, identified sub-processes. The generated traces are\nthen evaluated using common metrics for generative model assessment, along with\nadditional metrics to evaluate the quality of the conditional generation\n","authors":["Riccardo Graziosi","Massimiliano Ronzani","Andrei Buliga","Chiara Di Francescomarino","Francesco Folino","Chiara Ghidini","Francesca Meneghello","Luigi Pontieri"],"pdf_url":"https://arxiv.org/pdf/2411.02131v1.pdf","comment":"6th International Conference on Process Mining (ICPM) 2024\n  Copenhagen, Denmark 14-18 October 2024"},{"id":"http://arxiv.org/abs/2411.02127v1","updated":"2024-11-04T14:38:43Z","published":"2024-11-04T14:38:43Z","title":"Supervised Transfer Learning Framework for Fault Diagnosis in Wind\n  Turbines","summary":"  Common challenges in fault diagnosis include the lack of labeled data and the\nneed to build models for each domain, resulting in many models that require\nsupervision. Transfer learning can help tackle these challenges by learning\ncross-domain knowledge. Many approaches still require at least some labeled\ndata in the target domain, and often provide unexplainable results. To this\nend, we propose a supervised transfer learning framework for fault diagnosis in\nwind turbines that operates in an Anomaly-Space. This space was created using\nSCADA data and vibration data and was built and provided to us by our research\npartner. Data within the Anomaly-Space can be interpreted as anomaly scores for\neach component in the wind turbine, making each value intuitive to understand.\nWe conducted cross-domain evaluation on the train set using popular supervised\nclassifiers like Random Forest, Light-Gradient-Boosting-Machines and Multilayer\nPerceptron as metamodels for the diagnosis of bearing and sensor faults. The\nMultilayer Perceptron achieved the highest classification performance. This\nmodel was then used for a final evaluation in our test set. The results show,\nthat the proposed framework is able to detect cross-domain faults in the test\nset with a high degree of accuracy by using one single classifier, which is a\nsignificant asset to the diagnostic team.\n","authors":["Kenan Weber","Christine Preisach"],"pdf_url":"https://arxiv.org/pdf/2411.02127v1.pdf","comment":"9 pages, 4 figures, to be published in: Upper Rhine AI Symposium\n  (URAI) 2024"},{"id":"http://arxiv.org/abs/2411.02126v1","updated":"2024-11-04T14:37:07Z","published":"2024-11-04T14:37:07Z","title":"Unsupervised detection of semantic correlations in big data","summary":"  In real-world data, information is stored in extremely large feature vectors.\nThese variables are typically correlated due to complex interactions involving\nmany features simultaneously. Such correlations qualitatively correspond to\nsemantic roles and are naturally recognized by both the human brain and\nartificial neural networks. This recognition enables, for instance, the\nprediction of missing parts of an image or text based on their context. We\npresent a method to detect these correlations in high-dimensional data\nrepresented as binary numbers. We estimate the binary intrinsic dimension of a\ndataset, which quantifies the minimum number of independent coordinates needed\nto describe the data, and is therefore a proxy of semantic complexity. The\nproposed algorithm is largely insensitive to the so-called curse of\ndimensionality, and can therefore be used in big data analysis. We test this\napproach identifying phase transitions in model magnetic systems and we then\napply it to the detection of semantic correlations of images and text inside\ndeep neural networks.\n","authors":["Santiago Acevedo","Alex Rodriguez","Alessandro Laio"],"pdf_url":"https://arxiv.org/pdf/2411.02126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02125v1","updated":"2024-11-04T14:36:51Z","published":"2024-11-04T14:36:51Z","title":"Revisiting K-mer Profile for Effective and Scalable Genome\n  Representation Learning","summary":"  Obtaining effective representations of DNA sequences is crucial for genome\nanalysis. Metagenomic binning, for instance, relies on genome representations\nto cluster complex mixtures of DNA fragments from biological samples with the\naim of determining their microbial compositions. In this paper, we revisit\nk-mer-based representations of genomes and provide a theoretical analysis of\ntheir use in representation learning. Based on the analysis, we propose a\nlightweight and scalable model for performing metagenomic binning at the genome\nread level, relying only on the k-mer compositions of the DNA fragments. We\ncompare the model to recent genome foundation models and demonstrate that while\nthe models are comparable in performance, the proposed model is significantly\nmore effective in terms of scalability, a crucial aspect for performing\nmetagenomic binning of real-world datasets.\n","authors":["Abdulkadir Celikkanat","Andres R. Masegosa","Thomas D. Nielsen"],"pdf_url":"https://arxiv.org/pdf/2411.02125v1.pdf","comment":"Accepted to the Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.02124v1","updated":"2024-11-04T14:36:24Z","published":"2024-11-04T14:36:24Z","title":"Adaptive Sparse Allocation with Mutual Choice & Feature Choice Sparse\n  Autoencoders","summary":"  Sparse autoencoders (SAEs) are a promising approach to extracting features\nfrom neural networks, enabling model interpretability as well as causal\ninterventions on model internals. SAEs generate sparse feature representations\nusing a sparsifying activation function that implicitly defines a set of\ntoken-feature matches. We frame the token-feature matching as a resource\nallocation problem constrained by a total sparsity upper bound. For example,\nTopK SAEs solve this allocation problem with the additional constraint that\neach token matches with at most $k$ features. In TopK SAEs, the $k$ active\nfeatures per token constraint is the same across tokens, despite some tokens\nbeing more difficult to reconstruct than others. To address this limitation, we\npropose two novel SAE variants, Feature Choice SAEs and Mutual Choice SAEs,\nwhich each allow for a variable number of active features per token. Feature\nChoice SAEs solve the sparsity allocation problem under the additional\nconstraint that each feature matches with at most $m$ tokens. Mutual Choice\nSAEs solve the unrestricted allocation problem where the total sparsity budget\ncan be allocated freely between tokens and features. Additionally, we introduce\na new auxiliary loss function, $\\mathtt{aux\\_zipf\\_loss}$, which generalises\nthe $\\mathtt{aux\\_k\\_loss}$ to mitigate dead and underutilised features. Our\nmethods result in SAEs with fewer dead features and improved reconstruction\nloss at equivalent sparsity levels as a result of the inherent adaptive\ncomputation. More accurate and scalable feature extraction methods provide a\npath towards better understanding and more precise control of foundation\nmodels.\n","authors":["Kola Ayonrinde"],"pdf_url":"https://arxiv.org/pdf/2411.02124v1.pdf","comment":"10 pages (18 w/ appendices), 7 figures. Preprint"},{"id":"http://arxiv.org/abs/2411.02120v1","updated":"2024-11-04T14:35:14Z","published":"2024-11-04T14:35:14Z","title":"Bridge-IF: Learning Inverse Protein Folding with Markov Bridges","summary":"  Inverse protein folding is a fundamental task in computational protein\ndesign, which aims to design protein sequences that fold into the desired\nbackbone structures. While the development of machine learning algorithms for\nthis task has seen significant success, the prevailing approaches, which\npredominantly employ a discriminative formulation, frequently encounter the\nerror accumulation issue and often fail to capture the extensive variety of\nplausible sequences. To fill these gaps, we propose Bridge-IF, a generative\ndiffusion bridge model for inverse folding, which is designed to learn the\nprobabilistic dependency between the distributions of backbone structures and\nprotein sequences. Specifically, we harness an expressive structure encoder to\npropose a discrete, informative prior derived from structures, and establish a\nMarkov bridge to connect this prior with native sequences. During the inference\nstage, Bridge-IF progressively refines the prior sequence, culminating in a\nmore plausible design. Moreover, we introduce a reparameterization perspective\non Markov bridge models, from which we derive a simplified loss function that\nfacilitates more effective training. We also modulate protein language models\n(PLMs) with structural conditions to precisely approximate the Markov bridge\nprocess, thereby significantly enhancing generation performance while\nmaintaining parameter-efficient training. Extensive experiments on\nwell-established benchmarks demonstrate that Bridge-IF predominantly surpasses\nexisting baselines in sequence recovery and excels in the design of plausible\nproteins with high foldability. The code is available at\nhttps://github.com/violet-sto/Bridge-IF.\n","authors":["Yiheng Zhu","Jialu Wu","Qiuyi Li","Jiahuan Yan","Mingze Yin","Wei Wu","Mingyang Li","Jieping Ye","Zheng Wang","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2411.02120v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.11562v2","updated":"2024-11-04T14:32:26Z","published":"2024-07-16T10:15:35Z","title":"RobotKeyframing: Learning Locomotion with High-Level Objectives via\n  Mixture of Dense and Sparse Rewards","summary":"  This paper presents a novel learning-based control framework that uses\nkeyframing to incorporate high-level objectives in natural locomotion for\nlegged robots. These high-level objectives are specified as a variable number\nof partial or complete pose targets that are spaced arbitrarily in time. Our\nproposed framework utilizes a multi-critic reinforcement learning algorithm to\neffectively handle the mixture of dense and sparse rewards. Additionally, it\nemploys a transformer-based encoder to accommodate a variable number of input\ntargets, each associated with specific time-to-arrivals. Throughout simulation\nand hardware experiments, we demonstrate that our framework can effectively\nsatisfy the target keyframe sequence at the required times. In the experiments,\nthe multi-critic method significantly reduces the effort of hyperparameter\ntuning compared to the standard single-critic alternative. Moreover, the\nproposed transformer-based architecture enables robots to anticipate future\ngoals, which results in quantitative improvements in their ability to reach\ntheir targets.\n","authors":["Fatemeh Zargarbashi","Jin Cheng","Dongho Kang","Robert Sumner","Stelian Coros"],"pdf_url":"https://arxiv.org/pdf/2407.11562v2.pdf","comment":"This paper has been accepted to 8th Conference on Robot Learning\n  (CoRL 2024). Project website: https://sites.google.com/view/robot-keyframing"},{"id":"http://arxiv.org/abs/2303.06753v3","updated":"2024-11-04T14:32:02Z","published":"2023-03-12T21:01:54Z","title":"Modular Quantization-Aware Training for 6D Object Pose Estimation","summary":"  Edge applications, such as collaborative robotics and spacecraft rendezvous,\ndemand efficient 6D object pose estimation on resource-constrained embedded\nplatforms. Existing 6D pose estimation networks are often too large for such\ndeployments, necessitating compression while maintaining reliable performance.\nTo address this challenge, we introduce Modular Quantization-Aware Training\n(MQAT), an adaptive and mixed-precision quantization-aware training strategy\nthat exploits the modular structure of modern 6D pose estimation architectures.\nMQAT guides a systematic gradated modular quantization sequence and determines\nmodule-specific bit precisions, leading to quantized models that outperform\nthose produced by state-of-the-art uniform and mixed-precision quantization\ntechniques. Our experiments showcase the generality of MQAT across datasets,\narchitectures, and quantization algorithms. Remarkably, MQAT-trained quantized\nmodels achieve a significant accuracy boost (>7%) over the baseline\nfull-precision network while reducing model size by a factor of 4x or more. Our\nproject website is at: https://saqibjaved1.github.io/MQAT_/\n","authors":["Saqib Javed","Chengkun Li","Andrew Price","Yinlin Hu","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2303.06753v3.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR), 2024"},{"id":"http://arxiv.org/abs/2408.05500v2","updated":"2024-11-04T14:30:03Z","published":"2024-08-10T09:31:58Z","title":"PointNCBW: Towards Dataset Ownership Verification for Point Clouds via\n  Negative Clean-label Backdoor Watermark","summary":"  Recently, point clouds have been widely used in computer vision, whereas\ntheir collection is time-consuming and expensive. As such, point cloud datasets\nare the valuable intellectual property of their owners and deserve protection.\nTo detect and prevent unauthorized use of these datasets, especially for\ncommercial or open-sourced ones that cannot be sold again or used commercially\nwithout permission, we intend to identify whether a suspicious third-party\nmodel is trained on our protected dataset under the black-box setting. We\nachieve this goal by designing a scalable clean-label backdoor-based dataset\nwatermark for point clouds that ensures both effectiveness and stealthiness.\nUnlike existing clean-label watermark schemes, which are susceptible to the\nnumber of categories, our method could watermark samples from all classes\ninstead of only from the target one. Accordingly, it can still preserve high\neffectiveness even on large-scale datasets with many classes. Specifically, we\nperturb selected point clouds with non-target categories in both shape-wise and\npoint-wise manners before inserting trigger patterns without changing their\nlabels. The features of perturbed samples are similar to those of benign\nsamples from the target class. As such, models trained on the watermarked\ndataset will have a distinctive yet stealthy backdoor behavior, i.e.,\nmisclassifying samples from the target class whenever triggers appear, since\nthe trained DNNs will treat the inserted trigger pattern as a signal to deny\npredicting the target label. We also design a hypothesis-test-guided dataset\nownership verification based on the proposed watermark. Extensive experiments\non benchmark datasets are conducted, verifying the effectiveness of our method\nand its resistance to potential removal methods.\n","authors":["Cheng Wei","Yang Wang","Kuofeng Gao","Shuo Shao","Yiming Li","Zhibo Wang","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2408.05500v2.pdf","comment":"This paper was accepted by IEEE Transactions on Information Forensics\n  and Security (TIFS), 2024. 16 pages"},{"id":"http://arxiv.org/abs/2411.02115v1","updated":"2024-11-04T14:29:04Z","published":"2024-11-04T14:29:04Z","title":"FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation","summary":"  Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server.\n","authors":["Ziwei Zhan","Wenkuan Zhao","Yuanqing Li","Weijie Liu","Xiaoxi Zhang","Chee Wei Tan","Chuan Wu","Deke Guo","Xu Chen"],"pdf_url":"https://arxiv.org/pdf/2411.02115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02114v1","updated":"2024-11-04T14:29:02Z","published":"2024-11-04T14:29:02Z","title":"Semiparametric conformal prediction","summary":"  Many risk-sensitive applications require well-calibrated prediction sets over\nmultiple, potentially correlated target variables, for which the prediction\nalgorithm may report correlated non-conformity scores. In this work, we treat\nthe scores as random vectors and aim to construct the prediction set accounting\nfor their joint correlation structure. Drawing from the rich literature on\nmultivariate quantiles and semiparametric statistics, we propose an algorithm\nto estimate the $1-\\alpha$ quantile of the scores, where $\\alpha$ is the\nuser-specified miscoverage rate. In particular, we flexibly estimate the joint\ncumulative distribution function (CDF) of the scores using nonparametric vine\ncopulas and improve the asymptotic efficiency of the quantile estimate using\nits influence function. The vine decomposition allows our method to scale well\nto a large number of targets. We report desired coverage and competitive\nefficiency on a range of real-world regression problems, including those with\nmissing-at-random labels in the calibration set.\n","authors":["Ji Won Park","Robert Tibshirani","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2411.02114v1.pdf","comment":"9 pages (+12 appendix), 11 figures, submitted to AISTATS 2025"},{"id":"http://arxiv.org/abs/2411.02112v1","updated":"2024-11-04T14:27:10Z","published":"2024-11-04T14:27:10Z","title":"Multi-modal biometric authentication: Leveraging shared layer\n  architectures for enhanced security","summary":"  In this study, we introduce a novel multi-modal biometric authentication\nsystem that integrates facial, vocal, and signature data to enhance security\nmeasures. Utilizing a combination of Convolutional Neural Networks (CNNs) and\nRecurrent Neural Networks (RNNs), our model architecture uniquely incorporates\ndual shared layers alongside modality-specific enhancements for comprehensive\nfeature extraction. The system undergoes rigorous training with a joint loss\nfunction, optimizing for accuracy across diverse biometric inputs.\nFeature-level fusion via Principal Component Analysis (PCA) and classification\nthrough Gradient Boosting Machines (GBM) further refine the authentication\nprocess. Our approach demonstrates significant improvements in authentication\naccuracy and robustness, paving the way for advanced secure identity\nverification solutions.\n","authors":["Vatchala S","Yogesh C","Yeshwanth Govindarajan","Krithik Raja M","Vishal Pramav Amirtha Ganesan","Aashish Vinod A","Dharun Ramesh"],"pdf_url":"https://arxiv.org/pdf/2411.02112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02109v1","updated":"2024-11-04T14:23:59Z","published":"2024-11-04T14:23:59Z","title":"Training on test proteins improves fitness, structure, and function\n  prediction","summary":"  Data scarcity and distribution shifts often hinder the ability of machine\nlearning models to generalize when applied to proteins and other biological\ndata. Self-supervised pre-training on large datasets is a common method to\nenhance generalization. However, striving to perform well on all possible\nproteins can limit model's capacity to excel on any specific one, even though\npractitioners are often most interested in accurate predictions for the\nindividual protein they study. To address this limitation, we propose an\northogonal approach to achieve generalization. Building on the prevalence of\nself-supervised pre-training, we introduce a method for self-supervised\nfine-tuning at test time, allowing models to adapt to the test protein of\ninterest on the fly and without requiring any additional data. We study our\ntest-time training (TTT) method through the lens of perplexity minimization and\nshow that it consistently enhances generalization across different models,\ntheir scales, and datasets. Notably, our method leads to new state-of-the-art\nresults on the standard benchmark for protein fitness prediction, improves\nprotein structure prediction for challenging targets, and enhances function\nprediction accuracy.\n","authors":["Anton Bushuiev","Roman Bushuiev","Nikola Zadorozhny","Raman Samusevich","Hannes Stärk","Jiri Sedlar","Tomáš Pluskal","Josef Sivic"],"pdf_url":"https://arxiv.org/pdf/2411.02109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02099v1","updated":"2024-11-04T14:08:26Z","published":"2024-11-04T14:08:26Z","title":"Differentially Private Integrated Decision Gradients (IDG-DP) for\n  Radar-based Human Activity Recognition","summary":"  Human motion analysis offers significant potential for healthcare monitoring\nand early detection of diseases. The advent of radar-based sensing systems has\ncaptured the spotlight for they are able to operate without physical contact\nand they can integrate with pre-existing Wi-Fi networks. They are also seen as\nless privacy-invasive compared to camera-based systems. However, recent\nresearch has shown high accuracy in recognizing subjects or gender from radar\ngait patterns, raising privacy concerns. This study addresses these issues by\ninvestigating privacy vulnerabilities in radar-based Human Activity Recognition\n(HAR) systems and proposing a novel method for privacy preservation using\nDifferential Privacy (DP) driven by attributions derived with Integrated\nDecision Gradient (IDG) algorithm. We investigate Black-box Membership\nInference Attack (MIA) Models in HAR settings across various levels of\nattacker-accessible information. We extensively evaluated the effectiveness of\nthe proposed IDG-DP method by designing a CNN-based HAR model and rigorously\nassessing its resilience against MIAs. Experimental results demonstrate the\npotential of IDG-DP in mitigating privacy attacks while maintaining utility\nacross all settings, particularly excelling against label-only and shadow model\nblack-box MIA attacks. This work represents a crucial step towards balancing\nthe need for effective radar-based HAR with robust privacy protection in\nhealthcare environments.\n","authors":["Idris Zakariyya","Linda Tran","Kaushik Bhargav Sivangi","Paul Henderson","Fani Deligianni"],"pdf_url":"https://arxiv.org/pdf/2411.02099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13376v2","updated":"2024-11-04T14:06:02Z","published":"2024-08-23T21:23:22Z","title":"Reduce, Reuse, Recycle: Categories for Compositional Reinforcement\n  Learning","summary":"  In reinforcement learning, conducting task composition by forming cohesive,\nexecutable sequences from multiple tasks remains challenging. However, the\nability to (de)compose tasks is a linchpin in developing robotic systems\ncapable of learning complex behaviors. Yet, compositional reinforcement\nlearning is beset with difficulties, including the high dimensionality of the\nproblem space, scarcity of rewards, and absence of system robustness after task\ncomposition. To surmount these challenges, we view task composition through the\nprism of category theory -- a mathematical discipline exploring structures and\ntheir compositional relationships. The categorical properties of Markov\ndecision processes untangle complex tasks into manageable sub-tasks, allowing\nfor strategical reduction of dimensionality, facilitating more tractable reward\nstructures, and bolstering system robustness. Experimental results support the\ncategorical theory of reinforcement learning by enabling skill reduction,\nreuse, and recycling when learning complex robotic arm tasks.\n","authors":["Georgios Bakirtzis","Michail Savvas","Ruihan Zhao","Sandeep Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2408.13376v2.pdf","comment":"ECAI 2024"},{"id":"http://arxiv.org/abs/2411.02094v1","updated":"2024-11-04T13:56:54Z","published":"2024-11-04T13:56:54Z","title":"Alignment-Based Adversarial Training (ABAT) for Improving the Robustness\n  and Accuracy of EEG-Based BCIs","summary":"  Machine learning has achieved great success in electroencephalogram (EEG)\nbased brain-computer interfaces (BCIs). Most existing BCI studies focused on\nimproving the decoding accuracy, with only a few considering the adversarial\nsecurity. Although many adversarial defense approaches have been proposed in\nother application domains such as computer vision, previous research showed\nthat their direct extensions to BCIs degrade the classification accuracy on\nbenign samples. This phenomenon greatly affects the applicability of\nadversarial defense approaches to EEG-based BCIs. To mitigate this problem, we\npropose alignment-based adversarial training (ABAT), which performs EEG data\nalignment before adversarial training. Data alignment aligns EEG trials from\ndifferent domains to reduce their distribution discrepancies, and adversarial\ntraining further robustifies the classification boundary. The integration of\ndata alignment and adversarial training can make the trained EEG classifiers\nsimultaneously more accurate and more robust. Experiments on five EEG datasets\nfrom two different BCI paradigms (motor imagery classification, and event\nrelated potential recognition), three convolutional neural network classifiers\n(EEGNet, ShallowCNN and DeepCNN) and three different experimental settings\n(offline within-subject cross-block/-session classification, online\ncross-session classification, and pre-trained classifiers) demonstrated its\neffectiveness. It is very intriguing that adversarial attacks, which are\nusually used to damage BCI systems, can be used in ABAT to simultaneously\nimprove the model accuracy and robustness.\n","authors":["Xiaoqing Chen","Ziwei Wang","Dongrui Wu"],"pdf_url":"https://arxiv.org/pdf/2411.02094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06511v2","updated":"2024-11-04T13:52:23Z","published":"2024-10-09T03:26:11Z","title":"TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training","summary":"  The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines.\n","authors":["Wanchao Liang","Tianyu Liu","Less Wright","Will Constable","Andrew Gu","Chien-Chin Huang","Iris Zhang","Wei Feng","Howard Huang","Junjie Wang","Sanket Purandare","Gokul Nadathur","Stratos Idreos"],"pdf_url":"https://arxiv.org/pdf/2410.06511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02087v1","updated":"2024-11-04T13:49:26Z","published":"2024-11-04T13:49:26Z","title":"An Exponential Separation Between Quantum and Quantum-Inspired Classical\n  Algorithms for Machine Learning","summary":"  Achieving a provable exponential quantum speedup for an important machine\nlearning task has been a central research goal since the seminal HHL quantum\nalgorithm for solving linear systems and the subsequent quantum recommender\nsystems algorithm by Kerenidis and Prakash. These algorithms were initially\nbelieved to be strong candidates for exponential speedups, but a lower bound\nruling out similar classical improvements remained absent. In breakthrough work\nby Tang, it was demonstrated that this lack of progress in classical lower\nbounds was for good reasons. Concretely, she gave a classical counterpart of\nthe quantum recommender systems algorithm, reducing the quantum advantage to a\nmere polynomial. Her approach is quite general and was named quantum-inspired\nclassical algorithms. Since then, almost all the initially exponential quantum\nmachine learning speedups have been reduced to polynomial via new\nquantum-inspired classical algorithms. From the current state-of-affairs, it is\nunclear whether we can hope for exponential quantum speedups for any natural\nmachine learning task.\n  In this work, we present the first such provable exponential separation\nbetween quantum and quantum-inspired classical algorithms. We prove the\nseparation for the basic problem of solving a linear system when the input\nmatrix is well-conditioned and has sparse rows and columns.\n","authors":["Allan Grønlund","Kasper Green Larsen"],"pdf_url":"https://arxiv.org/pdf/2411.02087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02083v1","updated":"2024-11-04T13:43:24Z","published":"2024-11-04T13:43:24Z","title":"Regress, Don't Guess -- A Regression-like Loss on Number Tokens for\n  Language Models","summary":"  While language models have exceptional capabilities at text generation, they\nlack a natural inductive bias for emitting numbers and thus struggle in tasks\ninvolving reasoning over quantities, especially arithmetics. This has\nparticular relevance in scientific datasets where combinations of text and\nnumerical data are abundant. One fundamental limitation is the nature of the CE\nloss, which assumes a nominal (categorical) scale and thus cannot convey\nproximity between generated number tokens. As a remedy, we here present two\nversions of a number token loss. The first is based on an $L_p$ loss between\nthe ground truth token value and the weighted sum of the predicted class\nprobabilities. The second loss minimizes the Wasserstein-1 distance between the\ndistribution of the predicted output probabilities and the ground truth\ndistribution. These regression-like losses can easily be added to any language\nmodel and extend the CE objective during training. We compare the proposed\nschemes on a mathematics dataset against existing tokenization, encoding, and\ndecoding schemes for improving number representation in language models. Our\nresults reveal a significant improvement in numerical accuracy when equipping a\nstandard T5 model with the proposed loss schemes.\n","authors":["Jonas Zausinger","Lars Pennig","Kacper Chlodny","Vincent Limbach","Anna Ketteler","Thorben Prein","Vishwa Mohan Singh","Michael Morris Danziger","Jannis Born"],"pdf_url":"https://arxiv.org/pdf/2411.02083v1.pdf","comment":"5-page version for NeurIPS 2024 (MathAI workshop)"},{"id":"http://arxiv.org/abs/2310.14216v2","updated":"2024-11-04T13:33:28Z","published":"2023-10-22T07:48:33Z","title":"UniMAP: Universal SMILES-Graph Representation Learning","summary":"  Molecular representation learning is fundamental for many drug related\napplications. Most existing molecular pre-training models are limited in using\nsingle molecular modality, either SMILES or graph representation. To\neffectively leverage both modalities, we argue that it is critical to capture\nthe fine-grained 'semantics' between SMILES and graph, because subtle\nsequence/graph differences may lead to contrary molecular properties. In this\npaper, we propose a universal SMILE-graph representation learning model, namely\nUniMAP. Firstly, an embedding layer is employed to obtain the token and\nnode/edge representation in SMILES and graph, respectively. A multi-layer\nTransformer is then utilized to conduct deep cross-modality fusion. Specially,\nfour kinds of pre-training tasks are designed for UniMAP, including Multi-Level\nCross-Modality Masking (CMM), SMILES-Graph Matching (SGM), Fragment-Level\nAlignment (FLA), and Domain Knowledge Learning (DKL). In this way, both global\n(i.e. SGM and DKL) and local (i.e. CMM and FLA) alignments are integrated to\nachieve comprehensive cross-modality fusion. We evaluate UniMAP on various\ndownstream tasks, i.e. molecular property prediction, drug-target affinity\nprediction and drug-drug interaction. Experimental results show that UniMAP\noutperforms current state-of-the-art pre-training methods.We also visualize the\nlearned representations to demonstrate the effect of multi-modality\nintegration.\n","authors":["Shikun Feng","Lixin Yang","Yanwen Huang","Yuyan Ni","Weiying Ma","Yanyan Lan"],"pdf_url":"https://arxiv.org/pdf/2310.14216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09381v2","updated":"2024-11-04T13:30:38Z","published":"2024-07-12T16:03:58Z","title":"The Effectiveness of Curvature-Based Rewiring and the Role of\n  Hyperparameters in GNNs Revisited","summary":"  Message passing is the dominant paradigm in Graph Neural Networks (GNNs). The\nefficiency of message passing, however, can be limited by the topology of the\ngraph. This happens when information is lost during propagation due to being\noversquashed when travelling through bottlenecks. To remedy this, recent\nefforts have focused on graph rewiring techniques, which disconnect the input\ngraph originating from the data and the computational graph, on which message\npassing is performed. A prominent approach for this is to use discrete graph\ncurvature measures, of which several variants have been proposed, to identify\nand rewire around bottlenecks, facilitating information propagation. While\noversquashing has been demonstrated in synthetic datasets, in this work we\nreevaluate the performance gains that curvature-based rewiring brings to\nreal-world datasets. We show that in these datasets, edges selected during the\nrewiring process are not in line with theoretical criteria identifying\nbottlenecks. This implies they do not necessarily oversquash information during\nmessage passing. Subsequently, we demonstrate that SOTA accuracies on these\ndatasets are outliers originating from sweeps of hyperparameters -- both the\nones for training and dedicated ones related to the rewiring algorithm --\ninstead of consistent performance gains. In conclusion, our analysis nuances\nthe effectiveness of curvature-based rewiring in real-world datasets and brings\na new perspective on the methods to evaluate GNN accuracy improvements.\n","authors":["Floriano Tori","Vincent Holst","Vincent Ginis"],"pdf_url":"https://arxiv.org/pdf/2407.09381v2.pdf","comment":"22 pages, 17 figures"},{"id":"http://arxiv.org/abs/2409.12799v2","updated":"2024-11-04T13:30:18Z","published":"2024-09-19T14:10:38Z","title":"The Central Role of the Loss Function in Reinforcement Learning","summary":"  This paper illustrates the central role of loss functions in data-driven\ndecision making, providing a comprehensive survey on their influence in\ncost-sensitive classification (CSC) and reinforcement learning (RL). We\ndemonstrate how different regression loss functions affect the sample\nefficiency and adaptivity of value-based decision making algorithms. Across\nmultiple settings, we prove that algorithms using the binary cross-entropy loss\nachieve first-order bounds scaling with the optimal policy's cost and are much\nmore efficient than the commonly used squared loss. Moreover, we prove that\ndistributional algorithms using the maximum likelihood loss achieve\nsecond-order bounds scaling with the policy variance and are even sharper than\nfirst-order bounds. This in particular proves the benefits of distributional\nRL. We hope that this paper serves as a guide analyzing decision making\nalgorithms with varying loss functions, and can inspire the reader to seek out\nbetter loss functions to improve any decision making algorithm.\n","authors":["Kaiwen Wang","Nathan Kallus","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2409.12799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02075v1","updated":"2024-11-04T13:27:32Z","published":"2024-11-04T13:27:32Z","title":"Towards certification: A complete statistical validation pipeline for\n  supervised learning in industry","summary":"  Methods of Machine and Deep Learning are gradually being integrated into\nindustrial operations, albeit at different speeds for different types of\nindustries. The aerospace and aeronautical industries have recently developed a\nroadmap for concepts of design assurance and integration of neural\nnetwork-related technologies in the aeronautical sector. This paper aims to\ncontribute to this paradigm of AI-based certification in the context of\nsupervised learning, by outlining a complete validation pipeline that\nintegrates deep learning, optimization and statistical methods. This pipeline\nis composed by a directed graphical model of ten steps. Each of these steps is\naddressed by a merging key concepts from different contributing disciplines\n(from machine learning or optimization to statistics) and adapting them to an\nindustrial scenario, as well as by developing computationally efficient\nalgorithmic solutions. We illustrate the application of this pipeline in a\nrealistic supervised problem arising in aerostructural design: predicting the\nlikelikood of different stress-related failure modes during different airflight\nmaneuvers based on a (large) set of features characterising the aircraft\ninternal loads and geometric parameters.\n","authors":["Lucas Lacasa","Abel Pardo","Pablo Arbelo","Miguel Sánchez","Pablo Yeste","Noelia Bascones","Alejandro Martínez-Cava","Gonzalo Rubio","Ignacio Gómez","Eusebio Valero","Javier de Vicente"],"pdf_url":"https://arxiv.org/pdf/2411.02075v1.pdf","comment":"38 pages, 17 figures"},{"id":"http://arxiv.org/abs/2401.10158v3","updated":"2024-11-04T13:26:36Z","published":"2024-01-15T13:00:48Z","title":"DISTINQT: A Distributed Privacy Aware Learning Framework for QoS\n  Prediction for Future Mobile and Wireless Networks","summary":"  Beyond 5G and 6G networks are expected to support new and challenging use\ncases and applications that depend on a certain level of Quality of Service\n(QoS) to operate smoothly. Predicting the QoS in a timely manner is of high\nimportance, especially for safety-critical applications as in the case of\nvehicular communications. Although until recent years the QoS prediction has\nbeen carried out by centralized Artificial Intelligence (AI) solutions, a\nnumber of privacy, computational, and operational concerns have emerged.\nAlternative solutions have surfaced (e.g. Split Learning, Federated Learning),\ndistributing AI tasks of reduced complexity across nodes, while preserving the\nprivacy of the data. However, new challenges rise when it comes to scalable\ndistributed learning approaches, taking into account the heterogeneous nature\nof future wireless networks. The current work proposes DISTINQT, a novel\nmulti-headed input privacy-aware distributed learning framework for QoS\nprediction. Our framework supports multiple heterogeneous nodes, in terms of\ndata types and model architectures, by sharing computations across them. This\nenables the incorporation of diverse knowledge into a sole learning process\nthat will enhance the robustness and generalization capabilities of the final\nQoS prediction model. DISTINQT also contributes to data privacy preservation by\nencoding any raw input data into highly complex, compressed, and irreversible\nlatent representations before any transmission. Evaluation results showcase\nthat DISTINQT achieves a statistically identical performance compared to its\ncentralized version, while also proving the validity of the privacy preserving\nclaims. DISTINQT manages to achieve a reduction in prediction error of up to\n65% on average against six state-of-the-art centralized baseline solutions\npresented in the Tele-Operated Driving use case.\n","authors":["Nikolaos Koursioumpas","Lina Magoula","Ioannis Stavrakakis","Nancy Alonistioti","M. A. Gutierrez-Estevez","Ramin Khalili"],"pdf_url":"https://arxiv.org/pdf/2401.10158v3.pdf","comment":"12 Pages Double Column, 10 Figures, (Minor Revised Version) Accepted\n  for publication in the IEEE Transactions on Vehicular Technology (IEEE TVT)"},{"id":"http://arxiv.org/abs/2406.09952v2","updated":"2024-11-04T13:26:07Z","published":"2024-06-14T11:58:49Z","title":"BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval","summary":"  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work, we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts significantly improves over the base\nmodel in SugarCrepe and in BiVLC for both retrieval directions. The gap to\nhuman performance in BiVLC confirms that Vision-Language Compositionality is\nstill a challenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n","authors":["Imanol Miranda","Ander Salaberria","Eneko Agirre","Gorka Azkune"],"pdf_url":"https://arxiv.org/pdf/2406.09952v2.pdf","comment":"Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page\n  at: https://imirandam.github.io/BiVLC_project_page/"},{"id":"http://arxiv.org/abs/2411.02068v1","updated":"2024-11-04T13:15:28Z","published":"2024-11-04T13:15:28Z","title":"Model Integrity when Unlearning with T2I Diffusion Models","summary":"  The rapid advancement of text-to-image Diffusion Models has led to their\nwidespread public accessibility. However these models, trained on large\ninternet datasets, can sometimes generate undesirable outputs. To mitigate\nthis, approximate Machine Unlearning algorithms have been proposed to modify\nmodel weights to reduce the generation of specific types of images,\ncharacterized by samples from a ``forget distribution'', while preserving the\nmodel's ability to generate other images, characterized by samples from a\n``retain distribution''. While these methods aim to minimize the influence of\ntraining data in the forget distribution without extensive additional\ncomputation, we point out that they can compromise the model's integrity by\ninadvertently affecting generation for images in the retain distribution.\nRecognizing the limitations of FID and CLIPScore in capturing these effects, we\nintroduce a novel retention metric that directly assesses the perceptual\ndifference between outputs generated by the original and the unlearned models.\nWe then propose unlearning algorithms that demonstrate superior effectiveness\nin preserving model integrity compared to existing baselines. Given their\nstraightforward implementation, these algorithms serve as valuable benchmarks\nfor future advancements in approximate Machine Unlearning for Diffusion Models.\n","authors":["Andrea Schioppa","Emiel Hoogeboom","Jonathan Heek"],"pdf_url":"https://arxiv.org/pdf/2411.02068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02066v1","updated":"2024-11-04T13:13:25Z","published":"2024-11-04T13:13:25Z","title":"Collaborative Cognitive Diagnosis with Disentangled Representation\n  Learning for Learner Modeling","summary":"  Learners sharing similar implicit cognitive states often display comparable\nobservable problem-solving performances. Leveraging collaborative connections\namong such similar learners proves valuable in comprehending human learning.\nMotivated by the success of collaborative modeling in various domains, such as\nrecommender systems, we aim to investigate how collaborative signals among\nlearners contribute to the diagnosis of human cognitive states (i.e., knowledge\nproficiency) in the context of intelligent education. The primary challenges\nlie in identifying implicit collaborative connections and disentangling the\nentangled cognitive factors of learners for improved explainability and\ncontrollability in learner Cognitive Diagnosis (CD). However, there has been no\nwork on CD capable of simultaneously modeling collaborative and disentangled\ncognitive states. To address this gap, we present Coral, a Collaborative\ncognitive diagnosis model with disentangled representation learning.\nSpecifically, Coral first introduces a disentangled state encoder to achieve\nthe initial disentanglement of learners' states. Subsequently, a meticulously\ndesigned collaborative representation learning procedure captures collaborative\nsignals. It dynamically constructs a collaborative graph of learners by\niteratively searching for optimal neighbors in a context-aware manner. Using\nthe constructed graph, collaborative information is extracted through node\nrepresentation learning. Finally, a decoding process aligns the initial\ncognitive states and collaborative states, achieving co-disentanglement with\npractice performance reconstructions. Extensive experiments demonstrate the\nsuperior performance of Coral, showcasing significant improvements over\nstate-of-the-art methods across several real-world datasets. Our code is\navailable at https://github.com/bigdata-ustc/Coral.\n","authors":["Weibo Gao","Qi Liu","Linan Yue","Fangzhou Yao","Hao Wang","Yin Gu","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02066v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2411.00119v2","updated":"2024-11-04T13:09:42Z","published":"2024-10-31T18:17:39Z","title":"Soft Condorcet Optimization for Ranking of General Agents","summary":"  A common way to drive progress of AI models and agents is to compare their\nperformance on standardized benchmarks. Comparing the performance of general\nagents requires aggregating their individual performances across a potentially\nwide variety of different tasks. In this paper, we describe a novel ranking\nscheme inspired by social choice frameworks, called Soft Condorcet Optimization\n(SCO), to compute the optimal ranking of agents: the one that makes the fewest\nmistakes in predicting the agent comparisons in the evaluation data. This\noptimal ranking is the maximum likelihood estimate when evaluation data (which\nwe view as votes) are interpreted as noisy samples from a ground truth ranking,\na solution to Condorcet's original voting system criteria. SCO ratings are\nmaximal for Condorcet winners when they exist, which we show is not necessarily\ntrue for the classical rating system Elo. We propose three optimization\nalgorithms to compute SCO ratings and evaluate their empirical performance.\nWhen serving as an approximation to the Kemeny-Young voting method, SCO\nrankings are on average 0 to 0.043 away from the optimal ranking in normalized\nKendall-tau distance across 865 preference profiles from the PrefLib open\nranking archive. In a simulated noisy tournament setting, SCO achieves accurate\napproximations to the ground truth ranking and the best among several baselines\nwhen 59\\% or more of the preference data is missing. Finally, SCO ranking\nprovides the best approximation to the optimal ranking, measured on held-out\ntest sets, in a problem containing 52,958 human players across 31,049 games of\nthe classic seven-player game of Diplomacy.\n","authors":["Marc Lanctot","Kate Larson","Michael Kaisers","Quentin Berthet","Ian Gemp","Manfred Diaz","Roberto-Rafael Maura-Rivero","Yoram Bachrach","Anna Koop","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2411.00119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13956v2","updated":"2024-11-04T13:09:06Z","published":"2024-10-17T18:27:51Z","title":"Benchmarking Transcriptomics Foundation Models for Perturbation Analysis\n  : one PCA still rules them all","summary":"  Understanding the relationships among genes, compounds, and their\ninteractions in living organisms remains limited due to technological\nconstraints and the complexity of biological data. Deep learning has shown\npromise in exploring these relationships using various data types. However,\ntranscriptomics, which provides detailed insights into cellular states, is\nstill underused due to its high noise levels and limited data availability.\nRecent advancements in transcriptomics sequencing provide new opportunities to\nuncover valuable insights, especially with the rise of many new foundation\nmodels for transcriptomics, yet no benchmark has been made to robustly evaluate\nthe effectiveness of these rising models for perturbation analysis. This\narticle presents a novel biologically motivated evaluation framework and a\nhierarchy of perturbation analysis tasks for comparing the performance of\npretrained foundation models to each other and to more classical techniques of\nlearning from transcriptomics data. We compile diverse public datasets from\ndifferent sequencing techniques and cell lines to assess models performance.\nOur approach identifies scVI and PCA to be far better suited models for\nunderstanding biological perturbations in comparison to existing foundation\nmodels, especially in their application in real-world scenarios.\n","authors":["Ihab Bendidi","Shawn Whitfield","Kian Kenyon-Dean","Hanene Ben Yedder","Yassir El Mesbahi","Emmanuel Noutahi","Alisandra K. Denton"],"pdf_url":"https://arxiv.org/pdf/2410.13956v2.pdf","comment":"Neurips 2024 AIDrugX Workshop"},{"id":"http://arxiv.org/abs/2411.02064v1","updated":"2024-11-04T13:06:46Z","published":"2024-11-04T13:06:46Z","title":"Amortized Bayesian Experimental Design for Decision-Making","summary":"  Many critical decisions, such as personalized medical diagnoses and product\npricing, are made based on insights gained from designing, observing, and\nanalyzing a series of experiments. This highlights the crucial role of\nexperimental design, which goes beyond merely collecting information on system\nparameters as in traditional Bayesian experimental design (BED), but also plays\na key part in facilitating downstream decision-making. Most recent BED methods\nuse an amortized policy network to rapidly design experiments. However, the\ninformation gathered through these methods is suboptimal for down-the-line\ndecision-making, as the experiments are not inherently designed with downstream\nobjectives in mind. In this paper, we present an amortized decision-aware BED\nframework that prioritizes maximizing downstream decision utility. We introduce\na novel architecture, the Transformer Neural Decision Process (TNDP), capable\nof instantly proposing the next experimental design, whilst inferring the\ndownstream decision, thus effectively amortizing both tasks within a unified\nworkflow. We demonstrate the performance of our method across several tasks,\nshowing that it can deliver informative designs and facilitate accurate\ndecision-making.\n","authors":["Daolang Huang","Yujia Guo","Luigi Acerbi","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2411.02064v1.pdf","comment":"19 pages, 6 figures. Accepted at the 38th Conference on Neural\n  Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.02063v1","updated":"2024-11-04T13:06:17Z","published":"2024-11-04T13:06:17Z","title":"Scalable Efficient Training of Large Language Models with\n  Low-dimensional Projected Attention","summary":"  Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer.\n","authors":["Xingtai Lv","Ning Ding","Kaiyan Zhang","Ermo Hua","Ganqu Cui","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.02063v1.pdf","comment":"Accepted to EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2411.02059v1","updated":"2024-11-04T13:03:13Z","published":"2024-11-04T13:03:13Z","title":"TableGPT2: A Large Multimodal Model with Tabular Data Integration","summary":"  The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.\n","authors":["Aofeng Su","Aowen Wang","Chao Ye","Chen Zhou","Ga Zhang","Guangcheng Zhu","Haobo Wang","Haokai Xu","Hao Chen","Haoze Li","Haoxuan Lan","Jiaming Tian","Jing Yuan","Junbo Zhao","Junlin Zhou","Kaizhe Shou","Liangyu Zha","Lin Long","Liyao Li","Pengzuo Wu","Qi Zhang","Qingyi Huang","Saisai Yang","Tao Zhang","Wentao Ye","Wufang Zhu","Xiaomeng Hu","Xijun Gu","Xinjie Sun","Xiang Li","Yuhang Yang","Zhiqing Xiao"],"pdf_url":"https://arxiv.org/pdf/2411.02059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04317v2","updated":"2024-11-04T13:02:33Z","published":"2024-03-07T08:34:57Z","title":"Online Adaptation of Language Models with a Memory of Amortized Contexts","summary":"  Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. To\naddress the crucial need to keep models updated, online learning has emerged as\na critical tool when utilizing LLMs for real-world applications. However, given\nthe ever-expanding corpus of unseen documents and the large parameter space of\nmodern LLMs, efficient adaptation is essential. To address these challenges, we\npropose Memory of Amortized Contexts (MAC), an efficient and effective online\nadaptation framework for LLMs with strong knowledge retention. We propose a\nfeature extraction and memory-augmentation approach to compress and extract\ninformation from new documents into compact modulations stored in a memory\nbank. When answering questions, our model attends to and extracts relevant\nknowledge from this memory bank. To learn informative modulations in an\nefficient manner, we utilize amortization-based meta-learning, which\nsubstitutes an otherwise required optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. In addition, we show how MAC can be combined with and improve the\nperformance of popular alternatives such as retrieval augmented generations\n(RAGs). Code is available at: https://github.com/jihoontack/MAC.\n","authors":["Jihoon Tack","Jaehyung Kim","Eric Mitchell","Jinwoo Shin","Yee Whye Teh","Jonathan Richard Schwarz"],"pdf_url":"https://arxiv.org/pdf/2403.04317v2.pdf","comment":"Published as a conference proceeding for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02058v1","updated":"2024-11-04T13:01:13Z","published":"2024-11-04T13:01:13Z","title":"Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional\n  Trajectories Through Manifold Learning","summary":"  A data-driven approach based on unsupervised machine learning is proposed to\ninfer the intrinsic dimensions $m^{\\ast}$ of the high-dimensional trajectories\nof the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis\n(PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints,\nof the FPUT $\\beta$ model with $N = 32$ coupled oscillators, revealing a\ncritical relationship between $m^{\\ast}$ and the model's nonlinear strength.\nFor weak nonlinearities, $m^{\\ast} \\ll n$, where $n = 2N$. In contrast, for\nstrong nonlinearities, $m^{\\ast} \\rightarrow n - 1$, consistently with the\nergodic hypothesis. Furthermore, one of the potential limitations of PCA is\naddressed through an analysis with t-distributed stochastic neighbor embedding\n($t$-SNE). Accordingly, we found strong evidence suggesting that the datapoints\nlie near or on a curved low-dimensional manifold for weak nonlinearities.\n","authors":["Gionni Marchetti"],"pdf_url":"https://arxiv.org/pdf/2411.02058v1.pdf","comment":"20 pages, 18 figures"},{"id":"http://arxiv.org/abs/2411.02051v1","updated":"2024-11-04T12:56:35Z","published":"2024-11-04T12:56:35Z","title":"R+R:Understanding Hyperparameter Effects in DP-SGD","summary":"  Research on the effects of essential hyperparameters of DP-SGD lacks\nconsensus, verification, and replication. Contradictory and anecdotal\nstatements on their influence make matters worse. While DP-SGD is the standard\noptimization algorithm for privacy-preserving machine learning, its adoption is\nstill commonly challenged by low performance compared to non-private learning\napproaches. As proper hyperparameter settings can improve the privacy-utility\ntrade-off, understanding the influence of the hyperparameters promises to\nsimplify their optimization towards better performance, and likely foster\nacceptance of private learning. To shed more light on these influences, we\nconduct a replication study: We synthesize extant research on hyperparameter\ninfluences of DP-SGD into conjectures, conduct a dedicated factorial study to\nindependently identify hyperparameter effects, and assess which conjectures can\nbe replicated across multiple datasets, model architectures, and differential\nprivacy budgets. While we cannot (consistently) replicate conjectures about the\nmain and interaction effects of the batch size and the number of epochs, we\nwere able to replicate the conjectured relationship between the clipping\nthreshold and learning rate. Furthermore, we were able to quantify the\nsignificant importance of their combination compared to the other\nhyperparameters.\n","authors":["Felix Morsbach","Jan Reubold","Thorsten Strufe"],"pdf_url":"https://arxiv.org/pdf/2411.02051v1.pdf","comment":"Accepted at the 40th Annual Computer Security Applications Conference\n  (ACSAC 24)"},{"id":"http://arxiv.org/abs/2411.02047v1","updated":"2024-11-04T12:51:57Z","published":"2024-11-04T12:51:57Z","title":"Theory-inspired Label Shift Adaptation via Aligned Distribution Mixture","summary":"  As a prominent challenge in addressing real-world issues within a dynamic\nenvironment, label shift, which refers to the learning setting where the source\n(training) and target (testing) label distributions do not match, has recently\nreceived increasing attention. Existing label shift methods solely use\nunlabeled target samples to estimate the target label distribution, and do not\ninvolve them during the classifier training, resulting in suboptimal\nutilization of available information. One common solution is to directly blend\nthe source and target distributions during the training of the target\nclassifier. However, we illustrate the theoretical deviation and limitations of\nthe direct distribution mixture in the label shift setting. To tackle this\ncrucial yet unexplored issue, we introduce the concept of aligned distribution\nmixture, showcasing its theoretical optimality and generalization error bounds.\nBy incorporating insights from generalization theory, we propose an innovative\nlabel shift framework named as Aligned Distribution Mixture (ADM). Within this\nframework, we enhance four typical label shift methods by introducing\nmodifications to the classifier training process. Furthermore, we also propose\na one-step approach that incorporates a pioneering coupling weight estimation\nstrategy. Considering the distinctiveness of the proposed one-step approach, we\ndevelop an efficient bi-level optimization strategy. Experimental results\ndemonstrate the effectiveness of our approaches, together with their\neffectiveness in COVID-19 diagnosis applications.\n","authors":["Ruidong Fan","Xiao Ouyang","Hong Tao","Yuhua Qian","Chenping Hou"],"pdf_url":"https://arxiv.org/pdf/2411.02047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02038v1","updated":"2024-11-04T12:40:18Z","published":"2024-11-04T12:40:18Z","title":"Addressing Representation Collapse in Vector Quantized Models with One\n  Linear Layer","summary":"  Vector Quantization (VQ) is a widely used method for converting continuous\nrepresentations into discrete codes, which has become fundamental in\nunsupervised representation learning and latent generative models. However, VQ\nmodels are often hindered by the problem of representation collapse in the\nlatent space, which leads to low codebook utilization and limits the\nscalability of the codebook for large-scale training. Existing methods designed\nto mitigate representation collapse typically reduce the dimensionality of\nlatent space at the expense of model capacity, which do not fully resolve the\ncore issue. In this study, we conduct a theoretical analysis of representation\ncollapse in VQ models and identify its primary cause as the disjoint\noptimization of the codebook, where only a small subset of code vectors are\nupdated through gradient descent. To address this issue, we propose\n\\textbf{SimVQ}, a novel method which reparameterizes the code vectors through a\nlinear transformation layer based on a learnable latent basis. This\ntransformation optimizes the \\textit{entire linear space} spanned by the\ncodebook, rather than merely updating \\textit{the code vector} selected by the\nnearest-neighbor search in vanilla VQ models. Although it is commonly\nunderstood that the multiplication of two linear matrices is equivalent to\napplying a single linear layer, our approach works surprisingly well in\nresolving the collapse issue in VQ models with just one linear layer. We\nvalidate the efficacy of SimVQ through extensive experiments across various\nmodalities, including image and audio data with different model architectures.\nOur code is available at \\url{https://github.com/youngsheen/SimVQ}.\n","authors":["Yongxin Zhu","Bocheng Li","Yifei Xin","Linli Xu"],"pdf_url":"https://arxiv.org/pdf/2411.02038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02023v1","updated":"2024-11-04T12:20:13Z","published":"2024-11-04T12:20:13Z","title":"Optimal Classification under Performative Distribution Shift","summary":"  Performative learning addresses the increasingly pervasive situations in\nwhich algorithmic decisions may induce changes in the data distribution as a\nconsequence of their public deployment. We propose a novel view in which these\nperformative effects are modelled as push-forward measures. This general\nframework encompasses existing models and enables novel performative gradient\nestimation methods, leading to more efficient and scalable learning strategies.\nFor distribution shifts, unlike previous models which require full\nspecification of the data distribution, we only assume knowledge of the shift\noperator that represents the performative changes. This approach can also be\nintegrated into various change-of-variablebased models, such as VAEs or\nnormalizing flows. Focusing on classification with a linear-in-parameters\nperformative effect, we prove the convexity of the performative risk under a\nnew set of assumptions. Notably, we do not limit the strength of performative\neffects but rather their direction, requiring only that classification becomes\nharder when deploying more accurate models. In this case, we also establish a\nconnection with adversarially robust classification by reformulating the\nminimization of the performative risk as a min-max variational problem.\nFinally, we illustrate our approach on synthetic and real datasets.\n","authors":["Edwige Cyffers","Muni Sreenivas Pydi","Jamal Atif","Olivier Cappé"],"pdf_url":"https://arxiv.org/pdf/2411.02023v1.pdf","comment":"38th Conference on Neural Information Processing Systems, Dec 2024,\n  Vancouver (Canada), Canada"},{"id":"http://arxiv.org/abs/2411.02019v1","updated":"2024-11-04T12:14:35Z","published":"2024-11-04T12:14:35Z","title":"Modulating State Space Model with SlowFast Framework for\n  Compute-Efficient Ultra Low-Latency Speech Enhancement","summary":"  Deep learning-based speech enhancement (SE) methods often face significant\ncomputational challenges when needing to meet low-latency requirements because\nof the increased number of frames to be processed. This paper introduces the\nSlowFast framework which aims to reduce computation costs specifically when\nlow-latency enhancement is needed. The framework consists of a slow branch that\nanalyzes the acoustic environment at a low frame rate, and a fast branch that\nperforms SE in the time domain at the needed higher frame rate to match the\nrequired latency. Specifically, the fast branch employs a state space model\nwhere its state transition process is dynamically modulated by the slow branch.\nExperiments on a SE task with a 2 ms algorithmic latency requirement using the\nVoice Bank + Demand dataset show that our approach reduces computation cost by\n70% compared to a baseline single-branch network with equivalent parameters,\nwithout compromising enhancement performance. Furthermore, by leveraging the\nSlowFast framework, we implemented a network that achieves an algorithmic\nlatency of just 60 {\\mu}s (one sample point at 16 kHz sample rate) with a\ncomputation cost of 100 M MACs/s, while scoring a PESQ-NB of 3.12 and SISNR of\n16.62.\n","authors":["Longbiao Cheng","Ashutosh Pandey","Buye Xu","Tobi Delbruck","Vamsi Krishna Ithapu","Shih-Chii Liu"],"pdf_url":"https://arxiv.org/pdf/2411.02019v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.16998v2","updated":"2024-11-04T11:44:29Z","published":"2024-09-25T15:03:22Z","title":"PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in\n  Endoscopic Pituitary Surgery","summary":"  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow\nfor anaesthetists to more accurately decide when to administer anaesthetic\nagents and drugs, as well as to notify hospital staff to send in the next\npatient. Therefore RSD plays an important role in improving patient care and\nminimising surgical theatre costs via efficient scheduling. In endoscopic\npituitary surgery, it is uniquely challenging due to variable workflow\nsequences with a selection of optional steps contributing to high variability\nin surgery duration. This paper presents PitRSDNet for predicting RSD during\npituitary surgery, a spatio-temporal neural network model that learns from\nhistorical data focusing on workflow sequences. PitRSDNet integrates workflow\nknowledge into RSD prediction in two forms: 1) multi-task learning for\nconcurrently predicting step and RSD; and 2) incorporating prior steps as\ncontext in temporal learning and inference. PitRSDNet is trained and evaluated\non a new endoscopic pituitary surgery dataset with 88 videos to show\ncompetitive performance improvements over previous statistical and machine\nlearning methods. The findings also highlight how PitRSDNet improve RSD\nprecision on outlier cases utilising the knowledge of prior steps.\n","authors":["Anjana Wijekoon","Adrito Das","Roxana R. Herrera","Danyal Z. Khan","John Hanrahan","Eleanor Carter","Valpuri Luoma","Danail Stoyanov","Hani J. Marcus","Sophia Bano"],"pdf_url":"https://arxiv.org/pdf/2409.16998v2.pdf","comment":"Accepted to the Augmented Environments for Computer-Assisted\n  Interventions (AE-CAI) Workshop at the Medical Image Computing and\n  Computer-Assisted Interventions (MICCAI) Conference 2024"},{"id":"http://arxiv.org/abs/2402.06922v2","updated":"2024-11-04T11:42:26Z","published":"2024-02-10T11:07:24Z","title":"Whispers in the Machine: Confidentiality in LLM-integrated Systems","summary":"  Large Language Models (LLMs) are increasingly augmented with external tools\nand commercial services into LLM-integrated systems. While these interfaces can\nsignificantly enhance the capabilities of the models, they also introduce a new\nattack surface. Manipulated integrations, for example, can exploit the model\nand compromise sensitive data accessed through other interfaces. While previous\nwork primarily focused on attacks targeting a model's alignment or the leakage\nof training data, the security of data that is only available during inference\nhas escaped scrutiny so far. In this work, we demonstrate the vulnerabilities\nassociated with external components and introduce a systematic approach to\nevaluate confidentiality risks in LLM-integrated systems. We identify two\nspecific attack scenarios unique to these systems and formalize these into a\ntool-robustness framework designed to measure a model's ability to protect\nsensitive information. Our findings show that all examined models are highly\nvulnerable to confidentiality attacks, with the risk increasing significantly\nwhen models are used together with external tools.\n","authors":["Jonathan Evertz","Merlin Chlosta","Lea Schönherr","Thorsten Eisenhofer"],"pdf_url":"https://arxiv.org/pdf/2402.06922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02003v1","updated":"2024-11-04T11:42:25Z","published":"2024-11-04T11:42:25Z","title":"Against Multifaceted Graph Heterogeneity via Asymmetric Federated Prompt\n  Learning","summary":"  Federated Graph Learning (FGL) aims to collaboratively and privately optimize\ngraph models on divergent data for different tasks. A critical challenge in FGL\nis to enable effective yet efficient federated optimization against\nmultifaceted graph heterogeneity to enhance mutual performance. However,\nexisting FGL works primarily address graph data heterogeneity and perform\nincapable of graph task heterogeneity. To address the challenge, we propose a\nFederated Graph Prompt Learning (FedGPL) framework to efficiently enable\nprompt-based asymmetric graph knowledge transfer between multifaceted\nheterogeneous federated participants. Generally, we establish a split federated\nframework to preserve universal and domain-specific graph knowledge,\nrespectively. Moreover, we develop two algorithms to eliminate task and data\nheterogeneity for advanced federated knowledge preservation. First, a\nHierarchical Directed Transfer Aggregator (HiDTA) delivers cross-task\nbeneficial knowledge that is hierarchically distilled according to the\ndirectional transferability. Second, a Virtual Prompt Graph (VPG) adaptively\ngenerates graph structures to enhance data utility by distinguishing dominant\nsubgraphs and neutralizing redundant ones. We conduct theoretical analyses and\nextensive experiments to demonstrate the significant accuracy and efficiency\neffectiveness of FedGPL against multifaceted graph heterogeneity compared to\nstate-of-the-art baselines on large-scale federated graph datasets.\n","authors":["Zhuoning Guo","Ruiqian Han","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2411.02003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02001v1","updated":"2024-11-04T11:38:27Z","published":"2024-11-04T11:38:27Z","title":"Local Loss Optimization in the Infinite Width: Stable Parameterization\n  of Predictive Coding Networks and Target Propagation","summary":"  Local learning, which trains a network through layer-wise local targets and\nlosses, has been studied as an alternative to backpropagation (BP) in neural\ncomputation. However, its algorithms often become more complex or require\nadditional hyperparameters because of the locality, making it challenging to\nidentify desirable settings in which the algorithm progresses in a stable\nmanner. To provide theoretical and quantitative insights, we introduce the\nmaximal update parameterization ($\\mu$P) in the infinite-width limit for two\nrepresentative designs of local targets: predictive coding (PC) and target\npropagation (TP). We verified that $\\mu$P enables hyperparameter transfer\nacross models of different widths. Furthermore, our analysis revealed unique\nand intriguing properties of $\\mu$P that are not present in conventional BP. By\nanalyzing deep linear networks, we found that PC's gradients interpolate\nbetween first-order and Gauss-Newton-like gradients, depending on the\nparameterization. We demonstrate that, in specific standard settings, PC in the\ninfinite-width limit behaves more similarly to the first-order gradient. For\nTP, even with the standard scaling of the last layer, which differs from\nclassical $\\mu$P, its local loss optimization favors the feature learning\nregime over the kernel regime.\n","authors":["Satoki Ishikawa","Rio Yokota","Ryo Karakida"],"pdf_url":"https://arxiv.org/pdf/2411.02001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01996v1","updated":"2024-11-04T11:31:18Z","published":"2024-11-04T11:31:18Z","title":"Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task","summary":"  The advent of Large Language Models (LLMs) have shown promise in various\ncreative domains, including culinary arts. However, many LLMs still struggle to\ndeliver the desired level of culinary creativity, especially when tasked with\nadapting recipes to meet specific cultural requirements. This study focuses on\ncuisine transfer-applying elements of one cuisine to another-to assess LLMs'\nculinary creativity. We employ a diverse set of LLMs to generate and evaluate\nculturally adapted recipes, comparing their evaluations against LLM and human\njudgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark\nto evaluate LLMs' recipe generation abilities in the cuisine transfer task,\nassessing their cultural accuracy and creativity in the culinary domain. Our\nfindings reveal crucial insights into both generative and evaluative\ncapabilities of LLMs in the culinary domain, highlighting strengths and\nlimitations in understanding and applying cultural nuances in recipe creation.\nThe code and dataset used in this project will be openly available in\n\\url{http://github.com/dmis-lab/CulinaryASH}.\n","authors":["Hoonick Lee","Mogan Gim","Donghyeon Park","Donghee Choi","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2411.01996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01992v1","updated":"2024-11-04T11:26:38Z","published":"2024-11-04T11:26:38Z","title":"Ask, and it shall be given: Turing completeness of prompting","summary":"  Since the success of GPT, large language models (LLMs) have been\nrevolutionizing machine learning and have initiated the so-called LLM prompting\nparadigm. In the era of LLMs, people train a single general-purpose LLM and\nprovide the LLM with different prompts to perform different tasks. However,\nsuch empirical success largely lacks theoretical understanding. Here, we\npresent the first theoretical study on the LLM prompting paradigm to the best\nof our knowledge. In this work, we show that prompting is in fact\nTuring-complete: there exists a finite-size Transformer such that for any\ncomputable function, there exists a corresponding prompt following which the\nTransformer computes the function. Furthermore, we show that even though we use\nonly a single finite-size Transformer, it can still achieve nearly the same\ncomplexity bounds as that of the class of all unbounded-size Transformers.\nOverall, our result reveals that prompting can enable a single finite-size\nTransformer to be efficiently universal, which establishes a theoretical\nunderpinning for prompt engineering in practice.\n","authors":["Ruizhong Qiu","Zhe Xu","Wenxuan Bao","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2411.01992v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2311.10671v3","updated":"2024-11-04T11:24:51Z","published":"2023-11-17T17:43:11Z","title":"Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based\n  Inference","summary":"  We present multimodal neural posterior estimation (MultiNPE), a method to\nintegrate heterogeneous data from different sources in simulation-based\ninference with neural networks. Inspired by advances in deep fusion, it allows\nresearchers to analyze data from different domains and infer the parameters of\ncomplex mathematical models with increased accuracy. We consider three fusion\napproaches for MultiNPE (early, late, hybrid) and evaluate their performance in\nthree challenging experiments. MultiNPE not only outperforms single-source\nbaselines on a reference task, but also achieves superior inference on\nscientific models from cognitive neuroscience and cardiology. We systematically\ninvestigate the impact of partially missing data on the different fusion\nstrategies. Across our experiments, late and hybrid fusion techniques emerge as\nthe methods of choice for practical applications of multimodal simulation-based\ninference.\n","authors":["Marvin Schmitt","Leona Odole","Stefan T. Radev","Paul-Christian Bürkner"],"pdf_url":"https://arxiv.org/pdf/2311.10671v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17687v2","updated":"2024-11-04T11:23:46Z","published":"2024-09-26T09:51:29Z","title":"Graph Edit Distance with General Costs Using Neural Set Divergence","summary":"  Graph Edit Distance (GED) measures the (dis-)similarity between two given\ngraphs, in terms of the minimum-cost edit sequence that transforms one graph to\nthe other. However, the exact computation of GED is NP-Hard, which has recently\nmotivated the design of neural methods for GED estimation. However, they do not\nexplicitly account for edit operations with different costs. In response, we\npropose GRAPHEDX, a neural GED estimator that can work with general costs\nspecified for the four edit operations, viz., edge deletion, edge addition,\nnode deletion and node addition. We first present GED as a quadratic assignment\nproblem (QAP) that incorporates these four costs. Then, we represent each graph\nas a set of node and edge embeddings and use them to design a family of neural\nset divergence surrogates. We replace the QAP terms corresponding to each\noperation with their surrogates. Computing such neural set divergence require\naligning nodes and edges of the two graphs. We learn these alignments using a\nGumbel-Sinkhorn permutation generator, additionally ensuring that the node and\nedge alignments are consistent with each other. Moreover, these alignments are\ncognizant of both the presence and absence of edges between node-pairs.\nExperiments on several datasets, under a variety of edit cost settings, show\nthat GRAPHEDX consistently outperforms state-of-the-art methods and heuristics\nin terms of prediction error.\n","authors":["Eeshaan Jain","Indradyumna Roy","Saswat Meher","Soumen Chakrabarti","Abir De"],"pdf_url":"https://arxiv.org/pdf/2409.17687v2.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.18137v2","updated":"2024-11-04T11:16:38Z","published":"2024-05-28T12:51:01Z","title":"Exploiting LLM Quantization","summary":"  Quantization leverages lower-precision weights to reduce the memory usage of\nlarge language models (LLMs) and is a key technique for enabling their\ndeployment on commodity hardware. While LLM quantization's impact on utility\nhas been extensively explored, this work for the first time studies its adverse\neffects from a security perspective. We reveal that widely used quantization\nmethods can be exploited to produce a harmful quantized LLM, even though the\nfull-precision counterpart appears benign, potentially tricking users into\ndeploying the malicious quantized model. We demonstrate this threat using a\nthree-staged attack framework: (i) first, we obtain a malicious LLM through\nfine-tuning on an adversarial task; (ii) next, we quantize the malicious model\nand calculate constraints that characterize all full-precision models that map\nto the same quantized model; (iii) finally, using projected gradient descent,\nwe tune out the poisoned behavior from the full-precision model while ensuring\nthat its weights satisfy the constraints computed in step (ii). This procedure\nresults in an LLM that exhibits benign behavior in full precision but when\nquantized, it follows the adversarial behavior injected in step (i). We\nexperimentally demonstrate the feasibility and severity of such an attack\nacross three diverse scenarios: vulnerable code generation, content injection,\nand over-refusal attack. In practice, the adversary could host the resulting\nfull-precision model on an LLM community hub such as Hugging Face, exposing\nmillions of users to the threat of deploying its malicious quantized version on\ntheir devices.\n","authors":["Kazuki Egashira","Mark Vero","Robin Staab","Jingxuan He","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2405.18137v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07099v2","updated":"2024-11-04T11:15:28Z","published":"2024-06-18T07:46:13Z","title":"Nash CoT: Multi-Path Inference with Preference Equilibrium","summary":"  Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na competitive system on each path that balances the generation from\nrole-specific LLMs' and the general LLMs' generation, thereby ensuring both\neffective role adoption and diversity in LLM generation further maintaining the\nperformance of multi-path inference while reducing the requirement of the\nnumber of inference paths. We evaluate Nash CoT across various inference tasks,\nincluding Arabic Reasoning, Commonsense Question Answering, and Symbolic\nInference, achieving results that are comparable to or better than those of\nmulti-path CoT with the equal number of inference paths.\n","authors":["Ziqi Zhang","Cunxiang Wang","Xiong Xiao","Yue Zhang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05989v3","updated":"2024-11-04T11:14:34Z","published":"2023-06-09T15:59:27Z","title":"QBSD: Quartile-Based Seasonality Decomposition for Cost-Effective RAN\n  KPI Forecasting","summary":"  Forecasting time series patterns, such as cell key performance indicators\n(KPIs) of radio access networks (RAN), plays a vital role in enhancing service\nquality and operational efficiency. State-of-the-art forecasting approaches\nprioritize accuracy at the expense of computational performance, rendering them\nless suitable for data-intensive applications encompassing systems with a\nmultitude of time series variables. They also do not capture the effect of\ndynamic operating ranges that vary with time. To address this issue, we\nintroduce QBSD, a live single-step forecasting approach tailored to optimize\nthe trade-off between accuracy and computational complexity. The method has\nshown significant success with our real network RAN KPI datasets of over\nseveral thousand cells. In this article, we showcase the performance of QBSD in\ncomparison to other forecasting approaches on a dataset we have made publicly\navailable. The results demonstrate that the proposed method excels in runtime\nefficiency compared to the leading algorithms available while maintaining\ncompetitive forecast accuracy that rivals neural forecasting methods.\n","authors":["Ebenezer RHP Isaac","Bulbul Singh"],"pdf_url":"https://arxiv.org/pdf/2306.05989v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10618v2","updated":"2024-11-04T11:11:49Z","published":"2024-04-16T14:42:49Z","title":"Private Attribute Inference from Images with Vision-Language Models","summary":"  As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that LLMs can make accurate privacy-infringing\ninferences from previously unseen texts. With the rise of vision-language\nmodels (VLMs), capable of understanding both images and text, a key question is\nwhether this concern transfers to the previously unexplored domain of benign\nimages posted online. To answer this question, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the privacy risks posed by VLMs beyond traditional human attribute\nrecognition, our dataset consists of images where the inferable private\nattributes do not stem from direct depictions of humans. On this dataset, we\nevaluate 7 state-of-the-art VLMs, finding that they can infer various personal\nattributes at up to 77.6% accuracy. Concerningly, we observe that accuracy\nscales with the general capabilities of the models, implying that future models\ncan be misused as stronger inferential adversaries, establishing an imperative\nfor the development of adequate defenses.\n","authors":["Batuhan Tömekçe","Mark Vero","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2404.10618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01982v1","updated":"2024-11-04T11:09:58Z","published":"2024-11-04T11:09:58Z","title":"Learning Controlled Stochastic Differential Equations","summary":"  Identification of nonlinear dynamical systems is crucial across various\nfields, facilitating tasks such as control, prediction, optimization, and fault\ndetection. Many applications require methods capable of handling complex\nsystems while providing strong learning guarantees for safe and reliable\nperformance. However, existing approaches often focus on simplified scenarios,\nsuch as deterministic models, known diffusion, discrete systems,\none-dimensional dynamics, or systems constrained by strong structural\nassumptions such as linearity. This work proposes a novel method for estimating\nboth drift and diffusion coefficients of continuous, multidimensional,\nnonlinear controlled stochastic differential equations with non-uniform\ndiffusion. We assume regularity of the coefficients within a Sobolev space,\nallowing for broad applicability to various dynamical systems in robotics,\nfinance, climate modeling, and biology. Leveraging the Fokker-Planck equation,\nwe split the estimation into two tasks: (a) estimating system dynamics for a\nfinite set of controls, and (b) estimating coefficients that govern those\ndynamics. We provide strong theoretical guarantees, including finite-sample\nbounds for \\(L^2\\), \\(L^\\infty\\), and risk metrics, with learning rates\nadaptive to coefficients' regularity, similar to those in nonparametric\nleast-squares regression literature. The practical effectiveness of our\napproach is demonstrated through extensive numerical experiments. Our method is\navailable as an open-source Python library.\n","authors":["Luc Brogat-Motte","Riccardo Bonalli","Alessandro Rudi"],"pdf_url":"https://arxiv.org/pdf/2411.01982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07217v2","updated":"2024-11-04T11:06:40Z","published":"2024-06-11T12:50:53Z","title":"A Synthetic Dataset for Personal Attribute Inference","summary":"  Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users world-wide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose -- the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. We take two steps to\naddress this problem: (i) we construct a simulation framework for the popular\nsocial media platform Reddit using LLM agents seeded with synthetic personal\nprofiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic\ndataset of over 7800 comments manually labeled for personal attributes. We\nvalidate our dataset with a human study showing that humans barely outperform\nrandom guessing on the task of distinguishing our synthetic comments from real\nones. Further, we verify that our dataset enables meaningful personal attribute\ninference research by showing across 18 state-of-the-art LLMs that our\nsynthetic comments allow us to draw the same conclusions as real-world data.\nCombined, our experimental results, dataset and pipeline form a strong basis\nfor future privacy-preserving research geared towards understanding and\nmitigating inference-based privacy threats that LLMs pose.\n","authors":["Hanna Yukhymenko","Robin Staab","Mark Vero","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2406.07217v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05440v3","updated":"2024-11-04T11:03:30Z","published":"2023-12-09T02:14:12Z","title":"Consistency Models for Scalable and Fast Simulation-Based Inference","summary":"  Simulation-based inference (SBI) is constantly in search of more expressive\nand efficient algorithms to accurately infer the parameters of complex\nsimulation models. In line with this goal, we present consistency models for\nposterior estimation (CMPE), a new conditional sampler for SBI that inherits\nthe advantages of recent unconstrained architectures and overcomes their\nsampling inefficiency at inference time. CMPE essentially distills a continuous\nprobability flow and enables rapid few-shot inference with an unconstrained\narchitecture that can be flexibly tailored to the structure of the estimation\nproblem. We provide hyperparameters and default architectures that support\nconsistency training over a wide range of different dimensions, including\nlow-dimensional ones which are important in SBI workflows but were previously\ndifficult to tackle even with unconditional consistency models. Our empirical\nevaluation demonstrates that CMPE not only outperforms current state-of-the-art\nalgorithms on hard low-dimensional benchmarks, but also achieves competitive\nperformance with much faster sampling speed on two realistic estimation\nproblems with high data and/or parameter dimensions.\n","authors":["Marvin Schmitt","Valentin Pratz","Ullrich Köthe","Paul-Christian Bürkner","Stefan T Radev"],"pdf_url":"https://arxiv.org/pdf/2312.05440v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01978v1","updated":"2024-11-04T10:58:41Z","published":"2024-11-04T10:58:41Z","title":"Understanding Variational Autoencoders with Intrinsic Dimension and\n  Information Imbalance","summary":"  This work presents an analysis of the hidden representations of Variational\nAutoencoders (VAEs) using the Intrinsic Dimension (ID) and the Information\nImbalance (II). We show that VAEs undergo a transition in behaviour once the\nbottleneck size is larger than the ID of the data, manifesting in a double\nhunchback ID profile and a qualitative shift in information processing as\ncaptured by the II. Our results also highlight two distinct training phases for\narchitectures with sufficiently large bottleneck sizes, consisting of a rapid\nfit and a slower generalisation, as assessed by a differentiated behaviour of\nID, II, and KL loss. These insights demonstrate that II and ID could be\nvaluable tools for aiding architecture search, for diagnosing underfitting in\nVAEs, and, more broadly, they contribute to advancing a unified understanding\nof deep generative models through geometric analysis.\n","authors":["Charles Camboulin","Diego Doimo","Aldo Glielmo"],"pdf_url":"https://arxiv.org/pdf/2411.01978v1.pdf","comment":"4 pages, 3 figures, accepted at the Unifying Representations in\n  Neural Models (UniReps) workshop of NeurIPS 2024 (https://unireps.org/2024/)"},{"id":"http://arxiv.org/abs/2411.01974v1","updated":"2024-11-04T10:50:37Z","published":"2024-11-04T10:50:37Z","title":"On the phase diagram of extensive-rank symmetric matrix denoising beyond\n  rotational invariance","summary":"  Matrix denoising is central to signal processing and machine learning. Its\nanalysis when the matrix to infer has a factorised structure with a rank\ngrowing proportionally to its dimension remains a challenge, except when it is\nrotationally invariant. In this case the information theoretic limits and a\nBayes-optimal denoising algorithm, called rotational invariant estimator [1,2],\nare known. Beyond this setting few results can be found. The reason is that the\nmodel is not a usual spin system because of the growing rank dimension, nor a\nmatrix model due to the lack of rotation symmetry, but rather a hybrid between\nthe two. In this paper we make progress towards the understanding of Bayesian\nmatrix denoising when the hidden signal is a factored matrix $XX^\\intercal$\nthat is not rotationally invariant. Monte Carlo simulations suggest the\nexistence of a denoising-factorisation transition separating a phase where\ndenoising using the rotational invariant estimator remains Bayes-optimal due to\nuniversality properties of the same nature as in random matrix theory, from one\nwhere universality breaks down and better denoising is possible by exploiting\nthe signal's prior and factorised structure, though algorithmically hard. We\nalso argue that it is only beyond the transition that factorisation, i.e.,\nestimating $X$ itself, becomes possible up to sign and permutation ambiguities.\nOn the theoretical side, we combine mean-field techniques in an interpretable\nmultiscale fashion in order to access the minimum mean-square error and mutual\ninformation. Interestingly, our alternative method yields equations which can\nbe reproduced using the replica approach of [3]. Using numerical insights, we\nthen delimit the portion of the phase diagram where this mean-field theory is\nreliable, and correct it using universality when it is not. Our ansatz matches\nwell the numerics when accounting for finite size effects.\n","authors":["Jean Barbier","Francesco Camilli","Justin Ko","Koki Okajima"],"pdf_url":"https://arxiv.org/pdf/2411.01974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01973v1","updated":"2024-11-04T10:50:03Z","published":"2024-11-04T10:50:03Z","title":"The Certainty Ratio $C_ρ$: a novel metric for assessing the\n  reliability of classifier predictions","summary":"  Evaluating the performance of classifiers is critical in machine learning,\nparticularly in high-stakes applications where the reliability of predictions\ncan significantly impact decision-making. Traditional performance measures,\nsuch as accuracy and F-score, often fail to account for the uncertainty\ninherent in classifier predictions, leading to potentially misleading\nassessments. This paper introduces the Certainty Ratio ($C_\\rho$), a novel\nmetric designed to quantify the contribution of confident (certain) versus\nuncertain predictions to any classification performance measure. By integrating\nthe Probabilistic Confusion Matrix ($CM^\\star$) and decomposing predictions\ninto certainty and uncertainty components, $C_\\rho$ provides a more\ncomprehensive evaluation of classifier reliability. Experimental results across\n26 datasets and multiple classifiers, including Decision Trees, Naive-Bayes,\n3-Nearest Neighbors, and Random Forests, demonstrate that $C_\\rho$ reveals\ncritical insights that conventional metrics often overlook. These findings\nemphasize the importance of incorporating probabilistic information into\nclassifier evaluation, offering a robust tool for researchers and practitioners\nseeking to improve model trustworthiness in complex environments.\n","authors":["Jesus S. Aguilar-Ruiz"],"pdf_url":"https://arxiv.org/pdf/2411.01973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16749v2","updated":"2024-11-04T10:42:47Z","published":"2024-06-24T15:57:49Z","title":"Inferring stochastic low-rank recurrent neural networks from neural data","summary":"  A central aim in computational neuroscience is to relate the activity of\nlarge populations of neurons to an underlying dynamical system. Models of these\nneural dynamics should ideally be both interpretable and fit the observed data\nwell. Low-rank recurrent neural networks (RNNs) exhibit such interpretability\nby having tractable dynamics. However, it is unclear how to best fit low-rank\nRNNs to data consisting of noisy observations of an underlying stochastic\nsystem. Here, we propose to fit stochastic low-rank RNNs with variational\nsequential Monte Carlo methods. We validate our method on several datasets\nconsisting of both continuous and spiking neural data, where we obtain lower\ndimensional latent dynamics than current state of the art methods.\nAdditionally, for low-rank models with piecewise linear nonlinearities, we show\nhow to efficiently identify all fixed points in polynomial rather than\nexponential cost in the number of units, making analysis of the inferred\ndynamics tractable for large RNNs. Our method both elucidates the dynamical\nsystems underlying experimental recordings and provides a generative model\nwhose trajectories match observed variability.\n","authors":["Matthijs Pals","A Erdem Sağtekin","Felix Pei","Manuel Gloeckler","Jakob H Macke"],"pdf_url":"https://arxiv.org/pdf/2406.16749v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01966v1","updated":"2024-11-04T10:42:21Z","published":"2024-11-04T10:42:21Z","title":"UnSegMedGAT: Unsupervised Medical Image Segmentation using Graph\n  Attention Networks Clustering","summary":"  The data-intensive nature of supervised classification drives the interest of\nthe researchers towards unsupervised approaches, especially for problems such\nas medical image segmentation, where labeled data is scarce. Building on the\nrecent advancements of Vision transformers (ViT) in computer vision, we propose\nan unsupervised segmentation framework using a pre-trained Dino-ViT. In the\nproposed method, we leverage the inherent graph structure within the image to\nrealize a significant performance gain for segmentation in medical images. For\nthis, we introduce a modularity-based loss function coupled with a Graph\nAttention Network (GAT) to effectively capture the inherent graph topology\nwithin the image. Our method achieves state-of-the-art performance, even\nsignificantly surpassing or matching that of existing (semi)supervised\ntechnique such as MedSAM which is a Segment Anything Model in medical images.\nWe demonstrate this using two challenging medical image datasets ISIC-2018 and\nCVC-ColonDB. This work underscores the potential of unsupervised approaches in\nadvancing medical image analysis in scenarios where labeled data is scarce. The\ngithub repository of the code is available on\n[https://github.com/mudit-adityaja/UnSegMedGAT].\n","authors":["A. Mudit Adityaja","Saurabh J. Shigwan","Nitin Kumar"],"pdf_url":"https://arxiv.org/pdf/2411.01966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01958v1","updated":"2024-11-04T10:31:03Z","published":"2024-11-04T10:31:03Z","title":"N-Gram Induction Heads for In-Context RL: Improving Stability and\n  Reducing Data Needs","summary":"  In-context learning allows models like transformers to adapt to new tasks\nfrom a few examples without updating their weights, a desirable trait for\nreinforcement learning (RL). However, existing in-context RL methods, such as\nAlgorithm Distillation (AD), demand large, carefully curated datasets and can\nbe unstable and costly to train due to the transient nature of in-context\nlearning abilities. In this work we integrated the n-gram induction heads into\ntransformers for in-context RL. By incorporating these n-gram attention\npatterns, we significantly reduced the data required for generalization - up to\n27 times fewer transitions in the Key-to-Door environment - and eased the\ntraining process by making models less sensitive to hyperparameters. Our\napproach not only matches but often surpasses the performance of AD,\ndemonstrating the potential of n-gram induction heads to enhance the efficiency\nof in-context RL.\n","authors":["Ilya Zisman","Alexander Nikulin","Andrei Polubarov","Nikita Lyubaykin","Vladislav Kurenkov"],"pdf_url":"https://arxiv.org/pdf/2411.01958v1.pdf","comment":"Accepted at Adaptive Foundation Models Workshop; NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01956v1","updated":"2024-11-04T10:28:38Z","published":"2024-11-04T10:28:38Z","title":"EXAGREE: Towards Explanation Agreement in Explainable Machine Learning","summary":"  Explanations in machine learning are critical for trust, transparency, and\nfairness. Yet, complex disagreements among these explanations limit the\nreliability and applicability of machine learning models, especially in\nhigh-stakes environments. We formalize four fundamental ranking-based\nexplanation disagreement problems and introduce a novel framework, EXplanation\nAGREEment (EXAGREE), to bridge diverse interpretations in explainable machine\nlearning, particularly from stakeholder-centered perspectives. Our approach\nleverages a Rashomon set for attribution predictions and then optimizes within\nthis set to identify Stakeholder-Aligned Explanation Models (SAEMs) that\nminimize disagreement with diverse stakeholder needs while maintaining\npredictive performance. Rigorous empirical analysis on synthetic and real-world\ndatasets demonstrates that EXAGREE reduces explanation disagreement and\nimproves fairness across subgroups in various domains. EXAGREE not only\nprovides researchers with a new direction for studying explanation disagreement\nproblems but also offers data scientists a tool for making better-informed\ndecisions in practical applications.\n","authors":["Sichao Li","Quanling Deng","Amanda S. Barnard"],"pdf_url":"https://arxiv.org/pdf/2411.01956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00543v2","updated":"2024-11-04T10:21:57Z","published":"2024-11-01T12:50:38Z","title":"3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction","summary":"  Determining the 3D orientations of an object in an image, known as\nsingle-image pose estimation, is a crucial task in 3D vision applications.\nExisting methods typically learn 3D rotations parametrized in the spatial\ndomain using Euler angles or quaternions, but these representations often\nintroduce discontinuities and singularities. SO(3)-equivariant networks enable\nthe structured capture of pose patterns with data-efficient learning, but the\nparametrizations in spatial domain are incompatible with their architecture,\nparticularly spherical CNNs, which operate in the frequency domain to enhance\ncomputational efficiency. To overcome these issues, we propose a\nfrequency-domain approach that directly predicts Wigner-D coefficients for 3D\nrotation regression, aligning with the operations of spherical CNNs. Our\nSO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial\nparameterizations, ensuring consistent pose estimation under arbitrary\nrotations. Trained with a frequency-domain regression loss, our method achieves\nstate-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+,\nwith significant improvements in accuracy, robustness, and data efficiency.\n","authors":["Jongmin Lee","Minsu Cho"],"pdf_url":"https://arxiv.org/pdf/2411.00543v2.pdf","comment":"Accepted to NeurIPS 2024, Project webpage at\n  http://cvlab.postech.ac.kr/research/3D_EquiPose"},{"id":"http://arxiv.org/abs/2411.01947v1","updated":"2024-11-04T10:16:59Z","published":"2024-11-04T10:16:59Z","title":"HACD: Harnessing Attribute Semantics and Mesoscopic Structure for\n  Community Detection","summary":"  Community detection plays a pivotal role in uncovering closely connected\nsubgraphs, aiding various real-world applications such as recommendation\nsystems and anomaly detection. With the surge of rich information available for\nentities in real-world networks, the community detection problem in attributed\nnetworks has attracted widespread attention. While previous research has\neffectively leveraged network topology and attribute information for attributed\ncommunity detection, these methods overlook two critical issues: (i) the\nsemantic similarity between node attributes within the community, and (ii) the\ninherent mesoscopic structure, which differs from the pairwise connections of\nthe micro-structure. To address these limitations, we propose HACD, a novel\nattributed community detection model based on heterogeneous graph attention\nnetworks. HACD treats node attributes as another type of node, constructs\nattributed networks into heterogeneous graph structures and employs\nattribute-level attention mechanisms to capture semantic similarity.\nFurthermore, HACD introduces a community membership function to explore\nmesoscopic community structures, enhancing the robustness of detected\ncommunities. Extensive experiments demonstrate the effectiveness and efficiency\nof HACD, outperforming state-of-the-art methods in attributed community\ndetection tasks. Our code is publicly available at\nhttps://github.com/Anniran1/HACD1-wsdm.\n","authors":["Anran Zhang","Xingfen Wang","Yuhan Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.01947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05964v2","updated":"2024-11-04T10:14:22Z","published":"2024-08-12T07:33:11Z","title":"Target Detection of Safety Protective Gear Using the Improved YOLOv5","summary":"  In high-risk railway construction, personal protective equipment monitoring\nis critical but challenging due to small and frequently obstructed targets. We\npropose YOLO-EA, an innovative model that enhances safety measure detection by\nintegrating ECA into its backbone's convolutional layers, improving discernment\nof minuscule objects like hardhats. YOLO-EA further refines target recognition\nunder occlusion by replacing GIoU with EIoU loss. YOLO-EA's effectiveness was\nempirically substantiated using a dataset derived from real-world railway\nconstruction site surveillance footage. It outperforms YOLOv5, achieving 98.9%\nprecision and 94.7% recall, up 2.5% and 0.5% respectively, while maintaining\nreal-time performance at 70.774 fps. This highly efficient and precise YOLO-EA\nholds great promise for practical application in intricate construction\nscenarios, enforcing stringent safety compliance during complex railway\nconstruction projects.\n","authors":["Hao Liu","Xue Qin"],"pdf_url":"https://arxiv.org/pdf/2408.05964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14780v2","updated":"2024-11-04T10:02:49Z","published":"2024-05-23T16:48:06Z","title":"Metric Flow Matching for Smooth Interpolations on the Data Manifold","summary":"  Matching objectives underpin the success of modern generative models and rely\non constructing conditional paths that transform a source distribution into a\ntarget distribution. Despite being a fundamental building block, conditional\npaths have been designed principally under the assumption of Euclidean\ngeometry, resulting in straight interpolations. However, this can be\nparticularly restrictive for tasks such as trajectory inference, where straight\npaths might lie outside the data manifold, thus failing to capture the\nunderlying dynamics giving rise to the observed marginals. In this paper, we\npropose Metric Flow Matching (MFM), a novel simulation-free framework for\nconditional flow matching where interpolants are approximate geodesics learned\nby minimizing the kinetic energy of a data-induced Riemannian metric. This way,\nthe generative model matches vector fields on the data manifold, which\ncorresponds to lower uncertainty and more meaningful interpolations. We\nprescribe general metrics to instantiate MFM, independent of the task, and test\nit on a suite of challenging problems including LiDAR navigation, unpaired\nimage translation, and modeling cellular dynamics. We observe that MFM\noutperforms the Euclidean baselines, particularly achieving SOTA on single-cell\ntrajectory prediction.\n","authors":["Kacper Kapuśniak","Peter Potaptchik","Teodora Reu","Leo Zhang","Alexander Tong","Michael Bronstein","Avishek Joey Bose","Francesco Di Giovanni"],"pdf_url":"https://arxiv.org/pdf/2405.14780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04739v2","updated":"2024-11-04T10:02:09Z","published":"2024-06-07T08:39:40Z","title":"A survey and benchmark of high-dimensional Bayesian optimization of\n  discrete sequences","summary":"  Optimizing discrete black-box functions is key in several domains, e.g.\nprotein engineering and drug design. Due to the lack of gradient information\nand the need for sample efficiency, Bayesian optimization is an ideal candidate\nfor these tasks. Several methods for high-dimensional continuous and\ncategorical Bayesian optimization have been proposed recently. However, our\nsurvey of the field reveals highly heterogeneous experimental set-ups across\nmethods and technical barriers for the replicability and application of\npublished algorithms to real-world tasks. To address these issues, we develop a\nunified framework to test a vast array of high-dimensional Bayesian\noptimization methods and a collection of standardized black-box functions\nrepresenting real-world application domains in chemistry and biology. These two\ncomponents of the benchmark are each supported by flexible, scalable, and\neasily extendable software libraries (poli and poli-baselines), allowing\npractitioners to readily incorporate new optimization objectives or discrete\noptimizers. Project website:\nhttps://machinelearninglifescience.github.io/hdbo_benchmark\n","authors":["Miguel González-Duque","Richard Michael","Simon Bartels","Yevgen Zainchkovskyy","Søren Hauberg","Wouter Boomsma"],"pdf_url":"https://arxiv.org/pdf/2406.04739v2.pdf","comment":"Accepted at the Datasets and Benchmarks track of NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.22392v2","updated":"2024-11-04T09:56:16Z","published":"2024-10-29T17:56:05Z","title":"EfficientNet with Hybrid Attention Mechanisms for Enhanced Breast\n  Histopathology Classification: A Comprehensive Approach","summary":"  Breast cancer histopathology image classification is crucial for early cancer\ndetection, offering the potential to reduce mortality rates through timely\ndiagnosis. This paper introduces a novel approach integrating Hybrid\nEfficientNet models with advanced attention mechanisms, including Convolutional\nBlock Attention Module (CBAM), Self-Attention, and Deformable Attention, to\nenhance feature extraction and focus on critical image regions. We evaluate the\nperformance of our models across multiple magnification scales using publicly\navailable histopathological datasets. Our method achieves significant\nimprovements, with accuracy reaching 98.42% at 400X magnification, surpassing\nseveral state-of-the-art models, including VGG and ResNet architectures. The\nresults are validated using metrics such as accuracy, F1-score, precision, and\nrecall, demonstrating the clinical potential of our model in improving\ndiagnostic accuracy. Furthermore, the proposed method shows increased\ncomputational efficiency, making it suitable for integration into real-time\ndiagnostic workflows.\n","authors":["Naren Sengodan"],"pdf_url":"https://arxiv.org/pdf/2410.22392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01931v1","updated":"2024-11-04T09:53:03Z","published":"2024-11-04T09:53:03Z","title":"Differentially private and decentralized randomized power method","summary":"  The randomized power method has gained significant interest due to its\nsimplicity and efficient handling of large-scale spectral analysis and\nrecommendation tasks. As modern datasets contain sensitive private information,\nwe need to give formal guarantees on the possible privacy leaks caused by this\nmethod. This paper focuses on enhancing privacy preserving variants of the\nmethod. We propose a strategy to reduce the variance of the noise introduced to\nachieve Differential Privacy (DP). We also adapt the method to a decentralized\nframework with a low computational and communication overhead, while preserving\nthe accuracy. We leverage Secure Aggregation (a form of Multi-Party\nComputation) to allow the algorithm to perform computations using data\ndistributed among multiple users or devices, without revealing individual data.\nWe show that it is possible to use a noise scale in the decentralized setting\nthat is similar to the one in the centralized setting. We improve upon existing\nconvergence bounds for both the centralized and decentralized versions. The\nproposed method is especially relevant for decentralized applications such as\ndistributed recommender systems, where privacy concerns are paramount.\n","authors":["Julien Nicolas","César Sabater","Mohamed Maouche","Sonia Ben Mokhtar","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2411.01931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01929v1","updated":"2024-11-04T09:51:10Z","published":"2024-11-04T09:51:10Z","title":"Exploring the Landscape for Generative Sequence Models for Specialized\n  Data Synthesis","summary":"  Artificial Intelligence (AI) research often aims to develop models that\ngeneralize reliably across complex datasets, yet this remains challenging in\nfields where data is scarce, intricate, or inaccessible. This paper introduces\na novel approach leveraging three generative models of varying complexity to\nsynthesize one of the most demanding structured datasets: Malicious Network\nTraffic. Our approach transforms numerical data into text, reframing data\ngeneration as a language modeling task, which enhances data regularization and\nsignificantly improves generalization and the quality of the synthetic data.\nExtensive statistical analyses demonstrate that our method surpasses\nstate-of-the-art generative models in producing high-fidelity synthetic data.\nAdditionally, we conduct a comprehensive study on synthetic data applications,\neffectiveness, and evaluation strategies, offering valuable insights into its\nrole across various domains. Our code and pre-trained models are openly\naccessible at\nhttps://github.com/Moe-Zbeeb/Exploring-the-landscape-for-generative-models-for-specialized-data-generation,\nenabling further exploration and application of our methodology.\n  Index Terms: Data synthesis, machine learning, traffic generation,\nprivacy-preserving data, generative models.\n","authors":["Mohammad Zbeeb","Mohammad Ghorayeb","Mariam Salman"],"pdf_url":"https://arxiv.org/pdf/2411.01929v1.pdf","comment":"25 pages, 7 figures, 3 tables, 1 algorithm. code @\n  https://github.com/Moe-Zbeeb/Exploring-the-landscape-for-generative-models-for-specialized-data-generation.git"},{"id":"http://arxiv.org/abs/2405.16528v4","updated":"2024-11-04T09:50:00Z","published":"2024-05-26T11:29:57Z","title":"LoQT: Low-Rank Adapters for Quantized Pretraining","summary":"  Despite advances using low-rank adapters and quantization, pretraining of\nlarge models on consumer hardware has not been possible without model sharding,\noffloading during training, or per-layer gradient updates. To address these\nlimitations, we propose Low-Rank Adapters for Quantized Training (LoQT), a\nmethod for efficiently training quantized models. LoQT uses gradient-based\ntensor factorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning models. We demonstrate this for\nlanguage modeling and downstream task adaptation, finding that LoQT enables\nefficient training of models up to 7B parameters on a 24GB GPU. We also\ndemonstrate the feasibility of training a 13B model using per-layer gradient\nupdates on the same hardware.\n","authors":["Sebastian Loeschcke","Mads Toftrup","Michael J. Kastoryano","Serge Belongie","Vésteinn Snæbjarnarson"],"pdf_url":"https://arxiv.org/pdf/2405.16528v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07266v2","updated":"2024-11-04T09:48:15Z","published":"2024-04-10T18:00:17Z","title":"Sequential Decision Making with Expert Demonstrations under Unobserved\n  Heterogeneity","summary":"  We study the problem of online sequential decision-making given auxiliary\ndemonstrations from experts who made their decisions based on unobserved\ncontextual information. These demonstrations can be viewed as solving related\nbut slightly different problems than what the learner faces. This setting\narises in many application domains, such as self-driving cars, healthcare, and\nfinance, where expert demonstrations are made using contextual information,\nwhich is not recorded in the data available to the learning agent. We model the\nproblem as zero-shot meta-reinforcement learning with an unknown distribution\nover the unobserved contextual variables and a Bayesian regret minimization\nobjective, where the unobserved variables are encoded as parameters with an\nunknown prior. We propose the Experts-as-Priors algorithm (ExPerior), an\nempirical Bayes approach that utilizes expert data to establish an informative\nprior distribution over the learner's decision-making problem. This prior\ndistribution enables the application of any Bayesian approach for online\ndecision-making, such as posterior sampling. We demonstrate that our strategy\nsurpasses existing behaviour cloning, online, and online-offline baselines for\nmulti-armed bandits, Markov decision processes (MDPs), and partially observable\nMDPs, showcasing the broad reach and utility of ExPerior in using expert\ndemonstrations across different decision-making setups.\n","authors":["Vahid Balazadeh","Keertana Chidambaram","Viet Nguyen","Rahul G. Krishnan","Vasilis Syrgkanis"],"pdf_url":"https://arxiv.org/pdf/2404.07266v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.02334v1","updated":"2024-11-04T17:58:54Z","published":"2024-11-04T17:58:54Z","title":"Diffusion-based Generative Multicasting with Intent-aware Semantic\n  Decomposition","summary":"  Generative diffusion models (GDMs) have recently shown great success in\nsynthesizing multimedia signals with high perceptual quality enabling highly\nefficient semantic communications in future wireless networks. In this paper,\nwe develop an intent-aware generative semantic multicasting framework utilizing\npre-trained diffusion models. In the proposed framework, the transmitter\ndecomposes the source signal to multiple semantic classes based on the\nmulti-user intent, i.e. each user is assumed to be interested in details of\nonly a subset of the semantic classes. The transmitter then sends to each user\nonly its intended classes, and multicasts a highly compressed semantic map to\nall users over shared wireless resources that allows them to locally synthesize\nthe other classes, i.e. non-intended classes, utilizing pre-trained diffusion\nmodels. The signal retrieved at each user is thereby partially reconstructed\nand partially synthesized utilizing the received semantic map. This improves\nutilization of the wireless resources, with better preserving privacy of the\nnon-intended classes. We design a communication/computation-aware scheme for\nper-class adaptation of the communication parameters, such as the transmission\npower and compression rate to minimize the total latency of retrieving signals\nat multiple receivers, tailored to the prevailing channel conditions as well as\nthe users reconstruction/synthesis distortion/perception requirements. The\nsimulation results demonstrate significantly reduced per-user latency compared\nwith non-generative and intent-unaware multicasting benchmarks while\nmaintaining high perceptual quality of the signals retrieved at the users.\n","authors":["Xinkai Liu","Mahdi Boloursaz Mashhadi","Li Qiao","Yi Ma","Rahim Tafazolli","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2411.02334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02236v1","updated":"2024-11-04T16:30:14Z","published":"2024-11-04T16:30:14Z","title":"3D Audio-Visual Segmentation","summary":"  Recognizing the sounding objects in scenes is a longstanding objective in\nembodied AI, with diverse applications in robotics and AR/VR/MR. To that end,\nAudio-Visual Segmentation (AVS), taking as condition an audio signal to\nidentify the masks of the target sounding objects in an input image with\nsynchronous camera and microphone sensors, has been recently advanced. However,\nthis paradigm is still insufficient for real-world operation, as the mapping\nfrom 2D images to 3D scenes is missing. To address this fundamental limitation,\nwe introduce a novel research problem, 3D Audio-Visual Segmentation, extending\nthe existing AVS to the 3D output space. This problem poses more challenges due\nto variations in camera extrinsics, audio scattering, occlusions, and diverse\nacoustics across sounding object categories. To facilitate this research, we\ncreate the very first simulation based benchmark, 3DAVS-S34-O7, providing\nphotorealistic 3D scene environments with grounded spatial audio under\nsingle-instance and multi-instance settings, across 34 scenes and 7 object\ncategories. This is made possible by re-purposing the Habitat simulator to\ngenerate comprehensive annotations of sounding object locations and\ncorresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet,\ncharacterized by integrating the ready-to-use knowledge from pretrained 2D\naudio-visual foundation models synergistically with 3D visual scene\nrepresentation through spatial audio-aware mask alignment and refinement.\nExtensive experiments demonstrate that EchoSegnet can effectively segment\nsounding objects in 3D space on our new benchmark, representing a significant\nadvancement in the field of embodied AI. Project page:\nhttps://surrey-uplab.github.io/research/3d-audio-visual-segmentation/\n","authors":["Artem Sokolov","Swapnil Bhosale","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.02236v1.pdf","comment":"Accepted at the NeurIPS 2024 Workshop on Audio Imagination"},{"id":"http://arxiv.org/abs/2407.05645v3","updated":"2024-11-04T09:14:03Z","published":"2024-07-08T06:14:37Z","title":"OneDiff: A Generalist Model for Image Difference Captioning","summary":"  In computer vision, Image Difference Captioning (IDC) is crucial for\naccurately describing variations between closely related images. Traditional\nIDC methods often rely on specialist models, which restrict their applicability\nacross varied contexts. This paper introduces the OneDiff model, a novel\ngeneralist approach that utilizes a robust vision-language model architecture,\nintegrating a siamese image encoder with a Visual Delta Module. This innovative\nconfiguration allows for the precise detection and articulation of fine-grained\ndifferences between image pairs. OneDiff is trained through a dual-phase\nstrategy, encompassing Coupled Sample Training and multi-task learning across a\ndiverse array of data types, supported by our newly developed DiffCap Dataset.\nThis dataset merges real-world and synthetic data, enhancing the training\nprocess and bolstering the model's robustness. Extensive testing on diverse IDC\nbenchmarks, such as Spot-the-Diff, Image-Editing-Request, and Birds-to-Words,\nshows that OneDiff consistently outperforms existing state-of-the-art models in\naccuracy and adaptability, achieving improvements of up to 97% CIDEr points in\naverage. By setting a new benchmark in IDC, OneDiff paves the way for more\nversatile and effective applications in detecting and describing visual\ndifferences. The code, models, and data will be made publicly available.\n","authors":["Erdong Hu","Longteng Guo","Tongtian Yue","Zijia Zhao","Shuning Xue","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2407.05645v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01805v1","updated":"2024-11-04T05:17:44Z","published":"2024-11-04T05:17:44Z","title":"MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and\n  Correspondence","summary":"  Motion-to-music and music-to-motion have been studied separately, each\nattracting substantial research interest within their respective domains. The\ninteraction between human motion and music is a reflection of advanced human\nintelligence, and establishing a unified relationship between them is\nparticularly important. However, to date, there has been no work that considers\nthem jointly to explore the modality alignment within. To bridge this gap, we\npropose a novel framework, termed MoMu-Diffusion, for long-term and synchronous\nmotion-music generation. Firstly, to mitigate the huge computational costs\nraised by long sequences, we propose a novel Bidirectional Contrastive Rhythmic\nVariational Auto-Encoder (BiCoR-VAE) that extracts the modality-aligned latent\nrepresentations for both motion and music inputs. Subsequently, leveraging the\naligned latent spaces, we introduce a multi-modal Transformer-based diffusion\nmodel and a cross-guidance sampling strategy to enable various generation\ntasks, including cross-modal, multi-modal, and variable-length generation.\nExtensive experiments demonstrate that MoMu-Diffusion surpasses recent\nstate-of-the-art methods both qualitatively and quantitatively, and can\nsynthesize realistic, diverse, long-term, and beat-matched music or motion\nsequences. The generated samples and codes are available at\nhttps://momu-diffusion.github.io/\n","authors":["Fuming You","Minghui Fang","Li Tang","Rongjie Huang","Yongqi Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.01805v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2305.15748v2","updated":"2024-11-04T00:48:12Z","published":"2023-05-25T05:55:53Z","title":"ReactFace: Online Multiple Appropriate Facial Reaction Generation in\n  Dyadic Interactions","summary":"  In dyadic interaction, predicting the listener's facial reactions is\nchallenging as different reactions could be appropriate in response to the same\nspeaker's behaviour. Previous approaches predominantly treated this task as an\ninterpolation or fitting problem, emphasizing deterministic outcomes but\nignoring the diversity and uncertainty of human facial reactions. Furthermore,\nthese methods often failed to model short-range and long-range dependencies\nwithin the interaction context, leading to issues in the synchrony and\nappropriateness of the generated facial reactions. To address these\nlimitations, this paper reformulates the task as an extrapolation or prediction\nproblem, and proposes an novel framework (called ReactFace) to generate\nmultiple different but appropriate facial reactions from a speaker behaviour\nrather than merely replicating the corresponding listener facial behaviours.\nOur ReactFace generates multiple different but appropriate photo-realistic\nhuman facial reactions by: (i) learning an appropriate facial reaction\ndistribution representing multiple different but appropriate facial reactions;\nand (ii) synchronizing the generated facial reactions with the speaker verbal\nand non-verbal behaviours at each time stamp, resulting in realistic 2D facial\nreaction sequences. Experimental results demonstrate the effectiveness of our\napproach in generating multiple diverse, synchronized, and appropriate facial\nreactions from each speaker's behaviour. The quality of the generated facial\nreactions is intimately tied to the speaker's speech and facial expressions,\nachieved through our novel speaker-listener interaction modules. Our code is\nmade publicly available at \\url{https://github.com/lingjivoo/ReactFace}.\n","authors":["Cheng Luo","Siyang Song","Weicheng Xie","Micol Spitale","Zongyuan Ge","Linlin Shen","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2305.15748v2.pdf","comment":"Accepted to IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), 18 pages, 10 figures"}]},"2024-11-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.01713v1","updated":"2024-11-03T23:36:53Z","published":"2024-11-03T23:36:53Z","title":"Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models","summary":"  Modern optimizers such as AdamW, equipped with momentum and adaptive learning\nrate, are designed to escape local minima and explore the vast parameter space.\nThis exploration is beneficial for finding good loss basins when training from\nscratch. It is not necessarily ideal when resuming from a powerful foundation\nmodel because it can lead to large deviations from the pre-trained\ninitialization and, consequently, worse robustness and generalization. At the\nsame time, strong regularization on all parameters can lead to under-fitting.\nWe hypothesize that selectively regularizing the parameter space is the key to\nfitting and retraining the pre-trained knowledge. This paper proposes a new\nweight decay technique, Selective Projection Decay (SPD), that selectively\nimposes a strong penalty on certain layers while allowing others to change\nfreely. Intuitively, SPD expands and contracts the parameter search space for\nlayers with consistent and inconsistent loss reduction, respectively.\nExperimentally, when equipped with SPD, Adam consistently provides better\nin-distribution generalization and out-of-distribution robustness performance\non multiple popular vision and language benchmarks. Code available\nat~\\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}\n","authors":["Junjiao Tian","Chengyue Huang","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2411.01713v1.pdf","comment":"Accepted to Neurips 2024"},{"id":"http://arxiv.org/abs/2411.01710v1","updated":"2024-11-03T23:02:30Z","published":"2024-11-03T23:02:30Z","title":"SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation","summary":"  Spurred by the demand for interpretable models, research on eXplainable AI\nfor language technologies has experienced significant growth, with feature\nattribution methods emerging as a cornerstone of this progress. While prior\nwork in NLP explored such methods for classification tasks and textual\napplications, explainability intersecting generation and speech is lagging,\nwith existing techniques failing to account for the autoregressive nature of\nstate-of-the-art models and to provide fine-grained, phonetically meaningful\nexplanations. We address this gap by introducing Spectrogram Perturbation for\nExplainable Speech-to-text Generation (SPES), a feature attribution technique\napplicable to sequence generation tasks with autoregressive models. SPES\nprovides explanations for each predicted token based on both the input\nspectrogram and the previously generated tokens. Extensive evaluation on speech\nrecognition and translation demonstrates that SPES generates explanations that\nare faithful and plausible to humans.\n","authors":["Dennis Fucci","Marco Gaido","Beatrice Savoldi","Matteo Negri","Mauro Cettolo","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2411.01710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16337v3","updated":"2024-11-03T22:44:20Z","published":"2024-05-25T19:40:50Z","title":"Learning to Reason via Program Generation, Emulation, and Search","summary":"  Program synthesis with language models (LMs) has unlocked a large set of\nreasoning abilities; code-tuned LMs have proven adept at generating programs\nthat solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word\nconcatenation). However, not all reasoning tasks are easily expressible as\ncode, e.g. tasks involving commonsense reasoning, moral decision-making, and\nsarcasm understanding. Our goal is to extend an LM's program synthesis skills\nto such tasks and evaluate the results via pseudo-programs, namely Python\nprograms where some leaf function calls are left undefined. To that end, we\npropose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)\ntraining LMs to generate pseudo-programs, (2) teaching them to emulate their\ngenerated program's execution, including those leaf functions, allowing the\nLM's knowledge to fill in the execution gaps; and (3) using them to search over\nmany programs to find an optimal one. To adapt the CoGEX model to a new task,\nwe introduce a method for performing program search to find a single program\nwhose pseudo-execution yields optimal performance when applied to all the\ninstances of a given dataset. We show that our approach yields large\nimprovements compared to standard in-context learning approaches on a battery\nof tasks, both algorithmic and soft reasoning. This result thus demonstrates\nthat code synthesis can be applied to a much broader class of problems than\npreviously considered. Our released dataset, fine-tuned models, and\nimplementation can be found at \\url{https://github.com/nweir127/CoGEX}.\n","authors":["Nathaniel Weir","Muhammad Khalifa","Linlu Qiu","Orion Weller","Peter Clark"],"pdf_url":"https://arxiv.org/pdf/2405.16337v3.pdf","comment":"NeurIPS 2024 camera ready"},{"id":"http://arxiv.org/abs/2406.17261v2","updated":"2024-11-03T22:38:08Z","published":"2024-06-25T04:01:32Z","title":"TRAWL: Tensor Reduced and Approximated Weights for Large Language Models","summary":"  Recent research has shown that pruning large-scale language models for\ninference is an effective approach to improving model efficiency, significantly\nreducing model weights with minimal impact on performance. Interestingly,\npruning can sometimes even enhance accuracy by removing noise that accumulates\nduring training, particularly through matrix decompositions. However, recent\nwork has primarily focused on single matrix decompositions or lower precision\ntechniques, which may fail to fully capture structural patterns. To address\nthese limitations, we introduce TRAWL (Tensor Reduced and Approximated Weights\nfor Large Language Models), a technique that applies tensor decomposition\nacross multiple weight matrices to effectively denoise LLMs by capturing global\nstructural patterns. Our experiments show that TRAWL improves model performance\nby up to 16% over baseline models on benchmark datasets, without requiring\nadditional data, training, or fine-tuning.\n","authors":["Yiran Luo","Het Patel","Yu Fu","Dawon Ahn","Jia Chen","Yue Dong","Evangelos E. Papalexakis"],"pdf_url":"https://arxiv.org/pdf/2406.17261v2.pdf","comment":"4 pages. Submitted to NAACL 2025 and under review"},{"id":"http://arxiv.org/abs/2411.01706v1","updated":"2024-11-03T22:31:02Z","published":"2024-11-03T22:31:02Z","title":"Investigating Large Language Models for Complex Word Identification in\n  Multilingual and Multidomain Setups","summary":"  Complex Word Identification (CWI) is an essential step in the lexical\nsimplification task and has recently become a task on its own. Some variations\nof this binary classification task have emerged, such as lexical complexity\nprediction (LCP) and complexity evaluation of multi-word expressions (MWE).\nLarge language models (LLMs) recently became popular in the Natural Language\nProcessing community because of their versatility and capability to solve\nunseen tasks in zero/few-shot settings. Our work investigates LLM usage,\nspecifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and\nclosed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE\nsettings. We evaluate zero-shot, few-shot, and fine-tuning settings and show\nthat LLMs struggle in certain conditions or achieve comparable results against\nexisting methods. In addition, we provide some views on meta-learning combined\nwith prompt learning. In the end, we conclude that the current state of LLMs\ncannot or barely outperform existing methods, which are usually much smaller.\n","authors":["Răzvan-Alexandru Smădu","David-Gabriel Ion","Dumitru-Clementin Cercel","Florin Pop","Mihaela-Claudia Cercel"],"pdf_url":"https://arxiv.org/pdf/2411.01706v1.pdf","comment":"37 pages, 16 figures, Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.01705v1","updated":"2024-11-03T22:27:40Z","published":"2024-11-03T22:27:40Z","title":"Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors","summary":"  Despite significant advancements, large language models (LLMs) still struggle\nwith providing accurate answers when lacking domain-specific or up-to-date\nknowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by\nincorporating external knowledge bases, but it also introduces new attack\nsurfaces. In this paper, we investigate data extraction attacks targeting the\nknowledge databases of RAG systems. We demonstrate that previous attacks on RAG\nlargely depend on the instruction-following capabilities of LLMs, and that\nsimple fine-tuning can reduce the success rate of such attacks to nearly zero.\nThis makes these attacks impractical since fine-tuning is a common practice\nwhen deploying LLMs in specific domains. To further reveal the vulnerability,\nwe propose to backdoor RAG, where a small portion of poisoned data is injected\nduring the fine-tuning phase to create a backdoor within the LLM. When this\ncompromised LLM is integrated into a RAG system, attackers can exploit specific\ntriggers in prompts to manipulate the LLM to leak documents from the retrieval\ndatabase. By carefully designing the poisoned data, we achieve both verbatim\nand paraphrased document extraction. We show that with only 3\\% poisoned data,\nour method achieves an average success rate of 79.7\\% in verbatim extraction on\nLlama2-7B, with a ROUGE-L score of 64.21, and a 68.6\\% average success rate in\nparaphrased extraction, with an average ROUGE score of 52.6 across four\ndatasets. These results underscore the privacy risks associated with the supply\nchain when deploying RAG systems.\n","authors":["Yuefeng Peng","Junda Wang","Hong Yu","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2411.01705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01703v1","updated":"2024-11-03T22:19:20Z","published":"2024-11-03T22:19:20Z","title":"UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have revolutionized vision-language\nunderstanding but are vulnerable to multimodal jailbreak attacks, where\nadversaries meticulously craft inputs to elicit harmful or inappropriate\nresponses. We propose UniGuard, a novel multimodal safety guardrail that\njointly considers the unimodal and cross-modal harmful signals. UniGuard is\ntrained such that the likelihood of generating harmful responses in a toxic\ncorpus is minimized, and can be seamlessly applied to any input prompt during\ninference with minimal computational costs. Extensive experiments demonstrate\nthe generalizability of UniGuard across multiple modalities and attack\nstrategies. It demonstrates impressive generalizability across multiple\nstate-of-the-art MLLMs, including LLaVA, Gemini Pro, GPT-4, MiniGPT-4, and\nInstructBLIP, thereby broadening the scope of our solution.\n","authors":["Sejoon Oh","Yiqiao Jin","Megha Sharma","Donghyun Kim","Eric Ma","Gaurav Verma","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2411.01703v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.21647v3","updated":"2024-11-03T21:24:10Z","published":"2024-10-29T01:21:05Z","title":"Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'","summary":"  Large language models (LLMs) have achieved high accuracy, i.e., more than 90%\npass@1, in solving Python coding problems in HumanEval and MBPP. Thus, a\nnatural question is, whether LLMs achieve comparable code completion\nperformance compared to human developers? Unfortunately, one cannot answer this\nquestion using existing manual crafted or simple (e.g., single-line) code\ngeneration benchmarks, since such tasks fail to represent real-world software\ndevelopment tasks. In addition, existing benchmarks often use poor code\ncorrectness metrics, providing misleading conclusions.\n  To address these challenges, we create REPOCOD, a code generation benchmark\nwith 980 problems collected from 11 popular real-world projects, with more than\n58% of them requiring file-level or repository-level context information. In\naddition, REPOCOD has the longest average canonical solution length (331.6\ntokens) and the highest average cyclomatic complexity (9.00) compared to\nexisting benchmarks. Each task in REPOCOD includes 313.5 developer-written test\ncases on average for better correctness evaluation. In our evaluations of ten\nLLMs, none of the models achieve more than 30% pass@1 on REPOCOD, indicating\nthe necessity of building stronger LLMs that can help developers in real-world\nsoftware development. REPOCOD is available at\nhttps://github.com/lt-asset/REPOCOD\n","authors":["Shanchao Liang","Yiran Hu","Nan Jiang","Lin Tan"],"pdf_url":"https://arxiv.org/pdf/2410.21647v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12861v2","updated":"2024-11-03T20:58:35Z","published":"2024-07-10T11:31:20Z","title":"CiteME: Can Language Models Accurately Cite Scientific Claims?","summary":"  Thousands of new scientific papers are published each month. Such information\noverload complicates researcher efforts to stay current with the\nstate-of-the-art as well as to verify and correctly attribute claims. We pose\nthe following research question: Given a text excerpt referencing a paper,\ncould an LM act as a research assistant to correctly identify the referenced\npaper? We advance efforts to answer this question by building a benchmark that\nevaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,\nconsists of text excerpts from recent machine learning papers, each referencing\na single other paper. CiteME use reveals a large gap between frontier LMs and\nhuman performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.\nWe close this gap by introducing CiteAgent, an autonomous system built on the\nGPT-4o LM that can also search and read papers, which achieves an accuracy of\n35.3\\% on CiteME. Overall, CiteME serves as a challenging testbed for\nopen-ended claim attribution, driving the research community towards a future\nwhere any claim made by an LM can be automatically verified and discarded if\nfound to be incorrect.\n","authors":["Ori Press","Andreas Hochlehnert","Ameya Prabhu","Vishaal Udandarao","Ofir Press","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2407.12861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13743v3","updated":"2024-11-03T20:22:32Z","published":"2024-06-19T18:00:07Z","title":"GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual\n  Generation","summary":"  While text-to-visual models now produce photo-realistic images and videos,\nthey struggle with compositional text prompts involving attributes,\nrelationships, and higher-order reasoning such as logic and comparison. In this\nwork, we conduct an extensive human study on GenAI-Bench to evaluate the\nperformance of leading image and video generation models in various aspects of\ncompositional text-to-visual generation. We also compare automated evaluation\nmetrics against our collected human ratings and find that VQAScore -- a metric\nmeasuring the likelihood that a VQA model views an image as accurately\ndepicting the prompt -- significantly outperforms previous metrics such as\nCLIPScore. In addition, VQAScore can improve generation in a black-box manner\n(without finetuning) via simply ranking a few (3 to 9) candidate images.\nRanking by VQAScore is 2x to 3x more effective than other scoring methods like\nPickScore, HPSv2, and ImageReward at improving human alignment ratings for\nDALL-E 3 and Stable Diffusion, especially on compositional prompts that require\nadvanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with\nover 40,000 human ratings to evaluate scoring metrics on ranking images\ngenerated from the same prompt. Lastly, we discuss promising areas for\nimprovement in VQAScore, such as addressing fine-grained visual details. We\nwill release all human ratings (over 80,000) to facilitate scientific\nbenchmarking of both generative models and automated metrics.\n","authors":["Baiqi Li","Zhiqiu Lin","Deepak Pathak","Jiayao Li","Yixin Fei","Kewen Wu","Tiffany Ling","Xide Xia","Pengchuan Zhang","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2406.13743v3.pdf","comment":"We open-source our dataset, model, and code at:\n  https://linzhiqiu.github.io/papers/genai_bench ; Project page:\n  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first\n  introduced in arxiv:2404.01291. This article extends it with an additional\n  GenAI-Rank benchmark"},{"id":"http://arxiv.org/abs/2406.18776v2","updated":"2024-11-03T19:56:24Z","published":"2024-06-26T22:10:15Z","title":"Implicit Discourse Relation Classification For Nigerian Pidgin","summary":"  Despite attempts to make Large Language Models multi-lingual, many of the\nworld's languages are still severely under-resourced. This widens the\nperformance gap between NLP and AI applications aimed at well-financed, and\nthose aimed at less-resourced languages. In this paper, we focus on Nigerian\nPidgin (NP), which is spoken by nearly 100 million people, but has\ncomparatively very few NLP resources and corpora. We address the task of\nImplicit Discourse Relation Classification (IDRC) and systematically compare an\napproach translating NP data to English and then using a well-resourced IDRC\ntool and back-projecting the labels versus creating a synthetic discourse\ncorpus for NP, in which we translate PDTB and project PDTB labels, and then\ntrain an NP IDR classifier. The latter approach of learning a \"native\" NP\nclassifier outperforms our baseline by 13.27\\% and 33.98\\% in f$_{1}$ score for\n4-way and 11-way classification, respectively.\n","authors":["Muhammed Saeed","Peter Bourgonje","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2406.18776v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01663v1","updated":"2024-11-03T19:18:57Z","published":"2024-11-03T19:18:57Z","title":"Unlocking the Theory Behind Scaling 1-Bit Neural Networks","summary":"  Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an\nimpressive combination of efficiency and performance that rivals traditional\nLLMs. Research by Wang et al. (2023); Ma et al. (2024) indicates that the\nperformance of these 1-bit LLMs progressively improves as the number of\nparameters increases, hinting at the potential existence of a Scaling Law for\n1-bit Neural Networks. In this paper, we present the first theoretical result\nthat rigorously establishes this scaling law for 1-bit models. We prove that,\ndespite the constraint of weights restricted to $\\{-1, +1\\}$, the dynamics of\nmodel training inevitably align with kernel behavior as the network width\ngrows. This theoretical breakthrough guarantees convergence of the 1-bit model\nto an arbitrarily small loss as width increases. Furthermore, we introduce the\nconcept of the generalization difference, defined as the gap between the\noutputs of 1-bit networks and their full-precision counterparts, and\ndemonstrate that this difference maintains a negligible level as network width\nscales. Building on the work of Kaplan et al. (2020), we conclude by examining\nhow the training loss scales as a power-law function of the model size, dataset\nsize, and computational resources utilized for training. Our findings\nunderscore the promising potential of scaling 1-bit neural networks, suggesting\nthat int1 could become the standard in future neural network precision.\n","authors":["Majid Daliri","Zhao Song","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2411.01663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05266v3","updated":"2024-11-03T18:38:50Z","published":"2024-03-08T12:42:36Z","title":"ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models","summary":"  Large language models (LLMs) have achieved unprecedented performances in\nvarious applications, yet evaluating them is still challenging. Existing\nbenchmarks are either manually constructed or are automatic, but lack the\nability to evaluate the thought process of LLMs with arbitrary complexity. We\ncontend that utilizing existing relational databases based on the\nentity-relationship (ER) model is a promising approach for constructing\nbenchmarks as they contain structured knowledge that can be used to question\nLLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational\ndatabases have integrity constraints that can be used to better construct\ncomplex in-depth questions and verify answers: (1) functional dependencies can\nbe used to pinpoint critical keywords that an LLM must know to properly answer\na given question containing certain attribute values; and (2) foreign key\nconstraints can be used to join relations and construct multi-hop questions,\nwhich can be arbitrarily long and used to debug intermediate answers. We thus\npropose ERBench, which uses these integrity constraints to convert any database\ninto an LLM benchmark. ERBench supports continuous evaluation as databases\nchange, multimodal questions, and various prompt engineering techniques. In our\nexperiments, we construct LLM benchmarks using databases of multiple domains\nand make an extensive comparison of contemporary LLMs. We show how ERBench can\nproperly evaluate any LLM by not only checking for answer correctness, but also\neffectively verifying the rationales by looking for the right keywords.\n","authors":["Jio Oh","Soyeon Kim","Junseok Seo","Jindong Wang","Ruochen Xu","Xing Xie","Steven Euijong Whang"],"pdf_url":"https://arxiv.org/pdf/2403.05266v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01653v1","updated":"2024-11-03T18:37:35Z","published":"2024-11-03T18:37:35Z","title":"Diagnosing Medical Datasets with Training Dynamics","summary":"  This study explores the potential of using training dynamics as an automated\nalternative to human annotation for evaluating the quality of training data.\nThe framework used is Data Maps, which classifies data points into categories\nsuch as easy-to-learn, hard-to-learn, and ambiguous (Swayamdipta et al., 2020).\nSwayamdipta et al. (2020) highlight that difficult-to-learn examples often\ncontain errors, and ambiguous cases significantly impact model training. To\nconfirm the reliability of these findings, we replicated the experiments using\na challenging dataset, with a focus on medical question answering. In addition\nto text comprehension, this field requires the acquisition of detailed medical\nknowledge, which further complicates the task. A comprehensive evaluation was\nconducted to assess the feasibility and transferability of the Data Maps\nframework to the medical domain. The evaluation indicates that the framework is\nunsuitable for addressing datasets' unique challenges in answering medical\nquestions.\n","authors":["Laura Wenderoth"],"pdf_url":"https://arxiv.org/pdf/2411.01653v1.pdf","comment":"https://github.com/LauraWenderoth/training-dynamics"},{"id":"http://arxiv.org/abs/2405.18415v2","updated":"2024-11-03T18:23:45Z","published":"2024-05-28T17:57:06Z","title":"Why are Visually-Grounded Language Models Bad at Image Classification?","summary":"  Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.\n","authors":["Yuhui Zhang","Alyssa Unell","Xiaohan Wang","Dhruba Ghosh","Yuchang Su","Ludwig Schmidt","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2405.18415v2.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.09519v2","updated":"2024-11-03T17:48:48Z","published":"2024-06-13T18:12:01Z","title":"Talking Heads: Understanding Inter-layer Communication in Transformer\n  Language Models","summary":"  Although it is known that transformer language models (LMs) pass features\nfrom early layers to later layers, it is not well understood how this\ninformation is represented and routed by the model. We analyze a mechanism used\nin two LMs to selectively inhibit items in a context in one task, and find that\nit underlies a commonly used abstraction across many context-retrieval\nbehaviors. Specifically, we find that models write into low-rank subspaces of\nthe residual stream to represent features which are then read out by later\nlayers, forming low-rank communication channels (Elhage et al., 2021) between\nlayers. A particular 3D subspace in model activations in GPT-2 can be traversed\nto positionally index items in lists, and we show that this mechanism can\nexplain an otherwise arbitrary-seeming sensitivity of the model to the order of\nitems in the prompt. That is, the model has trouble copying the correct\ninformation from context when many items ``crowd\" this limited space. By\ndecomposing attention heads with the Singular Value Decomposition (SVD), we\nfind that previously described interactions between heads separated by one or\nmore layers can be predicted via analysis of their weight matrices alone. We\nshow that it is possible to manipulate the internal model representations as\nwell as edit model weights based on the mechanism we discover in order to\nsignificantly improve performance on our synthetic Laundry List task, which\nrequires recall from a list, often improving task accuracy by over 20%. Our\nanalysis reveals a surprisingly intricate interpretable structure learned from\nlanguage model pretraining, and helps us understand why sophisticated LMs\nsometimes fail in simple domains, facilitating future analysis of more complex\nbehaviors.\n","authors":["Jack Merullo","Carsten Eickhoff","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2406.09519v2.pdf","comment":"Neurips 2024"},{"id":"http://arxiv.org/abs/2411.01643v1","updated":"2024-11-03T17:37:06Z","published":"2024-11-03T17:37:06Z","title":"EcoAct: Economic Agent Determines When to Register What Action","summary":"  Recent advancements have enabled Large Language Models (LLMs) to function as\nagents that can perform actions using external tools. This requires\nregistering, i.e., integrating tool information into the LLM context prior to\ntaking actions. Current methods indiscriminately incorporate all candidate\ntools into the agent's context and retain them across multiple reasoning steps.\nThis process remains opaque to LLM agents and is not integrated into their\nreasoning procedures, leading to inefficiencies due to increased context length\nfrom irrelevant tools. To address this, we introduce EcoAct, a tool using\nalgorithm that allows LLMs to selectively register tools as needed, optimizing\ncontext use. By integrating the tool registration process into the reasoning\nprocedure, EcoAct reduces computational costs by over 50% in multiple steps\nreasoning tasks while maintaining performance, as demonstrated through\nextensive experiments. Moreover, it can be plugged into any reasoning pipeline\nwith only minor modifications to the prompt, making it applicable to LLM agents\nnow and future.\n","authors":["Shaokun Zhang","Jieyu Zhang","Dujian Ding","Mirian Hipolito Garcia","Ankur Mallick","Daniel Madrigal","Menglin Xia","Victor Rühle","Qingyun Wu","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01643v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.01636v1","updated":"2024-11-03T17:24:02Z","published":"2024-11-03T17:24:02Z","title":"Leveraging Microservices Architecture for Dynamic Pricing in the Travel\n  Industry: Algorithms, Scalability, and Impact on Revenue and Customer\n  Satisfaction","summary":"  This research investigates the implementation of a real-time,\nmicroservices-oriented dynamic pricing system for the travel sector. The system\nis designed to address factors such as demand, competitor pricing, and other\nexternal circumstances in real-time. Both controlled simulation and real-life\napplication showed a respectable gain of 22% in revenue generation and a 17%\nimprovement in pricing response time which concern the issues of scaling and\nflexibility of classical pricing mechanisms. Demand forecasting, competitor\npricing strategies, and event-based pricing were implemented as separate\nmicroservices to enhance their scalability and reduce resource consumption by\n30% during peak loads. Customers were also more content as depicted by a 15%\nincrease in satisfaction score post-implementation given the appreciation of\nmore appropriate pricing. This research enhances the existing literature with\npractical illustrations of the possible application of microservices technology\nin developing dynamic pricing solutions in a complex and data-driven context.\nThere exist however areas for improvement for instance inter-service latency\nand the need for extensive real-time data pipelines. The present research goes\non to suggest combining these with direct data capture from customer behavior\nat the same time as machine learning capacity developments in pricing\nalgorithms to assist in more accurate real time pricing. It is determined that\nthe use of microservices is a reasonable and efficient model for dynamic\npricing, allowing the tourism sector to employ evidence-based and customer\ncentric pricing techniques, which ensures that their profits are not\njeopardized because of the need for customers.\n","authors":["Biman Barua","M. Shamim Kaiser"],"pdf_url":"https://arxiv.org/pdf/2411.01636v1.pdf","comment":"19 pages, 18 figures"},{"id":"http://arxiv.org/abs/2407.11214v2","updated":"2024-11-03T17:14:37Z","published":"2024-07-15T19:57:15Z","title":"PutnamBench: Evaluating Neural Theorem-Provers on the Putnam\n  Mathematical Competition","summary":"  We present PutnamBench, a new multi-language benchmark for evaluating the\nability of neural theorem-provers to solve competition mathematics problems.\nPutnamBench consists of 1692 hand-constructed formalizations of 640 theorems\nsourced from the William Lowell Putnam Mathematical Competition, the premier\nundergraduate-level mathematics competition in North America. All the problems\nhave formalizations in Lean 4 and Isabelle; a substantial subset also has Coq\nformalizations. PutnamBench requires significant problem-solving ability and\nproficiency in a broad range of topics taught in undergraduate mathematics\ncourses. We use PutnamBench to evaluate several established neural and symbolic\ntheorem-provers. These approaches can only solve a handful of the PutnamBench\nproblems, establishing the benchmark as a difficult open challenge for research\non neural theorem-proving. PutnamBench is available at\nhttps://github.com/trishullab/PutnamBench.\n","authors":["George Tsoukalas","Jasper Lee","John Jennings","Jimmy Xin","Michelle Ding","Michael Jennings","Amitayush Thakur","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2407.11214v2.pdf","comment":"Accepted at NeurIPS 2024 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2406.06007v3","updated":"2024-11-03T16:54:14Z","published":"2024-06-10T04:07:09Z","title":"CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision\n  Language Models","summary":"  Artificial intelligence has significantly impacted medical applications,\nparticularly with the advent of Medical Large Vision Language Models\n(Med-LVLMs), sparking optimism for the future of automated and personalized\nhealthcare. However, the trustworthiness of Med-LVLMs remains unverified,\nposing significant risks for future model deployment. In this paper, we\nintroduce CARES and aim to comprehensively evaluate the Trustworthiness of\nMed-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs\nacross five dimensions, including trustfulness, fairness, safety, privacy, and\nrobustness. CARES comprises about 41K question-answer pairs in both closed and\nopen-ended formats, covering 16 medical image modalities and 27 anatomical\nregions. Our analysis reveals that the models consistently exhibit concerns\nregarding trustworthiness, often displaying factual inaccuracies and failing to\nmaintain fairness across different demographic groups. Furthermore, they are\nvulnerable to attacks and demonstrate a lack of privacy awareness. We publicly\nrelease our benchmark and code in https://cares-ai.github.io/.\n","authors":["Peng Xia","Ze Chen","Juanxi Tian","Yangrui Gong","Ruibo Hou","Yue Xu","Zhenbang Wu","Zhiyuan Fan","Yiyang Zhou","Kangyu Zhu","Wenhao Zheng","Zhaoyang Wang","Xiao Wang","Xuchao Zhang","Chetan Bansal","Marc Niethammer","Junzhou Huang","Hongtu Zhu","Yun Li","Jimeng Sun","Zongyuan Ge","Gang Li","James Zou","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2406.06007v3.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.20746v2","updated":"2024-11-03T16:19:49Z","published":"2024-10-28T05:25:50Z","title":"ElectionSim: Massive Population Election Simulation Powered by Large\n  Language Model Driven Agents","summary":"  The massive population election simulation aims to model the preferences of\nspecific groups in particular election scenarios. It has garnered significant\nattention for its potential to forecast real-world social trends. Traditional\nagent-based modeling (ABM) methods are constrained by their ability to\nincorporate complex individual background information and provide interactive\nprediction results. In this paper, we introduce ElectionSim, an innovative\nelection simulation framework based on large language models, designed to\nsupport accurate voter simulations and customized distributions, together with\nan interactive platform to dialogue with simulated voters. We present a\nmillion-level voter pool sampled from social media platforms to support\naccurate individual simulation. We also introduce PPE, a poll-based\npresidential election benchmark to assess the performance of our framework\nunder the U.S. presidential election scenario. Through extensive experiments\nand analyses, we demonstrate the effectiveness and robustness of our framework\nin U.S. presidential election simulations.\n","authors":["Xinnong Zhang","Jiayu Lin","Libo Sun","Weihong Qi","Yihang Yang","Yue Chen","Hanjia Lyu","Xinyi Mou","Siming Chen","Jiebo Luo","Xuanjing Huang","Shiping Tang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.20746v2.pdf","comment":"42 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.01612v1","updated":"2024-11-03T15:39:20Z","published":"2024-11-03T15:39:20Z","title":"Ontology Population using LLMs","summary":"  Knowledge graphs (KGs) are increasingly utilized for data integration,\nrepresentation, and visualization. While KG population is critical, it is often\ncostly, especially when data must be extracted from unstructured text in\nnatural language, which presents challenges, such as ambiguity and complex\ninterpretations. Large Language Models (LLMs) offer promising capabilities for\nsuch tasks, excelling in natural language understanding and content generation.\nHowever, their tendency to ``hallucinate'' can produce inaccurate outputs.\nDespite these limitations, LLMs offer rapid and scalable processing of natural\nlanguage data, and with prompt engineering and fine-tuning, they can\napproximate human-level performance in extracting and structuring data for KGs.\nThis study investigates LLM effectiveness for the KG population, focusing on\nthe Enslaved.org Hub Ontology. In this paper, we report that compared to the\nground truth, LLM's can extract ~90% of triples, when provided a modular\nontology as guidance in the prompts.\n","authors":["Sanaz Saki Norouzi","Adrita Barua","Antrea Christou","Nikita Gautam","Andrew Eells","Pascal Hitzler","Cogan Shimizu"],"pdf_url":"https://arxiv.org/pdf/2411.01612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01610v1","updated":"2024-11-03T15:31:44Z","published":"2024-11-03T15:31:44Z","title":"Explaining and Improving Contrastive Decoding by Extrapolating the\n  Probabilities of a Huge and Hypothetical LM","summary":"  Contrastive decoding (CD) (Li et al., 2023) improves the next-token\ndistribution of a large expert language model (LM) using a small amateur LM.\nAlthough CD is applied to various LMs and domains to enhance open-ended text\ngeneration, it is still unclear why CD often works well, when it could fail,\nand how we can make it better. To deepen our understanding of CD, we first\ntheoretically prove that CD could be viewed as linearly extrapolating the\nnext-token logits from a huge and hypothetical LM. We also highlight that the\nlinear extrapolation could make CD unable to output the most obvious answers\nthat have already been assigned high probabilities by the amateur LM.\n  To overcome CD's limitation, we propose a new unsupervised decoding method\ncalled $\\mathbf{A}$symptotic $\\mathbf{P}$robability $\\mathbf{D}$ecoding (APD).\nAPD explicitly extrapolates the probability curves from the LMs of different\nsizes to infer the asymptotic probabilities from an infinitely large LM without\ninducing more inference costs than CD. In FactualityPrompts, an open-ended text\ngeneration benchmark, sampling using APD significantly boosts factuality in\ncomparison to the CD sampling and its variants, and achieves state-of-the-art\nresults for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA\ndatasets, APD is often significantly better than CD and achieves a similar\neffect of using a larger LLM. For example, the perplexity of APD on top of\nPythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA\nand LAMBADA.\n","authors":["Haw-Shiuan Chang","Nanyun Peng","Mohit Bansal","Anil Ramakrishna","Tagyoung Chung"],"pdf_url":"https://arxiv.org/pdf/2411.01610v1.pdf","comment":"EMNLP 2024 Oral"},{"id":"http://arxiv.org/abs/2406.00799v5","updated":"2024-11-03T14:52:35Z","published":"2024-06-02T16:53:21Z","title":"Are you still on track!? Catching LLM Task Drift with Activations","summary":"  Large Language Models are commonly used in retrieval-augmented applications\nto execute user instructions based on data from external sources. For example,\nmodern search engines use LLMs to answer queries based on relevant search\nresults; email plugins summarize emails by processing their content through an\nLLM. However, the potentially untrusted provenance of these data sources can\nlead to prompt injection attacks, where the LLM is manipulated by natural\nlanguage instructions embedded in the external data, causing it to deviate from\nthe user's original instruction(s). We define this deviation as task drift.\nTask drift is a significant concern as it allows attackers to exfiltrate data\nor influence the LLM's output for other users. We study LLM activations as a\nsolution to detect task drift, showing that activation deltas - the difference\nin activations before and after processing external data - are strongly\ncorrelated with this phenomenon. Through two probing methods, we demonstrate\nthat a simple linear classifier can detect drift with near-perfect ROC AUC on\nan out-of-distribution test set. We evaluate these methods by making minimal\nassumptions about how user's tasks, system prompts, and attacks can be phrased.\nWe observe that this approach generalizes surprisingly well to unseen task\ndomains, such as prompt injections, jailbreaks, and malicious instructions,\nwithout being trained on any of these attacks. Interestingly, the fact that\nthis solution does not require any modifications to the LLM (e.g.,\nfine-tuning), as well as its compatibility with existing meta-prompting\nsolutions, makes it cost-efficient and easy to deploy. To encourage further\nresearch on activation-based task inspection, decoding, and interpretability,\nwe release our large-scale TaskTracker toolkit, featuring a dataset of over\n500K instances, representations from six SoTA language models, and inspection\ntools.\n","authors":["Sahar Abdelnabi","Aideen Fay","Giovanni Cherubin","Ahmed Salem","Mario Fritz","Andrew Paverd"],"pdf_url":"https://arxiv.org/pdf/2406.00799v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10671v3","updated":"2024-11-03T14:42:13Z","published":"2024-06-15T15:28:02Z","title":"Augmenting Biomedical Named Entity Recognition with General-domain\n  Resources","summary":"  Training a neural network-based biomedical named entity recognition (BioNER)\nmodel usually requires extensive and costly human annotations. While several\nstudies have employed multi-task learning with multiple BioNER datasets to\nreduce human effort, this approach does not consistently yield performance\nimprovements and may introduce label ambiguity in different biomedical corpora.\nWe aim to tackle those challenges through transfer learning from easily\naccessible resources with fewer concept overlaps with biomedical datasets. In\nthis paper, we proposed GERBERA, a simple-yet-effective method that utilized a\ngeneral-domain NER dataset for training. Specifically, we performed multi-task\nlearning to train a pre-trained biomedical language model with both the target\nBioNER dataset and the general-domain dataset. Subsequently, we fine-tuned the\nmodels specifically for the BioNER dataset. We systematically evaluated GERBERA\non five datasets of eight entity types, collectively consisting of 81,410\ninstances. Despite using fewer biomedical resources, our models demonstrated\nsuperior performance compared to baseline models trained with multiple\nadditional BioNER datasets. Specifically, our models consistently outperformed\nthe baselines in six out of eight entity types, achieving an average\nimprovement of 0.9% over the best baseline performance across eight biomedical\nentity types sourced from five different corpora. Our method was especially\neffective in amplifying performance on BioNER datasets characterized by limited\ndata, with a 4.7% improvement in F1 scores on the JNLPBA-RNA dataset.\n","authors":["Yu Yin","Hyunjae Kim","Xiao Xiao","Chih Hsuan Wei","Jaewoo Kang","Zhiyong Lu","Hua Xu","Meng Fang","Qingyu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.10671v3.pdf","comment":"Published in JBI 2024. We make data, codes, and models publicly\n  available via https://github.com/qingyu-qc/bioner_gerbera"},{"id":"http://arxiv.org/abs/2411.01562v1","updated":"2024-11-03T13:23:18Z","published":"2024-11-03T13:23:18Z","title":"Are LLMs good pragmatic speakers?","summary":"  Large language models (LLMs) are trained on data assumed to include natural\nlanguage pragmatics, but do they actually behave like pragmatic speakers? We\nattempt to answer this question using the Rational Speech Act (RSA) framework,\nwhich models pragmatic reasoning in human communication. Using the paradigm of\na reference game constructed from the TUNA corpus, we score candidate\nreferential utterances in both a state-of-the-art LLM (Llama3-8B-Instruct) and\nin the RSA model, comparing and contrasting these scores. Given that RSA\nrequires defining alternative utterances and a truth-conditional meaning\nfunction, we explore such comparison for different choices of each of these\nrequirements. We find that while scores from the LLM have some positive\ncorrelation with those from RSA, there isn't sufficient evidence to claim that\nit behaves like a pragmatic speaker. This initial study paves way for further\ntargeted efforts exploring different models and settings, including\nhuman-subject evaluation, to see if LLMs truly can, or be made to, behave like\npragmatic speakers.\n","authors":["Mingyue Jian","Siddharth Narayanaswamy"],"pdf_url":"https://arxiv.org/pdf/2411.01562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09645v2","updated":"2024-11-03T13:21:57Z","published":"2024-04-15T10:24:32Z","title":"Real-world Instance-specific Image Goal Navigation: Bridging Domain Gaps\n  via Contrastive Learning","summary":"  Improving instance-specific image goal navigation (InstanceImageNav), which\nlocates the identical object in a real-world environment from a query image, is\nessential for robotic systems to assist users in finding desired objects. The\nchallenge lies in the domain gap between low-quality images observed by the\nmoving robot, characterized by motion blur and low-resolution, and high-quality\nquery images provided by the user. Such domain gaps could significantly reduce\nthe task success rate but have not been the focus of previous work. To address\nthis, we propose a novel method called Few-shot Cross-quality Instance-aware\nAdaptation (CrossIA), which employs contrastive learning with an instance\nclassifier to align features between massive low- and few high-quality images.\nThis approach effectively reduces the domain gap by bringing the latent\nrepresentations of cross-quality images closer on an instance basis.\nAdditionally, the system integrates an object image collection with a\npre-trained deblurring model to enhance the observed image quality. Our method\nfine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We\nevaluated our method's effectiveness through an InstanceImageNav task with 20\ndifferent types of instances, where the robot identifies the same instance in a\nreal-world environment as a high-quality query image. Our experiments showed\nthat our method improves the task success rate by up to three times compared to\nthe baseline, a conventional approach based on SuperGlue. These findings\nhighlight the potential of leveraging contrastive learning and image\nenhancement techniques to bridge the domain gap and improve object localization\nin robotic applications. The project website is\nhttps://emergentsystemlabstudent.github.io/DomainBridgingNav/.\n","authors":["Taichi Sakaguchi","Akira Taniguchi","Yoshinobu Hagiwara","Lotfi El Hafi","Shoichi Hasegawa","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2404.09645v2.pdf","comment":"See website at\n  https://emergentsystemlabstudent.github.io/DomainBridgingNav/. Accepted to\n  IEEE IRC2024"},{"id":"http://arxiv.org/abs/2407.10657v3","updated":"2024-11-03T12:44:42Z","published":"2024-07-15T12:16:33Z","title":"An Empirical Study of Validating Synthetic Data for Formula Generation","summary":"  Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.\n","authors":["Usneek Singh","José Cambronero","Sumit Gulwani","Aditya Kanade","Anirudh Khatry","Vu Le","Mukul Singh","Gust Verbruggen"],"pdf_url":"https://arxiv.org/pdf/2407.10657v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01539v1","updated":"2024-11-03T12:03:12Z","published":"2024-11-03T12:03:12Z","title":"LLMs and the Madness of Crowds","summary":"  We investigate the patterns of incorrect answers produced by large language\nmodels (LLMs) during evaluation. These errors exhibit highly non-intuitive\nbehaviors unique to each model. By analyzing these patterns, we measure the\nsimilarities between LLMs and construct a taxonomy that categorizes them based\non their error correlations. Our findings reveal that the incorrect responses\nare not randomly distributed but systematically correlated across models,\nproviding new insights into the underlying structures and relationships among\nLLMs.\n","authors":["William F. Bradley"],"pdf_url":"https://arxiv.org/pdf/2411.01539v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.01533v1","updated":"2024-11-03T11:39:50Z","published":"2024-11-03T11:39:50Z","title":"Enhancing LLM Evaluations: The Garbling Trick","summary":"  As large language models (LLMs) become increasingly powerful, traditional\nevaluation metrics tend to saturate, making it challenging to distinguish\nbetween models based on their performance. We propose a general method to\ntransform existing LLM evaluations into a series of progressively more\ndifficult tasks. These enhanced evaluations emphasize reasoning capabilities\nand can reveal relative performance differences that are not apparent in the\noriginal assessments.\n  To demonstrate the effectiveness of our approach, we create a new\nmultiple-choice test corpus, extend it into a family of evaluations, and assess\na collection of LLMs. Our results offer insights into the comparative reasoning\nabilities of these models, particularly highlighting distinctions between\nOpenAI's o1-preview and Google's gemini-pro-1.5-002.\n","authors":["William F. Bradley"],"pdf_url":"https://arxiv.org/pdf/2411.01533v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.01531v1","updated":"2024-11-03T11:25:39Z","published":"2024-11-03T11:25:39Z","title":"DAG: Dictionary-Augmented Generation for Disambiguation of Sentences in\n  Endangered Uralic Languages using ChatGPT","summary":"  We showcase that ChatGPT can be used to disambiguate lemmas in two endangered\nlanguages ChatGPT is not proficient in, namely Erzya and Skolt Sami. We augment\nour prompt by providing dictionary translations of the candidate lemmas to a\nmajority language - Finnish in our case. This dictionary augmented generation\napproach results in 50\\% accuracy for Skolt Sami and 41\\% accuracy for Erzya.\nOn a closer inspection, many of the error types were of the kind even an\nuntrained human annotator would make.\n","authors":["Mika Hämäläinen"],"pdf_url":"https://arxiv.org/pdf/2411.01531v1.pdf","comment":"IWCLUL 2024"},{"id":"http://arxiv.org/abs/2405.09719v3","updated":"2024-11-03T11:12:14Z","published":"2024-05-15T22:28:23Z","title":"Spectral Editing of Activations for Large Language Model Alignment","summary":"  Large language models (LLMs) often exhibit undesirable behaviours, such as\ngenerating untruthful or biased content. Editing their internal representations\nhas been shown to be effective in mitigating such behaviours on top of the\nexisting alignment methods. We propose a novel inference-time editing method,\nnamely spectral editing of activations (SEA), to project the input\nrepresentations into directions with maximal covariance with the positive\ndemonstrations (e.g., truthful) while minimising covariance with the negative\ndemonstrations (e.g., hallucinated). We also extend our method to non-linear\nediting using feature functions. We run extensive experiments on benchmarks\nconcerning truthfulness and bias with six open-source LLMs of different sizes\nand model families. The results demonstrate the superiority of SEA in\neffectiveness, generalisation to similar tasks, as well as computation and data\nefficiency. We also show that SEA editing only has a limited negative impact on\nother model capabilities.\n","authors":["Yifu Qiu","Zheng Zhao","Yftah Ziser","Anna Korhonen","Edoardo M. Ponti","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2405.09719v3.pdf","comment":"24 pages, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01523v1","updated":"2024-11-03T11:03:52Z","published":"2024-11-03T11:03:52Z","title":"SinaTools: Open Source Toolkit for Arabic Natural Language Processing","summary":"  We introduce SinaTools, an open-source Python package for Arabic natural\nlanguage processing and understanding. SinaTools is a unified package allowing\npeople to integrate it into their system workflow, offering solutions for\nvarious tasks such as flat and nested Named Entity Recognition (NER),\nfully-flagged Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy\nExtractions and Evaluation, Lemmatization, Part-of-speech Tagging, Root\nTagging, and additional helper utilities such as corpus processing, text\nstripping methods, and diacritic-aware word matching. This paper presents\nSinaTools and its benchmarking results, demonstrating that SinaTools\noutperforms all similar tools on the aforementioned tasks, such as Flat NER\n(87.33%), Nested NER (89.42%), WSD (82.63%), Semantic Relatedness (0.49\nSpearman rank), Lemmatization (90.5%), POS tagging (97.5%), among others.\nSinaTools can be downloaded from (https://sina.birzeit.edu/sinatools).\n","authors":["Tymaa Hammouda","Mustafa Jarrar","Mohammed Khalilia"],"pdf_url":"https://arxiv.org/pdf/2411.01523v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.01511v1","updated":"2024-11-03T10:25:55Z","published":"2024-11-03T10:25:55Z","title":"Integration of Large Vision Language Models for Efficient Post-disaster\n  Damage Assessment and Reporting","summary":"  Traditional natural disaster response involves significant coordinated\nteamwork where speed and efficiency are key. Nonetheless, human limitations can\ndelay critical actions and inadvertently increase human and economic losses.\nAgentic Large Vision Language Models (LVLMs) offer a new avenue to address this\nchallenge, with the potential for substantial socio-economic impact,\nparticularly by improving resilience and resource access in underdeveloped\nregions. We introduce DisasTeller, the first multi-LVLM-powered framework\ndesigned to automate tasks in post-disaster management, including on-site\nassessment, emergency alerts, resource allocation, and recovery planning. By\ncoordinating four specialised LVLM agents with GPT-4 as the core model,\nDisasTeller autonomously implements disaster response activities, reducing\nhuman execution time and optimising resource distribution. Our evaluations\nthrough both LVLMs and humans demonstrate DisasTeller's effectiveness in\nstreamlining disaster response. This framework not only supports expert teams\nbut also simplifies access to disaster management processes for non-experts,\nbridging the gap between traditional response methods and LVLM-driven\nefficiency.\n","authors":["Zhaohui Chen","Elyas Asadi Shamsabadi","Sheng Jiang","Luming Shen","Daniel Dias-da-Costa"],"pdf_url":"https://arxiv.org/pdf/2411.01511v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.08763v4","updated":"2024-11-03T10:25:47Z","published":"2024-04-12T18:42:18Z","title":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language\n  Models","summary":"  Large Language Models (LLMs) have dramatically advanced AI applications, yet\ntheir deployment remains challenging due to their immense inference costs.\nRecent studies ameliorate the computational costs of LLMs by increasing their\nactivation sparsity but suffer from significant performance degradation on\ndownstream tasks. In this work, we introduce a new framework for sparsifying\nthe activations of base LLMs and reducing inference costs, dubbed Contextually\nAware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to\nimplement, and highly effective. At the heart of our framework is a new\nnon-linear activation function. We demonstrate that CATS can be applied to\nvarious base models, including Mistral-7B and Llama2-7B, and outperforms\nexisting sparsification techniques in downstream task performance. More\nprecisely, CATS-based models often achieve downstream task performance within\n1-2% of their base models without any fine-tuning and even at activation\nsparsity levels of 50%. Furthermore, CATS-based models converge faster and\ndisplay better task performance than competing techniques when fine-tuning is\napplied. Finally, we develop a custom GPU kernel for efficient implementation\nof CATS that translates the activation of sparsity of CATS to real wall-clock\ntime speedups. Our custom kernel implementation of CATS results in a ~15%\nimprovement in wall-clock inference latency of token generation on both\nLlama-7B and Mistral-7B.\n","authors":["Donghyun Lee","Je-Yong Lee","Genghan Zhang","Mo Tiwari","Azalia Mirhoseini"],"pdf_url":"https://arxiv.org/pdf/2404.08763v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11430v4","updated":"2024-11-03T09:42:35Z","published":"2024-06-17T11:35:16Z","title":"A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression","summary":"  The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.\n","authors":["Alessio Devoto","Yu Zhao","Simone Scardapane","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2406.11430v4.pdf","comment":"This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)"},{"id":"http://arxiv.org/abs/2410.23114v2","updated":"2024-11-03T09:35:12Z","published":"2024-10-30T15:25:06Z","title":"Unified Triplet-Level Hallucination Evaluation for Large Vision-Language\n  Models","summary":"  Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, in this paper we\ndesign a unified framework to measure object and relation hallucination in\nLVLMs simultaneously. The core idea of our framework is to conduct\nhallucination evaluation on (object, relation, object) triplets extracted from\nLVLMs' responses, and thus, could be easily generalized to different\nvision-language tasks. Based on our framework, we further introduce Tri-HE, a\nnovel Triplet-level Hallucination Evaluation benchmark which can be used to\nstudy both object and relation hallucination at the same time. We conduct\ncomprehensive evaluations on Tri-HE and observe that the relation hallucination\nissue is even more serious than object hallucination among existing LVLMs,\nhighlighting a previously neglected problem towards reliable LVLMs. Moreover,\nbased on our findings, we design a simple yet effective training-free approach\nto mitigate hallucinations for LVLMs, with which, we exceed all open-sourced\ncounterparts on Tri-HE, achieving comparable performance with the powerful\nGPT-4V. Our dataset and code for the reproduction of our experiments are\navailable publicly at https://github.com/wujunjie1998/Tri-HE.\n","authors":["Junjie Wu","Tsz Ting Chung","Kai Chen","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.23114v2.pdf","comment":"Project Page: https://kaichen1998.github.io/projects/tri-he/"},{"id":"http://arxiv.org/abs/2405.13911v2","updated":"2024-11-03T09:25:57Z","published":"2024-05-22T18:35:10Z","title":"TOPA: Extending Large Language Models for Video Understanding via\n  Text-Only Pre-Alignment","summary":"  Recent advancements in image understanding have benefited from the extensive\nuse of web image-text pairs. However, video understanding remains a challenge\ndespite the availability of substantial web video-text data. This difficulty\nprimarily arises from the inherent complexity of videos and the inefficient\nlanguage supervision in recent web-collected video-text datasets. In this\npaper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend\nlarge language models (LLMs) for video understanding, without the need for\npre-training on real video data. Specifically, we first employ an advanced LLM\nto automatically generate Textual Videos comprising continuous textual frames,\nalong with corresponding annotations to simulate real video-text data. Then,\nthese annotated textual videos are used to pre-align a language-only LLM with\nthe video modality. To bridge the gap between textual and real videos, we\nemploy the CLIP model as the feature extractor to align image and text\nmodalities. During text-only pre-alignment, the continuous textual frames,\nencoded as a sequence of CLIP text features, are analogous to continuous CLIP\nimage features, thus aligning the LLM with real video representation. Extensive\nexperiments, including zero-shot evaluation and finetuning on various video\nunderstanding tasks, demonstrate that TOPA is an effective and efficient\nframework for aligning video content with LLMs. In particular, without training\non any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%\non the challenging long-form video understanding benchmark, Egoschema. This\nperformance surpasses previous video-text pre-training approaches and proves\ncompetitive with recent GPT-3.5-based video agents.\n","authors":["Wei Li","Hehe Fan","Yongkang Wong","Mohan Kankanhalli","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2405.13911v2.pdf","comment":"NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2410.15365v2","updated":"2024-11-03T09:23:12Z","published":"2024-10-20T11:47:17Z","title":"BERTtime Stories: Investigating the Role of Synthetic Story Data in\n  Language pre-training","summary":"  We describe our contribution to the Strict and Strict-Small tracks of the 2nd\niteration of the BabyLM Challenge. The shared task is centered around efficient\npre-training given data constraints motivated by human development. In\nresponse, we study the effect of synthetic story data in language pre-training\nusing TinyStories: a recently introduced dataset of short stories. Initially,\nwe train GPT-Neo models on subsets of TinyStories, while varying the amount of\navailable data. We find that, even with access to less than 100M words, the\nmodels are able to generate high-quality, original completions to a given\nstory, and acquire substantial linguistic knowledge. To measure the effect of\nsynthetic story data, we train LTG-BERT encoder models on a combined dataset\nof: a subset of TinyStories, story completions generated by GPT-Neo, and a\nsubset of the BabyLM dataset. Our experimentation reveals that synthetic data\ncan occasionally offer modest gains, but overall have a negative influence on\nlinguistic understanding. Our work offers an initial study on synthesizing\nstory data in low resource settings and underscores their potential for\naugmentation in data-constrained language modeling. We publicly release our\nmodels and implementation on our GitHub.\n","authors":["Nikitas Theodoropoulos","Giorgos Filandrianos","Vassilis Lyberatos","Maria Lymperaiou","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2410.15365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01493v1","updated":"2024-11-03T09:18:28Z","published":"2024-11-03T09:18:28Z","title":"Sample-Efficient Alignment for LLMs","summary":"  We study methods for efficiently aligning large language models (LLMs) with\nhuman preferences given budgeted online feedback. We first formulate the LLM\nalignment problem in the frame of contextual dueling bandits. This formulation,\nsubsuming recent paradigms such as online RLHF and online DPO, inherently\nquests for sample-efficient algorithms that incorporate online active\nexploration. Leveraging insights from bandit theory, we introduce a unified\nalgorithm based on Thompson sampling and highlight its applications in two\ndistinct LLM alignment scenarios. The practical agent that efficiently\nimplements this algorithm, named SEA (Sample-Efficient Alignment), is\nempirically validated through extensive experiments across three model scales\n(1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The\nresults demonstrate that SEA achieves highly sample-efficient alignment with\noracle's preferences, outperforming recent active exploration methods for LLMs.\nAdditionally, we release the implementation of SEA together with an efficient\ncodebase designed for online alignment of LLMs, aiming to accelerate future\nresearch in this field.\n","authors":["Zichen Liu","Changyu Chen","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2411.01493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05137v2","updated":"2024-11-03T09:09:21Z","published":"2024-09-08T15:42:48Z","title":"READoc: A Unified Benchmark for Realistic Document Structured Extraction","summary":"  Document Structured Extraction (DSE) aims to extract structured content from\nraw documents. Despite the emergence of numerous DSE systems, their unified\nevaluation remains inadequate, significantly hindering the field's advancement.\nThis problem is largely attributed to existing benchmark paradigms, which\nexhibit fragmented and localized characteristics. To address these limitations\nand offer a thorough evaluation of DSE systems, we introduce a novel benchmark\nnamed READoc, which defines DSE as a realistic task of converting unstructured\nPDFs into semantically rich Markdown. The READoc dataset is derived from 2,233\ndiverse and real-world documents from arXiv and GitHub. In addition, we develop\na DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring\nmodules, to conduct a unified evaluation of state-of-the-art DSE approaches. By\nevaluating a range of pipeline tools, expert visual models, and general VLMs,\nwe identify the gap between current work and the unified, realistic DSE\nobjective for the first time. We aspire that READoc will catalyze future\nresearch in DSE, fostering more comprehensive and practical solutions.\n","authors":["Zichao Li","Aizier Abulaiti","Yaojie Lu","Xuanang Chen","Jia Zheng","Hongyu Lin","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2409.05137v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20092v2","updated":"2024-11-03T09:04:54Z","published":"2024-05-30T14:31:33Z","title":"Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in\n  Code Generation","summary":"  Despite recent progress made by large language models in code generation,\nthey still struggle with programs that meet complex requirements. Recent work\nutilizes plan-and-solve decomposition to decrease the complexity and leverage\nself-tests to refine the generated program. Yet, planning deep-inside\nrequirements in advance can be challenging, and the tests need to be accurate\nto accomplish self-improvement. To this end, we propose FunCoder, a code\ngeneration framework incorporating the divide-and-conquer strategy with\nfunctional consensus. Specifically, FunCoder recursively branches off\nsub-functions as smaller goals during code generation, represented by a tree\nhierarchy. These sub-functions are then composited to attain more complex\nobjectives. Additionally, we designate functions via a consensus formed by\nidentifying similarities in program behavior, mitigating error propagation.\nFunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval,\nMBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method\ndemonstrates superiority on smaller models: With FunCoder, StableCode-3b\nsurpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4's performance on\nHumanEval. Further analysis reveals that our proposed dynamic function\ndecomposition is capable of handling complex requirements, and the functional\nconsensus prevails over self-testing in correctness evaluation.\n","authors":["Jingchang Chen","Hongxuan Tang","Zheng Chu","Qianglong Chen","Zekun Wang","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2405.20092v2.pdf","comment":"NeurIPS 2024 oral"},{"id":"http://arxiv.org/abs/2411.01485v1","updated":"2024-11-03T08:57:41Z","published":"2024-11-03T08:57:41Z","title":"Domain-specific Guided Summarization for Mental Health Posts","summary":"  In domain-specific contexts, particularly mental health, abstractive\nsummarization requires advanced techniques adept at handling specialized\ncontent to generate domain-relevant and faithful summaries. In response to\nthis, we introduce a guided summarizer equipped with a dual-encoder and an\nadapted decoder that utilizes novel domain-specific guidance signals, i.e.,\nmental health terminologies and contextually rich sentences from the source\ndocument, to enhance its capacity to align closely with the content and context\nof guidance, thereby generating a domain-relevant summary. Additionally, we\npresent a post-editing correction model to rectify errors in the generated\nsummary, thus enhancing its consistency with the original content in detail.\nEvaluation on the MentSum dataset reveals that our model outperforms existing\nbaseline models in terms of both ROUGE and FactCC scores. Although the\nexperiments are specifically designed for mental health posts, the methodology\nwe've developed offers broad applicability, highlighting its versatility and\neffectiveness in producing high-quality domain-specific summaries.\n","authors":["Lu Qian","Yuqi Wang","Zimu Wang","Haiyang Zhang","Wei Wang","Ting Yu","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.01485v1.pdf","comment":"Accepted at PACLIC 2024. Camera-ready version"},{"id":"http://arxiv.org/abs/2411.01483v1","updated":"2024-11-03T08:49:55Z","published":"2024-11-03T08:49:55Z","title":"Teaching Models to Improve on Tape","summary":"  Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such ``corrective feedback''. Here we claim that\nthis skill of LLMs can be significantly enhanced via training. We introduce an\nRL framework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts.\n","authors":["Liat Bezalel","Eyal Orgad","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2411.01483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01477v1","updated":"2024-11-03T08:30:29Z","published":"2024-11-03T08:30:29Z","title":"DPCL-Diff: The Temporal Knowledge Graph Reasoning based on Graph Node\n  Diffusion Model with Dual-Domain Periodic Contrastive Learning","summary":"  Temporal knowledge graph (TKG) reasoning that infers future missing facts is\nan essential and challenging task. Predicting future events typically relies on\nclosely related historical facts, yielding more accurate results for repetitive\nor periodic events. However, for future events with sparse historical\ninteractions, the effectiveness of this method, which focuses on leveraging\nhigh-frequency historical information, diminishes. Recently, the capabilities\nof diffusion models in image generation have opened new opportunities for TKG\nreasoning. Therefore, we propose a graph node diffusion model with dual-domain\nperiodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)\nintroduces noise into sparsely related events to simulate new events,\ngenerating high-quality data that better conforms to the actual distribution.\nThis generative mechanism significantly enhances the model's ability to reason\nabout new events. Additionally, the dual-domain periodic contrastive learning\n(DPCL) maps periodic and non-periodic event entities to Poincar\\'e and\nEuclidean spaces, leveraging their characteristics to distinguish similar\nperiodic events effectively. Experimental results on four public datasets\ndemonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG\nmodels in event prediction, demonstrating our approach's effectiveness. This\nstudy also investigates the combined effectiveness of GNDiff and DPCL in TKG\ntasks.\n","authors":["Yukun Cao","Lisheng Wang","Luobing Huang"],"pdf_url":"https://arxiv.org/pdf/2411.01477v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.01474v1","updated":"2024-11-03T08:15:43Z","published":"2024-11-03T08:15:43Z","title":"MoCE: Adaptive Mixture of Contextualization Experts for Byte-based\n  Neural Machine Translation","summary":"  Byte-based machine translation systems have shown significant potential in\nmassively multilingual settings. Unicode encoding, which maps each character to\nspecific byte(s), eliminates the emergence of unknown words, even in new\nlanguages, enabling broad language scalability. However, byte-level\ntokenization results in sequences that are hard to interpret due to limited\nsemantic information per byte. Local contextualization has proven effective in\nassigning initial semantics to tokens, improving sentence comprehension.\nNevertheless, variations in encoding rules across languages necessitate an\nadaptive approach for effective contextualization. To this end, we propose\nAdaptive MultiScale-Headed Attention (Ada-MSHA), adaptively selecting and\nmixing attention heads, which are treated as contextualization experts. This\nenhances the flexibility of contextualization scales and improves the potential\nto discover a better strategy than previous methods. Experiment results show\nthat our method outperforms existing methods without extensive manual\nadjustment of hyper-parameters and surpasses subword-based models with fewer\nparameters in Ted-59 dataset. Our code is available at\nhttps://github.com/ictnlp/MoCE.\n","authors":["Langlin Huang","Mengyu Bu","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2411.01474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18634v2","updated":"2024-11-03T08:14:34Z","published":"2024-10-24T10:47:30Z","title":"Little Giants: Synthesizing High-Quality Embedding Data at Scale","summary":"  Synthetic data generation has become an increasingly popular way of training\nmodels without the need for large, manually labeled datasets. For tasks like\ntext embedding, synthetic data offers diverse and scalable training examples,\nsignificantly reducing the cost of human annotation. However, most current\napproaches rely heavily on proprietary models like GPT-4, which are expensive\nand inefficient for generating large-scale embedding data. In this paper, we\nintroduce SPEED, a framework that aligns open-source small models (8B) to\nefficiently generate large-scale synthetic embedding data. Through supervised\nfine-tuning, preference optimization, and self-improvement, SPEED enables small\nopen-source models to produce high-quality data. Remarkably, SPEED uses only\nless than 1/10 of the GPT API calls, outperforming the state-of-the-art\nembedding model E5_mistral when both are trained solely on their synthetic\ndata. Using this efficient generator, we conduct a comprehensive study on how\nvarious factors within the alignment pipeline impact data quality and reveal\nthe scaling law for synthetic embedding data.\n","authors":["Haonan Chen","Liang Wang","Nan Yang","Yutao Zhu","Ziliang Zhao","Furu Wei","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2410.18634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09824v3","updated":"2024-11-03T06:58:34Z","published":"2024-10-13T12:57:08Z","title":"Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation","summary":"  Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206.\n","authors":["Jiarui Ji","Runlin Lei","Jialing Bi","Zhewei Wei","Yankai Lin","Xuchen Pan","Yaliang Li","Bolin Ding"],"pdf_url":"https://arxiv.org/pdf/2410.09824v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16165v2","updated":"2024-11-03T06:03:56Z","published":"2024-10-21T16:31:23Z","title":"From Tokens to Materials: Leveraging Language Models for Scientific\n  Discovery","summary":"  Exploring the predictive capabilities of language models in material science\nis an ongoing interest. This study investigates the application of language\nmodel embeddings to enhance material property prediction in materials science.\nBy evaluating various contextual embedding methods and pre-trained models,\nincluding Bidirectional Encoder Representations from Transformers (BERT) and\nGenerative Pre-trained Transformers (GPT), we demonstrate that domain-specific\nmodels, particularly MatBERT significantly outperform general-purpose models in\nextracting implicit knowledge from compound names and material properties. Our\nfindings reveal that information-dense embeddings from the third layer of\nMatBERT, combined with a context-averaging approach, offer the most effective\nmethod for capturing material-property relationships from the scientific\nliterature. We also identify a crucial \"tokenizer effect,\" highlighting the\nimportance of specialized text processing techniques that preserve complete\ncompound names while maintaining consistent token counts. These insights\nunderscore the value of domain-specific training and tokenization in materials\nscience applications and offer a promising pathway for accelerating the\ndiscovery and development of new materials through AI-driven approaches.\n","authors":["Yuwei Wan","Tong Xie","Nan Wu","Wenjie Zhang","Chunyu Kit","Bram Hoex"],"pdf_url":"https://arxiv.org/pdf/2410.16165v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17283v2","updated":"2024-11-03T05:25:26Z","published":"2024-06-25T05:18:12Z","title":"A Recursive Encoding for Cuneiform Signs","summary":"  One of the most significant problems in cuneiform pedagogy is the process of\nlooking up unknown signs, which often involves a tedious page-by-page search\nthrough a sign list. This paper proposes a new \"recursive encoding\" for signs,\nwhich represents the arrangement of strokes in a way a computer can process. A\nseries of new algorithms then offers students a new way to look up signs by any\ndistinctive component, as well as providing new ways to render signs and\ntablets electronically.\n","authors":["Daniel M. Stelzer"],"pdf_url":"https://arxiv.org/pdf/2406.17283v2.pdf","comment":"27 pages, 29 figures, 5 tables"},{"id":"http://arxiv.org/abs/2402.15733v3","updated":"2024-11-03T05:00:52Z","published":"2024-02-24T06:05:15Z","title":"ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for\n  Arabic Characters","summary":"  Brain-computer interfaces is an important and hot research topic that\nrevolutionize how people interact with the world, especially for individuals\nwith neurological disorders. While extensive research has been done in EEG\nsignals of English letters and words, a major limitation remains: the lack of\npublicly available EEG datasets for many non-English languages, such as Arabic.\nAlthough Arabic is one of the most spoken languages worldwide, to the best of\nour knowledge, there is no publicly available dataset for EEG signals of Arabic\ncharacters until now. To address this gap, we introduce ArEEG_Chars, a novel\nEEG dataset for Arabic 31 characters collected from 30 participants (21 males\nand 9 females), these records were collected using Epoc X 14 channels device\nfor 10 seconds long for each char record. The number of recorded signals were\n930 EEG recordings. To make the EEG signals suitable for analyzing, each\nrecording has been split into multiple signals with a time duration of 250ms,\nrespectively. Therefore, a total of 39857 recordings of EEG signals have been\ncollected in this study. Moreover, ArEEG_Chars will be publicly available for\nresearchers. We do hope that this dataset will fill an important gap in the\nresearch of Arabic EEG benefiting Arabic-speaking individuals with\ndisabilities.\n","authors":["Hazem Darwish","Abdalrahman Al Malah","Khloud Al Jallad","Nada Ghneim"],"pdf_url":"https://arxiv.org/pdf/2402.15733v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10176v2","updated":"2024-11-03T03:48:02Z","published":"2024-02-15T18:26:11Z","title":"OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset","summary":"  Recent work has shown the immense potential of synthetically generated\ndatasets for training large language models (LLMs), especially for acquiring\ntargeted skills. Current large-scale math instruction tuning datasets such as\nMetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed\nusing outputs from closed-source LLMs with commercially restrictive licenses. A\nkey reason limiting the use of open-source LLMs in these data generation\npipelines has been the wide gap between the mathematical skills of the best\nclosed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on\nthe recent progress in open-source LLMs, our proposed prompting novelty, and\nsome brute-force scaling, we construct OpenMathInstruct-1, a math instruction\ntuning dataset with 1.8M problem-solution pairs. The dataset is constructed by\nsynthesizing code-interpreter solutions for GSM8K and MATH, two popular math\nreasoning benchmarks, using the recently released and permissively licensed\nMixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of\nOpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which\nis competitive with the best gpt-distilled models. We release our code, models,\nand the OpenMathInstruct-1 dataset under a commercially permissive license.\n","authors":["Shubham Toshniwal","Ivan Moshkov","Sean Narenthiran","Daria Gitman","Fei Jia","Igor Gitman"],"pdf_url":"https://arxiv.org/pdf/2402.10176v2.pdf","comment":"Camera-ready version for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.09395v6","updated":"2024-11-03T02:52:58Z","published":"2024-01-17T18:13:07Z","title":"Evaluating LLMs' Mathematical and Coding Competency through\n  Ontology-guided Interventions","summary":"  Recent advancements in Large Language Models (LLMs) have showcased striking\nresults on existing logical reasoning benchmarks, with some models even\nsurpassing human performance. However, the true depth of their competencies and\nrobustness in reasoning tasks remains an open question. To this end, in this\npaper, we focus on two popular reasoning tasks: arithmetic reasoning and code\ngeneration. Particularly, we introduce (i) a general ontology of perturbations\nfor math and coding questions, (ii) a semi-automatic method to apply these\nperturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,\nof perturbed math and coding problems to probe LLM capabilities in numeric\nreasoning and coding tasks. Through comprehensive evaluations of both\nclosed-source and open-source LLMs, we show a significant performance drop\nacross all the models against the perturbed questions, suggesting that the\ncurrent LLMs lack robust problem solving skills and structured reasoning\nabilities in many areas, as defined by our ontology. We open-source the\ndatasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest.\n","authors":["Pengfei Hong","Navonil Majumder","Deepanway Ghosal","Somak Aditya","Rada Mihalcea","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2401.09395v6.pdf","comment":"With o1 and GPT-4o results. Reformatted the data and presented more\n  analysis"},{"id":"http://arxiv.org/abs/2411.01409v1","updated":"2024-11-03T02:38:43Z","published":"2024-11-03T02:38:43Z","title":"Classifier-guided Gradient Modulation for Enhanced Multimodal Learning","summary":"  Multimodal learning has developed very fast in recent years. However, during\nthe multimodal training process, the model tends to rely on only one modality\nbased on which it could learn faster, thus leading to inadequate use of other\nmodalities. Existing methods to balance the training process always have some\nlimitations on the loss functions, optimizers and the number of modalities and\nonly consider modulating the magnitude of the gradients while ignoring the\ndirections of the gradients. To solve these problems, in this paper, we present\na novel method to balance multimodal learning with Classifier-Guided Gradient\nModulation (CGGM), considering both the magnitude and directions of the\ngradients. We conduct extensive experiments on four multimodal datasets:\nUPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification,\nregression and segmentation tasks. The results show that CGGM outperforms all\nthe baselines and other state-of-the-art methods consistently, demonstrating\nits effectiveness and versatility. Our code is available at\nhttps://github.com/zrguo/CGGM.\n","authors":["Zirun Guo","Tao Jin","Jingyuan Chen","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.01409v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.16038v3","updated":"2024-11-03T01:53:41Z","published":"2024-03-24T06:49:07Z","title":"Monotonic Paraphrasing Improves Generalization of Language Model\n  Prompting","summary":"  Performance of large language models (LLMs) may vary with different prompts\nor instructions of even the same task. One commonly recognized factor for this\nphenomenon is the model's familiarity with the given prompt or instruction,\nwhich is typically estimated by its perplexity. However, finding the prompt\nwith the lowest perplexity is challenging, given the enormous space of possible\nprompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),\nan end-to-end decoding strategy that paraphrases given prompts or instructions\ninto their lower perplexity counterparts based on an ensemble of a paraphrase\nLM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or\ninstruction executor) that constrains the generation for lower perplexity. The\nensemble decoding process can efficiently paraphrase the original prompt\nwithout altering its semantic meaning, while monotonically decreasing the\nperplexity of each generation as calculated by the target LM. We explore in\ndetail both greedy and search-based decoding as two alternative decoding\nschemes of MonoPara. Notably, MonoPara does not require any training and can\nmonotonically lower the perplexity of the paraphrased prompt or instruction,\nleading to improved performance of zero-shot LM prompting as evaluated on a\nwide selection of tasks. In addition, MonoPara is also shown to effectively\nimprove LMs' generalization on perturbed and unseen task instructions.\n","authors":["Qin Liu","Fei Wang","Nan Xu","Tianyi Yan","Tao Meng","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16038v3.pdf","comment":"EMNLP 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2409.11564v2","updated":"2024-11-03T01:51:57Z","published":"2024-09-17T21:28:51Z","title":"Preference Tuning with Human Feedback on Language, Speech, and Vision\n  Tasks: A Survey","summary":"  Preference tuning is a crucial process for aligning deep generative models\nwith human preferences. This survey offers a thorough overview of recent\nadvancements in preference tuning and the integration of human feedback. The\npaper is organized into three main sections: 1) introduction and preliminaries:\nan introduction to reinforcement learning frameworks, preference tuning tasks,\nmodels, and datasets across various modalities: language, speech, and vision,\nas well as different policy approaches, 2) in-depth exploration of each\npreference tuning approach: a detailed analysis of the methods used in\npreference tuning, and 3) applications, discussion, and future directions: an\nexploration of the applications of preference tuning in downstream tasks,\nincluding evaluation methods for different modalities, and an outlook on future\nresearch directions. Our objective is to present the latest methodologies in\npreference tuning and model alignment, enhancing the understanding of this\nfield for researchers and practitioners. We hope to encourage further\nengagement and innovation in this area.\n","authors":["Genta Indra Winata","Hanyang Zhao","Anirban Das","Wenpin Tang","David D. Yao","Shi-Xiong Zhang","Sambit Sahu"],"pdf_url":"https://arxiv.org/pdf/2409.11564v2.pdf","comment":"Survey paper"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.15590v2","updated":"2024-11-03T23:51:33Z","published":"2024-09-23T22:48:04Z","title":"MapEx: Indoor Structure Exploration with Probabilistic Information Gain\n  from Global Map Predictions","summary":"  Exploration is a critical challenge in robotics, centered on understanding\nunknown environments. In this work, we focus on robots exploring structured\nindoor environments which are often predictable and composed of repeating\npatterns. Most existing approaches, such as conventional frontier approaches,\nhave difficulty leveraging the predictability and explore with simple\nheuristics such as `closest first'. Recent works use deep learning techniques\nto predict unknown regions of the map, using these predictions for information\ngain calculation. However, these approaches are often sensitive to the\npredicted map quality or do not reason over sensor coverage. To overcome these\nissues, our key insight is to jointly reason over what the robot can observe\nand its uncertainty to calculate probabilistic information gain. We introduce\nMapEx, a new exploration framework that uses predicted maps to form\nprobabilistic sensor model for information gain estimation. MapEx generates\nmultiple predicted maps based on observed information, and takes into\nconsideration both the computed variances of predicted maps and estimated\nvisible area to estimate the information gain of a given viewpoint. Experiments\non the real-world KTH dataset showed on average 12.4% improvement than\nrepresentative map-prediction based exploration and 25.4% improvement than\nnearest frontier approach.\n","authors":["Cherie Ho","Seungchan Kim","Brady Moon","Aditya Parandekar","Narek Harutyunyan","Chen Wang","Katia Sycara","Graeme Best","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2409.15590v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2411.01713v1","updated":"2024-11-03T23:36:53Z","published":"2024-11-03T23:36:53Z","title":"Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models","summary":"  Modern optimizers such as AdamW, equipped with momentum and adaptive learning\nrate, are designed to escape local minima and explore the vast parameter space.\nThis exploration is beneficial for finding good loss basins when training from\nscratch. It is not necessarily ideal when resuming from a powerful foundation\nmodel because it can lead to large deviations from the pre-trained\ninitialization and, consequently, worse robustness and generalization. At the\nsame time, strong regularization on all parameters can lead to under-fitting.\nWe hypothesize that selectively regularizing the parameter space is the key to\nfitting and retraining the pre-trained knowledge. This paper proposes a new\nweight decay technique, Selective Projection Decay (SPD), that selectively\nimposes a strong penalty on certain layers while allowing others to change\nfreely. Intuitively, SPD expands and contracts the parameter search space for\nlayers with consistent and inconsistent loss reduction, respectively.\nExperimentally, when equipped with SPD, Adam consistently provides better\nin-distribution generalization and out-of-distribution robustness performance\non multiple popular vision and language benchmarks. Code available\nat~\\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}\n","authors":["Junjiao Tian","Chengyue Huang","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2411.01713v1.pdf","comment":"Accepted to Neurips 2024"},{"id":"http://arxiv.org/abs/2410.02401v4","updated":"2024-11-03T22:57:56Z","published":"2024-10-03T11:29:09Z","title":"SynCo: Synthetic Hard Negatives in Contrastive Learning for Better\n  Unsupervised Visual Representations","summary":"  Contrastive learning has become a dominant approach in self-supervised visual\nrepresentation learning. Hard negatives - samples closely resembling the anchor\n- are key to enhancing learned representations' discriminative power. However,\nefficiently leveraging hard negatives remains challenging. We introduce SynCo\n(Synthetic Negatives in Contrastive learning), a novel approach that improves\nmodel performance by generating synthetic hard negatives on the representation\nspace. Building on the MoCo framework, SynCo introduces six strategies for\ncreating diverse synthetic hard negatives on-the-fly with minimal computational\noverhead. SynCo achieves faster training and better representation learning,\nreaching 67.9% top-1 accuracy on ImageNet ILSVRC-2012 linear evaluation after\n200 pretraining epochs, surpassing MoCo's 67.5% using the same ResNet-50\nencoder. It also transfers more effectively to detection tasks: on PASCAL VOC,\nit outperforms both the supervised baseline and MoCo with 82.5% AP; on COCO, it\nsets new benchmarks with 40.9% AP for bounding box detection and 35.5% AP for\ninstance segmentation. Our synthetic hard negative generation approach\nsignificantly enhances visual representations learned through self-supervised\ncontrastive learning. Code is available at\nhttps://github.com/giakoumoglou/synco.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2410.02401v4.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.09474v2","updated":"2024-11-03T22:31:31Z","published":"2024-10-12T10:27:23Z","title":"Distilling Invariant Representations with Dual Augmentation","summary":"  Knowledge distillation (KD) has been widely used to transfer knowledge from\nlarge, accurate models (teachers) to smaller, efficient ones (students). Recent\nmethods have explored enforcing consistency by incorporating causal\ninterpretations to distill invariant representations. In this work, we extend\nthis line of research by introducing a dual augmentation strategy to promote\ninvariant feature learning in both teacher and student models. Our approach\nleverages different augmentations applied to both models during distillation,\npushing the student to capture robust, transferable features. This dual\naugmentation strategy complements invariant causal distillation by ensuring\nthat the learned representations remain stable across a wider range of data\nvariations and transformations. Extensive experiments on CIFAR-100 demonstrate\nthe effectiveness of this approach, achieving competitive results in\nsame-architecture KD.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2410.09474v2.pdf","comment":"This paper presents preliminary results from a project that we have\n  since discontinued, as our research focus has shifted to new directions"},{"id":"http://arxiv.org/abs/2403.14468v4","updated":"2024-11-03T21:16:54Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks","summary":"  In the dynamic field of digital content creation using generative models,\nstate-of-the-art video editing models still do not offer the level of quality\nand control that users desire. Previous works on video editing either extended\nfrom image-based generative models in a zero-shot manner or necessitated\nextensive fine-tuning, which can hinder the production of fluid video edits.\nFurthermore, these methods frequently rely on textual input as the editing\nguidance, leading to ambiguities and limiting the types of edits they can\nperform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free\nparadigm designed to simplify video editing into two primary steps: (1)\nemploying an off-the-shelf image editing model to modify the first frame, (2)\nutilizing an existing image-to-video generation model to generate the edited\nvideo through temporal feature injection. AnyV2V can leverage any existing\nimage editing tools to support an extensive array of video editing tasks,\nincluding prompt-based editing, reference-based style transfer, subject-driven\nediting, and identity manipulation, which were unattainable by previous\nmethods. AnyV2V can also support any video length. Our evaluation shows that\nAnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore,\nAnyV2V significantly outperformed these baselines in human evaluations,\ndemonstrating notable improvements in visual consistency with the source video\nwhile producing high-quality edits across all editing tasks.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Harry Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v4.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR 2024)\n  (11/2024)"},{"id":"http://arxiv.org/abs/2411.01683v1","updated":"2024-11-03T20:46:50Z","published":"2024-11-03T20:46:50Z","title":"ROAD-Waymo: Action Awareness at Scale for Autonomous Driving","summary":"  Autonomous Vehicle (AV) perception systems require more than simply seeing,\nvia e.g., object detection or scene segmentation. They need a holistic\nunderstanding of what is happening within the scene for safe interaction with\nother road users. Few datasets exist for the purpose of developing and training\nalgorithms to comprehend the actions of other road users. This paper presents\nROAD-Waymo, an extensive dataset for the development and benchmarking of\ntechniques for agent, action, location and event detection in road scenes,\nprovided as a layer upon the (US) Waymo Open dataset. Considerably larger and\nmore challenging than any existing dataset (and encompassing multiple cities),\nit comes with 198k annotated video frames, 54k agent tubes, 3.9M bounding boxes\nand a total of 12.4M labels. The integrity of the dataset has been confirmed\nand enhanced via a novel annotation pipeline designed for automatically\nidentifying violations of requirements specifically designed for this dataset.\nAs ROAD-Waymo is compatible with the original (UK) ROAD dataset, it provides\nthe opportunity to tackle domain adaptation between real-world road scenarios\nin different countries within a novel benchmark: ROAD++.\n","authors":["Salman Khan","Izzeddin Teeti","Reza Javanmard Alitappeh","Mihaela C. Stoian","Eleonora Giunchiglia","Gurkirt Singh","Andrew Bradley","Fabio Cuzzolin"],"pdf_url":"https://arxiv.org/pdf/2411.01683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00985v3","updated":"2024-11-03T20:34:35Z","published":"2024-06-03T04:43:56Z","title":"ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with\n  Attention Grouping","summary":"  Text-driven image synthesis has made significant advancements with the\ndevelopment of diffusion models, transforming how visual content is generated\nfrom text prompts. Despite these advances, text-driven image editing, a key\narea in computer graphics, faces unique challenges. A major challenge is making\nsimultaneous edits across multiple objects or attributes. Applying these\nmethods sequentially for multi-attribute edits increases computational demands\nand efficiency losses. In this paper, we address these challenges with\nsignificant contributions. Our main contribution is the development of\nParallelEdits, a method that seamlessly manages simultaneous edits across\nmultiple attributes. In contrast to previous approaches, ParallelEdits not only\npreserves the quality of single attribute edits but also significantly improves\nthe performance of multitasking edits. This is achieved through innovative\nattention distribution mechanism and multi-branch design that operates across\nseveral processing heads. Additionally, we introduce the PIE-Bench++ dataset,\nan expansion of the original PIE-Bench dataset, to better support evaluating\nimage-editing tasks involving multiple objects and attributes simultaneously.\nThis dataset is a benchmark for evaluating text-driven image editing methods in\nmultifaceted scenarios.\n","authors":["Mingzhen Huang","Jialing Cai","Shan Jia","Vishnu Suresh Lokhande","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2406.00985v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13743v3","updated":"2024-11-03T20:22:32Z","published":"2024-06-19T18:00:07Z","title":"GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual\n  Generation","summary":"  While text-to-visual models now produce photo-realistic images and videos,\nthey struggle with compositional text prompts involving attributes,\nrelationships, and higher-order reasoning such as logic and comparison. In this\nwork, we conduct an extensive human study on GenAI-Bench to evaluate the\nperformance of leading image and video generation models in various aspects of\ncompositional text-to-visual generation. We also compare automated evaluation\nmetrics against our collected human ratings and find that VQAScore -- a metric\nmeasuring the likelihood that a VQA model views an image as accurately\ndepicting the prompt -- significantly outperforms previous metrics such as\nCLIPScore. In addition, VQAScore can improve generation in a black-box manner\n(without finetuning) via simply ranking a few (3 to 9) candidate images.\nRanking by VQAScore is 2x to 3x more effective than other scoring methods like\nPickScore, HPSv2, and ImageReward at improving human alignment ratings for\nDALL-E 3 and Stable Diffusion, especially on compositional prompts that require\nadvanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with\nover 40,000 human ratings to evaluate scoring metrics on ranking images\ngenerated from the same prompt. Lastly, we discuss promising areas for\nimprovement in VQAScore, such as addressing fine-grained visual details. We\nwill release all human ratings (over 80,000) to facilitate scientific\nbenchmarking of both generative models and automated metrics.\n","authors":["Baiqi Li","Zhiqiu Lin","Deepak Pathak","Jiayao Li","Yixin Fei","Kewen Wu","Tiffany Ling","Xide Xia","Pengchuan Zhang","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2406.13743v3.pdf","comment":"We open-source our dataset, model, and code at:\n  https://linzhiqiu.github.io/papers/genai_bench ; Project page:\n  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first\n  introduced in arxiv:2404.01291. This article extends it with an additional\n  GenAI-Rank benchmark"},{"id":"http://arxiv.org/abs/2411.01669v1","updated":"2024-11-03T19:49:52Z","published":"2024-11-03T19:49:52Z","title":"MamT$^4$: Multi-view Attention Networks for Mammography Cancer\n  Classification","summary":"  In this study, we introduce a novel method, called MamT$^4$, which is used\nfor simultaneous analysis of four mammography images. A decision is made based\non one image of a breast, with attention also devoted to three additional\nimages: another view of the same breast and two images of the other breast.\nThis approach enables the algorithm to closely replicate the practice of a\nradiologist who reviews the entire set of mammograms for a patient.\nFurthermore, this paper emphasizes the preprocessing of images, specifically\nproposing a cropping model (U-Net based on ResNet-34) to help the method remove\nimage artifacts and focus on the breast region. To the best of our knowledge,\nthis study is the first to achieve a ROC-AUC of 84.0 $\\pm$ 1.7 and an F1 score\nof 56.0 $\\pm$ 1.3 on an independent test dataset of Vietnam digital mammography\n(VinDr-Mammo), which is preprocessed with the cropping model.\n","authors":["Alisher Ibragimov","Sofya Senotrusova","Arsenii Litvinov","Egor Ushakov","Evgeny Karpulevich","Yury Markin"],"pdf_url":"https://arxiv.org/pdf/2411.01669v1.pdf","comment":"The crop model is available here:\n  https://github.com/ispras/mammo_crop"},{"id":"http://arxiv.org/abs/2403.07536v2","updated":"2024-11-03T19:21:04Z","published":"2024-03-12T11:19:46Z","title":"LaB-GATr: geometric algebra transformers for large biomedical surface\n  and volume meshes","summary":"  Many anatomical structures can be described by surface or volume meshes.\nMachine learning is a promising tool to extract information from these 3D\nmodels. However, high-fidelity meshes often contain hundreds of thousands of\nvertices, which creates unique challenges in building deep neural network\narchitectures. Furthermore, patient-specific meshes may not be canonically\naligned which limits the generalisation of machine learning algorithms. We\npropose LaB-GATr, a transfomer neural network with geometric tokenisation that\ncan effectively learn with large-scale (bio-)medical surface and volume meshes\nthrough sequence compression and interpolation. Our method extends the recently\nproposed geometric algebra transformer (GATr) and thus respects all Euclidean\nsymmetries, i.e. rotation, translation and reflection, effectively mitigating\nthe problem of canonical alignment between patients. LaB-GATr achieves\nstate-of-the-art results on three tasks in cardiovascular hemodynamics\nmodelling and neurodevelopmental phenotype prediction, featuring meshes of up\nto 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful\narchitecture for learning with high-fidelity meshes which has the potential to\nenable interesting downstream applications. Our implementation is publicly\navailable.\n","authors":["Julian Suk","Baris Imre","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2403.07536v2.pdf","comment":"First published in \"Medical Image Computing and Computer Assisted\n  Intervention\" (MICCAI), pp 185-195, 2024 by Springer Nature"},{"id":"http://arxiv.org/abs/2410.22489v2","updated":"2024-11-03T19:00:34Z","published":"2024-10-29T19:28:41Z","title":"Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation","summary":"  Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to\nsegment novel categories with minimal annotated support samples. While existing\nFS-PCS methods have shown promise, they primarily focus on unimodal point cloud\ninputs, overlooking the potential benefits of leveraging multimodal\ninformation. In this paper, we address this gap by introducing a cost-free\nmultimodal FS-PCS setup, utilizing textual labels and the potentially available\n2D image modality. Under this easy-to-achieve setup, we present the MultiModal\nFew-Shot SegNet (MM-FSS), a model effectively harnessing complementary\ninformation from multiple modalities. MM-FSS employs a shared backbone with two\nheads to extract intermodal and unimodal visual features, and a pretrained text\nencoder to generate text embeddings. To fully exploit the multimodal\ninformation, we propose a Multimodal Correlation Fusion (MCF) module to\ngenerate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module\nto refine the correlations using text-aware semantic guidance. Additionally, we\npropose a simple yet effective Test-time Adaptive Cross-modal Calibration\n(TACC) technique to mitigate training bias, further improving generalization.\nExperimental results on S3DIS and ScanNet datasets demonstrate significant\nperformance improvements achieved by our method. The efficacy of our approach\nindicates the benefits of leveraging commonly-ignored free modalities for\nFS-PCS, providing valuable insights for future research. The code is available\nat https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot\n","authors":["Zhaochong An","Guolei Sun","Yun Liu","Runjia Li","Min Wu","Ming-Ming Cheng","Ender Konukoglu","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2410.22489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01656v1","updated":"2024-11-03T18:57:19Z","published":"2024-11-03T18:57:19Z","title":"Degradation-Aware Residual-Conditioned Optimal Transport for Unified\n  Image Restoration","summary":"  All-in-one image restoration has emerged as a practical and promising\nlow-level vision task for real-world applications. In this context, the key\nissue lies in how to deal with different types of degraded images\nsimultaneously. In this work, we present a Degradation-Aware\nResidual-Conditioned Optimal Transport (DA-RCOT) approach that models\n(all-in-one) image restoration as an optimal transport (OT) problem for\nunpaired and paired settings, introducing the transport residual as a\ndegradation-specific cue for both the transport cost and the transport map.\nSpecifically, we formalize image restoration with a residual-guided OT\nobjective by exploiting the degradation-specific patterns of the Fourier\nresidual in the transport cost. More crucially, we design the transport map for\nrestoration as a two-pass DA-RCOT map, in which the transport residual is\ncomputed in the first pass and then encoded as multi-scale residual embeddings\nto condition the second-pass restoration. This conditioning process injects\nintrinsic degradation knowledge (e.g., degradation type and level) and\nstructural information from the multi-scale residual embeddings into the OT\nmap, which thereby can dynamically adjust its behaviors for all-in-one\nrestoration. Extensive experiments across five degradations demonstrate the\nfavorable performance of DA-RCOT as compared to state-of-the-art methods, in\nterms of distortion measures, perceptual quality, and image structure\npreservation. Notably, DA-RCOT delivers superior adaptability to real-world\nscenarios even with multiple degradations and shows distinctive robustness to\nboth degradation levels and the number of degradations.\n","authors":["Xiaole Tang","Xiang Gu","Xiaoyi He","Xin Hu","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2411.01656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01652v1","updated":"2024-11-03T18:30:37Z","published":"2024-11-03T18:30:37Z","title":"Optimizing Gastrointestinal Diagnostics: A CNN-Based Model for VCE Image\n  Classification","summary":"  In recent years, the diagnosis of gastrointestinal (GI) diseases has advanced\ngreatly with the advent of high-tech video capsule endoscopy (VCE) technology,\nwhich allows for non-invasive observation of the digestive system. The MisaHub\nCapsule Vision Challenge encourages the development of vendor-independent\nartificial intelligence models that can autonomously classify GI anomalies from\nVCE images. This paper presents CNN architecture designed specifically for\nmulticlass classification of ten gut pathologies, including angioectasia,\nbleeding, erosion, erythema, foreign bodies, lymphangiectasia, polyps, ulcers,\nand worms as well as their normal state.\n","authors":["Vaneeta Ahlawat","Rohit Sharma"," Urush"],"pdf_url":"https://arxiv.org/pdf/2411.01652v1.pdf","comment":"11 pages, 7 figuers"},{"id":"http://arxiv.org/abs/2405.18415v2","updated":"2024-11-03T18:23:45Z","published":"2024-05-28T17:57:06Z","title":"Why are Visually-Grounded Language Models Bad at Image Classification?","summary":"  Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.\n","authors":["Yuhui Zhang","Alyssa Unell","Xiaohan Wang","Dhruba Ghosh","Yuchang Su","Ludwig Schmidt","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2405.18415v2.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.15563v3","updated":"2024-11-03T18:04:52Z","published":"2024-01-28T04:07:59Z","title":"BrepGen: A B-rep Generative Diffusion Model with Structured Latent\n  Geometry","summary":"  This paper presents BrepGen, a diffusion-based generative approach that\ndirectly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD)\nmodel. BrepGen represents a B-rep model as a novel structured latent geometry\nin a hierarchical tree. With the root node representing a whole CAD solid, each\nelement of a B-rep model (i.e., a face, an edge, or a vertex) progressively\nturns into a child-node from top to bottom. B-rep geometry information goes\ninto the nodes as the global bounding box of each primitive along with a latent\ncode describing the local geometric shape. The B-rep topology information is\nimplicitly represented by node duplication. When two faces share an edge, the\nedge curve will appear twice in the tree, and a T-junction vertex with three\nincident edges appears six times in the tree with identical node features.\nStarting from the root and progressing to the leaf, BrepGen employs\nTransformer-based diffusion models to sequentially denoise node features while\nduplicated nodes are detected and merged, recovering the B-Rep topology\ninformation. Extensive experiments show that BrepGen advances the task of CAD\nB-rep generation, surpassing existing methods on various benchmarks. Results on\nour newly collected furniture dataset further showcase its exceptional\ncapability in generating complicated geometry. While previous methods were\nlimited to generating simple prismatic shapes, BrepGen incorporates free-form\nand doubly-curved surfaces for the first time. Additional applications of\nBrepGen include CAD autocomplete and design interpolation. The code, pretrained\nmodels, and dataset are available at https://github.com/samxuxiang/BrepGen.\n","authors":["Xiang Xu","Joseph G. Lambourne","Pradeep Kumar Jayaraman","Zhengqing Wang","Karl D. D. Willis","Yasutaka Furukawa"],"pdf_url":"https://arxiv.org/pdf/2401.15563v3.pdf","comment":"Accepted to ACM SIGGRAPH 2024. Code at\n  https://github.com/samxuxiang/BrepGen"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.23300v2","updated":"2024-11-03T22:11:35Z","published":"2024-10-15T21:54:13Z","title":"Understanding and Scaling Collaborative Filtering Optimization from the\n  Perspective of Matrix Rank","summary":"  Collaborative Filtering (CF) methods dominate real-world recommender systems\ngiven their ability to learn high-quality, sparse ID-embedding tables that\neffectively capture user preferences. These tables scale linearly with the\nnumber of users and items, and are trained to ensure high similarity between\nembeddings of interacted user-item pairs, while maintaining low similarity for\nnon-interacted pairs. Despite their high performance, encouraging dispersion\nfor non-interacted pairs necessitates expensive regularization (e.g., negative\nsampling), hurting runtime and scalability. Existing research tends to address\nthese challenges by simplifying the learning process, either by reducing model\ncomplexity or sampling data, trading performance for runtime. In this work, we\nmove beyond model-level modifications and study the properties of the embedding\ntables under different learning strategies. Through theoretical analysis, we\nfind that the singular values of the embedding tables are intrinsically linked\nto different CF loss functions. These findings are empirically validated on\nreal-world datasets, demonstrating the practical benefits of higher stable\nrank, a continuous version of matrix rank which encodes the distribution of\nsingular values. Based on these insights, we propose an efficient warm-start\nstrategy that regularizes the stable rank of the user and item embeddings. We\nshow that stable rank regularization during early training phases can promote\nhigher-quality embeddings, resulting in training speed improvements of up to\n66%. Additionally, stable rank regularization can act as a proxy for negative\nsampling, allowing for performance gains of up to 21% over loss functions with\nsmall negative sampling ratios. Overall, our analysis unifies current CF\nmethods under a new perspective, their optimization of stable rank, motivating\na flexible regularization method.\n","authors":["Donald Loveland","Xinyi Wu","Tong Zhao","Danai Koutra","Neil Shah","Mingxuan Ju"],"pdf_url":"https://arxiv.org/pdf/2410.23300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01690v1","updated":"2024-11-03T21:32:07Z","published":"2024-11-03T21:32:07Z","title":"Co-clustering for Federated Recommender System","summary":"  As data privacy and security attract increasing attention, Federated\nRecommender System (FRS) offers a solution that strikes a balance between\nproviding high-quality recommendations and preserving user privacy. However,\nthe presence of statistical heterogeneity in FRS, commonly observed due to\npersonalized decision-making patterns, can pose challenges. To address this\nissue and maximize the benefit of collaborative filtering (CF) in FRS, it is\nintuitive to consider clustering clients (users) as well as items into\ndifferent groups and learning group-specific models. Existing methods either\nresort to client clustering via user representations-risking privacy leakage,\nor employ classical clustering strategies on item embeddings or gradients,\nwhich we found are plagued by the curse of dimensionality. In this paper, we\ndelve into the inefficiencies of the K-Means method in client grouping,\nattributing failures due to the high dimensionality as well as data sparsity\noccurring in FRS, and propose CoFedRec, a novel Co-clustering Federated\nRecommendation mechanism, to address clients heterogeneity and enhance the\ncollaborative filtering within the federated framework. Specifically, the\nserver initially formulates an item membership from the client-provided item\nnetworks. Subsequently, clients are grouped regarding a specific item category\npicked from the item membership during each communication round, resulting in\nan intelligently aggregated group model. Meanwhile, to comprehensively capture\nthe global inter-relationships among items, we incorporate an additional\nsupervised contrastive learning term based on the server-side generated item\nmembership into the local training phase for each client. Extensive experiments\non four datasets are provided, which verify the effectiveness of the proposed\nCoFedRec.\n","authors":["Xinrui He","Shuo Liu","Jackey Keung","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2411.01690v1.pdf","comment":"WWW '24: Proceedings of the ACM Web Conference 2024"},{"id":"http://arxiv.org/abs/2411.01611v1","updated":"2024-11-03T15:37:37Z","published":"2024-11-03T15:37:37Z","title":"Stochastic Communication Avoidance for Recommendation Systems","summary":"  One of the major bottlenecks for efficient deployment of neural network based\nrecommendation systems is the memory footprint of their embedding tables.\nAlthough many neural network based recommendation systems could benefit from\nthe faster on-chip memory access and increased computational power of hardware\naccelerators, the large embedding tables in these models often cannot fit on\nthe constrained memory of accelerators. Despite the pervasiveness of these\nmodels, prior methods in memory optimization and parallelism fail to address\nthe memory and communication costs of large embedding tables on accelerators.\nAs a result, the majority of models are trained on CPUs, while current\nimplementations of accelerators are hindered by issues such as bottlenecks in\ninter-device communication and main memory lookups. In this paper, we propose a\ntheoretical framework that analyses the communication costs of arbitrary\ndistributed systems that use lookup tables. We use this framework to propose\nalgorithms that maximize throughput subject to memory, computation, and\ncommunication constraints. Furthermore, we demonstrate that our method achieves\nstrong theoretical performance across dataset distributions and memory\nconstraints, applicable to a wide range of use cases from mobile federated\nlearning to warehouse-scale computation. We implement our framework and\nalgorithms in PyTorch and achieve up to 6x increases in training throughput on\nGPU systems over baselines, on the Criteo Terabytes dataset.\n","authors":["Lutfi Eren Erdogan","Vijay Anand Raghava Kanakagiri","Kurt Keutzer","Zhen Dong"],"pdf_url":"https://arxiv.org/pdf/2411.01611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01561v1","updated":"2024-11-03T13:23:07Z","published":"2024-11-03T13:23:07Z","title":"Multimodal Graph Neural Network for Recommendation with Dynamic\n  De-redundancy and Modality-Guided Feature De-noisy","summary":"  Graph neural networks (GNNs) have become crucial in multimodal recommendation\ntasks because of their powerful ability to capture complex relationships\nbetween neighboring nodes. However, increasing the number of propagation layers\nin GNNs can lead to feature redundancy, which may negatively impact the overall\nrecommendation performance. In addition, the existing recommendation task\nmethod directly maps the preprocessed multimodal features to the\nlow-dimensional space, which will bring the noise unrelated to user preference,\nthus affecting the representation ability of the model. To tackle the\naforementioned challenges, we propose Multimodal Graph Neural Network for\nRecommendation (MGNM) with Dynamic De-redundancy and Modality-Guided Feature\nDe-noisy, which is divided into local and global interaction. Initially, in the\nlocal interaction process,we integrate a dynamic de-redundancy (DDR) loss\nfunction which is achieved by utilizing the product of the feature coefficient\nmatrix and the feature matrix as a penalization factor. It reduces the feature\nredundancy effects of multimodal and behavioral features caused by the stacking\nof multiple GNN layers. Subsequently, in the global interaction process, we\ndeveloped modality-guided global feature purifiers for each modality to\nalleviate the impact of modality noise. It is a two-fold guiding mechanism\neliminating modality features that are irrelevant to user preferences and\ncaptures complex relationships within the modality. Experimental results\ndemonstrate that MGNM achieves superior performance on multimodal information\ndenoising and removal of redundant information compared to the state-of-the-art\nmethods.\n","authors":["Feng Mo","Lin Xiao","Qiya Song","Xieping Gao","Eryao Liang"],"pdf_url":"https://arxiv.org/pdf/2411.01561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01540v1","updated":"2024-11-03T12:10:20Z","published":"2024-11-03T12:10:20Z","title":"Efficient and Robust Regularized Federated Recommendation","summary":"  Recommender systems play a pivotal role across practical scenarios,\nshowcasing remarkable capabilities in user preference modeling. However, the\ncentralized learning paradigm predominantly used raises serious privacy\nconcerns. The federated recommender system (FedRS) addresses this by updating\nmodels on clients, while a central server orchestrates training without\naccessing private data. Existing FedRS approaches, however, face unresolved\nchallenges, including non-convex optimization, vulnerability, potential privacy\nleakage risk, and communication inefficiency. This paper addresses these\nchallenges by reformulating the federated recommendation problem as a convex\noptimization issue, ensuring convergence to the global optimum. Based on this,\nwe devise a novel method, RFRec, to tackle this optimization problem\nefficiently. In addition, we propose RFRecF, a highly efficient version that\nincorporates non-uniform stochastic gradient descent to improve communication\nefficiency. In user preference modeling, both methods learn local and global\nmodels, collaboratively learning users' common and personalized interests under\nthe federated learning setting. Moreover, both methods significantly enhance\ncommunication efficiency, robustness, and privacy protection, with theoretical\nsupport. Comprehensive evaluations on four benchmark datasets demonstrate RFRec\nand RFRecF's superior performance compared to diverse baselines.\n","authors":["Langming Liu","Wanyu Wang","Xiangyu Zhao","Zijian Zhang","Chunxu Zhang","Shanru Lin","Yiqi Wang","Lixin Zou","Zitao Liu","Xuetao Wei","Hongzhi Yin","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2411.01540v1.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2411.01537v1","updated":"2024-11-03T11:56:00Z","published":"2024-11-03T11:56:00Z","title":"LinRec: Linear Attention Mechanism for Long-term Sequential Recommender\n  Systems","summary":"  Transformer models have achieved remarkable success in sequential recommender\nsystems (SRSs). However, computing the attention matrix in traditional\ndot-product attention mechanisms results in a quadratic complexity with\nsequence lengths, leading to high computational costs for long-term sequential\nrecommendation. Motivated by the above observation, we propose a novel\nL2-Normalized Linear Attention for the Transformer-based Sequential Recommender\nSystems (LinRec), which theoretically improves efficiency while preserving the\nlearning capabilities of the traditional dot-product attention. Specifically,\nby thoroughly examining the equivalence conditions of efficient attention\nmechanisms, we show that LinRec possesses linear complexity while preserving\nthe property of attention mechanisms. In addition, we reveal its latent\nefficiency properties by interpreting the proposed LinRec mechanism through a\nstatistical lens. Extensive experiments are conducted based on two public\nbenchmark datasets, demonstrating that the combination of LinRec and\nTransformer models achieves comparable or even superior performance than\nstate-of-the-art Transformer-based SRS models while significantly improving\ntime and memory efficiency.\n","authors":["Langming Liu","Xiangyu Zhao","Chi Zhang","Jingtong Gao","Wanyu Wang","Wenqi Fan","Yiqi Wang","Ming He","Zitao Liu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2411.01537v1.pdf","comment":"SIGIR 2023"},{"id":"http://arxiv.org/abs/2410.18634v2","updated":"2024-11-03T08:14:34Z","published":"2024-10-24T10:47:30Z","title":"Little Giants: Synthesizing High-Quality Embedding Data at Scale","summary":"  Synthetic data generation has become an increasingly popular way of training\nmodels without the need for large, manually labeled datasets. For tasks like\ntext embedding, synthetic data offers diverse and scalable training examples,\nsignificantly reducing the cost of human annotation. However, most current\napproaches rely heavily on proprietary models like GPT-4, which are expensive\nand inefficient for generating large-scale embedding data. In this paper, we\nintroduce SPEED, a framework that aligns open-source small models (8B) to\nefficiently generate large-scale synthetic embedding data. Through supervised\nfine-tuning, preference optimization, and self-improvement, SPEED enables small\nopen-source models to produce high-quality data. Remarkably, SPEED uses only\nless than 1/10 of the GPT API calls, outperforming the state-of-the-art\nembedding model E5_mistral when both are trained solely on their synthetic\ndata. Using this efficient generator, we conduct a comprehensive study on how\nvarious factors within the alignment pipeline impact data quality and reveal\nthe scaling law for synthetic embedding data.\n","authors":["Haonan Chen","Liang Wang","Nan Yang","Yutao Zhu","Ziliang Zhao","Furu Wei","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2410.18634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01457v1","updated":"2024-11-03T06:47:45Z","published":"2024-11-03T06:47:45Z","title":"Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential\n  Recommendation","summary":"  Sequential recommendation (SR) systems excel at capturing users' dynamic\npreferences by leveraging their interaction histories. Most existing SR systems\nassign a single embedding vector to each item to represent its features, and\nvarious types of models are adopted to combine these item embeddings into a\nsequence representation vector to capture the user intent. However, we argue\nthat this representation alone is insufficient to capture an item's\nmulti-faceted nature (e.g., movie genres, starring actors). Besides, users\noften exhibit complex and varied preferences within these facets (e.g., liking\nboth action and musical films in the facet of genre), which are challenging to\nfully represent. To address the issues above, we propose a novel structure\ncalled Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential\nRecommendation (FAME). We leverage sub-embeddings from each head in the last\nmulti-head attention layer to predict the next item separately. This approach\ncaptures the potential multi-faceted nature of items without increasing model\ncomplexity. A gating mechanism integrates recommendations from each head and\ndynamically determines their importance. Furthermore, we introduce a\nMixture-of-Experts (MoE) network in each attention head to disentangle various\nuser preferences within each facet. Each expert within the MoE focuses on a\nspecific preference. A learnable router network is adopted to compute the\nimportance weight for each expert and aggregate them. We conduct extensive\nexperiments on four public sequential recommendation datasets and the results\ndemonstrate the effectiveness of our method over existing baseline models.\n","authors":["Mingrui Liu","Sixiao Zhang","Cheng Long"],"pdf_url":"https://arxiv.org/pdf/2411.01457v1.pdf","comment":"This paper has been accepted by WSDM'25. The final camera-ready\n  version will be available soon"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.14468v4","updated":"2024-11-03T21:16:54Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks","summary":"  In the dynamic field of digital content creation using generative models,\nstate-of-the-art video editing models still do not offer the level of quality\nand control that users desire. Previous works on video editing either extended\nfrom image-based generative models in a zero-shot manner or necessitated\nextensive fine-tuning, which can hinder the production of fluid video edits.\nFurthermore, these methods frequently rely on textual input as the editing\nguidance, leading to ambiguities and limiting the types of edits they can\nperform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free\nparadigm designed to simplify video editing into two primary steps: (1)\nemploying an off-the-shelf image editing model to modify the first frame, (2)\nutilizing an existing image-to-video generation model to generate the edited\nvideo through temporal feature injection. AnyV2V can leverage any existing\nimage editing tools to support an extensive array of video editing tasks,\nincluding prompt-based editing, reference-based style transfer, subject-driven\nediting, and identity manipulation, which were unattainable by previous\nmethods. AnyV2V can also support any video length. Our evaluation shows that\nAnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore,\nAnyV2V significantly outperformed these baselines in human evaluations,\ndemonstrating notable improvements in visual consistency with the source video\nwhile producing high-quality edits across all editing tasks.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Harry Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v4.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR 2024)\n  (11/2024)"},{"id":"http://arxiv.org/abs/2406.13743v3","updated":"2024-11-03T20:22:32Z","published":"2024-06-19T18:00:07Z","title":"GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual\n  Generation","summary":"  While text-to-visual models now produce photo-realistic images and videos,\nthey struggle with compositional text prompts involving attributes,\nrelationships, and higher-order reasoning such as logic and comparison. In this\nwork, we conduct an extensive human study on GenAI-Bench to evaluate the\nperformance of leading image and video generation models in various aspects of\ncompositional text-to-visual generation. We also compare automated evaluation\nmetrics against our collected human ratings and find that VQAScore -- a metric\nmeasuring the likelihood that a VQA model views an image as accurately\ndepicting the prompt -- significantly outperforms previous metrics such as\nCLIPScore. In addition, VQAScore can improve generation in a black-box manner\n(without finetuning) via simply ranking a few (3 to 9) candidate images.\nRanking by VQAScore is 2x to 3x more effective than other scoring methods like\nPickScore, HPSv2, and ImageReward at improving human alignment ratings for\nDALL-E 3 and Stable Diffusion, especially on compositional prompts that require\nadvanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with\nover 40,000 human ratings to evaluate scoring metrics on ranking images\ngenerated from the same prompt. Lastly, we discuss promising areas for\nimprovement in VQAScore, such as addressing fine-grained visual details. We\nwill release all human ratings (over 80,000) to facilitate scientific\nbenchmarking of both generative models and automated metrics.\n","authors":["Baiqi Li","Zhiqiu Lin","Deepak Pathak","Jiayao Li","Yixin Fei","Kewen Wu","Tiffany Ling","Xide Xia","Pengchuan Zhang","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2406.13743v3.pdf","comment":"We open-source our dataset, model, and code at:\n  https://linzhiqiu.github.io/papers/genai_bench ; Project page:\n  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first\n  introduced in arxiv:2404.01291. This article extends it with an additional\n  GenAI-Rank benchmark"},{"id":"http://arxiv.org/abs/2411.01561v1","updated":"2024-11-03T13:23:07Z","published":"2024-11-03T13:23:07Z","title":"Multimodal Graph Neural Network for Recommendation with Dynamic\n  De-redundancy and Modality-Guided Feature De-noisy","summary":"  Graph neural networks (GNNs) have become crucial in multimodal recommendation\ntasks because of their powerful ability to capture complex relationships\nbetween neighboring nodes. However, increasing the number of propagation layers\nin GNNs can lead to feature redundancy, which may negatively impact the overall\nrecommendation performance. In addition, the existing recommendation task\nmethod directly maps the preprocessed multimodal features to the\nlow-dimensional space, which will bring the noise unrelated to user preference,\nthus affecting the representation ability of the model. To tackle the\naforementioned challenges, we propose Multimodal Graph Neural Network for\nRecommendation (MGNM) with Dynamic De-redundancy and Modality-Guided Feature\nDe-noisy, which is divided into local and global interaction. Initially, in the\nlocal interaction process,we integrate a dynamic de-redundancy (DDR) loss\nfunction which is achieved by utilizing the product of the feature coefficient\nmatrix and the feature matrix as a penalization factor. It reduces the feature\nredundancy effects of multimodal and behavioral features caused by the stacking\nof multiple GNN layers. Subsequently, in the global interaction process, we\ndeveloped modality-guided global feature purifiers for each modality to\nalleviate the impact of modality noise. It is a two-fold guiding mechanism\neliminating modality features that are irrelevant to user preferences and\ncaptures complex relationships within the modality. Experimental results\ndemonstrate that MGNM achieves superior performance on multimodal information\ndenoising and removal of redundant information compared to the state-of-the-art\nmethods.\n","authors":["Feng Mo","Lin Xiao","Qiya Song","Xieping Gao","Eryao Liang"],"pdf_url":"https://arxiv.org/pdf/2411.01561v1.pdf","comment":null}]},"2024-11-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.10209v2","updated":"2024-11-02T23:19:18Z","published":"2024-06-14T17:44:22Z","title":"Be like a Goldfish, Don't Memorize! Mitigating Memorization in\n  Generative LLMs","summary":"  Large language models can memorize and repeat their training data, causing\nprivacy and copyright risks. To mitigate memorization, we introduce a subtle\nmodification to the next-token training objective that we call the goldfish\nloss. During training, randomly sampled subsets of tokens are excluded from the\nloss computation. These dropped tokens are not memorized by the model, which\nprevents verbatim reproduction of a complete chain of tokens from the training\nset. We run extensive experiments training billion-scale Llama-2 models, both\npre-trained and trained from scratch, and demonstrate significant reductions in\nextractable memorization with little to no impact on downstream benchmarks.\n","authors":["Abhimanyu Hans","Yuxin Wen","Neel Jain","John Kirchenbauer","Hamid Kazemi","Prajwal Singhania","Siddharth Singh","Gowthami Somepalli","Jonas Geiping","Abhinav Bhatele","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2406.10209v2.pdf","comment":"10 pages, 8 figures, and 1 table in the main body. Code available at\n  https://github.com/ahans30/goldfish-loss and checkpoints at\n  https://huggingface.co/collections/tomg-group-umd/goldfish-loss-mitigating-memorization-in-llms-66c175becb6aab07744f7272"},{"id":"http://arxiv.org/abs/2411.01369v1","updated":"2024-11-02T21:59:02Z","published":"2024-11-02T21:59:02Z","title":"Artificial Intelligence Driven Course Generation: A Case Study Using\n  ChatGPT","summary":"  This study explores Artificial Intelligence use, specifically ChatGPT, in\ncreating educational content. The study aims to elaborate on using ChatGPT to\ncreate course materials. The main objective is to assess the efficiency,\nquality, and impact of AI-driven course generation, and to create a Multimedia\nDatabases course as a case study. The study highlights the potential of AI to\nrevolutionize educational content creation, making it more accessible,\npersonalized, and efficient. The course content was generated in less than one\nday through iterative methods, using prompts for translation, content\nexpansion, practical examples, assignments, supplementary materials, and LaTeX\nformatting. Each part was verified immediately after generation to ensure\naccuracy. Post-generation analysis with Detectia and Turnitin showed similarity\nrates of 8.7% and 13%, indicating high originality. Experts and university\ncommittees reviewed and approved the course, with English university teachers\npraising its language quality. ChatGPT also created a well-structured and\ndiversified exam for the module. Key findings reveal significant time\nefficiency, comprehensive content coverage, and high flexibility. The study\nunderscores AI's transformative potential in education, addressing challenges\nrelated to data privacy, technology dependence, content accuracy, and\nalgorithmic biases. The conclusions emphasize the need for collaboration\nbetween educators, policymakers, and technology developers to harness AI's\nbenefits in education fully.\n","authors":["Djaber Rouabhia"],"pdf_url":"https://arxiv.org/pdf/2411.01369v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2403.11425v3","updated":"2024-11-02T21:12:03Z","published":"2024-03-18T02:42:01Z","title":"Narrative Feature or Structured Feature? A Study of Large Language\n  Models to Identify Cancer Patients at Risk of Heart Failure","summary":"  Cancer treatments are known to introduce cardiotoxicity, negatively impacting\noutcomes and survivorship. Identifying cancer patients at risk of heart failure\n(HF) is critical to improving cancer treatment outcomes and safety. This study\nexamined machine learning (ML) models to identify cancer patients at risk of HF\nusing electronic health records (EHRs), including traditional ML, Time-Aware\nlong short-term memory (T-LSTM), and large language models (LLMs) using novel\nnarrative features derived from the structured medical codes. We identified a\ncancer cohort of 12,806 patients from the University of Florida Health,\ndiagnosed with lung, breast, and colorectal cancers, among which 1,602\nindividuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the\nbest F1 scores, outperforming the traditional support vector machines by 39%,\nthe T-LSTM deep learning model by 7%, and a widely used transformer model,\nBERT, by 5.6%. The analysis shows that the proposed narrative features\nremarkably increased feature density and improved performance.\n","authors":["Ziyi Chen","Mengyuan Zhang","Mustafa Mohammed Ahmed","Yi Guo","Thomas J. George","Jiang Bian","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11425v3.pdf","comment":"10 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.10855v2","updated":"2024-11-02T21:07:54Z","published":"2024-10-06T20:13:11Z","title":"CogDevelop2K: Reversed Cognitive Development in Multimodal Large\n  Language Models","summary":"  Are Multi-modal Large Language Models (MLLMs) stochastic parrots? Do they\ngenuinely understand? This paper aims to explore the core cognitive abilities\nthat human intelligence builds upon to perceive, comprehend, and reason in\nMLLMs. To this end, we propose CogDevelop2K, a comprehensive benchmark that\nspans 12 sub-concepts from primitive knowledge like object permanence and\nboundary to more complex abilities like intentionality understanding,\nstructured via the developmental trajectory of a human mind. We evaluate 46\nMLLMs on our benchmarks. Surprisingly, we observe a reversed cognitive\ndevelopmental trajectory compared to humans. Comprehensively, we further\nevaluate the influence of evaluation strategies and prompting techniques.\nWebsite with this $\\href{https://growing-ai-like-a-child.github.io/}{link}$.\n","authors":["Yijiang Li","Qingying Gao","Haoran Sun","Haiyun Lyu","Dezhi Luo","Hokin Deng"],"pdf_url":"https://arxiv.org/pdf/2410.10855v2.pdf","comment":"Website with this\n  $\\href{https://growing-ai-like-a-child.github.io/}{link}$"},{"id":"http://arxiv.org/abs/2411.01354v1","updated":"2024-11-02T20:05:31Z","published":"2024-11-02T20:05:31Z","title":"Online and Offline Evaluations of Collaborative Filtering and Content\n  Based Recommender Systems","summary":"  Recommender systems are widely used AI applications designed to help users\nefficiently discover relevant items. The effectiveness of such systems is tied\nto the satisfaction of both users and providers. However, user satisfaction is\ncomplex and cannot be easily framed mathematically using information retrieval\nand accuracy metrics. While many studies evaluate accuracy through offline\ntests, a growing number of researchers argue that online evaluation methods\nsuch as A/B testing are better suited for this purpose. We have employed a\nvariety of algorithms on different types of datasets divergent in size and\nsubject, producing recommendations in various platforms, including media\nstreaming services, digital publishing websites, e-commerce systems, and news\nbroadcasting networks. Notably, our target websites and datasets are in Persian\n(Farsi) language.\n  This study provides a comparative analysis of a large-scale recommender\nsystem that has been operating for the past year across about 70 websites in\nIran, processing roughly 300 requests per second collectively. The system\nemploys user-based and item-based recommendations using content-based,\ncollaborative filtering, trend-based methods, and hybrid approaches. Through\nboth offline and online evaluations, we aim to identify where these algorithms\nperform most efficiently and determine the best method for our specific needs,\nconsidering the dataset and system scale. Our methods of evaluation include\nmanual evaluation, offline tests including accuracy and ranking metrics like\nhit-rate@k and nDCG, and online tests consisting of click-through rate (CTR).\nAdditionally we analyzed and proposed methods to address cold-start and\npopularity bias.\n","authors":["Ali Elahi","Armin Zirak"],"pdf_url":"https://arxiv.org/pdf/2411.01354v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.01343v1","updated":"2024-11-02T19:14:19Z","published":"2024-11-02T19:14:19Z","title":"AMREx: AMR for Explainable Fact Verification","summary":"  With the advent of social media networks and the vast amount of information\ncirculating through them, automatic fact verification is an essential component\nto prevent the spread of misinformation. It is even more useful to have fact\nverification systems that provide explanations along with their classifications\nto ensure accurate predictions. To address both of these requirements, we\nimplement AMREx, an Abstract Meaning Representation (AMR)-based veracity\nprediction and explanation system for fact verification using a combination of\nSmatch, an AMR evaluation metric to measure meaning containment and textual\nsimilarity, and demonstrate its effectiveness in producing partially\nexplainable justifications using two community standard fact verification\ndatasets, FEVER and AVeriTeC. AMREx surpasses the AVeriTec baseline accuracy\nshowing the effectiveness of our approach for real-world claim verification. It\nfollows an interpretable pipeline and returns an explainable AMR node mapping\nto clarify the system's veracity predictions when applicable. We further\ndemonstrate that AMREx output can be used to prompt LLMs to generate\nnatural-language explanations using the AMR mappings as a guide to lessen the\nprobability of hallucinations.\n","authors":["Chathuri Jayaweera","Sangpil Youm","Bonnie Dorr"],"pdf_url":"https://arxiv.org/pdf/2411.01343v1.pdf","comment":"This study implements, evaluates, and analyzes an Abstract Meaning\n  Representation (AMR) based partially explainable system for fact\n  verification/ veracity classification. Accepted by EMNLP Workshop on Fact\n  Extraction and VERification (FEVER) 2024, 11 pages, 7 figures,"},{"id":"http://arxiv.org/abs/2405.17653v4","updated":"2024-11-02T19:13:06Z","published":"2024-05-27T20:53:22Z","title":"InversionView: A General-Purpose Method for Reading Information from\n  Neural Activations","summary":"  The inner workings of neural networks can be better understood if we can\nfully decipher the information encoded in neural activations. In this paper, we\nargue that this information is embodied by the subset of inputs that give rise\nto similar activations. We propose InversionView, which allows us to\npractically inspect this subset by sampling from a trained decoder model\nconditioned on activations. This helps uncover the information content of\nactivation vectors, and facilitates understanding of the algorithms implemented\nby transformer models. We present four case studies where we investigate models\nranging from small transformers to GPT-2. In these studies, we show that\nInversionView can reveal clear information contained in activations, including\nbasic information about tokens appearing in the context, as well as more\ncomplex information, such as the count of certain tokens, their relative\npositions, and abstract knowledge about the subject. We also provide causally\nverified circuits to confirm the decoded information.\n","authors":["Xinting Huang","Madhur Panwar","Navin Goyal","Michael Hahn"],"pdf_url":"https://arxiv.org/pdf/2405.17653v4.pdf","comment":"NeurIPS 2024; ICML 2024 Mechanistic Interpretability Workshop oral"},{"id":"http://arxiv.org/abs/2410.16531v3","updated":"2024-11-02T19:06:53Z","published":"2024-10-21T21:45:22Z","title":"Bayesian scaling laws for in-context learning","summary":"  In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a family of novel Bayesian scaling laws for ICL. In\nexperiments with \\mbox{GPT-2} models of different sizes, our scaling laws\nexceed or match existing scaling laws in accuracy while also offering\ninterpretable terms for task priors, learning efficiency, and per-example\nprobabilities. To illustrate the analytic power that such interpretable scaling\nlaws provide, we report on controlled synthetic dataset experiments designed to\ninform real-world studies of safety alignment. In our experimental protocol, we\nuse SFT to suppress an unwanted existing model capability and then use ICL to\ntry to bring that capability back (many-shot jailbreaking). We then experiment\non real-world instruction-tuned LLMs using capabilities benchmarks as well as a\nnew many-shot jailbreaking dataset. In all cases, Bayesian scaling laws\naccurately predict the conditions under which ICL will cause the suppressed\nbehavior to reemerge, which sheds light on the ineffectiveness of post-training\nat increasing LLM safety.\n","authors":["Aryaman Arora","Dan Jurafsky","Christopher Potts","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2410.16531v3.pdf","comment":"10 pages main text, 26 pages total"},{"id":"http://arxiv.org/abs/2410.24049v2","updated":"2024-11-02T19:02:32Z","published":"2024-10-31T15:45:23Z","title":"Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs","summary":"  Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.\n","authors":["Muhammed Saeed","Elgizouli Mohamed","Mukhtar Mohamed","Shaina Raza","Shady Shehata","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2410.24049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20221v3","updated":"2024-11-02T18:46:54Z","published":"2024-10-26T16:27:34Z","title":"Generative linguistics contribution to artificial intelligence: Where\n  this contribution lies?","summary":"  This article aims to characterize Generative linguistics (GL) contribution to\nartificial intelligence (AI), alluding to the debate among linguists and AI\nscientists on whether linguistics belongs to humanities or science. In this\narticle, I will try not to be biased as a linguist, studying the phenomenon\nfrom an independent scientific perspective. The article walks the\nresearcher/reader through the scientific theorems and rationales involved in AI\nwhich belong from GL, specifically the Chomsky School. It, thus, provides good\nevidence from syntax, semantics, language faculty, Universal Grammar,\ncomputational system of human language, language acquisition, human brain,\nprogramming languages (e.g. Python), Large Language Models, and unbiased AI\nscientists that this contribution is huge, and that this contribution cannot be\ndenied. It concludes that however the huge GL contribution to AI, there are\nstill points of divergence including the nature and type of language input.\n","authors":["Mohammed Q. Shormani"],"pdf_url":"https://arxiv.org/pdf/2410.20221v3.pdf","comment":"28 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.00297v5","updated":"2024-11-02T18:03:03Z","published":"2024-03-30T09:20:43Z","title":"A hybrid transformer and attention based recurrent neural network for\n  robust and interpretable sentiment analysis of tweets","summary":"  Sentiment analysis is crucial for understanding public opinion and consumer\nbehavior. Existing models face challenges with linguistic diversity,\ngeneralizability, and explainability. We propose TRABSA, a hybrid framework\nintegrating transformer-based architectures, attention mechanisms, and BiLSTM\nnetworks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge\ngaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.\nAugmenting datasets with tweets from 32 countries and US states, we compare six\nword-embedding techniques and three lexicon-based labeling techniques,\nselecting the best for optimal sentiment analysis. TRABSA outperforms\ntraditional ML and deep learning models with 94% accuracy and significant\nprecision, recall, and F1-score gains. Evaluation across diverse datasets\ndemonstrates consistent superiority and generalizability. SHAP and LIME\nanalyses enhance interpretability, improving confidence in predictions. Our\nstudy facilitates pandemic resource management, aiding resource planning,\npolicy formation, and vaccination tactics.\n","authors":["Md Abrar Jahin","Md Sakib Hossain Shovon","M. F. Mridha","Md Rashedul Islam","Yutaka Watanobe"],"pdf_url":"https://arxiv.org/pdf/2404.00297v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12624v4","updated":"2024-11-02T17:07:06Z","published":"2024-06-18T13:49:54Z","title":"Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges","summary":"  Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores.\n","authors":["Aman Singh Thakur","Kartik Choudhary","Venkat Srinik Ramayapally","Sankaran Vaidyanathan","Dieuwke Hupkes"],"pdf_url":"https://arxiv.org/pdf/2406.12624v4.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.01376v1","updated":"2024-11-02T22:59:36Z","published":"2024-11-02T22:59:36Z","title":"Multi-Channel Hypergraph Contrastive Learning for Matrix Completion","summary":"  Rating is a typical user explicit feedback that visually reflects how much a\nuser likes a related item. The (rating) matrix completion is essentially a\nrating prediction process, which is also a significant problem in recommender\nsystems. Recently, graph neural networks (GNNs) have been widely used in matrix\ncompletion, which captures users' preferences over items by formulating a\nrating matrix as a bipartite graph. However, existing methods are susceptible\ndue to data sparsity and long-tail distribution in real-world scenarios.\nMoreover, the messaging mechanism of GNNs makes it difficult to capture\nhigh-order correlations and constraints between nodes, which are essentially\nuseful in recommendation tasks. To tackle these challenges, we propose a\nMulti-Channel Hypergraph Contrastive Learning framework for matrix completion,\nnamed MHCL. Specifically, MHCL adaptively learns hypergraph structures to\ncapture high-order correlations between nodes and jointly captures local and\nglobal collaborative relationships through attention-based cross-view\naggregation. Additionally, to consider the magnitude and order information of\nratings, we treat different rating subgraphs as different channels, encourage\nalignment between adjacent ratings, and further achieve the mutual enhancement\nbetween different ratings through multi-channel cross-rating contrastive\nlearning. Extensive experiments on five public datasets demonstrate that the\nproposed method significantly outperforms the current state-of-the-art\napproaches.\n","authors":["Xiang Li","Changsheng Shui","Yanwei Yu","Chao Huang","Zhongying Zhao","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2411.01376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01368v1","updated":"2024-11-02T21:53:20Z","published":"2024-11-02T21:53:20Z","title":"Combining Financial Data and News Articles for Stock Price Movement\n  Prediction Using Large Language Models","summary":"  Predicting financial markets and stock price movements requires analyzing a\ncompany's performance, historic price movements, industry-specific events\nalongside the influence of human factors such as social media and press\ncoverage. We assume that financial reports (such as income statements, balance\nsheets, and cash flow statements), historical price data, and recent news\narticles can collectively represent aforementioned factors. We combine\nfinancial data in tabular format with textual news articles and employ\npre-trained Large Language Models (LLMs) to predict market movements. Recent\nresearch in LLMs has demonstrated that they are able to perform both tabular\nand text classification tasks, making them our primary model to classify the\nmulti-modal data. We utilize retrieval augmentation techniques to retrieve and\nattach relevant chunks of news articles to financial metrics related to a\ncompany and prompt the LLMs in zero, two, and four-shot settings. Our dataset\ncontains news articles collected from different sources, historic stock price,\nand financial report data for 20 companies with the highest trading volume\nacross different industries in the stock market. We utilized recently released\nlanguage models for our LLM-based classifier, including GPT- 3 and 4, and\nLLaMA- 2 and 3 models. We introduce an LLM-based classifier capable of\nperforming classification tasks using combination of tabular (structured) and\ntextual (unstructured) data. By using this model, we predicted the movement of\na given stock's price in our dataset with a weighted F1-score of 58.5% and\n59.1% and Matthews Correlation Coefficient of 0.175 for both 3-month and\n6-month periods.\n","authors":["Ali Elahi","Fatemeh Taghvaei"],"pdf_url":"https://arxiv.org/pdf/2411.01368v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.01354v1","updated":"2024-11-02T20:05:31Z","published":"2024-11-02T20:05:31Z","title":"Online and Offline Evaluations of Collaborative Filtering and Content\n  Based Recommender Systems","summary":"  Recommender systems are widely used AI applications designed to help users\nefficiently discover relevant items. The effectiveness of such systems is tied\nto the satisfaction of both users and providers. However, user satisfaction is\ncomplex and cannot be easily framed mathematically using information retrieval\nand accuracy metrics. While many studies evaluate accuracy through offline\ntests, a growing number of researchers argue that online evaluation methods\nsuch as A/B testing are better suited for this purpose. We have employed a\nvariety of algorithms on different types of datasets divergent in size and\nsubject, producing recommendations in various platforms, including media\nstreaming services, digital publishing websites, e-commerce systems, and news\nbroadcasting networks. Notably, our target websites and datasets are in Persian\n(Farsi) language.\n  This study provides a comparative analysis of a large-scale recommender\nsystem that has been operating for the past year across about 70 websites in\nIran, processing roughly 300 requests per second collectively. The system\nemploys user-based and item-based recommendations using content-based,\ncollaborative filtering, trend-based methods, and hybrid approaches. Through\nboth offline and online evaluations, we aim to identify where these algorithms\nperform most efficiently and determine the best method for our specific needs,\nconsidering the dataset and system scale. Our methods of evaluation include\nmanual evaluation, offline tests including accuracy and ranking metrics like\nhit-rate@k and nDCG, and online tests consisting of click-through rate (CTR).\nAdditionally we analyzed and proposed methods to address cold-start and\npopularity bias.\n","authors":["Ali Elahi","Armin Zirak"],"pdf_url":"https://arxiv.org/pdf/2411.01354v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.07865v6","updated":"2024-11-02T16:56:36Z","published":"2023-03-14T12:56:47Z","title":"Predicting the Geolocation of Tweets Using transformer models on\n  Customized Data","summary":"  This research is aimed to solve the tweet/user geolocation prediction task\nand provide a flexible methodology for the geotagging of textual big data. The\nsuggested approach implements neural networks for natural language processing\n(NLP) to estimate the location as coordinate pairs (longitude, latitude) and\ntwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models\nhas been finetuned on a Twitter dataset using pretrained Bidirectional Encoder\nRepresentations from Transformers (BERT) as base models. Performance metrics\nshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15\nkm on the US-level datasets for the models trained and evaluated on text\nfeatures of tweets' content and metadata context. Our source code and data are\navailable at https://github.com/K4TEL/geo-twitter.git\n","authors":["Kateryna Lutsai","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2303.07865v6.pdf","comment":"31 pages, 5 tables, 9 figures"},{"id":"http://arxiv.org/abs/2411.01304v1","updated":"2024-11-02T16:39:45Z","published":"2024-11-02T16:39:45Z","title":"Towards a Knowledge Graph for Teaching Knowledge Graphs","summary":"  This poster paper describes the ongoing research project for the creation of\na use-case-driven Knowledge Graph resource tailored to the needs of teaching\neducation in Knowledge Graphs (KGs). We gather resources related to KG courses\nfrom lectures offered by the Semantic Web community, with the help of the COST\nAction Distributed Knowledge Graphs and the interest group on KGs at The Alan\nTuring Institute. Our goal is to create a resource-focused KG with multiple\ninterconnected semantic layers that interlink topics, courses, and materials\nwith each lecturer. Our approach formulates a domain KG in teaching and relates\nit with multiple Personal KGs created for the lecturers.\n","authors":["Eleni Ilkou","Ernesto Jiménez-Ruiz"],"pdf_url":"https://arxiv.org/pdf/2411.01304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22844v2","updated":"2024-11-02T15:23:36Z","published":"2024-10-30T09:23:14Z","title":"Understanding and Improving Adversarial Collaborative Filtering for\n  Robust Recommendation","summary":"  Adversarial Collaborative Filtering (ACF), which typically applies\nadversarial perturbations at user and item embeddings through adversarial\ntraining, is widely recognized as an effective strategy for enhancing the\nrobustness of Collaborative Filtering (CF) recommender systems against\npoisoning attacks. Besides, numerous studies have empirically shown that ACF\ncan also improve recommendation performance compared to traditional CF. Despite\nthese empirical successes, the theoretical understanding of ACF's effectiveness\nin terms of both performance and robustness remains unclear. To bridge this\ngap, in this paper, we first theoretically show that ACF can achieve a lower\nrecommendation error compared to traditional CF with the same training epochs\nin both clean and poisoned data contexts. Furthermore, by establishing bounds\nfor reductions in recommendation error during ACF's optimization process, we\nfind that applying personalized magnitudes of perturbation for different users\nbased on their embedding scales can further improve ACF's effectiveness.\nBuilding on these theoretical understandings, we propose Personalized Magnitude\nAdversarial Collaborative Filtering (PamaCF). Extensive experiments demonstrate\nthat PamaCF effectively defends against various types of poisoning attacks\nwhile significantly enhancing recommendation performance.\n","authors":["Kaike Zhang","Qi Cao","Yunfan Wu","Fei Sun","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.22844v2.pdf","comment":"To appear in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01182v1","updated":"2024-11-02T08:50:11Z","published":"2024-11-02T08:50:11Z","title":"Graph Cross-Correlated Network for Recommendation","summary":"  Collaborative filtering (CF) models have demonstrated remarkable performance\nin recommender systems, which represent users and items as embedding vectors.\nRecently, due to the powerful modeling capability of graph neural networks for\nuser-item interaction graphs, graph-based CF models have gained increasing\nattention. They encode each user/item and its subgraph into a single super\nvector by combining graph embeddings after each graph convolution. However,\neach hop of the neighbor in the user-item subgraphs carries a specific semantic\nmeaning. Encoding all subgraph information into single vectors and inferring\nuser-item relations with dot products can weaken the semantic information\nbetween user and item subgraphs, thus leaving untapped potential. Exploiting\nthis untapped potential provides insight into improving performance for\nexisting recommendation models. To this end, we propose the Graph\nCross-correlated Network for Recommendation (GCR), which serves as a general\nrecommendation paradigm that explicitly considers correlations between\nuser/item subgraphs. GCR first introduces the Plain Graph Representation (PGR)\nto extract information directly from each hop of neighbors into corresponding\nPGR vectors. Then, GCR develops Cross-Correlated Aggregation (CCA) to construct\npossible cross-correlated terms between PGR vectors of user/item subgraphs.\nFinally, GCR comprehensively incorporates the cross-correlated terms for\nrecommendations. Experimental results show that GCR outperforms\nstate-of-the-art models on both interaction prediction and click-through rate\nprediction tasks.\n","authors":["Hao Chen","Yuanchen Bei","Wenbing Huang","Shengyuan Chen","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2411.01182v1.pdf","comment":"14 pages, accepted by TKDE"},{"id":"http://arxiv.org/abs/2411.01178v1","updated":"2024-11-02T08:36:16Z","published":"2024-11-02T08:36:16Z","title":"LLM4PR: Improving Post-Ranking in Search Engine with Large Language\n  Models","summary":"  Alongside the rapid development of Large Language Models (LLMs), there has\nbeen a notable increase in efforts to integrate LLM techniques in information\nretrieval (IR) and search engines (SE). Recently, an additional post-ranking\nstage is suggested in SE to enhance user satisfaction in practical\napplications. Nevertheless, research dedicated to enhancing the post-ranking\nstage through LLMs remains largely unexplored. In this study, we introduce a\nnovel paradigm named Large Language Models for Post-Ranking in search engine\n(LLM4PR), which leverages the capabilities of LLMs to accomplish the\npost-ranking task in SE. Concretely, a Query-Instructed Adapter (QIA) module is\ndesigned to derive the user/item representation vectors by incorporating their\nheterogeneous features. A feature adaptation step is further introduced to\nalign the semantics of user/item representations with the LLM. Finally, the\nLLM4PR integrates a learning to post-rank step, leveraging both a main task and\nan auxiliary task to fine-tune the model to adapt the post-ranking task.\nExperiment studies demonstrate that the proposed framework leads to significant\nimprovements and exhibits state-of-the-art performance compared with other\nalternatives.\n","authors":["Yang Yan","Yihao Wang","Chi Zhang","Wenyuan Hou","Kang Pan","Xingkai Ren","Zelun Wu","Zhixin Zhai","Enyun Yu","Wenwu Ou","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2411.01178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17769v2","updated":"2024-11-02T08:06:32Z","published":"2024-04-27T03:37:12Z","title":"Two-stage Conformal Risk Control with Application to Ranked Retrieval","summary":"  Many practical machine learning systems, such as ranking and recommendation\nsystems, consist of two concatenated stages: retrieval and ranking. These\nsystems present significant challenges in accurately assessing and managing the\nuncertainty inherent in their predictions. To address these challenges, we\nextend the recently developed framework of conformal risk control, originally\ndesigned for single-stage problems, to accommodate the more complex two-stage\nsetup. We first demonstrate that a straightforward application of conformal\nrisk control, treating each stage independently, may fail to maintain risk at\ntheir pre-specified levels. Therefore, we propose an integrated approach that\nconsiders both stages simultaneously, devising algorithms to control the risk\nof each stage by jointly identifying thresholds for both stages. Our algorithm\nfurther optimizes for a weighted combination of prediction set sizes across all\nfeasible thresholds, resulting in more effective prediction sets. Finally, we\napply the proposed method to the critical task of two-stage ranked retrieval.\nWe validate the efficacy of our method through extensive experiments on two\nlarge-scale public datasets, MSLR-WEB and MS MARCO, commonly used for ranked\nretrieval tasks.\n","authors":["Yunpeng Xu","Mufang Ying","Wenge Guo","Zhi Wei"],"pdf_url":"https://arxiv.org/pdf/2404.17769v2.pdf","comment":"13 pages, 3 figures; 5 supplementary pages, 3 supplementary figures"},{"id":"http://arxiv.org/abs/2411.01135v1","updated":"2024-11-02T04:44:27Z","published":"2024-11-02T04:44:27Z","title":"Music Foundation Model as Generic Booster for Music Downstream Tasks","summary":"  We demonstrate the efficacy of using intermediate representations from a\nsingle foundation model to enhance various music downstream tasks. We introduce\nSoniDo , a music foundation model (MFM) designed to extract hierarchical\nfeatures from target music samples. By leveraging hierarchical intermediate\nfeatures, SoniDo constrains the information granularity, leading to improved\nperformance across various downstream tasks including both understanding and\ngenerative tasks. We specifically evaluated this approach on representative\ntasks such as music tagging, music transcription, music source separation, and\nmusic mixing. Our results reveal that the features extracted from foundation\nmodels provide valuable enhancements in training downstream task models. This\nhighlights the capability of using features extracted from music foundation\nmodels as a booster for downstream tasks. Our approach not only benefits\nexisting task-specific models but also supports music downstream tasks\nconstrained by data scarcity. This paves the way for more effective and\naccessible music processing solutions.\n","authors":["WeiHsiang Liao","Yuhta Takida","Yukara Ikemiya","Zhi Zhong","Chieh-Hsin Lai","Giorgio Fabbro","Kazuki Shimada","Keisuke Toyama","Kinwai Cheuk","Marco Martinez","Shusuke Takahashi","Stefan Uhlich","Taketo Akama","Woosung Choi","Yuichiro Koyama","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2411.01135v1.pdf","comment":"41 pages with 14 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.00562v2","updated":"2024-11-02T22:31:05Z","published":"2024-08-31T23:22:30Z","title":"Comparative Analysis of Modality Fusion Approaches for Audio-Visual\n  Person Identification and Verification","summary":"  Multimodal learning involves integrating information from various modalities\nto enhance learning and comprehension. We compare three modality fusion\nstrategies in person identification and verification by processing two\nmodalities: voice and face. In this paper, a one-dimensional convolutional\nneural network is employed for x-vector extraction from voice, while the\npre-trained VGGFace2 network and transfer learning are utilized for face\nmodality. In addition, gammatonegram is used as speech representation in\nengagement with the Darknet19 pre-trained network. The proposed systems are\nevaluated using the K-fold cross-validation technique on the 118 speakers of\nthe test set of the VoxCeleb2 dataset. The comparative evaluations are done for\nsingle-modality and three proposed multimodal strategies in equal situations.\nResults demonstrate that the feature fusion strategy of gammatonegram and\nfacial features achieves the highest performance, with an accuracy of 98.37% in\nthe person identification task. However, concatenating facial features with the\nx-vector reaches 0.62% for EER in verification tasks.\n","authors":["Aref Farhadipour","Masoumeh Chapariniya","Teodora Vukovic","Volker Dellwo"],"pdf_url":"https://arxiv.org/pdf/2409.00562v2.pdf","comment":"This paper was accepted at the ICNLSP2024 conference"},{"id":"http://arxiv.org/abs/2410.22023v3","updated":"2024-11-02T12:52:23Z","published":"2024-10-29T13:13:30Z","title":"Multi-modal Speech Emotion Recognition via Feature Distribution\n  Adaptation Network","summary":"  In this paper, we propose a novel deep inductive transfer learning framework,\nnamed feature distribution adaptation network, to tackle the challenging\nmulti-modal speech emotion recognition problem. Our method aims to use deep\ntransfer learning strategies to align visual and audio feature distributions to\nobtain consistent representation of emotion, thereby improving the performance\nof speech emotion recognition. In our model, the pre-trained ResNet-34 is\nutilized for feature extraction for facial expression images and acoustic Mel\nspectrograms, respectively. Then, the cross-attention mechanism is introduced\nto model the intrinsic similarity relationships of multi-modal features.\nFinally, the multi-modal feature distribution adaptation is performed\nefficiently with feed-forward network, which is extended using the local\nmaximum mean discrepancy loss. Experiments are carried out on two benchmark\ndatasets, and the results demonstrate that our model can achieve excellent\nperformance compared with existing ones.\n","authors":["Shaokai Li","Yixuan Ji","Peng Song","Haoqin Sun","Wenming Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.22023v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18709v3","updated":"2024-11-02T11:09:37Z","published":"2023-10-28T13:37:52Z","title":"Audio-Visual Instance Segmentation","summary":"  In this paper, we propose a new multi-modal task, termed audio-visual\ninstance segmentation (AVIS), which aims to simultaneously identify, segment\nand track individual sounding object instances in audible videos. To facilitate\nthis research, we introduce a high-quality benchmark named AVISeg, containing\nover 90K instance masks from 26 semantic categories in 926 long videos.\nAdditionally, we propose a strong baseline model for this task. Our model first\nlocalizes sound source within each frame, and condenses object-specific\ncontexts into concise tokens. Then it builds long-range audio-visual\ndependencies between these tokens using window-based attention, and tracks\nsounding objects among the entire video sequences. Extensive experiments reveal\nthat our method performs best on AVISeg, surpassing the existing methods from\nrelated tasks. We further conduct the evaluation on several multi-modal large\nmodels; however, they exhibits subpar performance on instance-level sound\nsource localization and temporal perception. We expect that AVIS will inspire\nthe community towards a more comprehensive multi-modal understanding. The\ndataset and code will soon be released on https://github.com/ruohaoguo/avis.\n","authors":["Ruohao Guo","Xianghua Ying","Yaru Chen","Dantong Niu","Guangyao Li","Liao Qu","Yanyu Qi","Jinxing Zhou","Bowei Xing","Wenzhen Yue","Ji Shi","Qixun Wang","Peiliang Zhang","Buwen Liang"],"pdf_url":"https://arxiv.org/pdf/2310.18709v3.pdf","comment":"Project page: https://github.com/ruohaoguo/avis"},{"id":"http://arxiv.org/abs/2406.11161v2","updated":"2024-11-02T02:30:50Z","published":"2024-06-17T03:01:22Z","title":"Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with\n  Instruction Tuning","summary":"  Accurate emotion perception is crucial for various applications, including\nhuman-computer interaction, education, and counseling. However, traditional\nsingle-modality approaches often fail to capture the complexity of real-world\nemotional expressions, which are inherently multimodal. Moreover, existing\nMultimodal Large Language Models (MLLMs) face challenges in integrating audio\nand recognizing subtle facial micro-expressions. To address this, we introduce\nthe MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained\nannotated samples across diverse emotional categories. This dataset enables\nmodels to learn from varied scenarios and generalize to real-world\napplications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly\nintegrates audio, visual, and textual inputs through emotion-specific encoders.\nBy aligning features into a shared space and employing a modified LLaMA model\nwith instruction tuning, Emotion-LLaMA significantly enhances both emotional\nrecognition and reasoning capabilities. Extensive evaluations show\nEmotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap\n(7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI\nchallenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations\non DFEW dataset.\n","authors":["Zebang Cheng","Zhi-Qi Cheng","Jun-Yan He","Jingdong Sun","Kai Wang","Yuxiang Lin","Zheng Lian","Xiaojiang Peng","Alexander Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2406.11161v2.pdf","comment":"Accepted at NeurIPS 2024. 49 pages, 13 figures, Project:\n  https://github.com/ZebangCheng/Emotion-LLaMA, Demo:\n  https://huggingface.co/spaces/ZebangCheng/Emotion-LLaMA"}]},"2024-11-01T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.13047v2","updated":"2024-11-01T23:39:14Z","published":"2024-10-16T21:17:18Z","title":"LLM Confidence Evaluation Measures in Zero-Shot CSS Classification","summary":"  Assessing classification confidence is critical for leveraging large language\nmodels (LLMs) in automated labeling tasks, especially in the sensitive domains\npresented by Computational Social Science (CSS) tasks. In this paper, we make\nthree key contributions: (1) we propose an uncertainty quantification (UQ)\nperformance measure tailored for data annotation tasks, (2) we compare, for the\nfirst time, five different UQ strategies across three distinct LLMs and CSS\ndata annotation tasks, (3) we introduce a novel UQ aggregation strategy that\neffectively identifies low-confidence LLM annotations and disproportionately\nuncovers data incorrectly labeled by the LLMs. Our results demonstrate that our\nproposed UQ aggregation strategy improves upon existing methods andcan be used\nto significantly improve human-in-the-loop data annotation processes.\n","authors":["David Farr","Iain Cruickshank","Nico Manzonelli","Nicholas Clark","Kate Starbird","Jevin West"],"pdf_url":"https://arxiv.org/pdf/2410.13047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01039v1","updated":"2024-11-01T21:14:04Z","published":"2024-11-01T21:14:04Z","title":"Enhancing Question Answering Precision with Optimized Vector Retrieval\n  and Instructions","summary":"  Question-answering (QA) is an important application of Information Retrieval\n(IR) and language models, and the latest trend is toward pre-trained large\nneural networks with embedding parameters. Augmenting QA performances with\nthese LLMs requires intensive computational resources for fine-tuning. We\npropose an innovative approach to improve QA task performances by integrating\noptimized vector retrievals and instruction methodologies. Based on retrieval\naugmentation, the process involves document embedding, vector retrieval, and\ncontext construction for optimal QA results. We experiment with different\ncombinations of text segmentation techniques and similarity functions, and\nanalyze their impacts on QA performances. Results show that the model with a\nsmall chunk size of 100 without any overlap of the chunks achieves the best\nresult and outperforms the models based on semantic segmentation using\nsentences. We discuss related QA examples and offer insight into how model\nperformances are improved within the two-stage framework.\n","authors":["Lixiao Yang","Mengyang Xu","Weimao Ke"],"pdf_url":"https://arxiv.org/pdf/2411.01039v1.pdf","comment":"6 pages, 4 tables"},{"id":"http://arxiv.org/abs/2411.00744v1","updated":"2024-11-01T17:11:16Z","published":"2024-11-01T17:11:16Z","title":"CORAG: A Cost-Constrained Retrieval Optimization System for\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) have demonstrated remarkable generation\ncapabilities but often struggle to access up-to-date information, which can\nlead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this\nissue by incorporating knowledge from external databases, enabling more\naccurate and relevant responses. Due to the context window constraints of LLMs,\nit is impractical to input the entire external database context directly into\nthe model. Instead, only the most relevant information, referred to as chunks,\nis selectively retrieved. However, current RAG research faces three key\nchallenges. First, existing solutions often select each chunk independently,\noverlooking potential correlations among them. Second, in practice the utility\nof chunks is non-monotonic, meaning that adding more chunks can decrease\noverall utility. Traditional methods emphasize maximizing the number of\nincluded chunks, which can inadvertently compromise performance. Third, each\ntype of user query possesses unique characteristics that require tailored\nhandling, an aspect that current approaches do not fully consider. To overcome\nthese challenges, we propose a cost constrained retrieval optimization system\nCORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search\n(MCTS) based policy framework to find optimal chunk combinations sequentially,\nallowing for a comprehensive consideration of correlations among chunks.\nAdditionally, rather than viewing budget exhaustion as a termination condition,\nwe integrate budget constraints into the optimization of chunk combinations,\neffectively addressing the non-monotonicity of chunk utility.\n","authors":["Ziting Wang","Haitao Yuan","Wei Dong","Gao Cong","Feifei Li"],"pdf_url":"https://arxiv.org/pdf/2411.00744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00702v1","updated":"2024-11-01T16:05:59Z","published":"2024-11-01T16:05:59Z","title":"A graph-based approach to extracting narrative signals from public\n  discourse","summary":"  Narratives are key interpretative devices by which humans make sense of\npolitical reality. As the significance of narratives for understanding current\nsocietal issues such as polarization and misinformation becomes increasingly\nevident, there is a growing demand for methods that support their empirical\nanalysis. To this end, we propose a graph-based formalism and machine-guided\nmethod for extracting, representing, and analyzing selected narrative signals\nfrom digital textual corpora, based on Abstract Meaning Representation (AMR).\nThe formalism and method introduced here specifically cater to the study of\npolitical narratives that figure in texts from digital media such as archived\npolitical speeches, social media posts, political manifestos and transcripts of\nparliamentary debates. We conceptualize these political narratives as a type of\nontological narratives: stories by which actors position themselves as\npolitical beings, and which are akin to political worldviews in which actors\npresent their normative vision of the world, or aspects thereof. We approach\nthe study of such political narratives as a problem of information retrieval:\nstarting from a textual corpus, we first extract a graph-like representation of\nthe meaning of each sentence in the corpus using AMR. Drawing on transferable\nconcepts from narratology, we then apply a set of heuristics to filter these\ngraphs for representations of 1) actors, 2) the events in which these actors\nfigure, and 3) traces of the perspectivization of these events. We approach\nthese references to actors, events, and instances of perspectivization as core\nnarrative signals that initiate a further analysis by alluding to larger\npolitical narratives. By means of a case study of State of the European Union\naddresses, we demonstrate how the formalism can be used to inductively surface\nsignals of political narratives from public discourse.\n","authors":["Armin Pournaki","Tom Willaert"],"pdf_url":"https://arxiv.org/pdf/2411.00702v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.00677v1","updated":"2024-11-01T15:36:52Z","published":"2024-11-01T15:36:52Z","title":"Making Sense of Metadata Mess: Alignment & Risk Assessment for Diatom\n  Data Use Case","summary":"  Biologists study Diatoms, a fundamental algae, to assess the health of\naquatic systems. Diatom specimens have traditionally been preserved on analog\nslides, where a single slide can contain thousands of these microscopic\norganisms. Digitization of these collections presents both metadata challenges\nand opportunities. This paper reports on metadata research aimed at providing\naccess to a digital portion of the Academy of Natural Sciences' Diatom\nHerbarium, Drexel University. We report results of a 3-part study covering 1) a\nreview of relevant metadata standards and a microscopy metadata framework\nshared by Hammer et al., 2) a baseline metadata alignment mapping current\ndiatom metadata properties to standard metadata types, and 3) a metadata risk\nanalysis associated with the course of standard data curation practices. This\nresearch is part of an effort involving the transfer of these digital slides to\nan new system, DataFed, to support global accessible. The final section of this\npaper includes a conclusion and discusses next steps.\n","authors":["Kio Polson","Marina Potapova","Uttam Meena","Chad Peiper","Joshua Brown","Joshua Agar","Jane Greenberg"],"pdf_url":"https://arxiv.org/pdf/2411.00677v1.pdf","comment":"13 pages, 2 figures, 1 table, to be published in MTSR 2024 conference\n  proceedings"},{"id":"http://arxiv.org/abs/2411.00676v1","updated":"2024-11-01T15:35:56Z","published":"2024-11-01T15:35:56Z","title":"Enhancing Semantic Interoperability Across Materials Science With\n  HIVE4MAT","summary":"  HIVE4MAT is a linked data interactive application for navigating ontologies\nof value to materials science. HIVE enables automatic indexing of textual\nresources with standardized terminology. This article presents the motivation\nunderlying HIVE4MAT, explains the system architecture, reports on two\nevaluations, and discusses future plans.\n","authors":["Jane Greenberg","Kio Polson","Scott McClellan","Xintong Zhao","Alex Kalinowski","Yuan An"],"pdf_url":"https://arxiv.org/pdf/2411.00676v1.pdf","comment":"11 pages, 1 figures, 3 tables, to be published in SeMatS 2024\n  workshop proceedings"},{"id":"http://arxiv.org/abs/2407.10691v2","updated":"2024-11-01T14:08:31Z","published":"2024-07-15T13:04:09Z","title":"$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity","summary":"  Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based\nretrievers, respectively, averaged on queries containing multiple subqueries\nfrom five scientific retrieval datasets. Moreover, the efficacy of two\ndownstream scientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The\ncode and experimental datasets are available.\n","authors":["Fengyu Cai","Xinran Zhao","Tong Chen","Sihao Chen","Hongming Zhang","Iryna Gurevych","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2407.10691v2.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2411.00556v1","updated":"2024-11-01T13:09:30Z","published":"2024-11-01T13:09:30Z","title":"LLM-KT: A Versatile Framework for Knowledge Transfer from Large Language\n  Models to Collaborative Filtering","summary":"  We present LLM-KT, a flexible framework designed to enhance collaborative\nfiltering (CF) models by seamlessly integrating LLM (Large Language\nModel)-generated features. Unlike existing methods that rely on passing\nLLM-generated features as direct inputs, our framework injects these features\ninto an intermediate layer of any CF model, allowing the model to reconstruct\nand leverage the embeddings internally. This model-agnostic approach works with\na wide range of CF models without requiring architectural changes, making it\nadaptable to various recommendation scenarios. Our framework is built for easy\nintegration and modification, providing researchers and developers with a\npowerful tool for extending CF model capabilities through efficient knowledge\ntransfer. We demonstrate its effectiveness through experiments on the MovieLens\nand Amazon datasets, where it consistently improves baseline CF models.\nExperimental studies showed that LLM-KT is competitive with the\nstate-of-the-art methods in context-aware settings but can be applied to a\nbroader range of CF models than current approaches.\n","authors":["Nikita Severin","Aleksei Ziablitsev","Yulia Savelyeva","Valeriy Tashchilin","Ivan Bulychev","Mikhail Yushkov","Artem Kushneruk","Amaliya Zaryvnykh","Dmitrii Kiselev","Andrey Savchenko","Ilya Makarov"],"pdf_url":"https://arxiv.org/pdf/2411.00556v1.pdf","comment":"accepted at ICDM 2024 (demo track)"},{"id":"http://arxiv.org/abs/2411.00469v1","updated":"2024-11-01T09:34:36Z","published":"2024-11-01T09:34:36Z","title":"MIRFLEX: Music Information Retrieval Feature Library for Extraction","summary":"  This paper introduces an extendable modular system that compiles a range of\nmusic feature extraction models to aid music information retrieval research.\nThe features include musical elements like key, downbeats, and genre, as well\nas audio characteristics like instrument recognition, vocals/instrumental\nclassification, and vocals gender detection. The integrated models are\nstate-of-the-art or latest open-source. The features can be extracted as latent\nor post-processed labels, enabling integration into music applications such as\ngenerative music, recommendation, and playlist generation. The modular design\nallows easy integration of newly developed systems, making it a good\nbenchmarking and comparison tool. This versatile toolkit supports the research\ncommunity in developing innovative solutions by providing concrete musical\nfeatures.\n","authors":["Anuradha Chopra","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2411.00469v1.pdf","comment":"2 pages, 4 tables, submitted to Extended Abstracts for the\n  Late-Breaking Demo Session of the 25th Int. Society for Music Information\n  Retrieval Conf., San Francisco, United States, 2024"},{"id":"http://arxiv.org/abs/2411.00451v1","updated":"2024-11-01T08:57:29Z","published":"2024-11-01T08:57:29Z","title":"Improving Few-Shot Cross-Domain Named Entity Recognition by Instruction\n  Tuning a Word-Embedding based Retrieval Augmented Large Language Model","summary":"  Few-Shot Cross-Domain NER is the process of leveraging knowledge from\ndata-rich source domains to perform entity recognition on data scarce target\ndomains. Most previous state-of-the-art (SOTA) approaches use pre-trained\nlanguage models (PLMs) for cross-domain NER. However, these models are often\ndomain specific. To successfully use these models for new target domains, we\nneed to modify either the model architecture or perform model finetuning using\ndata from the new domains. Both of these result in the creation of entirely new\nNER models for each target domain which is infeasible for practical scenarios.\nRecently,several works have attempted to use LLMs to solve Few-Shot\nCross-Domain NER. However, most of these are either too expensive for practical\npurposes or struggle to follow LLM prompt instructions. In this paper, we\npropose IF-WRANER (Instruction Finetuned Word-embedding based Retrieval\nAugmented large language model for Named Entity Recognition), a retrieval\naugmented LLM, finetuned for the NER task. By virtue of the regularization\ntechniques used during LLM finetuning and the adoption of word-level embedding\nover sentence-level embedding during the retrieval of in-prompt examples,\nIF-WRANER is able to outperform previous SOTA Few-Shot Cross-Domain NER\napproaches. We have demonstrated the effectiveness of our model by benchmarking\nits performance on the open source CrossNER dataset, on which it shows more\nthan 2% F1 score improvement over the previous SOTA model. We have deployed the\nmodel for multiple customer care domains of an enterprise. Accurate entity\nprediction through IF-WRANER helps direct customers to automated workflows for\nthe domains, thereby reducing escalations to human agents by almost 15% and\nleading to millions of dollars in yearly savings for the company.\n","authors":["Subhadip Nandi","Neeraj Agrawal"],"pdf_url":"https://arxiv.org/pdf/2411.00451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00395v1","updated":"2024-11-01T06:49:39Z","published":"2024-11-01T06:49:39Z","title":"DivNet: Diversity-Aware Self-Correcting Sequential Recommendation\n  Networks","summary":"  As the last stage of a typical \\textit{recommendation system},\n\\textit{collective recommendation} aims to give the final touches to the\nrecommended items and their layout so as to optimize overall objectives such as\ndiversity and whole-page relevance. In practice, however, the interaction\ndynamics among the recommended items, their visual appearances and meta-data\nsuch as specifications are often too complex to be captured by experts'\nheuristics or simple models. To address this issue, we propose a\n\\textit{\\underline{div}ersity-aware self-correcting sequential recommendation\n\\underline{net}works} (\\textit{DivNet}) that is able to estimate utility by\ncapturing the complex interactions among sequential items and diversify\nrecommendations simultaneously. Experiments on both offline and online settings\ndemonstrate that \\textit{DivNet} can achieve better results compared to\nbaselines with or without collective recommendations.\n","authors":["Shuai Xiao","Zaifan Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.00395v1.pdf","comment":"Published at CIKM"},{"id":"http://arxiv.org/abs/2408.10159v3","updated":"2024-11-01T03:47:59Z","published":"2024-08-19T17:09:32Z","title":"Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation","summary":"  Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA.\n","authors":["Xiaoyu Kong","Jiancan Wu","An Zhang","Leheng Sheng","Hui Lin","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2408.10159v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00341v1","updated":"2024-11-01T03:43:50Z","published":"2024-11-01T03:43:50Z","title":"A Survey on Bundle Recommendation: Methods, Applications, and Challenges","summary":"  In recent years, bundle recommendation systems have gained significant\nattention in both academia and industry due to their ability to enhance user\nexperience and increase sales by recommending a set of items as a bundle rather\nthan individual items. This survey provides a comprehensive review on bundle\nrecommendation, beginning by a taxonomy for exploring product bundling. We\nclassify it into two categories based on bundling strategy from various\napplication domains, i.e., discriminative and generative bundle recommendation.\nThen we formulate the corresponding tasks of the two categories and\nsystematically review their methods: 1) representation learning from bundle and\nitem levels and interaction modeling for discriminative bundle recommendation;\n2) representation learning from item level and bundle generation for generative\nbundle recommendation. Subsequently, we survey the resources of bundle\nrecommendation including datasets and evaluation metrics, and conduct\nreproducibility experiments on mainstream models. Lastly, we discuss the main\nchallenges and highlight the promising future directions in the field of bundle\nrecommendation, aiming to serve as a useful resource for researchers and\npractitioners. Our code and datasets are publicly available at\nhttps://github.com/WUT-IDEA/bundle-recommendation-survey.\n","authors":["Meng Sun","Lin Li","Ming Li","Xiaohui Tao","Dong Zhang","Peipei Wang","Jimmy Xiangji Huang"],"pdf_url":"https://arxiv.org/pdf/2411.00341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20646v2","updated":"2024-11-01T03:12:44Z","published":"2024-05-31T07:24:42Z","title":"LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential\n  Recommendation","summary":"  Sequential recommender systems (SRS) aim to predict users' subsequent choices\nbased on their historical interactions and have found applications in diverse\nfields such as e-commerce and social media. However, in real-world systems,\nmost users interact with only a handful of items, while the majority of items\nare seldom consumed. These two issues, known as the long-tail user and\nlong-tail item challenges, often pose difficulties for existing SRS. These\nchallenges can adversely affect user experience and seller benefits, making\nthem crucial to address. Though a few works have addressed the challenges, they\nstill struggle with the seesaw or noisy issues due to the intrinsic scarcity of\ninteractions. The advancements in large language models (LLMs) present a\npromising solution to these problems from a semantic perspective. As one of the\npioneers in this field, we propose the Large Language Models Enhancement\nframework for Sequential Recommendation (LLM-ESR). This framework utilizes\nsemantic embeddings derived from LLMs to enhance SRS without adding extra\ninference load from LLMs. To address the long-tail item challenge, we design a\ndual-view modeling framework that combines semantics from LLMs and\ncollaborative signals from conventional SRS. For the long-tail user challenge,\nwe propose a retrieval augmented self-distillation method to enhance user\npreference representation using more informative interactions from similar\nusers. To verify the effectiveness and versatility of our proposed enhancement\nframework, we conduct extensive experiments on three real-world datasets using\nthree popular SRS models. The results show that our method surpasses existing\nbaselines consistently, and benefits long-tail users and items especially. The\nimplementation code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/LLM-ESR.\n","authors":["Qidong Liu","Xian Wu","Yejing Wang","Zijian Zhang","Feng Tian","Yefeng Zheng","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.20646v2.pdf","comment":"accepted by NeruIPS'24 (Spotlight)"},{"id":"http://arxiv.org/abs/2411.00331v1","updated":"2024-11-01T03:09:28Z","published":"2024-11-01T03:09:28Z","title":"Beyond Utility: Evaluating LLM as Recommender","summary":"  With the rapid development of Large Language Models (LLMs), recent studies\nemployed LLMs as recommenders to provide personalized information services for\ndistinct users. Despite efforts to improve the accuracy of LLM-based\nrecommendation models, relatively little attention is paid to beyond-utility\ndimensions. Moreover, there are unique evaluation aspects of LLM-based\nrecommendation models, which have been largely ignored. To bridge this gap, we\nexplore four new evaluation dimensions and propose a multidimensional\nevaluation framework. The new evaluation dimensions include: 1) history length\nsensitivity, 2) candidate position bias, 3) generation-involved performance,\nand 4) hallucinations. All four dimensions have the potential to impact\nperformance, but are largely unnecessary for consideration in traditional\nsystems. Using this multidimensional evaluation framework, along with\ntraditional aspects, we evaluate the performance of seven LLM-based\nrecommenders, with three prompting strategies, comparing them with six\ntraditional models on both ranking and re-ranking tasks on four datasets. We\nfind that LLMs excel at handling tasks with prior knowledge and shorter input\nhistories in the ranking setting, and perform better in the re-ranking setting,\nbeating traditional models across multiple dimensions. However, LLMs exhibit\nsubstantial candidate position bias issues, and some models hallucinate\nnon-existent items much more often than others. We intend our evaluation\nframework and observations to benefit future research on the use of LLMs as\nrecommenders. The code and data are available at\nhttps://github.com/JiangDeccc/EvaLLMasRecommender.\n","authors":["Chumeng Jiang","Jiayin Wang","Weizhi Ma","Charles L. A. Clarke","Shuai Wang","Chuhan Wu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.00331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15235v3","updated":"2024-11-01T02:00:49Z","published":"2024-02-23T09:57:20Z","title":"MACRec: a Multi-Agent Collaboration Framework for Recommendation","summary":"  LLM-based agents have gained considerable attention for their decision-making\nskills and ability to handle complex tasks. Recognizing the current gap in\nleveraging agent capabilities for multi-agent collaboration in recommendation\nsystems, we introduce MACRec, a novel framework designed to enhance\nrecommendation systems through multi-agent collaboration. Unlike existing work\non using agents for user/item simulation, we aim to deploy multi-agents to\ntackle recommendation tasks directly. In our framework, recommendation tasks\nare addressed through the collaborative efforts of various specialized agents,\nincluding Manager, User/Item Analyst, Reflector, Searcher, and Task\nInterpreter, with different working flows. Furthermore, we provide application\nexamples of how developers can easily use MACRec on various recommendation\ntasks, including rating prediction, sequential recommendation, conversational\nrecommendation, and explanation generation of recommendation results. The\nframework and demonstration video are publicly available at\nhttps://github.com/wzf2000/MACRec.\n","authors":["Zhefan Wang","Yuanqing Yu","Wendi Zheng","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.15235v3.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2410.23683v2","updated":"2024-11-01T01:21:04Z","published":"2024-10-31T07:19:22Z","title":"Unveiling User Satisfaction and Creator Productivity Trade-Offs in\n  Recommendation Platforms","summary":"  On User-Generated Content (UGC) platforms, recommendation algorithms\nsignificantly impact creators' motivation to produce content as they compete\nfor algorithmically allocated user traffic. This phenomenon subtly shapes the\nvolume and diversity of the content pool, which is crucial for the platform's\nsustainability. In this work, we demonstrate, both theoretically and\nempirically, that a purely relevance-driven policy with low exploration\nstrength boosts short-term user satisfaction but undermines the long-term\nrichness of the content pool. In contrast, a more aggressive exploration policy\nmay slightly compromise user satisfaction but promote higher content creation\nvolume. Our findings reveal a fundamental trade-off between immediate user\nsatisfaction and overall content production on UGC platforms. Building on this\nfinding, we propose an efficient optimization method to identify the optimal\nexploration strength, balancing user and creator engagement. Our model can\nserve as a pre-deployment audit tool for recommendation algorithms on UGC\nplatforms, helping to align their immediate objectives with sustainable,\nlong-term goals.\n","authors":["Fan Yao","Yiming Liao","Jingzhou Liu","Shaoliang Nie","Qifan Wang","Haifeng Xu","Hongning Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00275v1","updated":"2024-11-01T00:13:46Z","published":"2024-11-01T00:13:46Z","title":"Improving Musical Instrument Classification with Advanced Machine\n  Learning Techniques","summary":"  Musical instrument classification, a key area in Music Information Retrieval,\nhas gained considerable interest due to its applications in education, digital\nmusic production, and consumer media. Recent advances in machine learning,\nspecifically deep learning, have enhanced the capability to identify and\nclassify musical instruments from audio signals. This study applies various\nmachine learning methods, including Naive Bayes, Support Vector Machines,\nRandom Forests, Boosting techniques like AdaBoost and XGBoost, as well as deep\nlearning models such as Convolutional Neural Networks and Artificial Neural\nNetworks. The effectiveness of these methods is evaluated on the NSynth\ndataset, a large repository of annotated musical sounds. By comparing these\napproaches, the analysis aims to showcase the advantages and limitations of\neach method, providing guidance for developing more accurate and efficient\nclassification systems. Additionally, hybrid model testing and discussion are\nincluded. This research aims to support further studies in instrument\nclassification by proposing new approaches and future research directions.\n","authors":["Joanikij Chulev"],"pdf_url":"https://arxiv.org/pdf/2411.00275v1.pdf","comment":"43 pages, 35 figures, 14 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.20012v2","updated":"2024-11-01T08:40:28Z","published":"2024-09-30T07:14:31Z","title":"Towards Robust Multimodal Sentiment Analysis with Incomplete Data","summary":"  The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an\nemerging direction seeking to tackle the issue of data incompleteness.\nRecognizing that the language modality typically contains dense sentiment\ninformation, we consider it as the dominant modality and present an innovative\nLanguage-dominated Noise-resistant Learning Network (LNLN) to achieve robust\nMSA. The proposed LNLN features a dominant modality correction (DMC) module and\ndominant modality based multimodal learning (DMML) module, which enhances the\nmodel's robustness across various noise scenarios by ensuring the quality of\ndominant modality representations. Aside from the methodical design, we perform\ncomprehensive experiments under random data missing scenarios, utilizing\ndiverse and meaningful settings on several popular datasets (\\textit{e.g.,}\nMOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and\nfairness compared to existing evaluations in the literature. Empirically, LNLN\nconsistently outperforms existing baselines, demonstrating superior performance\nacross these challenging and extensive evaluation metrics.\n","authors":["Haoyu Zhang","Wenbin Wang","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2409.20012v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.00304v1","updated":"2024-11-01T01:51:31Z","published":"2024-11-01T01:51:31Z","title":"Unified Generative and Discriminative Training for Multi-modal Large\n  Language Models","summary":"  In recent times, Vision-Language Models (VLMs) have been trained under two\npredominant paradigms. Generative training has enabled Multimodal Large\nLanguage Models (MLLMs) to tackle various complex tasks, yet issues such as\nhallucinations and weak object discrimination persist. Discriminative training,\nexemplified by models like CLIP, excels in zero-shot image-text classification\nand retrieval, yet struggles with complex scenarios requiring fine-grained\nsemantic differentiation. This paper addresses these challenges by proposing a\nunified approach that integrates the strengths of both paradigms. Considering\ninterleaved image-text sequences as the general format of input samples, we\nintroduce a structure-induced training strategy that imposes semantic\nrelationships between input samples and the MLLM's hidden state. This approach\nenhances the MLLM's ability to capture global semantics and distinguish\nfine-grained semantics. By leveraging dynamic sequence alignment within the\nDynamic Time Warping framework and integrating a novel kernel for fine-grained\nsemantic differentiation, our method effectively balances generative and\ndiscriminative tasks. Extensive experiments demonstrate the effectiveness of\nour approach, achieving state-of-the-art results in multiple generative tasks,\nespecially those requiring cognitive and discrimination abilities.\nAdditionally, our method surpasses discriminative benchmarks in interleaved and\nfine-grained retrieval tasks. By employing a retrieval-augmented generation\nstrategy, our approach further enhances performance in some generative tasks\nwithin one model, offering a promising direction for future research in\nvision-language modeling.\n","authors":["Wei Chow","Juncheng Li","Qifan Yu","Kaihang Pan","Hao Fei","Zhiqi Ge","Shuai Yang","Siliang Tang","Hanwang Zhang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2411.00304v1.pdf","comment":null}]}}